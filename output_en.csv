url,title,link,summary,content
https://news.ycombinator.com/rss,I'm Shadow Banned by DuckDuckGo (and Bing),https://daverupert.com/2023/01/shadow-banned-by-duckduckgo-and-bing/,Comments,"




I'm Shadow Banned by DuckDuckGo (and Bing)


January 14, 2023




It came to my attention that my site does not appear on DuckDuckGo search results. Even when searching for ‚Äúdaverupert.com‚Äù directly. After some digging, DuckDuckGo used to get their site index from Yandex, but now gets their site index from Bing and sure enough‚Ä¶ I didn‚Äôt appear on Bing either.
First of all‚Ä¶ rude. I‚Äôm the one person I know who actually uses Bing and I started using DuckDuckGo on my Mac‚Ä¶ and they have the audacity ‚Äînay, the cowardice!‚Äî to shadow ban me and my contributions to the Web!? I ‚Äîa southern gentleman‚Äî take the highest offense at this slighting and misconstruing of my character. I do declare.
SEO isn‚Äôt one of my top objectives with this site, so initially I dismissed it. But that nerdsnipe shot a signal flare up in my brain that spun into mystery I needed to solve. I mean‚Ä¶ there can be money from blogging. Surely I‚Äôve built some clout for my blog over the years‚Ä¶ right?

I have been blogging, almost weekly, for over a decade‚Ä¶
I co-host a somewhat successful web development podcast‚Ä¶
I‚Äôve been back linked from popular blogs like CSS-Tricks‚Ä¶
I‚Äôve been on hacker news a handful of times‚Ä¶
And probably most important‚Ä¶ I show up on Google!

Why on earth would Bing not index my site at all? To solve this, I took the first step and signed up for Bing Webmaster Tools to try to know what Bing knows about my site and sure enough: zero clicks, zero impressions, and zero indexed pages for my site. Awful.

The one clue I have to go off are some ‚ÄúErrors‚Äù according to Bing‚Äôs Crawler. 100% of those errors are ‚Äúmissing meta description‚Äù. That doesn‚Äôt seem like an SEO dealbreaker to me (I get a 91 on Lighthouse SEO), but does Bing super care about meta descriptions? Doesn‚Äôt seem like I should have 0 out of 418 pages in my sitemap.xml though.
One ‚Äúout there‚Äù reason I can think is that I use Amazon Affiliate links on my Bookshelf and my /Uses page and that triggers a shadow ban? I could see how that appears spammy and I question the ethics of Amazon links sometimes myself, but what would I do without those $ones of dollars that I make each year!? I keep it around as a money carrot incentive to motivate me to update the bookshelf, but perhaps it‚Äôs time I retire that monetization avenue.
Anyways, a mystery is afoot‚Ä¶ let the investigation begin! I will post a follow up if I ever solve this.



"
https://news.ycombinator.com/rss,SLT ‚Äì A Common Lisp Language Plugin for Jetbrains IDE Lineup,https://github.com/Enerccio/SLT,Comments,"








Enerccio

/

SLT

Public




 

Notifications



 

Fork
    0




 


          Star
 26
  









        SLT is an IDE Plugin for Itellij/Jetbrains IDE lineup implementing support for Common Lisp via SBCL and Slime/Swank 
      
License





     Apache-2.0 license
    






26
          stars
 



0
          forks
 



 


          Star

  





 

Notifications












Code







Issues
4






Pull requests
0






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







Enerccio/SLT









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











master





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








3
branches





1
tag







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit









Peter Vanusanik

fixed wrong import on StringUtils, tested building distributions




        ‚Ä¶
      




        4681185
      

Jan 15, 2023





fixed wrong import on StringUtils, tested building distributions


4681185



Git stats







19

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.run



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









gradle/wrapper



init



Jan 6, 2023









src



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









.gitignore



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









CodeOfConduct.md



rename file



Jan 14, 2023









LICENSE.txt



readme and such



Jan 14, 2023









README.md



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









build-distributions.sh



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









build.gradle.kts



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









gradle.properties



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









gradlew



init



Jan 6, 2023









gradlew.bat



init



Jan 6, 2023









settings.gradle.kts



init



Jan 6, 2023




    View code
 















SLT - A Common Lisp Language Plugin for Jetbrains IDE lineup
Requirements
Getting started
Compiling source
Planned features / goals
License





README.md




SLT - A Common Lisp Language Plugin for Jetbrains IDE lineup



THIS PLUGIN IS EXPERIMENTAL and can crash at any time! Please report all bugs!
This plugin is providing support for Common Lisp for JetBrains IDEs.
Using modified SLIME/Swank protocol to commmunicate with SBCL providing
IDE capabilities for Common Lisp.

Requirements

Intellij based IDE - tested on Intellij Idea Community/Ultimate but should workd on all major IDEs
Steel Bank Common Lisp installed
Quicklisp

Getting started
Download plugin for your IDE from releases and install it via file.
To find out which release applies to you check this table:



Jetbrains IDE Variant
Plugin name pattern




CLion
slt-version-signed-CL.zip


GoLand
slt-version-signed-GO.zip


Intellij Community
slt-version-signed-IC.zip


Intellij Ultimate
slt-version-signed-IU.zip


PyCharm
slt-version-signed-PY.zip


PyCharm Community
slt-version-signed-PC.zip


Rider
slt-version-signed-RD.zip



PhpStorm is coming when I read how to build it correctly since just swapping
the type does not work.
Compiling source
Clone the repository and change gradle.properties for your IDE.
Then use gradle to build the plugin.
You can also open this as a project in Intellij Idea.
Planned features / goals

 Upload to marketplace when it has enough features
 REPL
 Interactive debugging
 Walkable debugger without actions
 Breakpoints
 Documentation
 Macro expand in documentation
 Find function by symbol name
 Search for symbols
 Back references
 Refactoring
 List of quicklisp installed packages / ASDF packages
 List of modified top level forms that are yet to be evaluated

License
This project is licensed under Apache License v2.









About

      SLT is an IDE Plugin for Itellij/Jetbrains IDE lineup implementing support for Common Lisp via SBCL and Slime/Swank 
    
Topics



  lisp


  integrated-development-environment


  jetbrains


  common-lisp


  sbcl


  intellij-plugin


  jetbrains-plugin



Resources





      Readme
 
License





     Apache-2.0 license
    



Stars





26
    stars

Watchers





3
    watching

Forks





0
    forks







    Releases





1
tags







    Packages 0


        No packages published 











Languages













Java
91.4%







Common Lisp
4.4%







Lex
3.3%







Other
0.9%











"
https://news.ycombinator.com/rss,Ask HN: How do you trust that your personal machine is not compromised?,https://news.ycombinator.com/item?id=34388866,Comments,"

Ask HN: How do you trust that your personal machine is not compromised? | Hacker News

Hacker News
new | past | comments | ask | show | jobs | submit 
login




 Ask HN: How do you trust that your personal machine is not compromised?
44 points by coderatlarge 1 hour ago  | hide | past | favorite | 29¬†comments 

""Compromised"" meaning that malware hasn't been installed or that it's not being accessed by malicious third parties.  This could be at the BIOS, firmware, OS, app or any other other level. 
 
  
 
gnfargbl 31 minutes ago  
             | next [‚Äì] 

Here's a short, fairly practical guide that you might find helpful: https://www.ncsc.gov.uk/files/Cyber-Essentials-Requirements-.... It is aimed mostly at small businesses, but I find a lot of the guidance to be pretty relevant to my personal IT.My even shorter (and incomplete) summary of the document would be: configure your router and firewall; remove default passwords and crapware from your devices; use a lock screen; don't run as root; use a password manager and decent passwords; enable 2FA everywhere you can; enable anti-malware if your OS has it built it; don't run software from untrusted sources; patch regularly.There are also other controls that you can choose to impose on yourself. For example, I require full-disk encryption, and I will only use mobile devices which get regular updates. Would be interested in hearing other things that HN'ers do to limit risk.
 
reply



  
 
amelius 16 minutes ago  
             | parent | next [‚Äì] 

Do you lock your computer every time you leave your desk?And do you always check for keylogger thumbdrives and such?
 
reply



  
 
Semaphor 5 minutes ago  
             | root | parent | next [‚Äì] 

For me: No, and no. But as that would require someone breaking into my apartment, I don‚Äôt worry too much.
 
reply



  
 
k3liutZu 5 minutes ago  
             | root | parent | prev | next [‚Äì] 

Yes (I don't bother when I'm working from home)I haven't used a use usb stick in +10 years.
 
reply



  
 
greggyb 14 minutes ago  
             | root | parent | prev | next [‚Äì] 

Yes. Why wouldn't you?
 
reply



  
 
NikolaNovak 22 minutes ago  
             | prev | next [‚Äì] 

Great question. I don't anymore. Decades ago when I had a 286 and knew what each file did and what all the software was, and threats were limited and crude, I had good confidence of controlling my machine. Today, when my laptop has millions of files and each website - even hacker news - could inject something malicious and my surface is so broad (browsers applications extensions libraries everything) and virtually anything I do involves network connections... I just don't have the confidence.FWIW, I try to segregate my machines for different categories of behaviour - this laptop is for work, this one is for photos and personal documents, this one is for porn, this one is if I want to try something. But even still my trust in e. G. software vlan on my router and access controls on my NAS etc are limited in this day and age.I feel today it's not about striving for zero risk (for 99.99 of people) , but picking the ratio of overhead and risk you're ok with. And backups. (bonus question - how to make backups safe in age of encrypting ransom ware).
 
reply



  
 
jl6 9 minutes ago  
             | parent | next [‚Äì] 

On the backup question, this is one reason why I have a set of backups that are physically disconnected and not automated.
 
reply



  
 
h2odragon 42 minutes ago  
             | prev | next [‚Äì] 

You really can't, anymore. You can watch traffic and hope that anything nasty isn't communicating with the outside world, but then there's all sorts of side channels that you may not know to watch.At some point you just have to admit there's limits to privacy and work with them. You paper journal could be stolen and read / rewritten too, yaknow? It's not a new problem, its just in a new context.
 
reply



  
 
ramraj07 3 minutes ago  
             | prev | next [‚Äì] 

There are two levels here: compromised by some national agency vs. compromised by anyone else.For the former, I don‚Äôt assume anything especially since I‚Äôm not an American citizen. I still believe with some certainty that my iPhone is safe from the government but not 100%
 
reply



  
 
adriancr 25 minutes ago  
             | prev | next [‚Äì] 

Just some generic things that should help avoid or clean up after a compromise.- clean reinstall every month, just pick a new flavor of Linux to try out. (also helps ensure I have proper backups and scripts for setting up environment)- Dev work I usually do in docker containers, easy to set up/nuke environments.- Open source router with open source bios (apu2), firewall on it, usually reinstall once in a while.- Spin up VMs via scripts for anything else. (games - windows VM with passthrough GPU for example)- automatic updates everywhere.
 
reply



  
 
867-5309 22 minutes ago  
             | parent | next [‚Äì] 

>Spin up VMs via scripts for .. gamesthis is not sustainable. you do this once and then pray nothing breaks!
 
reply



  
 
adriancr 20 minutes ago  
             | root | parent | next [‚Äì] 

I just have a clone of a clean windows VM.If something breaks or I get bored, nuke the active one and start clone, update it and make another backup, then reinstall games again.On the other hand, gpu pass-through breaks once in a while and is annoying to fix.
 
reply



  
 
albntomat0 6 minutes ago  
             | prev | next [‚Äì] 

The biggest thing is being deliberate about your threat model.  Who would want to get onto your systems, and how much do they care about you in particular?From there, take appropriate actions.  For the vast, vast majority of us, that means using good passwords, updating software, and not running weird things from the internet.If you‚Äôre worried about 0 click RCE in Chrome/Windows/iOS, you either should be getting better advice from folks outside of HN, or are being unrealistic about who is coming after you.
 
reply



  
 
lifthrasiir 36 minutes ago  
             | prev | next [‚Äì] 

I'm reasonably sure that my personal machine is less compromised than the average, but I can't and will never be able to ensure that it is not compromised because I have no way to know everything the machine trying to do. This remains true even when you have an entirely free and directly inspectable hardware; you simply have no knowledge and time to verify everything. Just keep a reasonable amount of precaution and skepticism.
 
reply



  
 
jl6 30 minutes ago  
             | prev | next [‚Äì] 

I don‚Äôt have ultimate trust in any software or hardware, but I get to ‚Äúgood enough‚Äù by deciding which providers I trust:* Software: Canonical, Google, Microsoft, Valve, Oracle, Dropbox. I install software from their official repos. Anything 3rd-party/unofficial/experimental/GitHub goes in a VM.* Hardware: I built my main PC from mainstream commodity components. I have no way of knowing if there are secret backdoors but I consider it unlikely.I‚Äôm also privileged enough to not be a ‚Äúperson of interest‚Äù so don‚Äôt feel the need to take any extraordinary precautions.Yes, I‚Äôm aware of VM escapes. Yes, I‚Äôve read Reflections on Trusting Trust. I choose to trust regardless because life‚Äôs too short for paranoia. As Frank Drebin said:‚ÄúYou take a chance getting up in the morning, crossing the street, or sticking your face in a fan.‚Äù
 
reply



  
 
rhn_mk1 17 minutes ago  
             | parent | next [‚Äì] 

What about publicly known backdoors in your hardware?https://www.techrepublic.com/article/is-the-intel-management...There is hardware that doesn't contain those at least, but it doesn't break power records.
 
reply



  
 
codetrotter 38 minutes ago  
             | prev | next [‚Äì] 

Noone has drained my crypto from my wallets yet.So either my personal machine is not compromised, or they think the amount of crypto in the wallets is too low.Jokes on them though, cause I am moving my crypto to a hardware wallet eventually
 
reply



  
 
progval 14 minutes ago  
             | parent | next [‚Äì] 

Joke's on you, you just told them they should hurry up before you do ;)
 
reply



  
 
tluyben2 31 minutes ago  
             | parent | prev | next [‚Äì] 

Quite an interesting honeypot really.
 
reply



  
 
mimimi31 23 minutes ago  
             | root | parent | next [‚Äì] 

More like a canary I think.
 
reply



  
 
adg001 25 minutes ago  
             | prev | next [‚Äì] 

The reality is that you cannot trust that your machines are not compromised.The only option we are left with is to operate under the assumption that, indeed, our machines are permanently compromised.
 
reply



  
 
rekrsiv 4 minutes ago  
             | prev | next [‚Äì] 

You don't. Treat your personal machine(s) as compromised by default and take it from there.
 
reply



  
 
PaulHoule 35 minutes ago  
             | prev | next [‚Äì] 

Reminds me of the time I was watching a creepypasta horror movie about some guy who gets strange phone calls and my phone rang.I think this guy had gotten my phone number from my HN profile and he thought I might be able to help him.  He thought his android phone was infected by malware and he knew who did it.  I told him the people who repair cell phones at the mall could do a system reset on his phone‚Ä¶. Unless he was dealing with state-level actors in which case it might be an advanced persistent threat and it might be permanent.
 
reply



  
 
treebeard901 43 minutes ago  
             | prev | next [‚Äì] 

You should assume all devices are compromised
 
reply



  
 
wadayano 34 minutes ago  
             | parent | next [‚Äì] 

*compromisable
 
reply



  
 
baobabKoodaa 40 minutes ago  
             | parent | prev | next [‚Äì] 

Not helpful
 
reply



  
 
twaw 14 minutes ago  
             | root | parent | next [‚Äì] 

Why not? It still possible to communicate securely using compromised devices and networks.
 
reply



  
 
cube2222 11 minutes ago  
             | prev | next [‚Äì] 

I try to follow what others already mentioned, but still, for any personal high-security stuff I use a device whose OS  puts strong limits on apps, like an iPad.
 
reply



  
 
crims0n 22 minutes ago  
             | prev [‚Äì] 

Keep it air gapped, only way to be sure!Only half kidding, unfortunately.
 
reply







Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
Search:  


"
https://news.ycombinator.com/rss,Ubuntu 22.04 LTS servers and phased apt updates,https://utcc.utoronto.ca/~cks/space/blog/linux/Ubuntu2204ServerPhasedUpdates,Comments,"
 
 Chris's Wiki :: blog/linux/Ubuntu2204ServerPhasedUpdates 






Chris Siebenmann ::
CSpace ¬ª
       blog ¬ª
       linux ¬ª
       Ubuntu2204ServerPhasedUpdates
Welcome, guest.




Ubuntu 22.04 LTS servers and phased apt updates
January 13, 2023

I was working on getting one of our 22.04 LTS servers up to date,
even for packages we normally hold, when I hit a mystery and
posted about it on the Fediverse:
Why does apt on this 22.04 Ubuntu machine want to hold back a bunch of
package updates even with '--with-new-pkgs --ignore-hold'? Who knows,
it won't tell me why it doesn't like any or all of:
open-vm-tools openssh-client openssh-server openssh-sftp-server
osinfo-db python3-software-properties software-properties-common
(Apt is not my favorite package manager for many reasons, this among
them.)

Steve suggested that it was Ubuntu's ""Phased Update"" system, which is what it turned
out to be. This set me off to do some investigations, and it turns
out that phased (apt) updates explain some other anomalies we've
seen with package updates on our Ubuntu 22.04 machines.
The basic idea of phased updates is explained in the ""Phasing""
section of Ubuntu's page on Stable Release Updates (SRUs); it's a
progressive rollout of the package to more and more of the system
base. Ubuntu introduced phased updates in 2013 (cf) but initially they weren't
directly supported by apt, only by the desktop upgrade programs.
Ubuntu 21.04 added apt support for phased updates and
Ubuntu 22.04 LTS is thus the first LTS version to subject servers
to phased updates. More explanations of phased updates are in this
askubuntu answer, which includes
one way to work around them.
(Note that as far as I know and have seen, security updates are not
released as phased updates; if it's a security update, everyone
gets it right away. Phased updates are only used for regular,
non-security updates.)
Unfortunately apt (or apt-get) won't tell you if an update is being
held back because of phasing. This user-hostile apt issue is tracked
in Ubuntu bug #1988819 and
you should add yourself as someone it affects if this is relevant
to you. Ubuntu has a web page on what updates are currently in
phased release,
although packages are removed from this page once they reach 100%.
Having reached 100%, such a package is no longer a phased update,
which will become relevant soon. If you can't see a reason for a
package to be held back, it's probably a phased update but you can
check the page
to be sure.
(As covered in the ""Phasing"" section, packages
normally move forward through the phased rollout every six hours,
so you can have a package held back on some server in the morning
and then be not-held in the afternoon. This is great fun for
troubleshooting why a given server didn't get a particular update.)
Your place in a phased update is randomized across both different
servers and different packages. If you have a fleet of servers,
they will get each phased update at different times, and the order
won't be consistent from package to package. This explains an anomaly
we've been seeing in our package updates for some time, where
different 22.04 servers would get updates at different times without
any consistent pattern.
The phased update related apt settings available and some of the
technical details are mostly explained in this askubuntu answer. If you want to opt out of phased
updates entirely, you have two options; you can have your servers
install all phased updates right away (basically putting you at the
0% start line), or you can skip all phased updates and only install
such packages when they reach 100% and stop being considered phased
updates at all. Unfortunately, as of 22.04 there's no explicit
option to set your servers to have a particular order within all
updates (so that you can have, for example, a 'canary' server that
always installs updates at 0% or 10%, ahead of the rest of the
fleet).
For any given package update, machines are randomized based on the
contents of /etc/machine-id, which
can be overridden for apt by setting APT::Machine-ID to a 32 hex
digit value of your choice (the current version of apt appears to
only use the machine ID for phased updates).  If you set this to
the same value across your fleet, your fleet will update in sync
(although not at a predictable point in the phase process); you can
also set subsets of your fleet to different shared values so that
the groups will update at different times.  The assignment of a
particular machine to a point in the phased rollout is done through
a relatively straightforward approach; the package name, version,
and machine ID are all combined into a seed for a random number
generator, and then the random number generator is used to produce
a 0 to 100 value, which is your position in the phased rollout. The
inclusion of the package name and version means that a given machine
ID will be at different positions in the phased update for different
packages. All of this turns out to be officially documented in the
""Phased Updates"" section of apt_preferences(5),
although not in much detail.
(There is a somewhat different mechanism for desktop updates, covered
in the previously mentioned askubuntu answer.)
As far as I can see from looking at the current apt source code, apt doesn't log anything
at any verbosity if it holds a package back because the package is
a phased update and your machine doesn't qualify for it yet. The
fact that a package was a phased update the last time apt looked
may possibly be recorded in /var/log/apt/eipp.log.xz, but documentation
on this file is sparse.
Now that I've looked at all of this and read about APT::Machine-ID,
we'll probably set it to a single value across all of our fleet
because we find different machines getting updates at different
times to be confusing and annoying (and it potentially complicates
troubleshooting problems that are reported to us, since we normally
assume that all 22.04 machines have the same version of things like
OpenSSH). If we could directly control the position within a phased
rollout we'd probably set up some canary machines, but since we
can't I don't think there's a strong reason to have more than one
machine-id group of machines.
(We could set some very important machines to only get updates when
packages reach 100% and stop being phased updates, but Ubuntu has
a good record of not blowing things up with eg OpenSSH updates.)

(4 comments.)
Written on 13 January 2023. 

     ¬´   A browser tweak for system administrators doing (web) network debugging    
    Your server BMCs can need to be rebooted every so often   ¬ª     



 These are my WanderingThoughts 
(About the blog)
Full index of entries 
Recent comments
This is part of CSpace, and is written by ChrisSiebenmann. 
Mastodon: @cks 
Twitter: @thatcks
* * *
Categories: links, linux, programming, python, snark, solaris, spam, sysadmin, tech, unix, web 
Also: (Sub)topics
This is a DWiki. 
GettingAround 
(Help)
 
 Search:  



 Page tools: View Source, Add Comment. 

Search: 

Login: 
Password: 


 

Atom Syndication: Recent Comments.
 Last modified: Fri Jan 13 22:56:18 2023 
This dinky wiki is brought to you by the Insane Hackers
Guild, Python sub-branch.


"
https://news.ycombinator.com/rss,Single-file scripts that download their dependencies,https://dbohdan.com/scripts-with-dependencies,Comments,"



Single-file scripts that download their dependencies ¬∑ DBohdan.com











Toggle navigation




dbohdan



Home





 




Homescripts-with-dependencies





Single-file scripts that download their dependencies
An ideal distributable script is fully contained in a single file. It runs on any compatible operating system with an appropriate language runtime. It is plain text, and you can copy and paste it. It does not require mucking about with a package manager, or several, to run. It does not conflict with other scripts‚Äô packages or require managing a project environment to avoid such conflicts.
The classic way to get around all of these issues with scripts is to limit yourself to using the scripting language‚Äôs standard library. However, programmers writing scripts don‚Äôt want to; they want to use libraries that do not come with the language by default. Some scripting languages, runtimes, and environments resolve this conflict by offering a means to download and cache a script‚Äôs dependencies with just declarations in the script itself. This page lists such languages, runtimes, and environments. If you know more, drop me a line.
Contents



Anything with a Nix package


D


Groovy


JavaScript (Deno)


Kotlin (kscript)


Racket (Scripty)


Scala (Ammonite)



Anything with a Nix package
The Nix package manager can act as a #! interpreter and start another program with a list of dependencies available to it.
#! /usr/bin/env nix-shell
#! nix-shell -i python3 -p python3
print(""Hello, world!"".rjust(20, ""-""))
D
D‚Äôs official package manager DUB supports single-file packages.
#! /usr/bin/env dub
/+ dub.sdl:
name ""foo""
+/
import std.range : padLeft;
import std.stdio : writeln;
void main() {
    writeln(padLeft(""Hello, world!"", '-', 20));
}
Groovy
Groovy comes with an embedded JAR dependency manager.
#! /usr/bin/env groovy
@Grab(group='org.apache.commons', module='commons-lang3', version='3.12.0')
import org.apache.commons.lang3.StringUtils
println StringUtils.leftPad('Hello, world!', 20, '-')
JavaScript (Deno)
Deno downloads dependencies like a browser. Deno 1.28 and later can also import from NPM packages. Current versions of Deno require you to pass a run argument to deno. One way to accomplish this from a script is with a form of ‚Äúexec magic‚Äù. Here the magic is modified from a comment by Rafa≈Ç Pocztarski.
#! /bin/sh
"":"" //#; exec /usr/bin/env deno run ""$0"" ""$@""
import leftPad from ""npm:left-pad"";
console.log(leftPad(""Hello, world!"", 20, ""-""));
On Linux systems with recent GNU env(1) and on FreeBSD you can replace the magic with env -S.
#! /usr/bin/env -S deno run
import leftPad from ""npm:left-pad"";
console.log(leftPad(""Hello, world!"", 20, ""-""));
Kotlin (kscript)
kscript is an unofficial scripting tool for Kotlin that understands several comment-based directives, including one for dependencies.
#! /usr/bin/env kscript
//DEPS org.apache.commons:commons-lang3:3.12.0
import org.apache.commons.lang3.StringUtils
println(StringUtils.leftPad(""Hello, world!"", 20, ""-""))
Racket (Scripty)
Scripty interactively prompts you to install the missing dependencies for a script in any Racket language.
#! /usr/bin/env racket
#lang scripty
#:dependencies '(""base"" ""typed-racket-lib"" ""left-pad"")
------------------------------------------
#lang typed/racket/base
(require left-pad/typed)
(displayln (left-pad ""Hello, world!"" 20 ""-""))
Scala (Ammonite)
The scripting environment in Ammonite lets you import Ivy dependencies.
#! /usr/bin/env amm
import $ivy.`org.apache.commons:commons-lang3:3.12.0`,
  org.apache.commons.lang3.StringUtils
println(StringUtils.leftPad(""Hello, world!"", 20, ""-""))

Tags: list, programming.
 
 
 
 


Copyright 2013‚Äì2022 D. Bohdan.




 


"
https://news.ycombinator.com/rss,"The Fourier Transform, explained in one sentence",https://blog.revolutionanalytics.com/2014/01/the-fourier-transform-explained-in-one-sentence.html,Comments,"


































The Fourier Transform, explained in one sentence (Revolutions)













Revolutions

			Milestones in AI, Machine Learning, Data Science, and visualization with R and Python since 2008
		









¬´ Forecasting By Combining Expert Opinion |
	Main
	| Predictive Models in R Clustered By Tag Similarity ¬ª



January 03, 2014


The Fourier Transform, explained in one sentence


If, like me, you struggled to understand the Fourier Transformation when you first learned about it, this succinct one-sentence colour-coded explanation from Stuart Riffle probably comes several years too late:

Stuart provides a more detailed explanation here. This is the formula for the Discrete Fourier Transform, which converts sampled signals (like a digital sound recording) into the frequency domain (what tones are represented in the sound, and at what energies?). It's the mathematical engine behind a lot of the technology you use today, including mp3 files, file compression, and even how your old AM radio stays in tune.
The daunting formula involves imaginary numbers and complex summations, but Stuart's idea is simple. Imagine an enormous speaker, mounted on a pole, playing a repeating sound. The speaker is so large, you can see the cone move back and forth with the sound. Mark a point on the cone, and now rotate the pole. Trace the point from an above-ground view, if the resulting squiggly curve is off-center, then there is frequency corresponding the pole's rotational frequency is represented in the sound. This animated illustration (click to see it in action) illustrates the process:

The upper signal is make up of three frequencies (""notes""), but only the bottom-right squiggle is generated by a rotational frequency matching one of the component frequencies of the signal.
By the way, no-one uses that formula to actually calculate the Discrete Fourier Transform ‚Äî use the Fast Fourier Transform instead, as implemented by the fft function in R. As the name suggests, it's much faster.
AltDevBlog: Understanding the Fourier Transform¬†(note: updated link 20 Oct 2015 with active mirror)





Posted by David Smith at 13:30 in R, random  | Permalink











Comments

 You can follow this conversation by subscribing to the comment feed for this post.





Very interesting article, thank you. Please take a moment to rephrase the following key statement, if you would: ""...then there is frequency corresponding the pole's rotational frequency is represented in the sound.""


		Posted by:
		C. Griffith |
		January 04, 2014 at 10:09




polar form e^iŒ∏ is equal to the rectangular form cosŒ∏+isinŒ∏ and corresponds to the coordinates (cosŒ∏,sinŒ∏) such that 
e^i0    =  1 = (1,0)
e^iœÑ/4  =  i = (0,1)
e^iœÑ/2  = -1 = (-1,0)
e^iœÑ3/4 = -i = (0,-1)
e^iœÑ    =  1 = (1,0)


		Posted by:
		MasterG |
		January 04, 2014 at 16:33




May I suggest a  minor exception to your claim about FFT: most modern languages, R included, use some variation of the ""pure"" 2^N Cooley-Tukey FFT algorithm as appropriate to support factors of 3, 5, etc. in the length of the dataset, and even default to the ""raw"" DFT for other data lengths (unless specifically suppressed by the user).   
And, of course, the FFT is in fact that equation, just with gobs of like terms grouped together. :-)


		Posted by:
		Carl Witthoft |
		January 06, 2014 at 08:40











	The comments to this entry are closed.







Information


About this blog
Comments Policy
About Categories
About the Authors
Local R User Group Directory
Tips on Starting an R User Group





Search Revolutions Blog






















Got comments or suggestions for the blog editor? 
Email David Smith.
    




 Follow David on Twitter: @revodavid





Get this blog via email with 




Categories


academia (41)
advanced tips (218)
AI (62)
airoundups (20)
announcements (201)
applications (288)
beginner tips (106)
big data (272)
courses (60)
current events (126)
data science (227)
developer tips (90)
events (280)
finance (126)
government (25)
graphics (378)
high-performance computing (115)
life sciences (35)
Microsoft (314)
mlops (4)
open source (78)
other industry (58)
packages (388)
popularity (54)
predictive analytics (163)
profiles (15)
python (69)
R (2442)
R is Hot (8)
random (464)
reviews (22)
Revolution (422)
Rmedia (136)
roundups (121)
sports (55)
statistics (297)
user groups (127)


See More




R links


R on AzureDeveloper's guide and documentation
Find R packagesCRAN package directory at MRAN
Download Microsoft R OpenFree, high-performance R
R Project siteInformation about the R project






Recommended Sites


@RLangTipDaily tips on using R
FlowingDataModern data visualization
Probability and statistics blogMonte Carlo simulations in R
R BloggersDaily news and tutorials about R, contributed by R bloggers worldwide.
R Project group on analyticbridge.comCommunity and discussion forum
Statistical Modeling, Causal Inference, and Social ScienceAndrew Gelman's statistics blog






Archives


January 2023
August 2022
September 2021
July 2021
June 2021
April 2021
March 2021
February 2021
January 2021
December 2020





 Subscribe to this blog's feed




‚Äã
    













 








"
https://news.ycombinator.com/rss,Go FOSS: Information is power,https://gofoss.net/,Comments,"          gofoss.net      Home      Get started      Get started     Protect your digital freedom         Browse privately      Browse privately     Free your browser     Firefox     Tor Browser     VPN         Speak freely      Speak freely     Keep conversations private     Encrypted messages     Encrypted emails         Store safely      Store safely     Secure your data     Safe passwords     Backups     Encrypted files         Stay mobile & free      Stay mobile & free     Free your phone     FOSS apps     CalyxOS     LineageOS for microG         Unlock your computer      Unlock your computer     Free your computer     Ubuntu     Ubuntu apps         Own your cloud      Own your cloud     Free your cloud     Fediverse     Alternative cloud providers     Server hosting     Basic server security     Advanced server security     Secure access     Cloud storage     Photo gallery     Contacts, calendars & tasks     Media streaming     Server backups         About      About     Big Tech threatens privacy     The project     The team     Thanks     Contributing     Roadmap     Disclaimer         Origins      Origins     Hackers, 1984     The GNU Manifesto, 1985     The Techno-Revolution, 1986     The Conscience of a Hacker, 1986     The Crypto Anarchist Manifesto, 1988     A Cypherpunk's Manifesto, 1993     A Declaration of the Independence of Cyberspace, 1996     A Cyberpunk Manifesto, 1997     The Cathedral & The Bazaar, 1999     The dotCommunist Manifesto, 2003     A Hacker Manifesto, 2004     The Maker's Bill of Rights, 2006     Guerilla Open Access Manifesto, 2008     The Apache Way, 2009     Repair Manifesto, 2009     The Cult of Done Manifesto, 2009     Self-Repair Manifesto, 2010     The Hardware Hacker Manifesto, 2010     An Anonymous Manifesto, 2011     The Declaration of the Independence of the people of the Internet, 2012                          Back to top  "
https://news.ycombinator.com/rss,Constrain ‚Äì Interactive figures using declarative constraint solving,https://github.com/andrewcmyers/constrain,Comments,"








andrewcmyers

/

constrain

Public




 

Notifications



 

Fork
    2




 


          Star
 50
  









        Responsive, animated figures in JavaScript/HTML canvases
      





andrewcmyers.github.io/constrain







50
          stars
 



2
          forks
 



 


          Star

  





 

Notifications












Code







Issues
18






Pull requests
0






Discussions







Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Discussions
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







andrewcmyers/constrain









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











master





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








7
branches





6
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




andrewcmyers

Fix backprop for Smooth/Linear.




        ‚Ä¶
      




        130cac8
      

Jan 15, 2023





Fix backprop for Smooth/Linear.

Remove unnecessary tree constraints.
Add necessary constraints to example.

130cac8



Git stats







516

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








doc



doc improvement



Jan 13, 2023









examples



Fix backprop for Smooth/Linear.



Jan 15, 2023









images



Update image



Jun 27, 2022









reveal.js @ 9430a98



latest



Jul 25, 2019









tests



un/refix test



Jan 15, 2023









.gitignore



Reorg/clean up repo



Dec 25, 2020









.gitmodules



change to my version of Reveal



Jul 18, 2019









README.md



tree example



Jan 13, 2023









constrain-graph.js



argument checking



May 11, 2022









constrain-mathjax.js



formatting



Jul 27, 2022









constrain-pdf.js



Monkey-patch broken jsPDF calls



Apr 9, 2022









constrain-ps.js



Implement missing methods for PS rendering



Apr 9, 2022









constrain-reveal.js



Clean up comment



Apr 19, 2021









constrain-slide.css



More refactoring of Reveal mods



Aug 21, 2019









constrain-trees.js



Fix backprop for Smooth/Linear.



Jan 15, 2023









constrain.js



Fix backprop for Smooth/Linear.



Jan 15, 2023









numeric-1.2.6.js



execute bit should not be on



Jul 22, 2019




    View code
 















Constrain - a JS (ES6) library for animated, interactive figures, based on declarative constraint solving
Demos
Requirements





README.md




Constrain - a JS (ES6) library for animated, interactive figures, based on declarative constraint solving


Responsive, animated figures embedded in web pages
Figures implemented declaratively with time-dependent constraints on graphical objects
Integrates with Reveal.js presentations
GitHub repository
Reference manual
A short talk about Constrain

Demos
Using constraints to compute the Golden Ratio (Drag the diamond!)
A short talk about Constrain, using Reveal
Cornell University course notes using Constrain for embedded figures: CS 2112,
CS 4120/lexer generation,
CS 4120/bottom-up parsing
Interactive Pythagorean Theorem
Interactively computing centers of a triangle
TeX-style text formatting
Simple template page for using Constrain
Animated trees
Requirements

ES6-capable web browser

Tested on Chrome, Opera, Brave, Firefox, Safari (runs best on the first three)
Does not work on Internet Explorer or Opera Mini


Numeric.js version 1.2.6 (included)










About

      Responsive, animated figures in JavaScript/HTML canvases
    





andrewcmyers.github.io/constrain


Topics



  constraints


  web-pages


  animated-figures


  embedded-figures



Resources





      Readme
 


Stars





50
    stars

Watchers





4
    watching

Forks





2
    forks







    Releases
      5







Release 0.3.0

          Latest
 
Apr 21, 2021

 

        + 4 releases







    Packages 0


        No packages published 





Languages












JavaScript
95.1%







HTML
4.8%







CSS
0.1%











"
https://news.ycombinator.com/rss,Flightcontrol (YC W22) is hiring Developer Advocate (remote/fulltime),https://jobs.flightcontrol.dev/developer-advocate,Comments,"üõ´Developer Advocate at a Calm, Ambitious DevTools Startup¬†https://www.flightcontrol.dev¬†Flightcontrol is a 4 person devtool startup cofounded by the creator of Blitz.js. We‚Äôre not the macho, overworked team that ‚Äústartup‚Äù might bring to mind. We‚Äôre intentionally building the most life-giving and fulfilling company possible, and we want you to join us! :)¬†Photos from our last retreat in Italy ‚Äî¬†contrary to appearances, we also did real work :)Flightcontrol is transforming the app deployment landscapeFlightcontrol provides the deployment experience of a Platform-as-a-Service but without the limitations.Traditionally, users had to choose between a PaaS like Heroku, which is easy to use but is a closed-box with limitations and restrictions, or AWS which gives you full power and control but is a royal pain to set up. But with Flightcontrol they get the best of both worlds: great developer experience and full control and scalabilityOur product is so compelling that most of our users are migrating existing applications from Heroku, Render, Railway, Vercel and from custom AWS setups.Before you cringe at our current design, know that we are currently working with Overnice to completely overhaul our brand design, marketing site, and UI/UX. So rest assured, you‚Äôll soon have world-class brand materials to work with :)Since launching in January, over 350 users have deployed 30,000+ timesUsers of all sizes love the product, from solo indie hackers to enterprisesWe went through Y Combinator in W22 and have raised $3.5M.We‚Äôre default alive ‚Äî on track to become profitable this yearIntrigued? Read more about our company here.Deployments Per Month¬†Meet Our Team of 4Brandon Bayer, Cofounder and CEO. Dayton, Ohio. You might know me as the creator of Blitz.js. Although highly technical, my strengths are product design and marketing. My superpower is simplicity. My top values that define everything I do are excellence, equality, inclusion, and freedom. Outside of work I love traveling, flying airplanes and helicopters, and rock climbing. My intention is to build the best company to work for in the world. I‚Äôm here to support you and help make your dreams come true.Mina Abadir, Cofounder and CTO. Toronto, Canada. Mina is the technical genius that brings our core product features to life. He‚Äôs deeply authentic and caring, loves to laugh, and greatly enjoys a good video game. His superpower is empathy.Blake Bayer, Junior Software Engineer. Dayton, Ohio. Formerly a nurse, Blake joined at the end of April. This is his first job in tech, and he continually impresses everyone on the team with his ability to learn and implement complex things quickly. He loves rock climbing and learning new things.Camila Rondinini, Senior Backend Software Engineer. Spain. Since joining this past June, she is already an extremely foundational part of our team, having designed and shipped some of our most important features. She‚Äôs an incredible engineer and has made a massive impact on our engineering culture.You? üòâ¬†We need you to help us grow through awareness & educationAs our first devrel hire, this is your chance to really shine and help propel Flightcontrol into one of the most loved developer companies of this decade. This is a critical role in shaping FlightcontrolYour high level goal is to grow ‚Äútop of funnel‚Äù traffic through awareness and education, with written content as the foundation.We are building a product-led growth flywheel. Your primary role is feeding the flywheel via new top of funnel users. Your secondary role is improving the flywheel through education and documentation.So far Brandon has been doing all the devrel related tasks. You‚Äôll work closely with him to figure out where and how to invest your efforts. We don‚Äôt yet have the perfect content strategy. So we‚Äôll be experimenting to see what works and what doesn‚Äôt.In time, we want to be known as the cloud education company. The place where all devs turn to become maestros of the cloud. We believe that most engineers would love working with native cloud providers like AWS if only it was more approachable and more easily understood. Our vision is to empower an entirely new generation of engineers through our product and through our educational content.Essential Responsibilities Deeply understand our product, its strengths and weaknessesCreate and distribute compelling technical content, including documentation, documentation, guides, demos, and videosWe care about quality over quantityIdentify and work on collaborations and integrations with other companies and projectsExample: how to use Flightcontrol preview environments to run isolated Cypress e2e testsSecondary ResponsibilitiesHelp with customer success. As a small team, we all share this responsibility. Helping with this is one of the best ways to understand nitty-gritty details of our product, product improvement ideas, and documentation ideas.A customer recently said that we ‚Äúhave amazing support and developer success‚Äù and that it provides a tremendous amount of value to themPossibly, but not required: represent Flightcontrol at eventsIdentifying relevant events for Flightcontrol and organizing our participation (meetups, conferences, hackathons, workshops, etc.),Ideally participating in 4-8 of these events per year, as a speaker or sponsorRequirements1 year full-time devrel experience2 years full-time software engineering experience and are comfortable with fullstackSome working knowledge of some basic AWS services like EC2, S3, RDS, LambdaTeacher ‚Äî can explain complex things in as simple a manner as possibleWriter ‚Äî great at writing technical content, ideally for 1+ yearsGrit ‚Äî can ship content consistently over time through thick and thinEmpathy ‚Äî can learn and understand what‚Äôs important to developers and engineering organizationsCredible ‚Äî produces content that acknowledges the trade-offs and complexities of the real worldCan overlap with 10a-noon US Eastern time (EST) . You can work from anywhere in the world, but we have our company wide meetings in the 10a-12p EST time range.Nice to haveExperience in the AWS or cloud spaceGreat at creating videosConference speaking experienceGreat knowledge of the application hosting/deployment ecosystemAble to work with basic demos in several programming languagesExperience with our stack: Typescript, React, Next.js¬†You Are Someone WhoIs Kind. We are a team that seeks to work really well together by building deep relationships. We have each other‚Äôs backs. We care about and check in on each other, and we enjoy being together. We have company retreats 2-3 times per year for a week at a time.Is Collaborative. We all work closely together to design and develop the best product possible. We want someone who is humble but will bring your own ideas on how to be more excellent.Takes Ownership. We offer significant equity because we want you to think at a higher level than just your daily tasks. We want you to help us shape the business. We need someone who loves to dig in and do what it takes to figure things out. And we want someone who is good at turning vague ideas into magnificence. Has a Growth Mindset. It matters more where you are going than where you are today. We‚Äôre looking for someone who loves to grow, improve, and learn new things.¬†Your Typical Week at FlightcontrolOn Monday, depending on your timezone, you‚Äôll start your morning or afternoon with a coffee chat where everyone is together for causal conversation. After that, you‚Äôll join our Flightcontrol planning session with the entire Flightcontrol product team.Tuesday is usually meeting free, so you‚Äôll be focused on your work.On Wednesday you‚Äôll have your weekly 1 on 1 with Brandon, the CEO. This is your time to ask for what you want, bring up issues, ask hard questions, and give and receive feedback. Brandon takes feedback very seriously and is quick to make needed changes. Thursday and Friday are your time for deep work.Aside from being available 9a-12p EST, your work hours are flexible and up to you. Some of us work a standard 9-5 type of deal while others have varying schedules.You‚Äôll collaborate with Brandon as much as is needed.Since we‚Äôre a startup, the journey from idea to building to shipping to growing is certainly a bit of a roller coaster. But we're all on the roller coaster together, learning and iterating as quickly as we can. As long as we stick to our values and show up for each other with curiosity, compassion, and collaboration, we can likely overcome just about anything together.Every month we have a tech-debt cleanup day. Every other month we have a company hackathon.¬†You Can Grow With UsWe want you to grow with us as much as you desire. As we scale, you‚Äôll be able to grow into almost any role you can imagine. Want to become a team lead? We‚Äôll help train you. Want to become a manager? We‚Äôll make it happen. Want to be an executive? Let‚Äôs figure that out. We want you to be with us as long as you are extremely happy. If we get to place were you aren‚Äôt happy, we‚Äôll do everything we can to help you find a place where you are.¬†Our Code of ExcellenceGo above and beyond. We‚Äôre not here to half-way do anything. If we‚Äôre going to do something, we‚Äôre going to do a stellar job.Tell the truth even when it hurts. We don‚Äôt tell white lies, and we don‚Äôt deceive. Even when it costs.Take care of you and yours first, work second. Nothing matters more than family and close relationships. We never sacrifice them for work.Treat people better than they deserve. Kindness and generosity guides how we treat everyone, including teammates and customers.Give and receive feedback. Feedback is essential for growth. We highly value giving and receiving informal, constructive feedback between all members of the team, and then taking prompt action on that feedback.Have a life outside work. It can be anything, hobbies, side projects, reading, etc. As long as you have something and work isn‚Äôt all you live for.Eradicate stress. Stress is a killer, and we work to eliminate it through any means, including systems, exercise, and meditation.Nothing is impossible. We believe we can create any future we imagine, and we lean into solving the things that seem impossible.Build a legacy. We are here to do our very best work. Work that will inspire generations for years to come.¬†Salary & Benefits32 Hour Work Week - More and more companies are finding that people accomplish the same amount of work in 32 hours as in 40 hours.Salary: $110k ‚Äî $145k USD (same as our engineering roles)0.75% ‚Äî 1% Equity Stock Options. You‚Äôll be a $20+ millionaire if our growth continues like it isMinimum 4 Weeks PTO - It's critical to have good work life balance, so you must take at least 4 weeks PTO each year.Fully RemoteHealth Insurance Fully Paid For401k - We‚Äôre still working out the details on this, but will get it nailed down asap if it‚Äôs important to youMenstrual Leave - There's no use trying to be productive when you are suffering. Take the day(s) off as PTO, no explanation needed.Unlimited Sick Leave - If you are feeling crappy, you aren't going to be doing your best work. So rest, get better, then come back energized.2+ In-Person Company Retreats Per YearOpen Source - We are passionate about open-source and encourage you to contribute on company time to anything that will benefit the company.Equipment - We'll make sure you have all the equipment you need to have an ergonomic, productive environment, including a standing desk and external monitors.Conferences - We're a big fan of in-person conference experiences, and encourage you to speak at and attend them. We'll fully pay for you to attend 2 conferences per year.Education - Budget for books or courses that are at least tangentially related to your work.¬†¬†üî•Please apply here üëâ https://airtable.com/shrPet5euUinQ0uP4 üëà
Our process:You submit the application45 minute zoom with Brandon, CEO45 minute technical interview with Mina, CTONo LeetCode garbage ‚Äî we‚Äôll offer you a range of options so you can choose a style that you‚Äôll do best at45 minute technical interview with Camila1 hour zoom with Brandon, CEO ‚Äî a deep dive on your experience, devrel strategy, tactics, and information architectureAnother short call with Brandon for both of us to ask and answer questions in preparation for making an offer¬†üì£If you‚Äôd like to hear about future job openings, sign up here.¬†"
https://news.ycombinator.com/rss,"Porth, It's Like Forth but in Python",https://gitlab.com/tsoding/porth,Comments,"






P



porth






Project ID: 30419193








Star
250






1,189 Commits

1 Branch

0 Tags

16.1 MB Project Storage








Concatenative Programming Language for Computers


Read more
























Find file




Select Archive Format




Download source code


zip
tar.gz
tar.bz2
tar









Clone






Clone with SSH










Clone with HTTPS











Open in your IDE



Visual Studio Code (SSH)




Visual Studio Code (HTTPS)




IntelliJ IDEA (SSH)




IntelliJ IDEA (HTTPS)







Copy HTTPS clone URL





Copy SSH clone URLgit@gitlab.com:tsoding/porth.git


Copy HTTPS clone URLhttps://gitlab.com/tsoding/porth.git








README

MIT License

CONTRIBUTING





"
https://news.ycombinator.com/rss,I analyzed shuffling in a million games of MtG Arena (2020),https://old.reddit.com/r/MagicArena/comments/b21u3n/i_analyzed_shuffling_in_a_million_games/,Comments,"



Too Many Requests



whoa there, pardner!
we're sorry, but you appear to be a bot and we've seen too many requests
from you lately. we enforce a hard speed limit on requests that appear to come
from bots to prevent abuse.
if you are not a bot but are spoofing one via your browser's user agent
string: please change your user agent string to avoid seeing this message
again.
please wait 1 second(s) and try again.
as a reminder to developers, we recommend that clients make no
    more than one
    request every two seconds to avoid seeing this message.


"
https://news.ycombinator.com/rss,Faster than the filesystem (2021),https://www.sqlite.org/fasterthanfs.html,Comments,"




35% Faster Than The Filesystem









Small. Fast. Reliable.Choose any three.



Home
Menu
About
Documentation
Download
License
Support
Purchase

Search




About
Documentation
Download
Support
Purchase





Search Documentation
Search Changelog










35% Faster Than The Filesystem



‚ñ∫
Table Of Contents

1. Summary
1.1. Caveats
1.2. Related Studies
2. How These Measurements Are Made
2.1. Read Performance Measurements
2.2. Write Performance Measurements
2.3. Variations
3. General Findings
4. Additional Notes
4.1. Compiling And Testing on Android




1. Summary
SQLite reads and writes small blobs (for example, thumbnail images)
35% faster¬π than the same blobs
can be read from or written to individual files on disk using
fread() or fwrite().

Furthermore, a single SQLite database holding
10-kilobyte blobs uses about 20% less disk space than
storing the blobs in individual files.

The performance difference arises (we believe) because when
working from an SQLite database, the open() and close() system calls
are invoked only once, whereas
open() and close() are invoked once for each blob
when using blobs stored in individual files.  It appears that the
overhead of calling open() and close() is greater than the overhead
of using the database.  The size reduction arises from the fact that
individual files are padded out to the next multiple of the filesystem
block size, whereas the blobs are packed more tightly into an SQLite
database.


The measurements in this article were made during the week of 2017-06-05
using a version of SQLite in between 3.19.2 and 3.20.0.  You may expect
future versions of SQLite to perform even better.

1.1. Caveats


¬πThe 35% figure above is approximate.  Actual timings vary
depending on hardware, operating system, and the
details of the experiment, and due to random performance fluctuations
on real-world hardware.  See the text below for more detail.
Try the experiments yourself.  Report significant deviations on
the SQLite forum.


The 35% figure is based on running tests on every machine
that the author has easily at hand.
Some reviewers of this article report that SQLite has higher 
latency than direct I/O on their systems.  We do not yet understand
the difference.  We also see indications that SQLite does not
perform as well as direct I/O when experiments are run using
a cold filesystem cache.


So let your take-away be this: read/write latency for
SQLite is competitive with read/write latency of individual files on
disk.  Often SQLite is faster.  Sometimes SQLite is almost
as fast.  Either way, this article disproves the common
assumption that a relational database must be slower than direct
filesystem I/O.

1.2. Related Studies

Jim Gray
and others studied the read performance of BLOBs
versus file I/O for Microsoft SQL Server and found that reading BLOBs 
out of the 
database was faster for BLOB sizes less than between 250KiB and 1MiB.
(Paper).
In that study, the database still stores the filename of the content even
if the content is held in a separate file.  So the database is consulted
for every BLOB, even if it is only to extract the filename.  In this
article, the key for the BLOB is the filename, so no preliminary database
access is required.  Because the database is never used at all when
reading content from individual files in this article, the threshold
at which direct file I/O becomes faster is smaller than it is in Gray's
paper.


The Internal Versus External BLOBs article on this website is an
earlier investigation (circa 2011) that uses the same approach as the
Jim Gray paper ‚Äî storing the blob filenames as entries in the
database ‚Äî but for SQLite instead of SQL Server.



2. How These Measurements Are Made
I/O performance is measured using the
kvtest.c program
from the SQLite source tree.
To compile this test program, first gather the kvtest.c source file
into a directory with the SQLite amalgamation source
files ""sqlite3.c"" and ""sqlite3.h"".  Then on unix, run a command like
the following:

gcc -Os -I. -DSQLITE_DIRECT_OVERFLOW_READ \
  kvtest.c sqlite3.c -o kvtest -ldl -lpthread

Or on Windows with MSVC:

cl -I. -DSQLITE_DIRECT_OVERFLOW_READ kvtest.c sqlite3.c

Instructions for compiling for Android
are shown below.


Use the resulting ""kvtest"" program to
generate a test database with 100,000 random uncompressible
blobs, each with a random
size between 8,000 and 12,000 bytes
using a command like this:

./kvtest init test1.db --count 100k --size 10k --variance 2k


If desired, you can verify the new database by running this command:

./kvtest stat test1.db


Next, make copies of all the blobs into individual files in a directory
using a command like this:

./kvtest export test1.db test1.dir


At this point, you can measure the amount of disk space used by
the test1.db database and the space used by the test1.dir directory
and all of its content.  On a standard Ubuntu Linux desktop, the
database file will be 1,024,512,000 bytes in size and the test1.dir
directory will use 1,228,800,000 bytes of space (according to ""du -k""),
about 20% more than the database.


The ""test1.dir"" directory created above puts all the blobs into a single
folder.  It was conjectured that some operating systems would perform 
poorly when a single directory contains 100,000 objects.  To test this,
the kvtest program can also store the blobs in a hierarchy of folders with no
more than 100 files and/or subdirectories per folder.  The alternative
on-disk representation of the blobs can be created using the --tree
command-line option to the ""export"" command, like this:

./kvtest export test1.db test1.tree --tree


The test1.dir directory will contain 100,000 files
with names like ""000000"", ""000001"", ""000002"" and so forth but the
test1.tree directory will contain the same files in subdirectories like
""00/00/00"", ""00/00/01"", and so on.  The test1.dir and test1.test
directories take up approximately the same amount of space, though
test1.test is very slightly larger due to the extra directory entries.


All of the experiments that follow operate the same with either 
""test1.dir"" or ""test1.tree"".  Very little performance difference is
measured in either case, regardless of operating system.


Measure the performance for reading blobs from the database and from
individual files using these commands:

./kvtest run test1.db --count 100k --blob-api
./kvtest run test1.dir --count 100k --blob-api
./kvtest run test1.tree --count 100k --blob-api


Depending on your hardware and operating system, you should see that reads 
from the test1.db database file are about 35% faster than reads from 
individual files in the test1.dir or test1.tree folders.  Results can vary
significantly from one run to the next due to caching, so it is advisable
to run tests multiple times and take an average or a worst case or a best
case, depending on your requirements.

The --blob-api option on the database read test causes kvtest to use
the sqlite3_blob_read() feature of SQLite to load the content of the
blobs, rather than running pure SQL statements.  This helps SQLite to run
a little faster on read tests.  You can omit that option to compare the
performance of SQLite running SQL statements.
In that case, the SQLite still out-performs direct reads, though
by not as much as when using sqlite3_blob_read().
The --blob-api option is ignored for tests that read from individual disk
files.


Measure write performance by adding the --update option.  This causes
the blobs are overwritten in place with another random blob of
exactly the same size.

./kvtest run test1.db --count 100k --update
./kvtest run test1.dir --count 100k --update
./kvtest run test1.tree --count 100k --update


The writing test above is not completely fair, since SQLite is doing
power-safe transactions whereas the direct-to-disk writing is not.
To put the tests on a more equal footing, add either the --nosync
option to the SQLite writes to disable calling fsync() or
FlushFileBuffers() to force content to disk, or using the --fsync option
for the direct-to-disk tests to force them to invoke fsync() or
FlushFileBuffers() when updating disk files.


By default, kvtest runs the database I/O measurements all within
a single transaction.  Use the --multitrans option to run each blob
read or write in a separate transaction.  The --multitrans option makes
SQLite much slower, and uncompetitive with direct disk I/O.  This
option proves, yet again, that to get the most performance out of
SQLite, you should group as much database interaction as possible within
a single transaction.


There are many other testing options, which can be seen by running
the command:

./kvtest help

2.1. Read Performance Measurements
The chart below shows data collected using 
kvtest.c on five different
systems:


Win7: A circa-2009 Dell Inspiron laptop, Pentium dual-core
    at 2.30GHz, 4GiB RAM, Windows7.
Win10: A 2016 Lenovo YOGA 910, Intel i7-7500 at 2.70GHz,
    16GiB RAM, Windows10.
Mac: A 2015 MacBook Pro, 3.1GHz intel Core i7, 16GiB RAM,
    MacOS 10.12.5
Ubuntu: Desktop built from Intel i7-4770K at 3.50GHz, 32GiB RAM,
    Ubuntu 16.04.2 LTS
Android: Galaxy S3, ARMv7, 2GiB RAM

All machines use SSD except Win7 which has a
hard-drive. The test database is 100K blobs with sizes uniformly
distributed between 8K and 12K, for a total of about 1 gigabyte
of content.  The database page size
is 4KiB.  The -DSQLITE_DIRECT_OVERFLOW_READ compile-time option was
used for all of these tests.
Tests were run multiple times.
The first run was used to warm up the cache and its timings were discarded.


The chart below shows average time to read a blob directly from the
filesystem versus the time needed to read the same blob from the SQLite 
database.
The actual timings vary considerably from one system to another 
(the Ubuntu desktop is much
faster than the Galaxy S3 phone, for example).  
This chart shows the ratio of the
times needed to read blobs from a file divided by the time needed to
from the database.  The left-most column in the chart is the normalized
time to read from the database, for reference.


In this chart, an SQL statement (""SELECT v FROM kv WHERE k=?1"") 
is prepared once.  Then for each blob, the blob key value is bound 
to the ?1 parameter and the statement is evaluated to extract the
blob content.


The chart shows that on Windows10, content can be read from the SQLite
database about 5 times faster than it can be read directly from disk.
On Android, SQLite is only about 35% faster than reading from disk.






Chart 1:  SQLite read latency relative to direct filesystem reads.
100K blobs, avg 10KB each, random order using SQL


The performance can be improved slightly by bypassing the SQL layer
and reading the blob content directly using the
sqlite3_blob_read() interface, as shown in the next chart:






Chart 2:  SQLite read latency relative to direct filesystem reads.
100K blobs, avg size 10KB, random order
using sqlite3_blob_read().


Further performance improves can be made by using the
memory-mapped I/O feature of SQLite.  In the next chart, the
entire 1GB database file is memory mapped and blobs are read
(in random order) using the sqlite3_blob_read() interface.
With these optimizations, SQLite is twice as fast as Android
or MacOS-X and over 10 times faster than Windows.






Chart 3:  SQLite read latency relative to direct filesystem reads.
100K blobs, avg size 10KB, random order
using sqlite3_blob_read() from a memory-mapped database.


The third chart shows that reading blob content out of SQLite can be
twice as fast as reading from individual files on disk for Mac and
Android, and an amazing ten times faster for Windows.

2.2. Write Performance Measurements

Writes are slower.
On all systems, using both direct I/O and SQLite, write performance is
between 5 and 15 times slower than reads.


Write performance measurements were made by replacing (overwriting)
an entire blob with a different blob.  All of the blobs in these
experiment are random and incompressible.  Because writes are so much
slower than reads, only 10,000 of the 100,000 blobs in the database
are replaced.  The blobs to be replaced are selected at random and
are in no particular order.


The direct-to-disk writes are accomplished using fopen()/fwrite()/fclose().
By default, and in all the results shown below, the OS filesystem buffers are
never flushed to persistent storage using fsync() or
FlushFileBuffers().  In other words, there is no attempt to make the
direct-to-disk writes transactional or power-safe.
We found that invoking fsync() or FlushFileBuffers() on each file
written causes direct-to-disk storage
to be about 10 times or more slower than writes to SQLite.


The next chart compares SQLite database updates in WAL mode
against raw direct-to-disk overwrites of separate files on disk.
The PRAGMA synchronous setting is NORMAL.
All database writes are in a single transaction.
The timer for the database writes is stopped after the transaction
commits, but before a checkpoint is run.
Note that the SQLite writes, unlike the direct-to-disk writes,
are transactional and power-safe, though because the synchronous
setting is NORMAL instead of FULL, the transactions are not durable.






Chart 4:  SQLite write latency relative to direct filesystem writes.
10K blobs, avg size 10KB, random order,
WAL mode with synchronous NORMAL,
exclusive of checkpoint time


The android performance numbers for the write experiments are omitted
because the performance tests on the Galaxy S3 are so random.  Two
consecutive runs of the exact same experiment would give wildly different
times.  And, to be fair, the performance of SQLite on android is slightly
slower than writing directly to disk.


The next chart shows the performance of SQLite versus direct-to-disk
when transactions are disabled (PRAGMA journal_mode=OFF)
and PRAGMA synchronous is set to OFF.  These settings put SQLite on an
equal footing with direct-to-disk writes, which is to say they make the
data prone to corruption due to system crashes and power failures.






Chart 5:  SQLite write latency relative to direct filesystem writes.
10K blobs, avg size 10KB, random order,
journaling disabled, synchronous OFF.


In all of the write tests, it is important to disable anti-virus software
prior to running the direct-to-disk performance tests.  We found that
anti-virus software slows down direct-to-disk by an order of magnitude
whereas it impacts SQLite writes very little.  This is probably due to the
fact that direct-to-disk changes thousands of separate files which all need
to be checked by anti-virus, whereas SQLite writes only changes the single
database file.

2.3. Variations
The -DSQLITE_DIRECT_OVERFLOW_READ compile-time option causes SQLite
to bypass its page cache when reading content from overflow pages.  This
helps database reads of 10K blobs run a little faster, but not all that much
faster.  SQLite still holds a speed advantage over direct filesystem reads
without the SQLITE_DIRECT_OVERFLOW_READ compile-time option.

Other compile-time options such as using -O3 instead of -Os or
using -DSQLITE_THREADSAFE=0 and/or some of the other
recommended compile-time options might help SQLite to run even faster
relative to direct filesystem reads.

The size of the blobs in the test data affects performance.
The filesystem will generally be faster for larger blobs, since
the overhead of open() and close() is amortized over more bytes of I/O,
whereas the database will be more efficient in both speed and space
as the average blob size decreases.


3. General Findings


SQLite is competitive with, and usually faster than, blobs stored in
separate files on disk, for both reading and writing.


SQLite is much faster than direct writes to disk on Windows
when anti-virus protection is turned on.  Since anti-virus software
is and should be on by default in Windows, that means that SQLite
is generally much faster than direct disk writes on Windows.


Reading is about an order of magnitude faster than writing, for all
systems and for both SQLite and direct-to-disk I/O.


I/O performance varies widely depending on operating system and hardware.
Make your own measurements before drawing conclusions.


Some other SQL database engines advise developers to store blobs in separate
files and then store the filename in the database.  In that case, where
the database must first be consulted to find the filename before opening
and reading the file, simply storing the entire blob in the database
gives much faster read and write performance with SQLite.
See the Internal Versus External BLOBs article for more information.

4. Additional Notes

4.1. Compiling And Testing on Android

The kvtest program is compiled and run on Android as follows.
First install the Android SDK and NDK.  Then prepare a script
named ""android-gcc"" that looks approximately like this:

#!/bin/sh
#
NDK=/home/drh/Android/Sdk/ndk-bundle
SYSROOT=$NDK/platforms/android-16/arch-arm
ABIN=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin
GCC=$ABIN/arm-linux-androideabi-gcc
$GCC --sysroot=$SYSROOT -fPIC -pie $*

Make that script executable and put it on your $PATH.  Then
compile the kvtest program as follows:

android-gcc -Os -I. kvtest.c sqlite3.c -o kvtest-android

Next, move the resulting kvtest-android executable to the Android
device:

adb push kvtest-android /data/local/tmp

Finally use ""adb shell"" to get a shell prompt on the Android device,
cd into the /data/local/tmp directory, and begin running the tests
as with any other unix host.
This page last modified on  2021-03-01 12:55:48 UTC 
"
https://news.ycombinator.com/rss,The Inner Beauty of Basic Electronics,https://spectrum.ieee.org/open-circuits,Comments,"The Inner Beauty of Basic Electronics - IEEE SpectrumIEEE.orgIEEE Xplore Digital LibraryIEEE StandardsMore SitesSign InJoin IEEEThe Inner Beauty of Basic ElectronicsShareFOR THE TECHNOLOGY INSIDERSearch: Explore by topicAerospaceArtificial IntelligenceBiomedicalComputingConsumer ElectronicsEnergyHistory of TechnologyRoboticsSemiconductorsSensorsTelecommunicationsTransportationIEEE SpectrumFOR THE TECHNOLOGY INSIDERTopicsAerospaceArtificial IntelligenceBiomedicalComputingConsumer ElectronicsEnergyHistory of TechnologyRoboticsSemiconductorsSensorsTelecommunicationsTransportationSectionsFeaturesNewsOpinionCareersDIYThe Big PictureEngineering ResourcesMoreSpecial ReportsCollectionsExplainersPodcastsVideosNewslettersTop Programming LanguagesRobots GuideFor IEEE MembersCurrent IssueMagazine ArchiveThe InstituteTI ArchiveFor IEEE MembersCurrent IssueMagazine ArchiveThe InstituteTI ArchiveIEEE SpectrumAbout UsContact UsReprints & PermissionsAdvertisingFollow IEEE SpectrumSupport IEEE SpectrumIEEE Spectrum is the flagship publication of the IEEE ‚Äî the world‚Äôs largest professional organization devoted to engineering and applied sciences. Our articles, podcasts, and infographics inform our readers about developments in technology, engineering, and science.Join IEEESubscribeAbout IEEEContact & SupportAccessibilityNondiscrimination PolicyTermsIEEE Privacy Policy¬© Copyright 2023 IEEE ‚Äî All rights reserved. A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
                        view privacy policy
                    
                    accept & close
                Enjoy more free content and benefits by creating an accountSaving articles to read later requires an IEEE Spectrum accountThe Institute content is only available for membersDownloading full PDF issues is exclusive for IEEE MembersAccess to Spectrum's Digital Edition is exclusive for IEEE MembersFollowing topics is a feature exclusive for IEEE MembersAdding your response to an article requires an IEEE Spectrum accountCreate an account to access more content and features on IEEE Spectrum, including the ability to save articles to read later, download Spectrum Collections, and participate in conversations with readers and editors. For more exclusive content and features, consider Joining IEEE.Join the world‚Äôs largest professional organization devoted to engineering and applied sciences and get access to all of Spectrum‚Äôs articles, archives, PDF downloads, and other benefits. Learn more ‚ÜíCREATE AN ACCOUNTSIGN INJOIN IEEESIGN INCloseAccess Thousands of Articles ‚Äî Completely FreeCreate an account and get exclusive content and features: Save articles, download collections, and talk to tech insiders ‚Äî all free! For full access and benefits, join IEEE as a paying member.CREATE AN ACCOUNTSIGN INConsumer ElectronicsTopicMagazineTypeFeature
        The Inner Beauty of Basic Electronics
    Open Circuits showcases the surprising complexity of passive componentsEric SchlaepferWindell H. Oskay20h5 min readBlueEric Schlaepfer was trying to fix a broken piece of test equipment when he came across the cause of the problem‚Äîa troubled tantalum capacitor. The component had somehow shorted out, and he wanted to know why. So he polished it down for a look inside. He never found the source of the short, but he and his collaborator, Windell H. Oskay, discovered something even better: a breathtaking hidden world inside electronics. What followed were hours and hours of polishing, cleaning, and photography that resulted in Open Circuits: The Inner Beauty of Electronic Components (No Starch Press, 2022), an excerpt of which follows. As the authors write, everything about these components is deliberately designed to meet specific technical needs, but that design leads to ‚Äúaccidental beauty: the emergent aesthetics of things you were never expected to see.‚Äù

	From a book that spans the wide world of electronics, what we at 
	IEEE Spectrum found surprisingly compelling were the insides of things we don‚Äôt spend much time thinking about, passive components. Transistors, LEDs, and other semiconductors may be where the action is, but the simple physics of resistors, capacitors, and inductors have their own sort of splendor. 
            
                High-Stability Film Resistor
            
            
        All photos by Eric Schlaepfer & Windell H. OskayThis high-stability film resistor, about 4 millimeters in diameter, is made in much the same way as its inexpensive carbon-film cousin, but with exacting precision. A ceramic rod is coated with a fine layer of resistive film (thin metal, metal oxide, or carbon) and then a perfectly uniform helical groove is machined into the film.Instead of coating the resistor with an epoxy, it‚Äôs hermetically sealed in a lustrous little glass envelope. This makes the resistor more robust, ideal for specialized cases such as precision reference instrumentation, where long-term stability of the resistor is critical. The glass envelope provides better isolation against moisture and other environmental changes than standard coatings like epoxy.
            
                15-Turn Trimmer Potentiometer
            
            
        It takes 15 rotations of an adjustment screw to move a 15-turn trimmer potentiometer from one end of its resistive range to the other. Circuits that need to be adjusted with fine resolution control use this type of trimmer pot instead of the single-turn variety.The resistive element in this trimmer is a strip of cermet‚Äîa composite of ceramic and metal‚Äîsilk-screened on a white ceramic substrate. Screen-printed metal links each end of the strip to the connecting wires. It‚Äôs a flattened, linear version of the horseshoe-shaped resistive element in single-turn trimmers.Turning the adjustment screw moves a plastic slider along a track. The wiper is a spring finger, a spring-loaded metal contact, attached to the slider. It makes contact between a metal strip and the selected point on the strip of resistive film.
            
                Ceramic Disc Capacitor
            
            
        Capacitors are fundamental electronic components that store energy in the form of static electricity. They‚Äôre used in countless ways, including for bulk energy storage, to smooth out electronic signals, and as computer memory cells. The simplest capacitor consists of two parallel metal plates with a gap between them, but capacitors can take many forms so long as there are two conductive surfaces, called electrodes, separated by an insulator.A ceramic disc capacitor is a low-cost capacitor that is frequently found in appliances and toys. Its insulator is a ceramic disc, and its two parallel plates are extremely thin metal coatings that are evaporated or sputtered onto the disc‚Äôs outer surfaces. Connecting wires are attached using solder, and the whole assembly is dipped into a porous coating material that dries hard and protects the capacitor from damage.
            
                Film Capacitor
            
            
        Film capacitors are frequently found in high-quality audio equipment, such as headphone amplifiers, record players, graphic equalizers, and radio tuners. Their key feature is that the dielectric material is a plastic film, such as polyester or polypropylene.The metal electrodes of this film capacitor are vacuum-deposited on the surfaces of long strips of plastic film. After the leads are attached, the films are rolled up and dipped into an epoxy that binds the assembly together. Then the completed assembly is dipped in a tough outer coating and marked with its value.Other types of film capacitors are made by stacking flat layers of metallized plastic film, rather than rolling up layers of film.
            
                Dipped Tantalum Capacitor
            
            
        At the core of this capacitor is a porous pellet of tantalum metal. The pellet is made from tantalum powder and sintered, or compressed at a high temperature, into a dense, spongelike solid.Just like a kitchen sponge, the resulting pellet has a high surface area per unit volume. The pellet is then anodized, creating an insulating oxide layer with an equally high surface area. This process packs a lot of capacitance into a compact device, using spongelike geometry rather than the stacked or rolled layers that most other capacitors use.The device‚Äôs positive terminal, or anode, is connected directly to the tantalum metal. The negative terminal, or cathode, is formed by a thin layer of conductive manganese dioxide coating the pellet.
            
                Axial Inductor
            
            
        Inductors are fundamental electronic components that store energy in the form of a magnetic field. They‚Äôre used, for example, in some types of power supplies to convert between voltages by alternately storing and releasing energy. This energy-efficient design helps maximize the battery life of cellphones and other portable electronics.Inductors typically consist of a coil of insulated wire wrapped around a core of magnetic material like iron or ferrite, a ceramic filled with iron oxide. Current flowing around the core produces a magnetic field that acts as a sort of flywheel for current, smoothing out changes in the current as it flows through the inductor.This axial inductor has a number of turns of varnished copper wire wrapped around a ferrite form and soldered to copper leads on its two ends. It has several layers of protection: a clear varnish over the windings, a light-green coating around the solder joints, and a striking green outer coating to protect the whole component and provide a surface for the colorful stripes that indicate its inductance value.
            
                Power Supply Transformer
            
            
        This transformer has multiple sets of windings and is used in a power supply to create multiple output AC voltages from a single AC input such as a wall outlet.The small wires nearer the center are ‚Äúhigh impedance‚Äù turns of magnet wire. These windings carry a higher voltage but a lower current. They‚Äôre protected by several layers of tape, a copper-foil electrostatic shield, and more tape.The outer ‚Äúlow impedance‚Äù windings are made with thicker insulated wire and fewer turns. They handle a lower voltage but a higher current.All of the windings are wrapped around a black plastic bobbin. Two pieces of ferrite ceramic are bonded together to form the magnetic core at the heart of the transformer.From Your Site ArticlesDell Tried to Hide Bad Capacitors Problem 2003-2005 ‚Ä∫Hands On - IEEE Spectrum ‚Ä∫Watch: Laser Origami Makes Inductors ‚Ä∫Related Articles Around the WebOpen Circuits: The Inner Beauty of Electronic Components: Oskay ... ‚Ä∫Open Circuits | No Starch Press ‚Ä∫Open Circuits ‚Ä∫passive componentsArt of Electronicsresistorscapacitorsinductorsbooks{""imageShortcodeIds"":[]}Eric SchlaepferEric Schlaepfer runs the popular engineering Twitter account @TubeTimeUS, where he posts cross-section photos, shares his retrocomputing and reverse engineering projects, investigates engineering accidents, and even features the occasional vacuum tube or two. He is coauthor of Open Circuits: The Inner Beauty of Electronic Components (No Starch Press, 2022).Windell H. OskayWindell H. Oskay is the cofounder of Evil Mad Scientist Laboratories, where he designs robots for a living. He is coauthor of¬†Open Circuits: The Inner Beauty of Electronic Components¬†(No Starch Press, 2022).The Conversation (0)
        Video Friday: Robots at Night
    13 Jan 20233 min readAerospaceTopicTypeRoboticsNews
        Relativity Space Aims for Orbit
    13 Jan 20234 min readConsumer ElectronicsTopicTypeNews
        Paper Batteries, Blue Quantum Dots, and Other Enabling Technologies from CES 2023
    12 Jan 20233 min readThe InstituteTopicArticleTypeHistory of Technology
        How This Record Company Engineer Invented the CT Scanner
    The machine, made to image the human brain, won him a Nobel PrizeJoanna GoodrichJoanna Goodrich is the associate editor of The Institute, covering the work and accomplishments of IEEE members and IEEE and technology-related events. She has a master's degree in health communications from Rutgers University, in New Brunswick, N.J.12 Jan 20234 min readResearch engineer Godfrey Hounsfield invented the CT scanner to create three-dimensional brain images.
        PA Images/Getty Images
    ieee historyieee tech historyhistory of technologyct scannermedical devicesieee milestonetype:tiThe inspiration for computed tomography (CT) came from a chance conversation that research engineer Godfrey Hounsfield had with a doctor while on vacation in the 1960s. The physician complained that X-ray images of the brain were too grainy and only two-dimensional.Hounsfield worked at Electrical and Musical Industry in Hayes, England. Best known for producing and selling Beatles records, EMI also developed electronic equipment. Keep Reading ‚ÜìShow lessConsumer ElectronicsTopicTypeComputingSponsored Article
        Building the Future of Smart Home Security
    Engineers must invent new technology to enhance security products‚Äô abilitiesNate WilfertNate Wilfert is Vice President of Software Engineering at SimpliSafe.22 Mar 20224 min readIn this article, SimpliSafe‚Äôs VP of Software Engineering discusses his team‚Äôs focus on creating a safer future through enhanced technology.
        SimpliSafe
    smart homeiotconnected homesecuritysimplisafeThis is a sponsored article brought to you by SimpliSafe.It‚Äôs nearly impossible to find a household today that doesn‚Äôt have at least one connected smart home device installed. From video doorbells to robot vacuums, automated lighting, and voice assistants, smart home technology has invaded consumers‚Äô homes and shows no sign of disappearing anytime soon. Indeed, according to a study conducted by consulting firm Parks Associates, smart home device adoption has increased by more than 64 percent in the past two years, with 23 percent of households owning three or more smart home devices. This is particularly true for devices that provide security with 38 percent of Americans owning a home security product. This percentage is likely to increase as 7 in 10 homebuyers claimed that safety and security was the primary reason, after convenience, that they would be seeking out smart homes, according to a report published by Security.org last year.As the demand for smart home security grows, it‚Äôs pertinent that the engineers who build the products and services that keep millions of customers safe continue to experiment with new technologies that could enhance overall security and accessibility. At SimpliSafe, an award-winning home security company based in Boston, Mass., it is the pursuit of industry-leading protection that drives the entire organization to continue innovating.In this article, Nate Wilfert, VP of Software Engineering at SimpliSafe, discusses the complex puzzles his team is solving on a daily basis‚Äîsuch as applying artificial intelligence (AI) technology into cameras and building load-balancing solutions to handle server traffic‚Äîto push forward the company‚Äôs mission to make every home secure and advance the home security industry as a whole.Keep Reading ‚ÜìShow less
        Trending Stories
    The most-read stories on IEEE Spectrum right nowThe InstituteTopicArticleTypeHistory of Technology
        How This Record Company Engineer Invented the CT Scanner
    12 Jan 20234 min readAerospaceTopicTypeRoboticsNews
        Relativity Space Aims for Orbit
    13 Jan 20234 min readConsumer ElectronicsTopicTypeNews
        Paper Batteries, Blue Quantum Dots, and Other Enabling Technologies from CES 2023
    12 Jan 20233 min readConsumer ElectronicsTopicTypeNews
        CES 2023‚Äôs Four Wildest‚Äîand Catchiest‚ÄîGadgets
    11 Jan 20233 min readThe InstituteTopicTypeOpinionTelecommunications
        Examining the Impact of 6G Telecommunications on Society
    10 Jan 20233 min readSensorsTopicArtificial IntelligenceTypeNews
        Spray-on Smart Skin Reads Typing and Hand Gestures
    11 Jan 20233 min readTelecommunicationsTopicMagazineTypeFeature
        How Police Exploited the Capitol Riot‚Äôs Digital Records
    06 Jan 202311 min readConsumer ElectronicsTopicTypeNewsTransportation
        The Best Tech of CES 2023
    09 Jan 20236 min read"
https://news.ycombinator.com/rss,Running KDE Plasma on RISC-V VisionFive-2,https://cordlandwehr.wordpress.com/2023/01/14/running-plasma-on-visionfive-2/,Comments,"


Running Plasma on¬†VisionFive-2 

New year, new RISC-V Yocto blog post \o/ When I wrote my last post, I did really not expect my brand new VisionFive-2 board to find its way to me so soon‚Ä¶ But well, a week ago it was suddenly there. While unpacking I shortly pondered over my made plans to prepare a Plasma Bigscreen RaspberryPi 4 demo board for this year‚Äôs FOSDEM.
Obvious conclusion: ‚ÄúScrew it! Let‚Äôs do the demo on the VisionFive-2!‚Äù ‚Äî And there we are:
After some initial bumpy steps to boot up a first self-compiled U-boot and Kernel (If you unbox a new board, you need to do a bootloader and firmware update first! Otherwise it will not boot the latest VisionFive Kernel) it was surprisingly easy to prepare Yocto to build a core-image-minimal that really boots the whole way up.
Unfortunately after these first happy hours, the last week was full of handling the horrors of closed-source binary drivers for the GPU. Even though Imagination promised to provide an open source driver at some time, right now there is only the solution to use the closed source PVR driver. After quite a lot of trying, guessing and and comparing the boot and init sequences of the reference image to the dark screen in front of me, I came up with:

a new visionfive2-graphics Yocto package for the closed source driver blobs
a fork of Mesa that uses a very heavy patch set for the PVR driver adaptions; all patches are taken from the VisionFive 2 buildroot configurations
and a couple of configs for making the system start with doing an initial modeset

The result right now:

VisionFive-2 device with Plasma-Bigscreen (KWin running via Wayland), SD card image built via Yocto, KDE software via KDE‚Äôs Yocto layers, Kernel and U-Boot being the latest fork versions from StarFive
Actually, the full UI even feels much smoother than on my RPi4, which is quite cool. I am not sure where I will end in about 3 weeks with some more debugging and patching. But I am very confident that you can see a working RISC-V board with onboard GPU and running Plasma Shell, when you visit the KDE stall at FOSDEM in February üòâ
For people who are interested in Yocto, here is the WIP patch set: https://github.com/riscv/meta-riscv/pull/382
Share this:TwitterFacebookLike this:Like Loading...

Related
 

Posted on January 14, 2023January 14, 2023Author cordlandwehrCategories KDE, YoctoTags KDE 



Leave a Reply Cancel reply


Enter your comment here...




Fill in your details below or click an icon to log in:







 



 



 






 
 


Email (required) (Address never made public)



Name (required)



Website
















			You are commenting using your WordPress.com account.			
				(¬†Log¬†Out¬†/¬†
				Change¬†)
			
















			You are commenting using your Twitter account.			
				(¬†Log¬†Out¬†/¬†
				Change¬†)
			
















			You are commenting using your Facebook account.			
				(¬†Log¬†Out¬†/¬†
				Change¬†)
			






Cancel
Connecting to %s




 Notify me of new comments via email. Notify me of new posts via email.
 



Œî 



Post navigation
Previous Previous post: Getting a First Picture on my Nezha RISC-V¬†Board

"
https://news.ycombinator.com/rss,The Bibites: Artificial Life Simulation,https://leocaussan.itch.io/the-bibites,Comments,"The Bibites by The BibitesFollow The BibitesFollowFollowing The BibitesFollowingAdd To CollectionCollectionCommentsDevlogRelated gamesRelatedThe BibitesA downloadable project for Windows, macOS, and LinuxDownload NowName your own priceWelcome everyone!¬†
This is The Bibites ¬†
A simulation where you are able to watch evolution happen before your very eyes!¬†
Each¬†bibite (the small critters you see on the screen) starts off with an empty brain (they do nothing) and pretty basic genes (they all look alike).¬†
Through random mutations, one can be spawned with a brain connection that will link two neurons and might trigger a behavior, like going forward, which will allow them to eat food, and then reproduce with the energy gained.
You have reproduction, mutations, and natural selection, which leads to ...¬†

With time, this develops into complex behaviors, like following¬†pheromone trails to hunt other bibites, or stockpiling food in a specific area of the map.¬†

Present Features
VisionProcedural Sprites (generating a custom sprite for each bibite from their genes)
Self-awareness (state, health, energy, etc.)Pheromones (producing and sensing)Grabbing and Throwing stuff (pellets and other bibites)Materials and Digestion SimulationRealistic Energy System
The simulation is also interactive, allowing you to YEET bibites and pellets around. You can selectively kill bibites, feed them, force the laying of eggs, and so much more.
It's also highly customizable, allowing you to test a nearly infinite number of scenarios. How will they evolve if there is no drag (no friction)? What about if moving is extremely energy-costly? It's your job to test it all, I sure can't do it¬†by myself.
I'LL STATE CLEARLY THAT THIS IS THE REGULAR VERSION. I TRIED TO DISABLE ""name your own price"" AND SET IT TO¬†0.00$ BUT IT DON'T SEEM TO WORK...I ENCOURAGE YOU TO DOWNLOAD THIS FOR FREE, ONLY PAY SOMETHING IF YOU WANT TO THROW MONEY AT ME FOR NO OTHER REASON THAN TO SUPPORT THIS PROJECT. The best way to do so is to subscribe to my Patreon to provide me with reliable support and have access to the alpha updates as I develop them:¬†
Become a Patron to get alpha updates!¬†

Follow the development and see additional content on Youtube

Follow me on Twitter to see... whatever I do there

Join the subreddit community

Upcoming features¬†
Module-based systems for unbounded evolution and incredible performancesBiomes (environmental simulation)Evolving ecosystems (the plants/food evolves too)Rocks (Movable objects)And much more!

After trying it out, please give me some feedback

Or report bugs
More informationUpdated 28 days agoStatusIn developmentPlatformsWindows, macOS, LinuxRatingRated 4.6 out of 5 stars(69 total ratings)AuthorThe BibitesGenreSimulationMade withUnityTags2D, artificial-intelligence, evolution, interactive, Life Simulation, Pixel Art, Procedural Generation, Sandbox, UnityAverage sessionA few hoursLanguagesEnglishInputsKeyboard, MouseMultiplayerLocal multiplayerPlayer countSingleplayerLinksYouTube, Patreon, Twitter, CommunityDownloadDownload NowName your own priceClick download now to get access to the following files:The Bibites 0.4.2 - Windows 64x.zip 30 MB  The Bibites 0.4.2 - Linux.zip 41 MB  The Bibites 0.4.2 - Mac Universal.zip 36 MB  The Bibites 0.4.2 - Windows 32x.zip 27 MB  The Bibites 0.5.0 - Linux.zip 84 MB  The Bibites 0.5.0 - Windows 32x.zip 70 MB  The Bibites 0.5.0 - Mac Universal.zip 98 MB  The Bibites 0.5.0 - Windows 64x.zip 73 MB  Development logThe Bibites 0.5.0: Modernity and Progress 28 days agoThe Bibites  v0.4.2: Balance and stability Jun 19, 2022The Bibites  v0.4.1 Mar 28, 2022The Bibites 0.3.0 : Artificial Life With Herding and Viruses Jun 25, 2021It's official, this is launch üöÄüöÄüöÄ! Full-time on The Bibites May 20, 2021Roadmap for the future of the project Ep.3 Procedural Sprites! Jan 24, 2021Roadmap for the future of the project Ep.2 Modules! Jan 11, 2021Roadmap of the future of the Project Ep.1 Dec 28, 2020View all postsCommentsLog in with itch.io to leave a comment.Viewing most recent comments 1 to 40 of 112 ¬∑ Next page ¬∑ Last page Waterloo057 hours agoso dowload the game... from where do i enter to it?Reply Davket00520 hours ago!!!Reply Davket00520 hours agohow the hell do I download itReply Victoria_the_cool4 days agoi cant figure out how to save my progressReply Victoria_the_cool4 days agoyou should probably implement in-game save filesReply Riptides_storm2 days agohit settings top right, save gameleads to a menu where u can name the save file¬†for 0.5 btwReply JoeKing295 days agoWill you add android version? (if it is possible)Reply TheSmartBanana5 days agoIs there an opotion that allows you to paint or upload your own bit parts like texture packs in minecraft?Reply Filipcucumer18 days agoIf i have a problem how do i report it?Reply R0fael25 days ago(+1)It's the best simulator of lifeReply The Bibites25 days ago(+1)Thanks!Reply R0fael23 days ago (1 edit) Can you fix 0 fps when you have more than 100 bibitesReply johnnysmith10 days agoprobably a hardware issue (bad computer)Reply R0fael7 days agono, it can run windows 11Reply Riptides_storm2 days agothat dosn't really determine how good you computer is.Reply coryedora28 days ago(+1)(-3)how I download the game?Reply Crknite!26 days ago(+10)By completing elementary school.Reply Nikki_Devil31 days ago(+2)I wanted to know, will the Linux version also be compiled to Arm64 processors ? I'd really like to use this on my server but as of now I can't and am stuck with my 15yo 32bit pc :,)Reply EKKN38 days ago(+5)Great game, recommended.Even though there are bugs and uncompleted features, it is a decent and very interesting¬†game.Good luck on developing the game!Reply tosety56 days ago(+5)(-1)running on Ubuntu 20 I get a grey screen and cursor, but nothing elseReply Methisa53 days ago (1 edit) (+4)(-1)same for me on steamdeck running steam os. Happened on v 3.0 and 4.2Reply geomagas27 days ago(+1)Same here!Reply Friday_13th56 days ago(+1)I think we rly need some multi core optimizations. Program struggles a lot when there is a high birth rate bibite developedReply baulerbonduc63 days ago(+2)(-1)hey this don't¬†work on linux.please fix if you canReply raktul89 days ago(+1)I like the o & g binding to find oldest and highest generation respectfully, but would love to add more search features/ toggles between multiple bit bits of the same generation. Not sure how to support the development(as in offering my own time/skills to learn and implement)Reply Garyizcool103 days ago(+2)I got to play it once but now its not letting me go on. not sure if its my computer or some sort of glitch but I'm getting a new computer today so we will see if it works then :). if this has happened to anyone else and they know how to solve it could you please help?Reply ThemonstousBibiteengineer111 days ago(-2)how download simReply Skyper111 days ago(+1)Is there a way to save the simulation?Reply Victoria_the_cool2 days agoi hope he adds in-game save filesReply Magnet Boi117 days ago(+5)When will neural netork editor be available?Reply bloodytomb122 days ago(-1)my own thing is how do i use neural network editorReply Phoenix_185128 days ago(+2)How do you download this on linux/chromebook? It will be very helpful if someone can tell me.Reply sssemil130 days ago(+4)To fix the blue screen on Linux, run with the following parameter:¬†-force-vulkan. Cheers.Reply HotNoob130 days ago(+3)
./'The Bibites.x86_64' -force-vulkanWorks! thx.Reply jinnturtle130 days ago (1 edit) (+5)Stuck on a dark blue screen immediately after running the executable, nothing seems to change even if I let it sit there for a while.OS: distro is ArchLinux running on kernell v5.18.15GPU:¬†Nvidia GeForce 1060s ; driver version¬†515.57CPU: Intel i5-9400FGame version in question:¬†The Bibites 0.4.2The game/sim looks quite interesting from what I've seen and read of it, well done!Reply juega331131 days ago(+1)Is this going to be on Android at some point?Reply Fiddeou131 days ago(+4)I'm on mac Mojave. I open the game, and when loading it just stops at 50%Reply damiantyler8a58 days ago(+2)same, why does this happen?Reply ViyWolf56 days ago(+3)You need to put the file into the applications folder.Reply IIDisruptII133 days ago (1 edit) (+4)(-1)Linux version is broken.
After the unity flash screen goes away it get's stuck on a dark blue screen.
Nothing at all, gotta alt f4 or tab out to close.I'm on Ubuntu¬†22.04.1 LTS x86_64, I hope this get's fixed the game looks super dope.Reply TeDe3152 days ago (1 edit) (+2)Add bodyplans and abylity to change them!!!!!!!! PLSReply Morado161 days ago(+3)I'm having difficulties on macOS, the game hangs on 50%; any suggestion?Reply sunusl157 days ago(+1)I am having the same issueReply RottenLynx165 days ago(+3)The game does not work on linux. There's just a dark blue screen after the unity splash screen.Reply someguyplaysitchgames132 days ago(+1)yeah same hereReply R333999174 days ago(+2)make an android version pleaseReply lilyhavok179 days ago(+1)As a fan of Framsticks, Artificial LIfe ENvironment, and AL:RE this is definitely on my watchlist. I love everything so far, and it runs quite well.¬†Do you plan to allow creating Bibites through genetic programming?Reply String Studios184 days agoWhere source code?Reply helsy185 days ago(+1)wont load :( stuck on 50% permanentlyReply neccarus180 days ago(+1)Had this happen. Moved it to another folder location and it worked. Seems like it was a permissions errorReply helsy180 days ago(+1)i'll try it out! :D thanks for the replyReply Daevan189 days ago(+1)I tried to download it, sadly my Mac says it can't search for malware and the software needs to be updated to do that.¬†Reply Robotex4193 days ago (2 edits) (+4)some of my bibites evolved to ""herd"" with individual prey, chasing it to eat its meat once it dies. I kind of think that the herding node is too advanced and powerful (it seems to completely overpower things like ""pellet concentration angle""), it could have an internal neural network which can also evolve and change, or a better option would be to make it an input¬†node like pellet concentration angle¬†which would simply cause the bibite to accelerate towards its herd (but only when moving slower)¬†if connected to the accelerate node,¬†or turn towards its herd, if connected to the rotate node, this option could also come with making all input/output nodes also being able to be modifier nodes, allowing for strands like """"pellet concentration angle""-""herding""-""rotate"""" which would cause the bibite to turn towards pellets by a value altered by how close/far said pellet¬†is from the herdReply Robotex4193 days ago(+3)Here's my idea to make bibites able to be just a bit better: memory, for example,¬†¬†if they see a pellet somewhere but pass by it (and no longer see it). They could, with this adaptation, remember where it is anyway, acting as though they can see it even if it's out of view. A memorized thing would no longer need to be sensed to trigger something like ""pellet concentration angle"". This would all work through a ""commit to memory"" node, which would save the bibites location, direction, and everything it senses to a single memory slot, a ""bite"" if you will, but it would not need any extra nodes to call this info, instead, it would be called by the normal sensing nodes if nothing else is found. there would be a ""memory"" stat, which by default would be 0, and would control the number of ""bites"" a bibite can remember, and once a bibite fills its memory older memories are deleted. There could also be a node that would update/replace a memory slot, and another that would delete a memory slot. This memory system would save many splendid¬†bibites from a lonely fate in the void.Reply Jognh199 days ago(+1)I have a glitch with the latest windows version where no matter how much energy my bibites are getting they have a minimum energy loss. If they end up deep in the negative of energy consumption they still lose energy faster, but there seems to be a point (around 0.1 e/s) where it just doesnt lose less or gain any and all my bibites just dieReply EnchantedAxolotl204 days ago (1 edit) (+1)my strong bibites keep throwing themselves into the void :( they are kinda dumb, but i love this game!Reply Kammcorder205 days ago(+3)i am having the hardest time modding the game, can you please make the guide more easy to understand?Reply Lrapava205 days ago(+2)Fix Linux version plzReplyViewing most recent comments 1 to 40 of 112 ¬∑ Next page ¬∑ Last pageitch.io¬∑View all by The Bibites¬∑Report¬∑Embed¬∑Updated  28 days agoGames ‚Ä∫ Simulation ‚Ä∫ Free"
https://news.ycombinator.com/rss,VToonify: Controllable high-resolution portrait video style transfer,https://github.com/williamyang1991/VToonify,Comments,"








williamyang1991

/

VToonify

Public




 

Notifications



 

Fork
    238




 


          Star
 2.2k
  









        [SIGGRAPH Asia 2022] VToonify: Controllable High-Resolution Portrait Video Style Transfer
      
License





     View license
    






2.2k
          stars
 



238
          forks
 



 


          Star

  





 

Notifications












Code







Issues
7






Pull requests
1






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







williamyang1991/VToonify









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





0
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




williamyang1991

Update train_vtoonify_d.py




        ‚Ä¶
      




        cf993aa
      

Nov 15, 2022





Update train_vtoonify_d.py


cf993aa



Git stats







166

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








checkpoint



Update README.md



Sep 12, 2022









data



Add files via upload



Oct 3, 2022









environment



Add files via upload



Sep 14, 2022









model



Update align_all_parallel.py



Oct 2, 2022









notebooks



‰ΩøÁî® Colaboratory ÂàõÂª∫



Oct 7, 2022









output



Update readme.md



Sep 12, 2022









LICENSE.md



Update LICENSE.md



Sep 14, 2022









README.md



Update README.md



Oct 13, 2022









smooth_parsing_map.py



Add files via upload



Sep 12, 2022









style_transfer.py



Add files via upload



Sep 12, 2022









train_vtoonify_d.py



Update train_vtoonify_d.py



Nov 15, 2022









train_vtoonify_t.py



Update train_vtoonify_t.py



Sep 16, 2022









util.py



Update util.py



Oct 1, 2022









vtoonify_model.py



Update vtoonify_model.py



Oct 4, 2022




    View code
 


















VToonify - Official PyTorch Implementation
Updates
Web Demo
Installation
(1) Inference for Image/Video Toonification
Inference Notebook
Pre-trained Models
Style Transfer with VToonify-D
Style Transfer with VToonify-T
(2) Training VToonify
Train VToonify-D
Train VToonify-T
(3) Results
Citation
Acknowledgments





README.md




VToonify - Official PyTorch Implementation





overview.mp4





This repository provides the official PyTorch implementation for the following paper:
VToonify: Controllable High-Resolution Portrait Video Style Transfer
Shuai Yang, Liming Jiang, Ziwei Liu and Chen Change Loy
In ACM TOG (Proceedings of SIGGRAPH Asia), 2022.
Project Page | Paper | Supplementary Video | Input Data and Video Results 




Abstract: Generating high-quality artistic portrait videos is an important and desirable task in computer graphics and vision.
Although a series of successful portrait image toonification models built upon the powerful StyleGAN have been proposed,
these image-oriented methods have obvious limitations when applied to videos, such as the fixed frame size, the requirement of face alignment, missing non-facial details and temporal inconsistency.
In this work, we investigate the challenging controllable high-resolution portrait video style transfer by introducing a novel VToonify framework.
Specifically, VToonify leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details. The resulting fully convolutional architecture accepts non-aligned faces in videos of variable size as input, contributing to complete face regions with natural motions in the output.
Our framework is compatible with existing StyleGAN-based image toonification models to extend them to video toonification, and inherits appealing features of these models for flexible style control on color and intensity.
This work presents two instantiations of VToonify built upon Toonify and DualStyleGAN for collection-based and exemplar-based portrait video style transfer, respectively.
Extensive experimental results demonstrate the effectiveness of our proposed VToonify framework over existing methods in generating high-quality and temporally-coherent artistic portrait videos with flexible style controls.

Features:
High-Resolution Video (>1024, support unaligned faces) | Data-Friendly (no real training data) | Style Control

Updates

[10/2022] Integrate Gradio interface into Colab notebook. Enjoy the web demo!
[10/2022] Integrated to ü§ó Hugging Face. Enjoy the web demo!
[09/2022] Input videos and video results are released.
[09/2022] Paper is released.
[09/2022] Code is released.
[09/2022] This website is created.

Web Demo
Integrated into Huggingface Spaces ü§ó using Gradio. Try out the Web Demo 
Installation
Clone this repo:
git clone https://github.com/williamyang1991/VToonify.git
cd VToonify
Dependencies:
We have tested on:

CUDA 10.1
PyTorch 1.7.0
Pillow 8.3.1; Matplotlib 3.3.4; opencv-python 4.5.3; Faiss 1.7.1; tqdm 4.61.2; Ninja 1.10.2

All dependencies for defining the environment are provided in environment/vtoonify_env.yaml.
We recommend running this repository using Anaconda (you may need to modify vtoonify_env.yaml to install PyTorch that matches your own CUDA version following https://pytorch.org/):
conda env create -f ./environment/vtoonify_env.yaml
If you have a problem regarding the cpp extention (fused and upfirdn2d), or no GPU is available, you may refer to CPU compatible version.

(1) Inference for Image/Video Toonification
Inference Notebook

To help users get started, we provide a Jupyter notebook found in ./notebooks/inference_playground.ipynb that allows one to visualize the performance of VToonify.
The notebook will download the necessary pretrained models and run inference on the images found in ./data/.
Pre-trained Models
Pre-trained models can be downloaded from Google Drive, Baidu Cloud (access code: sigg) or Hugging Face:


BackboneModelDescription


DualStyleGANcartoonpre-trained VToonify-D models and 317 cartoon style codes


caricaturepre-trained VToonify-D models and 199 caricature style codes


arcanepre-trained VToonify-D models and 100 arcane style codes


comicpre-trained VToonify-D models and 101 comic style codes


pixarpre-trained VToonify-D models and 122 pixar style codes


illustrationpre-trained VToonify-D models and 156 illustration style codes


Toonifycartoonpre-trained VToonify-T model


caricaturepre-trained VToonify-T model


arcanepre-trained VToonify-T model


comicpre-trained VToonify-T model


pixarpre-trained VToonify-T model


Supporting model 


encoder.ptPixel2style2pixel encoder to map real faces into Z+ space of StyleGAN


faceparsing.pthBiSeNet for face parsing from face-parsing.PyTorch


The downloaded models are suggested to be arranged in this folder structure.
The VToonify-D models are named with suffixes to indicate the settings, where

_sXXX: supports only one fixed style with XXX the index of this style.

_s without XXX means the model supports examplar-based style transfer


_dXXX: supports only a fixed style degree of XXX.

_d without XXX means the model supports style degrees ranging from 0 to 1


_c: supports color transfer.

Style Transfer with VToonify-D
‚úî A quick start HERE
Transfer a default cartoon style onto a default face image ./data/077436.jpg:
python style_transfer.py --scale_image
The results are saved in the folder ./output/, where 077436_input.jpg is the rescaled input image to fit VToonify (this image can serve as the input without --scale_image) and 077436_vtoonify_d.jpg is the result.

Specify the content image and the model, control the style with the following options:

--content: path to the target face image or video
--style_id: the index of the style image (find the mapping between index and the style image here).
--style_degree (default: 0.5): adjust the degree of style.
--color_transfer(default: False): perform color transfer if loading a VToonify-Dsdc model.
--ckpt: path of the VToonify-D model. By default, a VToonify-Dsd trained on cartoon style is loaded.
--exstyle_path: path of the extrinsic style code. By default, codes in the same directory as --ckpt are loaded.
--scale_image: rescale the input image/video to fit VToonify (highly recommend).
--padding (default: 200, 200, 200, 200): left, right, top, bottom paddings to the eye center.

Here is an example of arcane style transfer:
python style_transfer.py --content ./data/038648.jpg \
       --scale_image --style_id 77 --style_degree 0.5 \
       --ckpt ./checkpoint/vtoonify_d_arcane/vtoonify_s_d.pt \
       --padding 600 600 600 600     # use large padding to avoid cropping the image

Specify --video to perform video toonification:
python style_transfer.py --scale_image --content ./data/YOUR_VIDEO.mp4 --video
The above style control options (--style_id, --style_degree, --color_transfer) also work for videos.
Style Transfer with VToonify-T
Specify --backbone as ''toonify'' to load and use a VToonify-T model.
python style_transfer.py --content ./data/038648.jpg \
       --scale_image --backbone toonify \
       --ckpt ./checkpoint/vtoonify_t_arcane/vtoonify.pt \
       --padding 600 600 600 600     # use large padding to avoid cropping the image

In VToonify-T, --style_id, --style_degree, --color_transfer, --exstyle_path are not used.
As with VToonify-D, specify --video to perform video toonification.

(2) Training VToonify
Download the supporting models to the ./checkpoint/ folder and arrange them in this folder structure:



Model
Description




stylegan2-ffhq-config-f.pt
StyleGAN model trained on FFHQ taken from rosinality


encoder.pt
Pixel2style2pixel encoder that embeds FFHQ images into StyleGAN2 Z+ latent code


faceparsing.pth
BiSeNet for face parsing from face-parsing.PyTorch


directions.npy
Editing vectors taken from LowRankGAN for editing face attributes


Toonify | DualStyleGAN
pre-trained stylegan-based toonification models



To customize your own style, you may need to train a new Toonify/DualStyleGAN model following here.
Train VToonify-D
Given the supporting models arranged in the default folder structure, we can simply pre-train the encoder and train the whole VToonify-D by running
# for pre-training the encoder
python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_d.py \
       --iter ITERATIONS --stylegan_path DUALSTYLEGAN_PATH --exstyle_path EXSTYLE_CODE_PATH \
       --batch BATCH_SIZE --name SAVE_NAME --pretrain
# for training VToonify-D given the pre-trained encoder
python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_d.py \
       --iter ITERATIONS --stylegan_path DUALSTYLEGAN_PATH --exstyle_path EXSTYLE_CODE_PATH \
       --batch BATCH_SIZE --name SAVE_NAME                  # + ADDITIONAL STYLE CONTROL OPTIONS
The models and the intermediate results are saved in ./checkpoint/SAVE_NAME/ and ./log/SAVE_NAME/, respectively.
VToonify-D provides the following STYLE CONTROL OPTIONS:

--fix_degree: if specified, model is trained with a fixed style degree (no degree adjustment)
--fix_style: if specified, model is trained with a fixed style image (no examplar-based style transfer)
--fix_color: if specified, model is trained with color preservation (no color transfer)
--style_id: the index of the style image (find the mapping between index and the style image here).
--style_degree (default: 0.5): the degree of style.

Here is an example to reproduce the VToonify-Dsd on Cartoon style and the VToonify-D specialized for a mild toonification on the 26th cartoon style:
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_d.py \
       --iter 30000 --stylegan_path ./checkpoint/cartoon/generator.pt --exstyle_path ./checkpoint/cartoon/refined_exstyle_code.npy \
       --batch 1 --name vtoonify_d_cartoon --pretrain      
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_d.py \
       --iter 2000 --stylegan_path ./checkpoint/cartoon/generator.pt --exstyle_path ./checkpoint/cartoon/refined_exstyle_code.npy \
       --batch 4 --name vtoonify_d_cartoon --fix_color 
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_d.py \
       --iter 2000 --stylegan_path ./checkpoint/cartoon/generator.pt --exstyle_path ./checkpoint/cartoon/refined_exstyle_code.npy \
       --batch 4 --name vtoonify_d_cartoon --fix_color --fix_degree --style_degree 0.5 --fix_style --style_id 26
Note that the pre-trained encoder is shared by different STYLE CONTROL OPTIONS. VToonify-D only needs to pre-train the encoder once for each DualStyleGAN model.
Eight GPUs are not necessary, one can train the model with a single GPU with larger --iter.
Tips: [how to find an ideal model] we can first train a versatile model VToonify-Dsd,
and navigate around different styles and degrees. After finding the ideal setting, we can then train the model specialized in that setting for high-quality stylization.
Train VToonify-T
The training of VToonify-T is similar to VToonify-D,
# for pre-training the encoder
python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_t.py \
       --iter ITERATIONS --finetunegan_path FINETUNED_MODEL_PATH \
       --batch BATCH_SIZE --name SAVE_NAME --pretrain       # + ADDITIONAL STYLE CONTROL OPTION
# for training VToonify-T given the pre-trained encoder
python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_t.py \
       --iter ITERATIONS --finetunegan_path FINETUNED_MODEL_PATH \
       --batch BATCH_SIZE --name SAVE_NAME                  # + ADDITIONAL STYLE CONTROL OPTION
VToonify-T only has one STYLE CONTROL OPTION:

--weight (default: 1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0): 18 numbers indicate how the 18 layers of the ffhq stylegan model and the finetuned model are blended to obtain the final Toonify model. Here is the --weight we use in the paper for different styles. Please refer to toonify for the details.

Here is an example to reproduce the VToonify-T model on Arcane style:
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_t.py \
       --iter 30000 --finetunegan_path ./checkpoint/arcane/finetune-000600.pt \
       --batch 1 --name vtoonify_t_arcane --pretrain --weight 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1 1 1 1 1 1 1 1 1 1 1
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_t.py \
       --iter 2000 --finetunegan_path ./checkpoint/arcane/finetune-000600.pt \
       --batch 4 --name vtoonify_t_arcane --weight 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1 1 1 1 1 1 1 1 1 1 1

(3) Results
Our framework is compatible with existing StyleGAN-based image toonification models to extend them to video toonification, and inherits their appealing features for flexible style control. With DualStyleGAN as the backbone, our VToonify is able to transfer the style of various reference images and adjust the style degree in one model.





joint.style.and.degree.control.mp4





Here are the color interpolated results of VToonify-D and VToonify-Dc on Arcane, Pixar and Comic styles.





styles.mp4





Citation
If you find this work useful for your research, please consider citing our paper:
@article{yang2022Vtoonify,
  title={VToonify: Controllable High-Resolution Portrait Video Style Transfer},
  author={Yang, Shuai and Jiang, Liming and Liu, Ziwei and Loy, Chen Change},
  journal={ACM Transactions on Graphics (TOG)},
  volume={41},
  number={6},
  articleno={203},
  pages={1--15},
  year={2022},
  publisher={ACM New York, NY, USA},
  doi={10.1145/3550454.3555437},
}
Acknowledgments
The code is mainly developed based on stylegan2-pytorch, pixel2style2pixel and DualStyleGAN.









About

      [SIGGRAPH Asia 2022] VToonify: Controllable High-Resolution Portrait Video Style Transfer
    
Topics



  style-transfer


  face


  siggraph-asia


  stylegan2


  toonify


  video-style-transfer



Resources





      Readme
 
License





     View license
    



Stars





2.2k
    stars

Watchers





54
    watching

Forks





238
    forks







    Releases

No releases published






    Packages 0


        No packages published 







    Contributors 4





¬†



¬†



¬†



¬†







Languages












Jupyter Notebook
91.0%







Python
8.3%







Other
0.7%











"
https://news.ycombinator.com/rss,ZSWatch ‚Äì Open-source Zephyr-based smartwatch,https://github.com/jakkra/ZSWatch,Comments,"








jakkra

/

ZSWatch

Public




 

Notifications



 

Fork
    5




 


          Star
 204
  









        ZSWatch - the Open Source Zephyr‚Ñ¢ based Smartwatch, including both HW and FW.
      
License





     MIT license
    






204
          stars
 



5
          forks
 



 


          Star

  





 

Notifications












Code







Issues
1






Pull requests
1






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







jakkra/ZSWatch









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





0
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




jakkra

Merge remote-tracking branch 'origin/main' into main




        ‚Ä¶
      




        b0bbf79
      

Jan 14, 2023





Merge remote-tracking branch 'origin/main' into main


b0bbf79



Git stats







123

                      commits
                    







Files

Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github


¬†


¬†









CAD


¬†


¬†









ZSWatch-kicad


¬†


¬†









app


¬†


¬†









schematic


¬†


¬†









.gitignore


¬†


¬†









LICENCE


¬†


¬†









README.md


¬†


¬†




    View code
 



















ZSWatch
Hardware Features in ZSWatch v1
Upcoming Hardware features in ZSWatch v2
Charger/Dock
Enclosure/Casing
Software Features
Larger not yet implemented SW Features and TODOs
Android phone communication
PCB
ZSWatch in action
Writing apps for the Application Manager
Dock





README.md





ZSWatch


  The ZSWatch v1



Smartwatch built from scratch, both hardware and software. Built on the Zephyr‚Ñ¢ Project RTOS, hence the name ZSWatch - Zephyr Smartwatch.

Hardware Features in ZSWatch v1

nRF52833 BLE chip (u-blox ANNA-B402 module).
1.28"" 240x240 IPS TFT Circular Display with GC9A01 driver.
Accelerometer for step counting etc. (LIS2DS12TR).
Pulse oximetry and heartrate using (MAX30101EFD)).
Vibration motor with haptics driver to give better vibration control (DRV2603RUNT).
External 8MB flash (MX25R6435FZNIL0).
Battery charger and battery supervisor (MAX1811ESA+ datasheet, TLV840MAPL3).
3 buttons for navigation (prev/next/enter)
220 mAh Li-Po battery.
Sapphire Crystal Glass to protect the display.

Upcoming Hardware features in ZSWatch v2

nRF5340 BLE chip (u-blox NORA-B10 module)
Touch screen with same size and features as v1
8MB external flash will probably be removed due to larger size of u-blox NORA-B10 vs. ANNA-B402.
Find another way to dock the clock for charging and programming, maybe can find some connector similar to what smartwatches normally have.

Charger/Dock
Basic pogo-pin dock that connects the power and SWD pins to the bottom of the watch.
Enclosure/Casing
3D printed casing with 3D printed buttons. Does it's job, but for revision v2 of the watch I'll probably do something CNC'd for nicer looks.
Software Features

Bluetooth LE communications with GadgetBridge Android app.
Also support Bluetooth Direction Finding so the watch can act as a tag and is trackable using any u-blox AoA antenna board
Watchface that shows:

Standard stuff as time, date, battery
Weather
Step count
Number unread notifications
Heart rate (not implemented yet however)


Pop-up notifications
Setting menu system, with easy extendability
Application picker and app concept

Music control app
Settings app
etc.


Step counting

Larger not yet implemented SW Features and TODOs

Heart rate, right now only samples the raw data, but no heart rate is calculated from it.
Proper BLE pairing, currently removed due to flash constraints (fixed by nRF5340 upgrade).
Watchface should also be an application.
Refactoring of main.c, should have way less logic, utlize Zephyr architecture more.

Android phone communication
Fortunately there is a great Android app called GadgetBridge which handles everything needed on the phone side, such as notifications management, music control and so much more... The ZSWatch right now pretends to be one of the supported Smart Watches in Gadgetbridge, following the same API as it does. In future there may be a point adding native support, we'll see.
PCB
A 4 layer board which measures 36mm in diameter designed in KiCad.








ZSWatch in action



Music control
Accelerometer for step count and tap detection




 object-fit=""cover""



Notifications from phone (Gmail here)
Settings







Writing apps for the Application Manager
Check out the sample application for the general app design. The main idea is each app have an <app_name>_app.c file which registers the app, chooses icon and drives the logic for the app. Then there should be one or more files named for example <app_name>_ui.c containing pure LVGL code with no dependencies to Zephyr or the watch software. The idea is that this UI code should be runnable in a LVGL simulator to speed up development of UI, however right now that's not set up yet. The <app_name>_app.c will do all logic and call functions in <app_name>_ui.c to update the UI accordingly.
Each application needs to have a way to close itself, for example a button, and then through callback tell the application_manager.c to close the app:
When user clicks an app in the app picker:

application_manager.c deletes it's UI elements and calls the application_start_fn.
<app_name>_app.c will do necessary init and then call the <app_name>_ui.c to draw the app UI.
User can now navigate arund and the application and do whatever.

When user for example presses a close button in the application:

Typically a callback from the UI code in <app_name>_ui.c will call <app_name>_app.c to tell that user requested to close the app. <app_name>_app.c will notify application_manager.c that it want to close itself. application_manager.c will then call <app_name>_app.c application_stop_fn and <app_name>_app.c will tell UI to close then do necessary de-init and return.
application_manager.c will now draw the app picker again.

The application manager can also at any time close a running application by calling it's application_stop_fn.
Dock
Very basic, will be re-worked for next watch revision v2.












About

      ZSWatch - the Open Source Zephyr‚Ñ¢ based Smartwatch, including both HW and FW.
    
Topics



  bluetooth


  ble


  smartwatch


  zephyr


  nrf52


  lvgl


  angle-of-arrival


  nrf-connect


  nordic-semiconductor


  nrf53


  nrf-connect-sdk


  zswatch



Resources





      Readme
 
License





     MIT license
    



Stars





204
    stars

Watchers





5
    watching

Forks





5
    forks







    Releases

No releases published




Languages











C
99.9%







Other
0.1%











"
https://news.ycombinator.com/rss,Four thousand weeks,https://leebyron.com/4000/,Comments,"



















Four Thousand Weeks










FourThousandWeeks


          A tribute to
          the book by
          Oliver Burkeman, an exploration of time management in the
          face of human finitude, and addressing the anxiety of ‚Äúgetting
          everything done.‚Äù
        

          To begin, enter when were you born
          This site uses no cookies nor saves your information




Scroll on...




We live our livesweek by week

















Yet a week feels frustratingly limited

        The pressure to be more productive and fit ever-increasing quantities of
        activity into a stubbornly non-increasing quantity of time leads to
        productivity anxiety, shriveled attention spans, and burn-out.
      


And there are alarmingly few of¬†them

        You would feel less anxious about wasting an evening doom-scrolling if
        you had an infinite amount of them. Somehow either doing too much or too
        little can create the sense of wasting time.
      

        Despite all this activity we sense there are important and fulfilling
        ways we could be spending our time, even if we can‚Äôt say exactly what
        they are. Yet, we systematically spend our time doing other things to
        get by instead.
      


          I (like many others) felt a wrongness in the world. Life, I knew, was
          supposed to be more joyful than this, more real, more meaningful. We
          were not supposed to hate Mondays and live for the weekends and
          holidays. We were not supposed to have to raise our hands to be
          allowed to pee.
        
Charles Eisenstien




The average human life is only four thousand weeks


        Scientists estimate that life, in some form, will persist for another
        1.5 billion years or more, until the intensifying heat of the sun
        condemns the last organism to death.
      

        But you? Assuming you live to be eighty, you‚Äôll have had about four
        thousand weeks. The rare few lucky enough to become a centenarian will
        see only five thousand.
      
That‚Äôs absurdly, terrifyingly, insultingly short.




          You have lived  of them so¬†far
        


        You likely have many more weeks ahead of you. The psychologist
        Erik Erikson suggests that at this phase of life you focus
        on the virtues of competence and fidelity. Allow yourself failures in
        the spirit of discovering and developing your personal identity and
        priorities so that your future weeks can be lived well with intention
        and purpose.
      

        That‚Äôs a significant amount of the weeks you‚Äôll see. The psychologist
        Erik Erikson suggests that at this phase of life you focus
        on the virtue of love. Share yourself more intimately with others and
        invest in happy relationships so that your future weeks can be lived
        well with companionship and purpose.
      

        That‚Äôs likely a majority of the weeks you‚Äôll see. The psychologist
        Erik Erikson suggests that at this phase of life you focus
        on the virtue of care. Spend your weeks ‚Äúmaking your mark‚Äù by
        intentionally nurturing things that will outlast you, raising children,
        mentoring others, becoming involved in your community and organizations,
        and creating positive change that benefits others.
      

        You‚Äôre likely well aware of your own finitude having lived the large
        majority of the weeks you‚Äôll see. The psychologist
        Erik Erikson suggests that at this phase of life you focus
        on the virtue of wisdom. Accept and appreciate your accomplishments so
        far as a life well lived. Continue to nurture things that will outlast
        you and mentor others, spend your weeks intentionally on your true
        priorities, and appreciate novelty in the mundane.
      

        You‚Äôre no doubt well aware of your own finitude as one of the lucky ones
        to live well past four thousand weeks. The psychologist
        Erik Erikson suggests that at this phase of life you focus
        on the virtue of wisdom. Accept and appreciate your accomplishments so
        far as a life well lived. Spend every remaining week intentionally on
        your true priorities and appreciate novelty in the mundane.
      


Productivity is a trap

        There are numerous techniques, products, and services to squeeze the
        most productivity from your week. The problem isn‚Äôt that these don‚Äôt
        work, it‚Äôs that they do work. And yet paradoxically you only
        feel busier, more anxious, and somehow emptier as a result.
      

        The day will never arrive when you finally have everything under
        control‚Äîwhen the flood of emails has been contained; when your to-do
        lists have stopped getting longer; when you‚Äôre meeting all your
        obligations at work and in your home life; when nobody‚Äôs angry with you
        for missing a deadline or dropping the ball; and when the fully
        optimized person you‚Äôve become can turn, at long last, to the things
        life is really supposed to be about.
      

        Let‚Äôs start by admitting defeat: none of this is ever going to happen.
      


          Time feels like an unstoppable conveyer belt, bringing us new tasks as
          fast as we can dispatch the old ones; and becoming ‚Äúmore productive‚Äù
          just seems to cause the belt to speed up.
        
Edward T. Hall



Adopt a limit-embracing attitude

        If you truly don‚Äôt have time for everything you want to do, or feel you
        ought to do, or that others are badgering you to do, then, well, you
        don‚Äôt have time‚Äîno matter how grave the consequences of failing to do it
        all might prove to be. So, technically, it‚Äôs irrational to feel troubled
        by an overwhelming to-do list. You‚Äôll do what you can, you won‚Äôt do what
        you can‚Äôt, and the tyrannical inner voice insisting that you must do
        everything is simply mistaken.
      

        We rarely stop to consider things so rationally, though, because that
        would mean confronting the painful truth of our limitations.
      

        Surrender to the reality that things just take the time they take, and
        that you can‚Äôt quiet your anxieties by working faster, because it isn‚Äôt
        within your power to force reality‚Äôs pace as much as you feel you need
        to, and because the faster you go, the faster you‚Äôll feel you need to
        go.
      


          Which of us truly lives on twenty-four hours a day? Which of us is not
          saying: ‚ÄúI shall alter that when I have a little more time?‚Äù We never
          shall have any more time. We have, and we have always had, all the
          time there is.
        
Arnold Bennett



How you spend your time is a choice

        We are forced to accept that there will always be too much to do; that
        you can‚Äôt make the world run at your preferred speed and so there are
        tough choices to be made: which balls to let drop, which people to
        disappoint, which cherished ambitions to abandon, which roles to fail
        at.
      

        Once you truly understand that you‚Äôre guaranteed to miss out on almost
        every experience the world has to offer, the fact that there are so many
        you still haven‚Äôt experienced stops feeling like a problem. Instead, you
        get to focus on fully enjoying the tiny slice of experiences you
        actually do have time for. Digging in to a challenging project that
        can‚Äôt be hurried becomes not a trigger for stressful emotions but a
        bracing act of choice.
      


The importance of¬†rest

        A real risk of doing too much is finding your work time, in attempt to
        be productive, encroaching on an evening‚Äôs rest. Rest as it turns
        out‚Äîwhether in the evening, over a weekend, or a long vacation‚Äîis
        critical for productive creative work. Its absence can lead to stress,
        burnout, and counterintuitively overall poor performance despite the
        extra hours worked.
      

        Though why should vacations or lazy mornings need defending in terms of
        improved work performance? Enjoying leisure for its own sake‚Äîwhich is
        the whole point of leisure‚Äîshould not feel as though you‚Äôre failing at
        life. Leisure is not merely an opportunity for recovery and
        replenishment for the purposes of further work, but for its intrinsic
        satisfactions.
      


          I have to die. If it is now, well then I die now; if later, then now I
          will take my lunch, since the hour for lunch has arrived - and dying I
          will tend to later.
        
Epictetus



The loneliness of temporal sovereignty

        Other human beings are always impinging on your time in countless
        frustrating ways. In an ideal world the only person making decisions
        about your time is you. However this comes at a cost that‚Äôs not worth
        paying.
      

        It‚Äôs good to have plenty of time, but having all the time in the world
        isn‚Äôt much use if you‚Äôre forced to experience it all on your own. To do
        countless important things with time: to socialize, go on dates, raise
        children, launch businesses, start movements; it has to be synchronized
        with other people. In fact, having large amounts of time but no
        opportunity to use it collaboratively can be actively unpleasant.
      

        We treat our time as something to hoard, when it‚Äôs better approached as
        something to share. Even if that means surrendering some of your power
        to decide exactly what you do with it and when.
      


          However, the two things must be mingled and varied, solitude and
          joining a crowd: the one will make us long for people and the other
          for ourselves, and each will be a remedy for the other; solitude will
          cure our distaste for a crowd, and a crowd will cure our boredom with
          solitude.
        
Seneca




        Ten tools for embracing finitude
      


1.
Adopt a fixed volume approach to productivity

        Tough choices are inevitable; focus on making them consciously and well.
      

        Keep two to-do lists: an ‚Äúopen‚Äù one for everything on your plate,
        doubtlessly nightmarishly long, and ‚Äúclosed‚Äù with a fixed number of
        entries, only moving tasks onto it when previous ones have been
        completed.
      

        You‚Äôll never get through all the tasks on the open list, but you were
        never going to in any case. The choice to leave them there is hard, but
        time spent on them is time not spent on the things you chose to focus
        on.
      

        Establish pre-determined time boundaries on your work, and make
        decisions in light of those limits. If your primary goal is to do what‚Äôs
        required to be finished by 5:30 you‚Äôll be aware of the constraints on
        your time and motivated to use it wisely.
      


2.
Serialize,serialize,serialize

        Focus on one big project at a time, and see it to completion before
        moving onto the next.
      

        It‚Äôs alluring to try to alleviate the anxiety of having too many
        responsibilities or ambitions by getting started on them all at once,
        but you‚Äôll make little progress that way. Instead, train yourself to get
        incrementally better at tolerating that anxiety by consciously
        postponing everything you possibly can except for one thing.
      

        Soon the satisfaction of completing important projects will make that
        anxiety feel worthwhile, and as you complete them you‚Äôll have less to be
        anxious about anyway.
      


3.
Strategic underachievement

        Simply because your time is finite, you‚Äôll inevitably underachieve at
        something. When you can‚Äôt do it all, you can feel ashamed and give up.
        When you decide in advance what to fail at, you remove the sting of
        shame.
      

        Nominate in advance whole areas of life in which you won‚Äôt expect
        excellence from yourself. Instead focus that time more effectively, and
        you won‚Äôt be surprised when you fail at what you planned to fail at all
        along.
      


4.
Celebrate wins

        The to-do list will never be finished. Inbox zero will inevitably
        refill. There‚Äôs an unhelpful assumption that you begin each morning with
        a productivity debt that you must pay off with hard work to achieve a
        zero-balance by evening.
      

        Keep a ‚Äúdone‚Äù list which starts empty and fills up over the day. You
        could have spent the day doing nothing remotely constructive, and look
        what you did instead! Lower the bar for what gets to count as an
        accomplishment; small wins accrue.
      


5.
Consolidate care

        The attention economy demands urgency, bringing a litany of demands for
        your care every day. Consciously choose your battles in industry,
        charity, activism, and politics.
      

        To make a real difference, you must focus your finite capacity for care.
      


6.
Embrace boring & single-purpose technology

        Modern digital devices offer distraction to a place where painful human
        limitations do not apply; you need never feel bored or constrained in
        your freedom of action‚Äîwhich isn‚Äôt the case when it comes to work that
        matters.
      

        Combat this by making your devices boring. Remove apps that distract
        (even consider Slack or Email). Switch your screen to grayscale. Use
        time-limiting reminders.
      

        Choose single-purpose devices like an e-reader where it‚Äôs tedious and
        awkward to do anything but read. If distracting apps are only a swipe
        away they‚Äôll prove impossible to resist when the first twinge of boredom
        or difficulty of focus arises.
      


7.
Seek novelty in the¬†mundane

        The fewer weeks we have left the faster we seem to lose them. The
        likeliest explanation for this phenomenon is that our brains encode the
        passing of time on the basis of how much information we process in any
        given interval.
      

        Cramming your life with novel experiences does work, but can also lead
        to existential overwhelm and is also impractical, especially if you have
        a job or children.
      

        Alternatively pay more attention to every moment no matter how mundane.
        Plunge into the life you already have with twice the intensity and your
        life will feel twice as full and will be remembered as lasting twice as
        long. Meditation, going on unplanned walks, photography, journaling,
        anything that draws your attention more fully to the present.
      


8.
Be a researcher in relationships

        When presented with a challenging or boring moment with another person,
        deliberately adopt an attitude of curiosity in which your goal isn‚Äôt to
        achieve any particular outcome or explain your position but to figure
        out who this human being is who we‚Äôre with.
      

        This curiosity is well suited to the unpredictability of life with
        others because it can be satisfied by their behaving in ways you like or
        dislike whereas the stance of demanding a certain result is frustrated
        each time things fail to go your way.
      


9.
Cultivate instantaneous generosity

        Whenever a generous impulse arises your mind: to give money, to check in
        on a friend, send an email praising someone‚Äôs work, act on that impulse
        right away. If you put it off for whatever reason, you‚Äôll likely not get
        back to it. The only acts of generosity that count are the ones you‚Äôre
        actually making.
      

        People are social creatures, and generous action reliably makes us feel
        much happier.
      


10.
Practice doing nothing

        When it comes to the challenge of using your four thousand weeks well,
        the capacity to do nothing is indispensable. If you can‚Äôt bear the
        discomfort of not acting you‚Äôre far more likely to make poor choices
        with your time simply to feel as if you‚Äôre acting. Calm down, gain
        autonomy over your choices, and make better ones.
      
Do nothing meditation

Set a timer, even for only five minutes.
Sit in a chair and then stop trying to do anything.

          Every time you notice you‚Äôre doing something, including thinking or
          focusing on your breathing, stop doing it.
        

          If you notice you‚Äôre criticizing yourself inwardly for doing things
          well‚Ä¶ that‚Äôs a thought too so stop doing that.
        
Keep on stopping until the timer goes off.





        Thanks for reading this tribute to
        Four Thousand Weeks, by Oliver Burkeman.
      

        This page is comprised of themes and excerpts from the book. If you‚Äôve
        scrolled this far you should absolutely read it in its entirety.
      

        Set in
        Playfair 2.0
        by Claus Eggers


        Made and open-sourced by
        Lee Byron
        with ‚ô• in San Francisco.
      
‚úåÔ∏é



"
https://news.ycombinator.com/rss,Use.GPU Goes Trad,https://acko.net/blog/use-gpu-goes-trad/,Comments,"



Use.GPU Goes Trad ‚Äî Acko.net































Hackery, Math &¬†Design
Steven Wittens i













Home







Home






January 14, 2023
Use.GPU Goes Trad


Old is new again




I've released a new version of Use.GPU, my experimental reactive/declarative WebGPU framework, now at version 0.8.
My goal is to make GPU rendering easier and more sane. I do this by applying the lessons and patterns learned from the React world, and basically turning them all up to 11, sometimes 12. This is done via my own Live run-time, which is like a martian React on steroids.
The previous 0.7 release was themed around compute, where I applied my shader linker to a few challenging use cases. It hopefully made it clear that Use.GPU is very good at things that traditional engines are kinda bad at.
In comparison, 0.8 will seem banal, because the theme was to fill the gaps and bring some traditional conveniences, like:

Scenes and nodes with matrices
Meshes with instancing
Shadow maps for lighting
Visibility culling for geometry






These were absent mostly because I didn't really need them, and they didn't seem like they'd push the architecture in novel directions. That's changed however, because there's one major refactor underpinning it all: the previously standard forward renderer is now entirely swappable. There is a shiny deferred-style renderer to showcase this ability, where lights are rendered separately, using a g-buffer with stenciling.
This new rendering pipeline is entirely component-driven, and fully dogfooded. There is no core renderer per-se: the way draws are realized depends purely on the components being used. It effectively realizes that most elusive of graphics grails, which established engines have had difficulty delivering on: a data-driven, scriptable render pipeline, that mortals can hopefully use.





Root of the App



Deep inside the tree


I've spent countless words on Use.GPU's effect-based architecture in prior posts, which I won't recap. Rather, I'll just summarize the one big trick: it's structured entirely as if it needs to produce only 1 frame. Then in order to be interactive, and animate, it selectively rewinds parts of the program, and reactively re-runs them. If it sounds crazy, that's because it is. And yet it works.
So the key point isn't the feature list above, but rather, how it does so. It continues to prove that this way of coding can pay off big. It has all the benefits of immediate-mode UI, with none of the downsides, and tons of extensibility. And there are some surprises along the way.
Real Reactivity
You might think: isn't this a solved problem? There are plenty of JS 3D engines. Hasn't React-Three-Fiber (R3F) shown how to make that declarative? And aren't these just web versions of what native engines like Unreal and Unity already do well, and better?
My answer is no, but it might not be clear why. Let me give an example from my current job.







My client needs a specialized 3D editing tool. In gaming terms you might think of it as a level design tool, except the levels are real buildings. The details don't really matter, only that they need a custom 3D editing UI. I've been using Three.js and R3F for it, because that's what works today and what other people know.
Three.js might seem like a great choice for the job: it has a 3D scene, editing controls and so on. But, my scene is not the source of truth, it's the output of a process. The actual source of truth being live-edited is another tree that sits before it. So I need to solve a two-way synchronization problem between both. This requires careful reasoning about state changes.





Change handlers in Three.js and R3F


Sadly, the way Three.js responds to changes is ill-defined. As is common, its objects have ""dirty"" flags. They are resolved and cleared when the scene is re-rendered. But this is not an iron rule: many methods do trigger a local refresh on the spot. Worse, certain properties have an invisible setter, which immediately triggers a ""change"" event when you assign a new value to it. This also causes derived state to update and cascade, and will be broadcast to any code that might be listening.
The coding principle applied here is ""better safe than sorry"". Each of these triggers was only added to fix a particular stale data bug, so their effects are incomplete, creating two big problems. Problem 1 is a mix of old and new state... but problem 2 is you can only make it worse, by adding even more pre-emptive partial updates, sprinkled around everywhere.
These ""change"" events are oblivious to the reason for the change, and this is actually key: if a change was caused by a user interaction, the rest of the app needs to respond to it. But if the change was computed from something else, then you explicitly don't want anything earlier to respond to it, because it would just create an endless cycle, which you need to detect and halt.


R3F introduces a declarative model on top, but can't fundamentally fix this. In fact it adds a few new problems of it own in trying to bridge the two worlds. The details are boring and too specific to dig into, but let's just say it took me a while to realize why my objects were moving around whenever I did a hot-reload, because the second render is not at all the same as the first.
Yet this is exactly what one-way data flow in reactive frameworks is meant to address. It creates a fundamental distinction between the two directions: cascading down (derived state) vs cascading up (user interactions). Instead of routing both through the same mutable objects, it creates a one-way reverse-path too, triggered only in specific circumstances, so that cause and effect are always unambigious, and cycles are impossible.
Three.js is good for classic 3D. But if you're trying to build applications with R3F it feels fragile, like there's something fundamentally wrong with it, that they'll never be able to fix. The big lesson is this: for code to be truly declarative, changes must not be allowed to travel backwards. They must also be resolved consistently, in one big pass. Otherwise it leads to endless bug whack-a-mole.
What reactivity really does is take cache invalidation, said to be the hardest problem, and turn the problem itself into the solution. You never invalidate a cache without immediately refreshing it, and you make that the sole way to cause anything to happen at all. Crazy, and yet it works.
When I tell people this, they often say ""well, it might work well for your domain, but it couldn't possibly work for mine."" And then I show them how to do it.


Figuring out which way your cube map points:just gfx programmer things.

And... Scene
One of the cool consequences of this architecture is that even the most traditional of constructs can suddenly bring neat, Lispy surprises.
The new scene system is a great example. Contrary to most other engines, it's actually entirely optional. But that's not the surprising part.
Normally you just have a tree where nodes contain other nodes, which eventually contain meshes, like this:
<Scene>
  <Node matrix={...}>
    <Mesh>
    <Mesh>
  <Node matrix={...}>
    <Mesh>
    <Node matrix={...}>
      <Mesh>
      <Mesh>


It's a way to compose matrices: they cascade and combine from parent to child. The 3D engine is then built to efficiently traverse and render this structure.
But what it ultimately does is define a transform for every mesh: a function vec3 => vec3 that maps one vertex position to another. So if you squint, <Mesh> is really just a marker for a place where you stop composing matrices and pass a composed matrix transform to something else.
Hence Use.GPU's equivalent, <Primitive>, could actually be called <Unscene>. What it does is escape from the scene model, mirroring the Lisp pattern of quote-unquote. A chain of <Node> parents is just a domain-specific-language (DSL) to produce a TransformContext with a shader function, one that applies a single combined matrix transform.
In turn, <Mesh> just becomes a combination of <Primitive> and a <FaceLayer>, i.e. triangle geometry that uses the transform. It all composes cleanly.
So if you just put meshes inside the scene tree, it works exactly like a traditional 3D engine. But if you put, say, a polar coordinate plot in there from the plot package, which is not a matrix transform, inside a primitive, then it will still compose cleanly. It will combine the transforms into a new shader function, and apply it to whatever's inside. You can unscene and scene repeatedly, because it's just exiting and re-entering a DSL.
In 3D this is complicated by the fact that tangents and normals transform differently from vertices. But, this was already addressed in 0.7 by pairing each transform with a differential function, and using shader fu to compose it. So this all just keeps working.
Another neat thing is how this works with instancing. There is now an <Instances> component, which is exactly like <Mesh>, except that it gives you a dynamic <Instance> to copy/paste via a render prop:
<Instances
   mesh={mesh}
   render={(Instance) => (<>
     <Instance position={[1, 2, 3]} />
     <Instance position={[3, 4, 5]} />
   </>)
 />


As you might expect, it will gather the transforms of all instances, stuff all of them into a single buffer, and then render them all with a single draw call. The neat part is this: you can still wrap individual <Instance> components in as many <Node> levels as you like. Because all <Instance> does is pass its matrix transform back up the tree to the parent it belongs to.





This is done using Live captures, which are React context providers in reverse. It doesn't violate one-way data flow, because captures will only run after all the children have finished running. Captures already worked previously, the semantics were just extended and formalized in 0.8 to allow this to compose with other reduction mechanisms.


But there's more. Not only can you wrap <Instance> in <Node>, you can also wrap either of them in <Animate>, which is Use.GPU's keyframe animator, entirely unchanged since 0.7:









<Instances
  mesh={mesh}
  render={(Instance) => (

    <Animate
      prop=""rotation""
      keyframes={ROTATION_KEYFRAMES}
      loop
      ease=""cosine""
    >
      <Node>
        {seq(20).map(i => (
          <Animate
            prop=""position""
            keyframes={POSITION_KEYFRAMES}
            loop
            delay={-i * 2}
            ease=""linear""
          >
            <Instance
              rotation={[
                Math.random()*360,
                Math.random()*360,
                Math.random()*360,
              ]}
              scale={[0.2, 0.2, 0.2]}
            />
          </Animate>
        ))}
      </Node>
    </Animate>

  )}
/>


The scene DSL and the instancing DSL and the animation DSL all compose directly, with nothing up my sleeve. Each of these <Components> are still just ordinary functions. On the inside they look like constructors with all the other code missing. There is zero special casing going on here, and none of them are explicitly walking the tree to reach each other. The only one doing that is the reactive run-time... and all it does is enforce one-way data flow by calling functions, gathering results and busting caches in tree order. Because a capture is a long-distance yeet.
Personally I find this pretty magical. It's not as efficient as a hand-rolled scene graph with instancing and built-in animation, but in terms of coding lift it's literally O(0) instead of OO. I needed to add zero lines of code to any of the 3 sub-systems, in order to combine them into one spinning whole.
The entire scene + instancing package clocks in at about 300 lines and that's including empties and generous formatting. I don't need to architect the rest of the framework around a base Object3D class that everything has to inherit from either, which is a-ok in my book.
This architecture will never reach Unreal or Unity levels of hundreds of thousands of draw calls, but then, it's not meant to do that. It embraces the idea of a unique shader for every draw call, and then walks that back if and when it's useful. The prototype map package for example does this, and can draw a whole 3D vector globe in 2 draw calls: fill and stroke. Adding labels would make it 3. And it's not static: it's doing the usual quad-tree of LOD'd mercator map tiles.










Multi-Pass
Next up, the modular renderer passes. Architecturally and reactively-speaking, there isn't much here. This was mainly an exercise in slicing apart the existing glue.
The key thing to grok is that in Use.GPU, the <Pass> component does not correspond to a literal GPU render pass. Rather, it's a virtual, logical render pass. It represents all the work needed to draw some geometry to a screen or off-screen buffer, in its fully shaded form. This seems like a useful abstraction, because it cleanly separates the nitty gritty rendering from later compositing (e.g. overlays).
For the forward renderer, this means first rendering a few shadow maps, and possibly rendering a picking buffer for interaction. For the deferred renderer, this involves rendering the g-buffer, stencils, lights, and so on.
My goal was for the toggle between the two to be as simple as replacing a <ForwardRenderer> with a <DeferredRenderer>... but also to have both of those be flexible enough that you could potentially add on, say, SSAO, or bloom, or a Space Engine-style black hole, as an afterthought. And each <Pass> can have its own renderer, rather than shoehorning everything into one big engine.
Neatly, that's mostly what it is now. The basic principle rests on three pillars.



Deferred rendering


First, there are a few different rendering modes, by default solid vs shaded vs ui. These define what kind of information is needed at every pixel, i.e. the classic varying attributes. But they have no opinion on where the data comes from or what it's used for: that's defined by the geometry layer being rendered. It renders a <Virtual> draw call, which it gives e.g. a getVertex and getFragment shader function with a particular signature for that mode. These functions are not complete shaders, just the core functions, which are linked into a stub. There are a few standard 'tropes' used here, not just these two.
Second, there are a few different rendering buckets, like opaque, transparent, shadow, picking and debug. These are used to group draws into. Different GPU render passes then pick and choose from that. opaque and transparent are drawn to the screen, while shadow is drawn repeatedly into all the shadow maps. This includes sorting front-to-back and back-to-front, as well as culling.
Finally, there's the renderer itself (forward vs deferred), and its associated pass components (e.g. <ColorPass>, <ShadowPass>, <PickingPass>, and so on). The renderer decides how to translate a particular ""mode + bucket"" combination into a concrete draw call, by lowering it into render components (e.g. <ShadedRender>). The pass components decide which buffer to actually render stuff to, and how. So the renderer itself doesn't actually render, it merely spawns and delegates to other components that do.


The forward path works mostly the same as before, only the culling and shadow maps are new... but it's now split up into all its logical parts. And I verified this design by adding the deferred renderer, which is a lot more convoluted, but still needs to do some forward rendering.
It works like a treat, and they use all the same lighting shaders. You can extend any of the 3 pillars just by replacing or injecting a new component. And you don't need to fork either renderer to do so: you can just pick and choose √† la carte by selectively overriding or extending its ""mode + bucket"" mapping table, or injecting a new actual render pass.










To really put a bow on top, I upgraded the Use.GPU inspector so that you can directly view any render target in a RenderDoc-like way. This will auto-apply useful colorization shaders, e.g. to visualize depth. This is itself implemented as a Use.GPU Live canvas, sitting inside the HTML-based inspector, sitting on top of Live, which makes this a Live-in-React-in-Live scenario.
For shits and giggles, you can also inspect the inspector's canvas, recursively, ad infinitum. Useful for debugging the debugger:







There are still of course some limitations. If, for example, you wanted to add a new light type, or add support for volumetric lights, you'd have to reach in more deeply to make that happen: the resulting code needs to be tightly optimized, because it runs per pixel and per light. But if you do, you're still going to be able to reuse 90% of the existing components as-is.
I do want a more comprehensive set of light types (e.g. line and area), I just didn't get around to it. Same goes for motion vectors and TXAA. However, with WebGPU finally nearing public release, maybe people will actually help out. Hint hint.







Port of a Reaction Diffusion system by Felix Woitzel.



A Clusterfuck of Textures
A final thing to talk about is 2D image effects and how they work. Or rather, the way they don't work. It seems simple, but in practice it's kind of ludicrous.
If you'd asked me a year ago, I'd have thought a very clean, composable post-effects pipeline was entirely within reach, with a unified API that mostly papered over the difference between compute and render. Given that I can link together all sorts of crazy shaders, this ought to be doable.
Well, I did upgrade the built-in fullscreen conveniences a bit, so that it's now easier to make e.g. a reaction diffusion sim like this (full code):



The devil here is in the details. If you want to process 2D images on a GPU, you basically have several choices:

Use a compute shader or render shader?
Which pixel format do you use?
Are you sampling one flat image or a MIP pyramid of pre-scaled copies?
Are you sampling color images, or depth/stencil images?
Use hardware filtering or emulate filtering in software?

The big problem is that there is no single approach that can handle all cases. Each has its own quirks. To give you a concrete example: if you wrote a float16 reaction-diffusion sim, and then decided you actually needed float32, you'd probably have to rewrite all your shaders, because float16 is always renderable and hardware filterable, but float32 is not.
Use.GPU has a pretty nice set of Compute/Stage/Kernel components, which are elegant on the outside; but they require you to write pretty gnarly shader code to actually use them. On the other side are the RenderToTexture/Pass/FullScreen components which conceptually do the same thing, and have much nicer shader code, but which don't work for a lot of scenarios. All of them can be broken by doing something seemingly obvious, that just isn't natively supported and difficult to check ahead of time.
Even just producing universal code to display any possible texture type on screen becomes a careful exercise in code-generation. If you're familiar with the history of these features, it's understandable how it got to this point, but nevertheless, the resulting API is abysmal to use, and is a never-ending show of surprise pitfalls.
Here's a non-exhaustive list of quirks:

Render shaders are the simplest, but can only be used to write those pixel formats that are ""renderable"".
Compute shaders must be dispatched in groups of N, even if the image size is not a multiple of N. You have to manually trim off the excess threads.
Hardware filtering only works on some formats, and some filtering functions only work in render shaders.
Hardware filtering (fast) uses [0..1] UV float coordinates, software emulation in a shader (slow) uses [0..N] XY uint coordinates.
Reading and writing from/to the same render texture is not allowed, you have to bounce between a read and write buffer.
Depth+stencil images have their own types and have an additional notion of ""aspect"" to select one or both.
Certain texture functions cannot be called conditionally, i.e. inside an if.
Copying from one texture to another doesn't work between certain formats and aspects.

My strategy so far has been to try and stick to native WGSL semantics as much as possible, meaning the shader code you do write gets inserted pretty much verbatim. But if you wanted to paper over all these differences, you'd have to invent a whole new shader dialect. This is a huge effort which I have not bothered with. As a result, compute vs render pretty much have to remain separate universes, even when they're doing 95% the same thing. There is also no easy way to explain to users which one they ought to use.
While it's unrealistic to expect GPU makers to support every possible format and feature on a fast path, there is little reason why they can't just pretend a little bit more. If a texture format isn't hardware filterable, somebody will have to emulate that in a shader, so it may as well be done once, properly, instead of in hundreds of other hand-rolled implementations.
If there is one overarching theme in this space, it's that limitations and quirks continue to be offloaded directly onto application developers, often with barely a shrug. To make matters worse, the ""next gen"" APIs like Metal and Vulkan, which WebGPU inherits from, do not improve this. They want you to become an expert at their own kind of busywork, instead of getting on with your own.
I can understand if the WebGPU designers have looked at the resulting venn-diagram of poorly supported features, and have had to pick their battles. But there's a few absurdities hidden in the API, and many non-obvious limitations, where the API spec suggests you can do a lot more than you actually can. It's a very mixed bag all things considered, and in certain parts, plain retarded. Ask me about minimum binding size. No wait, don't.
* * *
Most promising is that as Use.GPU grows to do more, I'm not touching extremely large parts of it. This to me is the sign of good architecture. I also continue to focus on specific use cases to validate it all, because that's the only way I know how to do it well.
There are some very interesting goodies lurking inside too. To give you an example... that R3F client app I mentioned at the start. It leverages Use.GPU's state package to implement a universal undo/redo system in 130 lines. A JS patcher is very handy to wrangle the WebGPU API's deep argument style, but it can do a lot more.
One more thing. As a side project to get away from the core architecting, I made a viewer for levels for Dark Engine games, i.e. Thief 1 (1998), System Shock 2 (1999) and Thief 2 (2000). I want to answer a question I've had for ages: how would those light-driven games have looked, if we'd had better lighting tech back then? So it actually relights the levels. It's still a work in progress, and so far I've only done slow-ass offline CPU bakes with it, using a BSP-tree based raytracer. But it works like a treat.









I basically don't have to do any heavy lifting if I want to draw something, be it normal geometry, in-place data/debug viz, or zoomable overlays. Integrating old-school lightmaps takes about 10 lines of shader code and 10 lines of JS, and the rest is off-the-shelf Use.GPU. I can spend my cycles working on the problem I actually want to be working on. That to me is the real value proposition here.
I've noticed that when you present people with refined code that is extremely simple, they often just do not believe you, or even themselves. They assume that the only way you're able to juggle many different concerns is through galaxy brain integration gymnastics. It's really quite funny. They go looking for the complexity, and they can't find it, so they assume they're missing something really vital. The realization that it's simply not there can take a very long time to sink in.
Visit usegpu.live for more and to view demos in a WebGPU capable browser.






Compute ¬†Data Flow ¬†GPU ¬†Latest ¬†Use.GPU

January 14, 2023











Home







Home









Subscribe










About
¬© 2003‚Äì2023







This article contains graphics made with WebGL, which your browser does not seem to support.
  Try Google Chrome or Mozilla Firefox.
  
  √ó



"
https://news.ycombinator.com/rss,DragonFlyBSD's HAMMER2 File-System Being Ported to NetBSD,https://www.phoronix.com/news/NetBSD-HAMMER2-Port,Comments,"







DragonFlyBSD's HAMMER2 File-System Being Ported To NetBSD - Phoronix






































 













Articles & Reviews
News Archive
Forums
Premium  Categories
Computers Display Drivers Graphics Cards Linux Gaming Memory Motherboards Processors Software Storage Operating Systems Peripherals Close








Articles & Reviews


News Archive


Forums


Premium
 
Contact


 Categories


Computers Display Drivers Graphics Cards Linux Gaming Memory Motherboards Processors Software Storage Operating Systems Peripherals 






























Show Your Support:  This site is primarily supported by advertisements. Ads are what have allowed this site to be maintained on a daily basis for the past 18+ years. We do our best to ensure only clean, relevant ads are shown, when any nasty ads are detected, we work to remove them ASAP. If you would like to view the site without ads while still supporting our work, please consider our ad-free Phoronix Premium.
DragonFlyBSD's HAMMER2 File-System Being Ported To NetBSD
Written by Michael Larabel in BSD on 11 January 2023 at 06:26 AM EST. 21 Comments


NetBSD continues using the FFS file-system by default while it's offered ZFS support that has been slowly improving -- in NetBSD-CURRENT is the ability to use ZFS as the root file-system if first booting to FFS, for example.  There may be another modern file-system option soon with an effort underway to port DragonFlyBSD's HAMMER2 over to NetBSD. HAMMER2 has been built-up over the past decade by Matthew Dillon and other DragonFlyBSD developers. HAMMER2 has been the default and working rather well with recent releases of DragonFlyBSD while now there is a port underway to try to get the file-system working good for NetBSD.HAMMER2 on DragonFlyBSD. NetBSD developer  Tomohiro Kusumi has started working on a HAMMER2 port to NetBSD, who had also worked on porting HAMMER2 to FreeBSD as another exercise. This port is intended to be built against recent NetBSD code, initially is only read-only support but write support will be tackled once the read support has stabilized.  More details on this still very early stage port of HAMMER2 to NetBSD can be found via this GitHub repository. It will be interesting to see how the HAMMER2 port to NetBSD goes and if eventually could become a viable file-system option for NetBSD installations.









21 Comments 




Tweet







Related News
DragonFlyBSD 6.4 Released With Many FixesNetBSD 10 Beta Brings Much Improved Performance, Long Overdue Hardware SupportFreeBSD 12.4 Released With Various Fixes & ImprovementsTrying Out The BSDs On The Intel Core i9 13900K ""Raptor Lake""FreeBSD Re-Introduces WireGuard Support Into Its KernelFreeBSD 12.4-BETA1 Released, Q3-2022 Status Report Issued
 






About The Author

Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via Twitter, LinkedIn, or contacted via MichaelLarabel.com.


Popular News This Week
DragonFlyBSD's HAMMER2 File-System Being Ported To NetBSDOpenZFS Lands A Very Nice Performance OptimizationA Developer Hopes To Restore GCC's Java Front-EndOBS Studio 29 Released With AV1 Encode Additions, Upward Compression FilterUbuntu's Real-Time Kernel Approaching GA StatusLinux Preparing To Disable Drivers For Microsoft's RNDIS ProtocolLinux 4.9.337 Released To End Out The 2016 LTS SeriesWine 8.0-rc3 Released With 28 Known Bug Fixes
 













Latest Linux News
Removing Some Old Arm Drivers & Board/Machine Code To Lighten The Kernel By 154k Lines


Linux 6.3 To Support Making Use Of Intel's New LKGS Instruction (Part Of FRED)


Linux 6.3 Will Better Handle Missing AMD Radeon Firmware / Unsupported Hardware


Basic OpenGL ES Compute Shader Support Begins Working For The Apple GPU Linux Driver


GNU Binutils 2.40 Released With AMD Zen 4 & Upcoming Intel Instructions, Zstd Support


MSI PRO Z690-A WiFi DDR5 Support Upstreamed To Coreboot


Intel Posts Linux Patches For Linear Address Space Separation (LASS)


AMD ROCm 5.4.2 Released As Another Small Update To The Compute Stack


KDE This Week: ""Pretty Juicy In The Eye Candy Department""


Linux Developers Eye Orphaning The JFS File-System










Show Your Support, Go Premium
Phoronix Premium allows ad-free access to the site, multi-page articles on a single page, and other features while supporting this site's continued operations.


Latest Featured Articles
Setting Up Intel 4th Gen Xeon Scalable ""Sapphire Rapids"" For Accelerator Use


AMD Radeon vs. Intel Arc Graphics With Linux 6.2 + Mesa 23.0


Intel Xeon Platinum 8490H ""Sapphire Rapids"" Performance Benchmarks


Intel Launches 4th Gen Xeon Scalable ""Sapphire Rapids"", Xeon CPU Max Series


AMD Ryzen 5 7600 / Ryzen 7 7700 / Ryzen 9 7900 Linux Performance







 


Support Phoronix
The mission at Phoronix since 2004 has centered around enriching the Linux hardware experience. In addition to supporting our site through advertisements, you can help by subscribing to Phoronix Premium. You can also contribute to Phoronix through a PayPal tip or tip via Stripe.











Phoronix Media


Contact
Michael Larabel
OpenBenchmarking.org



Phoronix Premium


Support Phoronix
While Having Ad-Free Browsing,
Single-Page Article Viewing



Share


Facebook
Twitter







Legal Disclaimer, Privacy Policy, Cookies | Contact
Copyright ¬© 2004 - 2023 by Phoronix Media.
All trademarks used are properties of their respective owners. All rights reserved.







"
https://news.ycombinator.com/rss,The joy of sets,https://www.prospectmagazine.co.uk/arts-and-books/the-joy-of-sets,Comments,"
403 Forbidden

403 Forbidden


"
https://news.ycombinator.com/rss,Server BMCs can need to be rebooted every so often,https://utcc.utoronto.ca/~cks/space/blog/sysadmin/BMCsCanNeedRebooting,Comments,"
 
 Chris's Wiki :: blog/sysadmin/BMCsCanNeedRebooting 






Chris Siebenmann ::
CSpace ¬ª
       blog ¬ª
       sysadmin ¬ª
       BMCsCanNeedRebooting
Welcome, guest.




Your server BMCs can need to be rebooted every so often
January 14, 2023

Over on the Fediverse I said:
A sysadmin tip: if your BMC/IPMI
is doing weird things, restart (reboot) it. Server BMCs are little
computers running ancient versions of Linux with software that's
probably terribly written and they stay running forever, which means
all sorts of opportunities for slow bugs. Reboot away!
This is brought to you by the BMC with a KVM-over-IP that wouldn't
accept '2' entered on the (virtual) keyboard in any way or form. Until
I rebooted the BMC. 
PS: Our IP addresses have 2s in them.

(This probably isn't the only weird BMC glitch we've experienced,
but it's the first one where I tried rebooting the BMC and that
fixed it.)
A number of people shared additional stories in the replies, and I
especially 'liked' @frederic@chaos.social's:
Same for IPMI hardware sensors: Thought the motherboard was damaged
because half the sensors were reported as ""n/a"".  Rebooting magically
fixed this. √∞≈∏‚Ñ¢ÀÜ

This happens for more or less the reasons I mentioned above. BMCs
naturally accumulate very large uptimes because they don't normally
reboot when your server reboots; if you don't do anything special,
your BMC will normally stay up for as long as the server has power.
In many places this can amount to years of uptime, and it's a rare
set of software that can stand up to that even if you don't use
them much. Server vendors typically don't want you to think about
this, and I don't believe 'BMC uptime' is generally exposed anywhere.
(Routinely querying the BMC's sensor readings via IPMI may actually
make this worse, since then the BMC's software is active to answer
those queries. I should probably make our metrics system notice when a server decreases the
number of IPMI metrics it exposes without a reboot.)
Modern BMCs can generally reboot themselves without rebooting their
host (the actual server), although you may want to test this to be
sure since apparently some vendors can do that differently.
PS: How I encountered this is that I was reinstalling a server using
KVM-over-IP, and I hit the portion of the base Ubuntu 22.04 install
when I had to enter the subnet and various associated IP addresses.
Our network has a '2' in it, so all of that failed. Helpfully, the
KVM-over-IP software had a virtual keyboard so I could see it wasn't
just some browser weirdness intercepting a '2' from my real keyboard;
even the virtual keyboard's '2' key wouldn't get through to the
Ubuntu 22.04 installer running on the server being reinstalled. Since
rebooting the BMC didn't reboot the host, I could verify that rebooting
the BMC alone fixed the problem; when the BMC rebooted, my KVM-over-IP
session could now enter all digits.
(I'm glad that it occurred to me to reboot the BMC, instead of just
grumble and go down to the machine room to do the install with the
physical console.)

(One comment.)
Written on 14 January 2023. 

     ¬´   Ubuntu 22.04 LTS servers and phased apt updates    
  



 These are my WanderingThoughts 
(About the blog)
Full index of entries 
Recent comments
This is part of CSpace, and is written by ChrisSiebenmann. 
Mastodon: @cks 
Twitter: @thatcks
* * *
Categories: links, linux, programming, python, snark, solaris, spam, sysadmin, tech, unix, web 
Also: (Sub)topics
This is a DWiki. 
GettingAround 
(Help)
 
 Search:  



 Page tools: View Source, Add Comment. 

Search: 

Login: 
Password: 


 

Atom Syndication: Recent Comments.
 Last modified: Sat Jan 14 22:02:49 2023 
This dinky wiki is brought to you by the Insane Hackers
Guild, Python sub-branch.


"
https://news.ycombinator.com/rss,Bayesian statistics and machine learning: How do they differ?,https://statmodeling.stat.columbia.edu/2023/01/14/bayesian-statistics-and-machine-learning-how-do-they-differ/,Comments,"
403 Forbidden

403 Forbidden
nginx


"
https://news.ycombinator.com/rss,Money creation in the modern economy (2014) [pdf],https://www.bankofengland.co.uk/-/media/boe/files/quarterly-bulletin/2014/money-creation-in-the-modern-economy.pdf?la=en&hash=9A8788FD44A62D8BB927123544205CE476E01654,Comments,"14  Quarterly Bulletin  2014 Q1  Money creation in the modern economy  By Michael McLeay, Amar Radia and Ryland Thomas of the Bank‚Äôs Monetary Analysis Directorate.(1)  ¬Å  This article explains how the majority of money in the modern economy is created by commercial  banks making loans.  ¬Å  Money creation in practice differs from some popular misconceptions ‚Äî banks do not act simply as intermediaries, lending out deposits that savers place with them, and nor do they ‚Äòmultiply up‚Äô central bank money to create new loans and deposits.  ¬Å  The amount of money created in the economy ultimately depends on the monetary policy of the central bank.  In normal times, this is carried out by setting interest rates.  The central bank can also affect the amount of money directly through purchasing assets or ‚Äòquantitative easing‚Äô.  Overview  In the modern economy, most money takes the form of bank deposits.  But how those bank deposits are created is often misunderstood:  the principal way is through commercial banks making loans.  Whenever a bank makes a loan, it simultaneously creates a matching deposit in the borrower‚Äôs bank account, thereby creating new money.  The reality of how money is created today differs from the description found in some economics textbooks:  ¬Å  Rather than banks receiving deposits when households save and then lending them out, bank lending creates deposits.  ¬Å  In normal times, the central bank does not fix the amount  of money in circulation, nor is central bank money ‚Äòmultiplied up‚Äô into more loans and deposits.  Although commercial banks create money through lending, they cannot do so freely without limit.  Banks are limited in how much they can lend if they are to remain profitable in a competitive banking system.  Prudential regulation also acts as a constraint on banks‚Äô activities in order to maintain the resilience of the financial system.  And the households and companies who receive the money created by new lending may take actions that affect the stock of money ‚Äî they could quickly ‚Äòdestroy‚Äô money by using it to repay their existing debt, for instance.  Monetary policy acts as the ultimate limit on money creation.  The Bank of England aims to make sure the amount of money creation in the economy is consistent with  low and stable inflation.  In normal times, the Bank of England implements monetary policy by setting the interest rate on central bank reserves.  This then influences a range of interest rates in the economy, including those on bank loans.  In exceptional circumstances, when interest rates are at theireffective lower bound, money creation and spending in the economy may still be too low to be consistent with the central bank‚Äôs monetary policy objectives.  One possible response is to undertake a series of asset purchases, or‚Äòquantitative easing‚Äô (QE).  QE is intended to boost theamount of money in the economy directly by purchasingassets, mainly from non-bank financial companies. QE initially increases the amount of bank deposits those companies hold (in place of the assets they sell).  Those companies will then wish to rebalance their portfolios ofassets by buying higher-yielding assets, raising the price ofthose assets and stimulating spending in the economy. As a by-product of QE, new central bank reserves arecreated.  But these are not an important part of thetransmission mechanism.  This article explains how, just as innormal times, these reserves cannot be multiplied into moreloans and deposits and how these reserves do not represent‚Äòfree money‚Äô for banks. Click here for a short video filmed in the Bank‚Äôs gold vaults that discusses some of the key topics from this article.  (1)  The authors would like to thank Lewis Kirkham for his help in producing this article.  Topical articles  Money creation in the modern economy  15  Introduction  ‚ÄòMoney in the modern economy:  an introduction‚Äô, a companion piece to this article, provides an overview of what is meant by money and the different types of money that exist in a modern economy, briefly touching upon how each type of money is created.  This article explores money creation in the modern economy in more detail.  The article begins by outlining two common misconceptions about money creation, and explaining how, in the modern economy, money is largely created by commercial banks making loans.(1)  The article then discusses the limits to the banking system‚Äôs ability to create money and the important role for central bank policies in ensuring that credit and money growth are consistent with monetary and financial stability in the economy.  The final section discusses the role of money in the monetary transmission mechanism during periods of quantitative easing (QE), and dispels some myths surrounding money creation and QE.  A short video explains some of the key topics covered in this article.(2)  Two misconceptions about money creation  The vast majority of money held by the public takes the form of bank deposits.  But where the stock of bank deposits comes from is often misunderstood.  One common misconception is that banks act simply as intermediaries, lending out the deposits that savers place with them.  In this view deposits are typically ‚Äòcreated‚Äô by the saving decisions of households, and banks then ‚Äòlend out‚Äô those existing deposits to borrowers, for example to companies looking to finance investment or individuals wanting to purchase houses.  In fact, when households choose to save more money in bank accounts, those deposits come simply at the expense of deposits that would have otherwise gone to companies in payment for goods and services.  Saving does not by itself increase the deposits or ‚Äòfunds available‚Äô for banks to lend. Indeed, viewing banks simply as intermediaries ignores the fact that, in reality in the modern economy, commercial banks are the creators of deposit money.  This article explains how, rather than banks lending out deposits that are placed with them, the act of lending creates deposits ‚Äî the reverse of the sequence typically described in textbooks.(3)  Another common misconception is that the central bank determines the quantity of loans and deposits in the economy by controlling the quantity of central bank money ‚Äî the so-called ‚Äòmoney multiplier‚Äô approach.  In that view, central banks implement monetary policy by choosing a quantity of reserves.  And, because there is assumed to be a constant ratio of broad money to base money, these reserves are then ‚Äòmultiplied up‚Äô to a much greater change in bank  loans and deposits.  For the theory to hold, the amount of reserves must be a binding constraint on lending, and the central bank must directly determine the amount of reserves. While the money multiplier theory can be a useful way of introducing money and banking in economic textbooks, it is not an accurate description of how money is created in reality. Rather than controlling the quantity of reserves, central banks today typically implement monetary policy by setting the price of reserves ‚Äî that is, interest rates.  In reality, neither are reserves a binding constraint on lending, nor does the central bank fix the amount of reserves that are available.  As with the relationship between deposits and loans, the relationship between reserves and loans typically operates in the reverse way to that described in some economics textbooks.  Banks first decide how much to lend depending on the profitable lending opportunities available to them ‚Äî which will, crucially, depend on the interest rate set by the Bank of England.  It is these lending decisions that determine how many bank deposits are created by the banking system.  The amount of bank deposits in turn influences how much central bank money banks want to hold in reserve (to meet withdrawals by the public, make payments to other banks, or meet regulatory liquidity requirements), which is then, in normal times, supplied on demand by the Bank of England.  The rest of this article discusses these practices in more detail.  Money creation in reality  Lending creates deposits ‚Äî broad money determination at the aggregate level As explained in ‚ÄòMoney in the modern economy:  an introduction‚Äô, broad money is a measure of the total amount of money held by households and companies in the economy. Broad money is made up of bank deposits ‚Äî which are essentially IOUs from commercial banks to households and companies ‚Äî and currency ‚Äî mostly IOUs from the central bank.(4)(5)  Of the two types of broad money, bank deposits make up the vast majority ‚Äî 97% of the amount currently in circulation.(6)  And in the modern economy, those bank deposits are mostly created by commercial banks themselves.  (1)  Throughout this article, ‚Äòbanks‚Äô and ‚Äòcommercial banks‚Äô are used to refer to banks and  building societies together.  (2)  See www.youtube.com/watch?v=CvRAqR2pAgw. (3)  There is a long literature that does recognise the ‚Äòendogenous‚Äô nature of money  creation in practice.  See, for example, Moore (1988), Howells (1995) and Palley (1996).  (4)  The definition of broad money used by the Bank of England, M4ex, also includes a  wider range of bank liabilities than regular deposits;  see Burgess and Janssen (2007) for more details.  For simplicity, this article describes all of these liabilities as deposits. A box later in this article provides details about a range of popular monetary aggregates in the United Kingdom.  (5)  Around 6% of the currency in circulation is made up of coins, which are produced by The Royal Mint.  Of the banknotes that circulate in the UK economy, some are issued by some Scottish and Northern Irish commercial banks, although these are fully matched by Bank of England money held at the Bank.  (6)  As of December 2013.  16  Quarterly Bulletin  2014 Q1  Commercial banks create money, in the form of bank deposits, by making new loans.  When a bank makes a loan, for example to someone taking out a mortgage to buy a house, it does not typically do so by giving them thousands of pounds worth of banknotes.  Instead, it credits their bank account with a bank deposit of the size of the mortgage.  At that moment, new money is created.  For this reason, some economists have referred to bank deposits as ‚Äòfountain pen money‚Äô, created at the stroke of bankers‚Äô pens when they approve loans.(1)  This process is illustrated in Figure 1, which shows how new lending affects the balance sheets of different sectors of the economy (similar balance sheet diagrams are introduced in ‚ÄòMoney in the modern economy:  an introduction‚Äô).  As shown in the third row of Figure 1, the new deposits increase the assets of the consumer (here taken to represent households and companies) ‚Äî the extra red bars ‚Äî and the new loan increases their liabilities ‚Äî the extra white bars.  New broad money has been created.  Similarly, both sides of the commercial banking sector‚Äôs balance sheet increase as new money and loans are created.  It is important to note that although the simplified diagram of Figure 1 shows the amount of new money created as being identical to the amount of new lending, in practice there will be several factors that may subsequently cause the amount of deposits to be different from the amount of lending.  These are discussed in detail in the next section.  While new broad money has been created on the consumer‚Äôs balance sheet, the first row of Figure 1 shows that this is without ‚Äî in the first instance, at least ‚Äî any change in the amount of central bank money or ‚Äòbase money‚Äô.  As discussed earlier, the higher stock of deposits may mean that banks want, or are required, to hold more central bank money in order to meet withdrawals by the public or make payments to other banks.  And reserves are, in normal times, supplied ‚Äòon demand‚Äô by the Bank of England to commercial banks in exchange for other assets on their balance sheets.  In no way does the aggregate quantity of reserves directly constrain the amount of bank lending or deposit creation.  This description of money creation contrasts with the notion that banks can only lend out pre-existing money, outlined in the previous section.  Bank deposits are simply a record of how much the bank itself owes its customers.  So they are a liability of the bank, not an asset that could be lent out.  A related misconception is that banks can lend out their reserves. Reserves can only be lent between banks, since consumers do not have access to reserves accounts at the Bank of England.(2)  Other ways of creating and destroying deposits Just as taking out a new loan creates money, the repayment of bank loans destroys money.(3)  For example, suppose a consumer has spent money in the supermarket throughout the month by using a credit card.  Each purchase made using the  Figure 1  Money creation by the aggregate banking sector making additional loans(a)  Before loans are made  After loans are made  Assets  Liabilities  Assets  Liabilities  Central bank(b)  Non-money  Reserves  Currency  Base money  Non-money  Reserves  Currency  Base money  Commercial banks(c)  Assets  Liabilities  New loans  New deposits  Assets  Liabilities  Reserves  Deposits  Reserves  Deposits  Currency  Currency  Consumers(d)  Assets  Liabilities  New deposits  New loans  Assets  Liabilities  Broad money  Broad money  Deposits  Non-money  Deposits  Non-money  Currency  Currency  (a)  Balance sheets are highly stylised for ease of exposition:  the quantities of each type of  money shown do not correspond to the quantities actually held on each sector‚Äôs balance sheet.  (b)  Central bank balance sheet only shows base money liabilities and the corresponding assets. In practice the central bank holds other non-money liabilities.  Its non-monetary assets are mostly made up of government debt.  Although that government debt is actually held by the Bank of England Asset Purchase Facility, so does not appear directly on the balance sheet. (c)  Commercial banks‚Äô balance sheets only show money assets and liabilities before any loans  are made.  (d)  Consumers represent the private sector of households and companies.  Balance sheet only shows broad money assets and corresponding liabilities ‚Äî real assets such as the house being transacted are not shown.  Consumers‚Äô non-money liabilities include existing secured and unsecured loans.  credit card will have increased the outstanding loans on the consumer‚Äôs balance sheet and the deposits on the supermarket‚Äôs balance sheet (in a similar way to that shown in Figure 1).  If the consumer were then to pay their credit card  (1)  Fountain pen money is discussed in Tobin (1963), who mentions it in the context of making an argument that banks cannot create unlimited amounts of money in practice.  (2)  Part of the confusion may stem from some economists‚Äô use of the term ‚Äòreserves‚Äô when referring to ‚Äòexcess reserves‚Äô ‚Äî balances held above those required by regulatory reserve requirements.  In this context, ‚Äòlending out reserves‚Äô could be a shorthand way of describing the process of increasing lending and deposits until the bank reaches its maximum ratio.  As there are no reserve requirements in the United Kingdom the process is less relevant for UK banks.  (3)  The fall in bank lending in the United Kingdom since 2008 is an important reason why  the growth of money in the economy has been so much lower than in the years leading up to the crisis, as discussed in Bridges, Rossiter and Thomas (2011) and Butt et al (2012).  Topical articles  Money creation in the modern economy  17  bill in full at the end of the month, its bank would reduce the amount of deposits in the consumer‚Äôs account by the value of the credit card bill, thus destroying all of the newly created money.  Banks making loans and consumers repaying them are the most significant ways in which bank deposits are created and destroyed in the modern economy.  But they are far from the only ways.  Deposit creation or destruction will also occur any time the banking sector (including the central bank) buys or sells existing assets from or to consumers, or, more often, from companies or the government.  Banks buying and selling government bonds is one particularly important way in which the purchase or sale of existing assets by banks creates and destroys money.  Banks often buy and hold government bonds as part of their portfolio of liquid assets that can be sold on quickly for central bank money if, for example, depositors want to withdraw currency in large amounts.(1)  When banks purchase government bonds from the non-bank private sector they credit the sellers with bank deposits.(2)  And, as discussed later in this article, central bank asset purchases, known as quantitative easing (QE), have similar implications for money creation.  Money can also be destroyed through the issuance of long-term debt and equity instruments by banks.  In addition to deposits, banks hold other liabilities on their balance sheets. Banks manage their liabilities to ensure that they have at least some capital and longer-term debt liabilities to mitigate certain risks and meet regulatory requirements.  Because these ‚Äònon-deposit‚Äô liabilities represent longer-term investments in the banking system by households and companies, they cannot be exchanged for currency as easily as bank deposits, and therefore increase the resilience of the bank.  When banks issue these longer-term debt and equity instruments to non-bank financial companies, those companies pay for them with bank deposits.  That reduces the amount of deposit, or money, liabilities on the banking sector‚Äôs balance sheet and increases their non-deposit liabilities.(3)  Buying and selling of existing assets and issuing longer-term liabilities may lead to a gap between lending and deposits in a closed economy.  Additionally, in an open economy such as the United Kingdom, deposits can pass from domestic residents to overseas residents, or sterling deposits could be converted into foreign currency deposits.  These transactions do not destroy money per se, but overseas residents‚Äô deposits and foreign currency deposits are not always counted as part of a country‚Äôs money supply.  Limits to broad money creation Although commercial banks create money through their lending behaviour, they cannot in practice do so without limit. In particular, the price of loans ‚Äî that is, the interest rate (plus  any fees) charged by banks ‚Äî determines the amount that households and companies will want to borrow.  A number of factors influence the price of new lending, not least the monetary policy of the Bank of England, which affects the level of various interest rates in the economy.  The limits to money creation by the banking system were discussed in a paper by Nobel Prize winning economist James Tobin and this topic has recently been the subject of debate among a number of economic commentators and bloggers.(4)  In the modern economy there are three main sets of constraints that restrict the amount of money that banks can create.  (i)  Banks themselves face limits on how much they can  lend.  In particular:  ¬Å  Market forces constrain lending because individual  banks have to be able to lend profitably in a competitive market.  ¬Å  Lending is also constrained because banks have to take  steps to mitigate the risks associated with making additional loans.  ¬Å  Regulatory policy acts as a constraint on banks‚Äô  activities in order to mitigate a build-up of risks that could pose a threat to the stability of the financial system.  (ii)  Money creation is also constrained by the behaviour of the money holders ‚Äî households and businesses. Households and companies who receive the newly created money might respond by undertaking transactions that immediately destroy it, for example by repaying outstanding loans.  (iii)  The ultimate constraint on money creation is monetary policy.  By influencing the level of interest rates in the economy, the Bank of England‚Äôs monetary policy affects how much households and companies want to borrow. This occurs both directly, through influencing the loan rates charged by banks, but also indirectly through the overall effect of monetary policy on economic activity in  (1)  It is for this reason that holdings of some government bonds are counted towards meeting prudential liquidity requirements, as described in more detail by Farag, Harland and Nixon (2013).  (2)  In a balance sheet diagram such as Figure 1, a purchase of government bonds from consumers by banks would be represented by a change in the composition of consumers‚Äô assets from government bonds to deposits and an increase in both deposits and government bonds on the commercial banks‚Äô balance sheet.  (3)  Commercial banks‚Äô purchases of government bonds and their issuance of long-term  debt and equity have both been important influences on broad money growth during the financial crisis as discussed in Bridges, Rossiter and Thomas (2011) and Butt et al (2012).  (4)  Tobin (1963) argued that banks do not possess a ‚Äòwidow‚Äôs cruse‚Äô, referring to a biblical story (earlier referenced in economics by John Maynard Keynes) in which a widow is able to miraculously refill a cruse (a pot or jar) of oil during a famine.  Tobin was arguing that there were limits to how many loans could be automatically matched by deposits.  18  Quarterly Bulletin  2014 Q1  the economy.  As a result, the Bank of England is able to ensure that money growth is consistent with its objective of low and stable inflation.  make many such loans every day.  So if a given bank financed all of its new loans in this way, it would soon run out of reserves.  The remainder of this section explains how each of these mechanisms work in practice.  (i) Limits on how much banks can lend Market forces facing individual banks Figure 1 showed how, for the aggregate banking sector, loans are initially created with matching deposits.  But that does not mean that any given individual bank can freely lend and create money without limit.  That is because banks have to be able to lend profitably in a competitive market, and ensure that they adequately manage the risks associated with making loans.  Banks receive interest payments on their assets, such as loans, but they also generally have to pay interest on their liabilities, such as savings accounts.  A bank‚Äôs business model relies on receiving a higher interest rate on the loans (or other assets) than the rate it pays out on its deposits (or other liabilities). Interest rates on both banks‚Äô assets and liabilities depend on the policy rate set by the Bank of England, which acts as the ultimate constraint on money creation.  The commercial bank uses the difference, or spread, between the expected return on their assets and liabilities to cover its operating costs and to make profits.(1)  In order to make extra loans, an individual bank will typically have to lower its loan rates relative to its competitors to induce households and companies to borrow more.  And once it has made the loan it may well ‚Äòlose‚Äô the deposits it has created to those competing banks.  Both of these factors affect the profitability of making a loan for an individual bank and influence how much borrowing takes place.  For example, suppose an individual bank lowers the rate it charges on its loans, and that attracts a household to take out a mortgage to buy a house.  The moment the mortgage loan is made, the household‚Äôs account is credited with new deposits. And once they purchase the house, they pass their new deposits on to the house seller.  This situation is shown in the first row of Figure 2.  The buyer is left with a new asset in the form of a house and a new liability in the form of a new loan. The seller is left with money in the form of bank deposits instead of a house.  It is more likely than not that the seller‚Äôs account will be with a different bank to the buyer‚Äôs.  So when the transaction takes place, the new deposits will be transferred to the seller‚Äôs bank, as shown in the second row of Figure 2.  The buyer‚Äôs bank would then have fewer deposits than assets.  In the first instance, the buyer‚Äôs bank settles with the seller‚Äôs bank by transferring reserves.  But that would leave the buyer‚Äôs bank with fewer reserves and more loans relative to its deposits than before.  This is likely to be problematic for the bank since it would increase the risk that it would not be able to meet all of its likely outflows.  And, in practice, banks  Banks therefore try to attract or retain additional liabilities to accompany their new loans.  In practice other banks would also be making new loans and creating new deposits, so one way they can do this is to try and attract some of those newly created deposits.  In a competitive banking sector, that may involve increasing the rate they offer to households on their savings accounts.  By attracting new deposits, the bank can increase its lending without running down its reserves, as shown in the third row of Figure 2.  Alternatively, a bank can borrow from other banks or attract other forms of liabilities, at least temporarily.  But whether through deposits or other liabilities, the bank would need to make sure it was attracting and retaining some kind of funds in order to keep expanding lending.  And the cost of that needs to be measured against the interest the bank expects to earn on the loans it is making, which in turn depends on the level of Bank Rate set by the Bank of England.  For example, if a bank continued to attract new borrowers and increase lending by reducing mortgage rates, and sought to attract new deposits by increasing the rates it was paying on its customers‚Äô deposits, it might soon find it unprofitable to keep expanding its lending.  Competition for loans and deposits, and the desire to make a profit, therefore limit money creation by banks.  Managing the risks associated with making loans Banks also need to manage the risks associated with making new loans.  One way in which they do this is by making sure that they attract relatively stable deposits to match their new loans, that is, deposits that are unlikely or unable to be withdrawn in large amounts.  This can act as an additional limit to how much banks can lend.  For example, if all of the deposits that a bank held were in the form of instant access accounts, such as current accounts, then the bank might run the risk of lots of these deposits being withdrawn in a short period of time.  Because banks tend to lend for periods of many months or years, the bank may not be able to repay all of those deposits ‚Äî it would face a great deal of liquidity risk. In order to reduce liquidity risk, banks try to make sure that some of their deposits are fixed for a certain period of time, or term.(2)  Consumers are likely to require compensation for the inconvenience of holding longer-term deposits, however, so these are likely to be more costly for banks, limiting the amount of lending banks wish to do.  And as discussed earlier, if banks guard against liquidity risk by issuing long-term liabilities, this may destroy money directly when companies pay for them using deposits.  (1)  See Button, Pezzini and Rossiter (2010) for an explanation of how banks price new  loans.  (2)  Banks also guard against liquidity risk by holding liquid assets (including reserves and currency), which either can be used directly to cover outflows, or if not can quickly and cheaply be converted into assets that can.  Although if banks purchase liquid assets such as government bonds from non-banks, this could create further deposits.  Topical articles  Money creation in the modern economy  19  Figure 2  Money creation for an individual bank making an additional loan(a)  Changes to the balance sheets of the house buyer and seller  House buyer  House seller  House buyer  House seller  House buyer  House seller  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Non-money (house)  GovernmentDeposits debt Currency  Deposits  Currency  Non-money  New deposit  New loan  Non-money (house)  Non-money (house)  New loan  New deposit  Non-money  Non-money  Non-money  Deposits  Currency  Non-money  GovernmentDeposits debt Currency  Deposits  Currency  Non-money  GovernmentDeposits debt Currency  Balance sheets before the loan is made.  The house buyer takes out a mortgage‚Ä¶  ‚Ä¶and uses its new deposits to pay the house seller.  Changes to the balance sheets of the house buyer and seller‚Äôs banks  Buyer‚Äôs bank  Seller‚Äôs bank  Buyer‚Äôs bank  Seller‚Äôs bank  Buyer‚Äôs bank  Seller‚Äôs bank  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  New loan  New deposit  Transferred reserves  New deposit  Reserves  Deposits  Reserves  Deposits  Reserves  Deposits  Reserves  Deposits  Currency  Currency  Currency  Currency  Balance sheets before the loan is made.  The mortgage lender creates new deposits‚Ä¶  New loan  Reserves  Currency  Deposits  Reserves  Currency  Deposits  ‚Ä¶which are transferred to the seller‚Äôs bank, along with reserves, which the buyer‚Äôs bank uses to settle the transaction.  But settling all transactions in this way would be unsustainable: ‚Ä¢  The buyer‚Äôs bank would have fewer reserves to meet its possible  outfows, for example from deposit withdrawals.  ‚Ä¢  And if it made many new loans it would eventually run out  of reserves.  Buyer‚Äôs bank  Seller‚Äôs bank  Assets  Liabilities  Assets  Liabilities  New loan  New deposit  Reserves  Reserves  Currency  Deposits  Reserves  Currency  Deposits  So the buyer‚Äôs bank will in practice seek to attract or retain new deposits (and reserves) ‚Äî in the example shown here, from the seller‚Äôs bank ‚Äî to accompany their new loans.  (a)  Balance sheets are highly stylised for ease of exposition:  the quantities of each type of money shown do not correspond to the quantities actually held on each sector‚Äôs balance sheet.  Individual banks‚Äô lending is also limited by considerations of credit risk.  This is the risk to the bank of lending to borrowers who turn out to be unable to repay their loans.  In part, banks can guard against credit risk by having sufficient capital to absorb any unexpected losses on their loans.  But since loans will always involve some risk to banks of incurring losses, the cost of these losses will be taken into account when pricing loans.  When a bank makes a loan, the interest rate it charges will typically include compensation for the average level of credit losses the bank expects to suffer.  The size of this component of the interest rate will be larger when banks estimate that they will suffer higher losses, for example when lending to mortgagors with a high loan to value ratio.  As banks expand lending, their average expected loss per loan is likely to increase, making those loans less profitable.  This further limits the amount of lending banks can profitably do, and the money they can therefore create.  Market forces do not always lead individual banks to sufficiently protect themselves against liquidity and credit risks.  Because of this, prudential regulation aims to ensure that banks do not take excessive risks when making new loans, including via requirements for banks‚Äô capital and liquidity positions.  These requirements can therefore act as an additional brake on how much money commercial banks create by lending.  The prudential regulatory framework, along with more detail on capital and liquidity, is described in Farag, Harland and Nixon (2013).  So far this section has considered the case of an individual bank making additional loans by offering competitive interest rates ‚Äî both on its loans and deposits.  But if all banks simultaneously decide to try to do more lending, money growth may not be limited in quite the same way.  Although an individual bank may lose deposits to other banks, it would itself be likely to gain some deposits as a result of the other banks making loans.     20  Quarterly Bulletin  2014 Q1  There are a number of reasons why many banks may choose to increase their lending markedly at the same time.  For example, the profitability of lending at given interest rates could increase because of a general improvement in economic conditions.  Alternatively, banks may decide to lend more if they perceive the risks associated with making loans to households and companies to have fallen.  This sort of development is sometimes argued to be one of the reasons why bank lending expanded so much in the lead up to the financial crisis.(1)  But if that perception of a less risky environment were unwarranted, the result could be a more fragile financial system.(2)  One of the responses to the crisis in the United Kingdom has been the creation of a macroprudential authority, the Financial Policy Committee, to identify, monitor and take action to reduce or remove risks which threaten the resilience of the financial system as a whole.(3)  (ii) Constraints arising from the response of households and companies In addition to the range of constraints facing banks that act to limit money creation, the behaviour of households and companies in response to money creation by the banking sector can also be important, as argued by Tobin.  The behaviour of the non-bank private sector influences the ultimate impact that credit creation by the banking sector has on the stock of money because more (or less) money may be created than they wish to hold relative to other assets (such as property or shares).  As the households and companies who take out loans do so because they want to spend more, they will quickly pass that money on to others as they do so.  How those households and companies then respond will determine the stock of money in the economy, and potentially have implications for spending and inflation.  There are two main possibilities for what could happen to newly created deposits.  First, as suggested by Tobin, the money may quickly be destroyed if the households or companies receiving the money after the loan is spent wish to use it to repay their own outstanding bank loans.  This is sometimes referred to as the ‚Äòreflux theory‚Äô.(4) For example, a first-time house buyer may take out a mortgage to purchase a house from an elderly person who, in turn, repays their existing mortgage and moves in with their family.  As discussed earlier, repaying bank loans destroys money just as making loans creates it.  So, in this case, the balance sheet of consumers in the economy would be returned to the position it was in before the loan was made.  in the economy.(5)  Instead, the money may initially pass to households or companies with positive holdings of financial assets:  the elderly person may have already paid off their mortgage, or a company receiving money as a payment may already have sufficient liquid assets to cover possible outgoings.  They may then be left holding more money than they desire, and attempt to reduce their ‚Äòexcess‚Äô money holdings by increasing their spending on goods and services. (In the case of a company it may instead buy other, higher-yielding, assets.)  These two scenarios for what happens to newly created money ‚Äî being quickly destroyed or being passed on via spending ‚Äî have very different implications for economic activity.  In the latter, the money may continue to be passed between different households and companies each of whom may, in turn, increase their spending.  This process ‚Äî sometimes referred to as the ‚Äòhot potato‚Äô effect ‚Äî can lead, other things equal, to increased inflationary pressure on the economy.(6)  In contrast, if the money is quickly destroyed as in the former scenario, there need be no further effects on the economy.  This section has so far discussed how the actions of banks, households and companies can affect the amount of money in the economy, and therefore inflationary pressure.  But the ultimate determinant of monetary conditions in the economy is the monetary policy of the central bank.  (iii) Monetary policy ‚Äî the ultimate constraint on money creation One of the Bank of England‚Äôs primary objectives is to ensure monetary stability by keeping consumer price inflation on track to meet the 2% target set by the Government.  And, as discussed in the box on pages 22‚Äì23, over some periods of time, various measures of money have grown at a similar rate to nominal spending, which determines inflationary pressure in the economy in the medium term.  So setting monetary policy appropriately to meet the inflation target should ultimately ensure a stable rate of credit and money creation consistent with meeting that target.  This section explains the relationship between monetary policy and different types of money.  In normal times, the Monetary Policy Committee (MPC), like most of its equivalents in other countries, implements monetary policy by setting short-term interest rates, specifically by setting the interest rate paid on central bank reserves held by commercial banks.  It is able to do so because  The second possible outcome is that the extra money creation by banks can lead to more spending in the economy.  For newly created money to be destroyed, it needs to pass to households and companies with existing loans who want to repay them.  But this will not always be the case, since asset and debt holdings tend to vary considerably across individuals  (1)  See, for example, Haldane (2009). (2)  Tucker (2009) discusses the possibility of such ‚Äòrisk illusion‚Äô in the financial system. (3)  Tucker, Hall and Pattani (2013) describe the new powers for macroprudential policymaking in the United Kingdom in the wake of the recent financial crisis.  (4)  See Kaldor and Trevithick (1981). (5)  See Kamath et al (2011). (6)  This mechanism is explained in more detail in papers including Laidler (1984),  Congdon (1992, 2005), Howells (1995), Laidler and Robson (1995), Bridges, Rossiter and Thomas (2011) and Bridges and Thomas (2012).  Topical articles  Money creation in the modern economy  21  of the Bank‚Äôs position as the monopoly provider of central bank money in the United Kingdom.  And it is because there is demand for central bank money ‚Äî the ultimate means of settlement for banks, the creators of broad money ‚Äî that the price of reserves has a meaningful impact on other interest rates in the economy.  The interest rate that commercial banks can obtain on money placed at the central bank influences the rate at which they are willing to lend on similar terms in sterling money markets ‚Äî the markets in which the Bank and commercial banks lend to each other and other financial institutions.  The exact details of how the Bank uses its money market operations to implement monetary policy has varied over time, and central bank operating procedures today differ somewhat from country to country, as discussed in Clews, Salmon and Weeken (2010).(1)  Changes in interbank interest rates then feed through to a wider range of interest rates in different markets and at different maturities, including the interest rates that banks charge borrowers for loans and offer savers for deposits.(2)  By influencing the price of credit in this way, monetary policy affects the creation of broad money.  This description of the relationship between monetary policy and money differs from the description in many introductory textbooks, where central banks determine the quantity of broad money via a ‚Äòmoney multiplier‚Äô by actively varying the quantity of reserves.(3)  In that view, central banks implement monetary policy by choosing the quantity of reserves.  And, because there is assumed to be a stable ratio of broad money to base money, these reserves are then ‚Äòmultiplied up‚Äô to a much greater change in bank deposits as banks increase lending and deposits.  Neither step in that story represents an accurate description of the relationship between money and monetary policy in the modern economy.  Central banks do not typically choose a quantity  of reserves to bring about the desired short-term interest rate.(4)  Rather, they focus on prices ‚Äî setting interest rates.(5)  The Bank of England controls interest rates by supplying and remunerating reserves at its chosen policy rate.  The supply of both reserves and currency (which together make up base money) is determined by banks‚Äô demand for reserves both for the settlement of payments and to meet demand for currency from their customers ‚Äî demand that the central bank typically accommodates.  This demand for base money is therefore more likely to be a consequence rather than a cause of banks making loans and creating broad money.  This is because banks‚Äô decisions to extend credit are based on the availability of profitable lending opportunities at any given point in time.  The profitability of making a loan will depend on a number of factors, as discussed earlier.  One of these is the cost of funds that banks face, which is closely related to the interest rate paid on reserves, the policy rate.  In contrast, the quantity of reserves already in the system does not constrain the creation of broad money through the act of lending.(6)  This leg of the money multiplier is sometimes motivated by appealing to central bank reserve requirements, whereby banks are obliged to hold a minimum amount of reserves equal to a fixed proportion of their holdings of deposits.  But reserve requirements are not an important aspect of monetary policy frameworks in most advanced economies today.(7)  A looser stance of monetary policy is likely to increase the stock of broad money by reducing loan rates and increasing the volume of loans.  And a larger stock of broad money, accompanied by an increased level of spending in the economy, may cause banks and customers to demand more reserves and currency.(8)  So, in reality, the theory of the money multiplier operates in the reverse way to that normally described.  QE ‚Äî creating broad money directly with monetary policy  The previous section discussed how monetary policy can be seen as the ultimate limit to money creation by commercial banks.  But commercial banks could alternatively create too little money to be consistent with the economy meeting the inflation target.  In normal times, the MPC can respond by lowering the policy rate to encourage more lending and hence more money creation.  But, in response to the financial crisis, the MPC cut Bank Rate to 0.5% ‚Äî the so-called effective lower bound.  Once short-term interest rates reach the effective lower bound, it is not possible for the central bank to provide further stimulus to the economy by lowering the rate at which reserves are remunerated.(9)  One possible way of providing further monetary stimulus to the economy is through a programme of asset purchases (QE).  Like reductions in Bank  (1)  The framework for the Bank‚Äôs operations in the sterling money markets is set out in  the Bank‚Äôs ‚ÄòRed Book‚Äô, available at www.bankofengland.co.uk/markets/Documents/money/publications/redbook.pdf. Recent developments in sterling money markets are discussed by Jackson and Sim (2013).  (2)  Bank of England (1999) discusses the transmission mechanism of monetary policy in  more detail.  (3)  Benes and Kumhof (2012) discuss the money multiplier myth in more detail. (4)  As discussed by Disyatat (2008). (5)  Bindseil (2004) provides a detailed account of how monetary policy implementation  works through short-term interest rates.  (6)  Carpenter and Demiralp (2012) show that changes in quantities of reserves are  unrelated to changes in quantities of loans in the United States.  (7)  The Bank of England currently has no formal reserve requirements, for example. (It does require banks to hold a proportion of non-interest bearing ‚Äòcash ratio deposits‚Äô with the Bank for a subset of their liabilities.  But the function of these cash ratio deposits is non-operational.  Their sole purpose is to provide income for the Bank.)  Bernanke (2007) discusses how reserve requirements now present less of a constraint than in the past in the United States.  (8)  Kydland and Prescott (1990) found that broad money aggregates led the cycle, while  base money aggregates tended to lag the cycle slightly.  (9)  If the central bank were to lower interest rates significantly below zero, banks could swap their bank reserves into currency, which would pay a higher interest rate (of zero, or slightly less after taking into account the costs of storing currency).  Or put another way, the demand for central bank reserves would disappear, so the central bank could no longer influence the economy by changing the price of those reserves.  22  Quarterly Bulletin  2014 Q1  The information content of different types of money and monetary aggregates  One of the Bank of England‚Äôs primary objectives is to ensure monetary stability by keeping inflation on track to meet the Government‚Äôs 2% target.  Milton Friedman (1963) famously argued that ‚Äòinflation is always and everywhere a monetary phenomenon‚Äô.  So changes in the money supply may contain valuable information about spending and inflationary pressure in the economy.  Since money is essential for buying goods and services, it is likely to contain corroborative information about the current level of nominal spending in the economy.  It may also provide incremental information about future movements in nominal spending, and so can be a useful indicator of future inflationary pressure.  Finally, the behaviour of money may help to reveal the nature of the monetary transmission mechanism, especially when monetary policy is operated through ‚Äòquantitative easing‚Äô (QE).  In practice, a key difficulty is assessing which measures of money are the appropriate ones to look at for each of the different purposes.  The Bank currently constructs a number of monetary aggregates and publishes a range of data that allow to be created, summarised in Table 1.  Chart A shows some long-run historical time series of the growth of monetary aggregates compared with that of nominal spending in the economy.(1)  Given the various changes in the UK monetary regime over the past 150 years, it is unlikely that a single monetary indicator perfectly captures both the corroborative and incremental information in money.  The UK financial sector has also undergone various structural changes that need to be taken into account when considering the underlying link between money and spending.  For example, during periods when the financial sector has grown relative to the rest of the economy (such as in the early 1980s and the 2000s), broad money has tended to grow persistently faster than nominal spending.  Narrower measures of money, such as notes and coin and sight deposits (accounts that can be withdrawn immediately without penalty) are, in principle, better corroborative indicators of spending, as these are likely to be the types of money used to carry out the majority of transactions in goods and services in the economy.  The sum of notes and coin and sight deposits held by the non-bank private sector is sometimes known as zero maturity money or ‚ÄòMZM‚Äô.(2)  Broader measures of money might be more appropriate as incremental indicators of future spending and more revealing about the nature of the transmission mechanism.  M2, for example, additionally includes household time deposits such as savings accounts.(3)  And M4 is an even broader measure, including all sight and time deposits held by non-financial companies and non-bank financial companies.  The main article describes how QE works by first increasing the deposits of financial companies.  As these companies rebalance their  portfolios, asset prices are likely to increase and, with a lag, lead to an increase in households‚Äô and companies‚Äô spending.  So monitoring broad money has been an important part of assessing the effectiveness of QE.(4)  A number of econometric studies have suggested that sectoral movements in broad money may also provide valuable incremental information about spending in the economy.(5)  For example, non-financial companies‚Äô deposits appear to be a leading indicator of business investment in the economy. One can also try and weight different types of narrow and broad money together using some metric of how much each type of money is used in transactions ‚Äî known as a Divisia index.(6)  In practice, the interest paid on a given type of money is typically used as a weighting metric.  That is because individuals and companies are only likely to hold money which earns a low interest rate relative to other financial instruments if it compensates them by providing greater transactions services.  Identifying the appropriate measurement of money has been complicated by the continued development of the financial sector.  This has both expanded the range of instruments that might serve as money and the range of financial institutions that borrow from and deposit with the traditional banking system.  For example, sale and repurchase agreements (known as repos) ‚Äî where a company agrees to buy a security from a bank with agreement to sell it back later ‚Äî are currently included in M4 since the claim held on the bank can be thought of as a secured deposit.  In addition, some economists have argued that a range of instruments that provide collateral for various types of borrowing and lending could also be included in a broader measure of money.(7)  Moreover, many of the non-bank institutions that hold deposits mainly intermediate between banks themselves.  The deposits of these institutions, known as ‚Äòintermediate other financial corporations‚Äô (IOFCs), are likely to reflect activities within the banking system that are not directly related to spending in the economy.(8)  For this reason, the Bank‚Äôs headline measure of broad money is M4ex, which excludes IOFC deposits.  (1)  These series involve splicing together current Bank of England data with historic data  on monetary aggregates.  A spreadsheet of the data is available at www.bankofengland.co.uk/publications/Documents/quarterlybulletin/2014/ longrunmoneydata.xls.  (2)  A narrower measure known as non-interest bearing M1 can also be constructed.  This measure has become a less useful aggregate as most sight deposits now pay some form of interest.  For example, during the financial crisis when interest rates fell close to zero, the growth of non-interest bearing M1 picked up markedly as the relative cost of holding a non-interest bearing deposit fell sharply compared to an interest-bearing one.  Focusing on M1 would have given a misleading signal about the growth of nominal spending in the economy.  (3)  M2 contains the non-bank private sector‚Äôs holdings of notes and coin plus ‚Äòretail‚Äô  deposits which are deposits that pay an advertised interest rate.  Those will largely be deposits held by households but will also apply to some corporate deposits.  (4)  See Bridges, Rossiter and Thomas (2011) and Butt et al (2012). (5)  See, for example, Astley and Haldane (1995), Thomas (1997a, b) and Brigden and  Mizen (2004).  (6)  See Hancock (2005), for example. (7)  See, for example, Singh (2013). (8)  See Burgess and Janssen (2007) and  www.bankofengland.co.uk/statistics/Pages/iadb/notesiadb/m4adjusted.aspx for more detail.  Topical articles  Money creation in the modern economy  23  Table 1  Popular monetary aggregates that can be constructed from available UK data(a)  Name  Definition  Description(b)  Availability  Notes and coin  M0  Notes and coin in circulation outside the Bank of England.  Notes and coin plus central bank reserves.  The narrowest measure of money and used as an indicator of cash-based transactions.  1870‚Äìpresent(c)  Historically the base measure of money used in money multiplier calculations.  Often used as an approximate measure of the size of the Bank of England‚Äôs balance sheet. No longer published by the Bank of England but can be reconstructed.(d)  1870‚Äìpresent(c)  Non-interest bearing M1  Notes and coin plus non-interest bearing sight deposits held by the non-bank private sector.  An indicator of transactions in goods and services in the economy, less useful now since most sight deposits pay some form of interest.  1921‚Äìpresent(c)  Not published by the Bank of England but can be constructed from published components.  MZM  Notes and coin plus all sight deposits held by the non-bank private sector.  M2 or retail M4  Notes and coin plus all retail deposits (including retail time deposits) held by the non-bank private sector.  Notes and coin plus all sight and time deposits held with banks (excluding building societies) by the non-bank private sector.  Notes and coin, deposits, certificates of deposit, repos and securities with a maturity of less than five years held by the non-bank private sector.  M3  M4  M4ex  Divisia  An indicator of transactions in goods and services in the economy.  1977‚Äìpresent  Not published by the Bank of England but can be constructed from published components. The Bank also produces a measure based on an ECB definition of M1.  A broader measure of money than MZM encompassing all retail deposits.  The key additions are household time deposits and some corporate retail time deposits.  1982‚Äìpresent  Published by the Bank of England.  The Bank also produces a measure based on an ECB definition of M2.  Up until 1987 the headline broad monetary aggregate constructed by the Bank of England.  1870‚Äì1990(c)  The Bank also produces a measure based on an ECB definition of M3.  Up until 2007 the headline broad monetary aggregate constructed by the Bank of England.  1963‚Äìpresent  M4 excluding the deposits of IOFCs.  Since 2007 the headline broad monetary aggregate constructed by the Bank of England.  1997‚Äìpresent  A weighted sum of different types of money.  Aims to weight the component assets of broad money according to the transactions services they provide.(e)  1977‚Äìpresent  (a)  All definitions refer to sterling instruments only.  Some of the definitions in this table were changed at various points in time.  For example the original M3 aggregate included public sector deposits and thesector‚Äôs holdings of deposits in foreign currency.  A more comprehensive history of the development of UK monetary aggregates can be found at www.bankofengland.co.uk/statistics/Documents/ms/articl  non-bank private es/art2jul03.pdf.  (b)  Published by the Bank of England unless otherwise stated. (c)  This series uses the data constructed by Capie and Webber (1985). (d)  Data on M0 were discontinued following reforms to the Bank of England‚Äôs money market operations in 2006.  See www.bankofengland.co.uk/statistics/Documents/ms/articles/artjun06.pdf for more detai(e)  The Divisia indices for other financial corporations and for the non-bank private sector were discontinued in 2013.  See www.bankofengland.co.uk/statistics/Documents/ms/articles/art1aug13.pdf for more  ls. details.  Chart A  Different monetary aggregates and nominal spending  Notes and coin(a) Non-interest bearing M1(b)  MZM(c) M2(d) Divisia M3/M4/M4ex(e)  Nominal GDP(f)  Percentage changes on a year earlier  1870  80  90  1900  10  20  30  40  50  60  70  80  90  2000 10  60  50  40  30  20  10 + 0 ‚Äì 10  20  30  Sources:  Bank of England, Capie and Webber (1985), Mitchell (1988), ONS, Sefton and Weale (1995), Solomou and Weale (1991) and Bank calculations.  All series seasonally adjusted and break-adjusted where possible.  Historical data seasonally adjusted using X12.  (a)  1969 Q2 to 2013 Q4 ‚Äî notes and coin in circulation.  1870 Q1 to 1969 Q2 ‚Äî M0 from Capie and Webber (1985). (b)  1977 Q1 to 2013 Q4 ‚Äî notes and coin held by the non-bank and building society private sector plus non-interest bearing deposits.  Prior to 2008 Q1, excludes deposits with  building societies.  1963 Q1 to 1977 Q1 ‚Äî historical M1 data from Bank of England Quarterly Bulletins.  1921 Q4 to 1963 Q1 ‚Äî Capie and Webber (1985). (c)  Notes and coin held by the non-bank and building society private sector plus total sight deposits.  Prior to 1998 Q4 excludes deposits with building societies. (d)  Notes and coin and retail deposits held by the non-bank and building society private sector. (e)  1997 Q4 to 2013 Q4 ‚Äî M4 excluding intermediate OFCs.  1963 Q1 to 1997 Q4 ‚Äî M4.  1870 Q2 to 1963 Q1 ‚Äî M3 from Capie and Webber (1985). (f)  Composite estimate of nominal GDP at market prices.  See appendix of Hills, Thomas and Dimsdale (2010) for details.  24  Quarterly Bulletin  2014 Q1  Rate, asset purchases are a way in which the MPC can loosen the stance of monetary policy in order to stimulate economic activity and meet its inflation target.  But the role of money in the two policies is not the same.  QE involves a shift in the focus of monetary policy to the quantity of money:  the central bank purchases a quantity of assets, financed by the creation of broad money and a corresponding increase in the amount of central bank reserves. The sellers of the assets will be left holding the newly created deposits in place of government bonds.  They will be likely to be holding more money than they would like, relative to other assets that they wish to hold.  They will therefore want to rebalance their portfolios, for example by using the new deposits to buy higher-yielding assets such as bonds and shares issued by companies ‚Äî leading to the ‚Äòhot potato‚Äô effect discussed earlier.  This will raise the value of those assets and lower the cost to companies of raising funds in these markets.  That, in turn, should lead to higher spending in the economy.(1)  The way in which QE works therefore differs from two common misconceptions about central bank asset purchases:  that QE involves giving banks ‚Äòfree money‚Äô;  and that the key aim of QE is to increase bank lending by providing more reserves to the banking system, as might be described by the money multiplier theory.  This section explains the relationship between money and QE and dispels these misconceptions.  The link between QE and quantities of money QE has a direct effect on the quantities of both base and broad money because of the way in which the Bank carries out its asset purchases.  The policy aims to buy assets, government bonds, mainly from non-bank financial companies, such as pension funds or insurance companies.  Consider, for example, the purchase of ¬£1 billion of government bonds from a pension fund.  One way in which the Bank could carry out the purchase would be to print ¬£1 billion of banknotes and swap these directly with the pension fund.  But transacting in such large quantities of banknotes is impractical.  These sorts of transactions are therefore carried out using electronic forms of money.  As the pension fund does not hold a reserves account with the Bank of England, the commercial bank with whom they hold a bank account is used as an intermediary.  The pension fund‚Äôs bank credits the pension fund‚Äôs account with ¬£1 billion of deposits in exchange for the government bonds.  This is shown in the first panel of Figure 3.  The Bank of England finances its purchase by crediting reserves to the pension fund‚Äôs bank ‚Äî it gives the commercial bank an IOU (second row).  The commercial bank‚Äôs balance sheet expands:  new deposit liabilities are matched with an asset in the form of new reserves (third row).  Figure 3  Impact of QE on balance sheets(a)  Before asset purchase  After asset purchase  Pension fund  Assets  Liabilities  Assets  Liabilities  Government debt  Other  Deposits  Other  Central bank(b)  Assets  Liabilities  Assets  Liabilities  Government debt  Reserves  Other assets  Reserves  Other assets  Commercial bank  Assets  Liabilities  Assets  Liabilities  Reserves  Deposits Reserves  Deposits (a)  Balance sheets are highly stylised for ease of exposition:  quantities of assets and liabilities shown do not correspond to the quantities actually held by those sectors.  The figure only shows assets and liabilities relevant to the transaction.  (b)  Government debt is actually purchased by the Bank of England‚Äôs Asset Purchase Facility using a loan from the Bank of England, so does not actually appear directly on the Bank‚Äôs official consolidated balance sheet.  Two misconceptions about how QE works Why the extra reserves are not ‚Äòfree money‚Äô for banks While the central bank‚Äôs asset purchases involve ‚Äî and affect ‚Äî commercial banks‚Äô balance sheets, the primary role of those banks is as an intermediary to facilitate the transaction between the central bank and the pension fund.  The additional reserves shown in Figure 3 are simply a by-product of this transaction.  It is sometimes argued that, because they are assets held by commercial banks that earn interest, these reserves represent ‚Äòfree money‚Äô for banks.  While banks do earn interest on the newly created reserves, QE also creates an accompanying liability for the bank in the form of the pension fund‚Äôs deposit, which the bank will itself typically have to pay interest on.  In other words, QE leaves banks with both a new IOU from the central bank but also a new, equally sized IOU to consumers (in this case, the pension fund), and the interest rates on both of these depend on Bank Rate.  Why the extra reserves are not multiplied up into new loans and broad money As discussed earlier, the transmission mechanism of QE relies on the effects of the newly created broad ‚Äî rather than base ‚Äî money.  The start of that transmission is the creation of  (1)  The ways in which QE affects the economy are covered in more detail in Benford et al (2009), Joyce, Tong and Woods (2011) and Bowdler and Radia (2012).  The role of money more specifically is described in Bridges, Rossiter and Thomas (2011), Bridges and Thomas (2012) and Butt et al (2012).  Topical articles  Money creation in the modern economy  25  bank deposits on the asset holder‚Äôs balance sheet in the place of government debt (Figure 3, first row).  Importantly, the reserves created in the banking sector (Figure 3, third row) do not play a central role.  This is because, as explained earlier, banks cannot directly lend out reserves.  Reserves are an IOU from the central bank to commercial banks.  Those banks can use them to make payments to each other, but they cannot ‚Äòlend‚Äô them on to consumers in the economy, who do not hold reserves accounts.  When banks make additional loans they are matched by extra deposits ‚Äî the amount of reserves does not change.  Moreover, the new reserves are not mechanically multiplied up into new loans and new deposits as predicted by the money multiplier theory.  QE boosts broad money without directly leading to, or requiring, an increase in lending.  While the first leg of the money multiplier theory does hold during QE ‚Äî the monetary stance mechanically determines the quantity of reserves ‚Äî the newly created reserves do not, by themselves, meaningfully change the incentives for the banks to create new broad money by lending.  It is possible that QE might indirectly affect the incentives facing banks to make new loans, for example by reducing their funding costs, or by increasing the quantity of credit by boosting activity.(1)  But equally, QE could lead to companies repaying bank credit, if they were to issue more bonds or equity and use those funds  to repay bank loans.  On balance, it is therefore possible for QE to increase or to reduce the amount of bank lending in the economy.  However these channels were not expected to be key parts of its transmission:  instead, QE works by circumventing the banking sector, aiming to increase private sector spending directly.(2)  Conclusion  This article has discussed how money is created in the modern economy.  Most of the money in circulation is created, not by the printing presses of the Bank of England, but by the commercial banks themselves:  banks create money whenever they lend to someone in the economy or buy an asset from consumers.  And in contrast to descriptions found in some textbooks, the Bank of England does not directly control the quantity of either base or broad money.  The Bank of England is nevertheless still able to influence the amount of money in the economy.  It does so in normal times by setting monetary policy ‚Äî through the interest rate that it pays on reserves held by commercial banks with the Bank of England.  More recently, though, with Bank Rate constrained by the effective lower bound, the Bank of England‚Äôs asset purchase programme has sought to raise the quantity of broad money in circulation. This in turn affects the prices and quantities of a range of assets in the economy, including money.  (1)  A similar mechanism whereby QE could increase bank lending by enabling banks to  attract more stable funding is discussed in Miles (2012).  (2)  These channels, along with the effect of QE on bank lending more broadly, are  discussed in detail in a box in Butt et al (2012).  26  Quarterly Bulletin  2014 Q1  References  Astley, M and Haldane, A (1995), ‚ÄòMoney as an indicator‚Äô, Bank of England Working Paper No. 35.  Bank of England (1999), ‚ÄòThe transmission mechanism of monetary policy‚Äô, available at www.bankofengland.co.uk/publications/ Documents/other/monetary/montrans.pdf.  Benes, J and Kumhof, M (2012), ‚ÄòThe Chicago Plan revisited‚Äô, IMF Working Paper No. 12/202.  Benford, J, Berry, S, Nikolov, K, Robson, M and Young, C (2009), ‚ÄòQuantitative easing‚Äô, Bank of England Quarterly Bulletin, Vol. 49, No. 2, pages 90‚Äì100.  Bernanke, B (2007), ‚ÄòThe financial accelerator and the credit channel‚Äô, speech at a conference on The Credit Channel of Monetary Policy in the Twenty-first Century, Federal Reserve Bank of Atlanta.  Bindseil, U (2004), ‚ÄòThe operational target of monetary policy and the rise and fall of the reserve position doctrine‚Äô, ECB Working Paper No. 372.  Bowdler, C and Radia, A (2012), ‚ÄòUnconventional monetary policy: the assessment‚Äô, Oxford Review of Economic Policy, Vol. 28, No. 4, pages 603‚Äì21.  Clews, R, Salmon, C and Weeken, O (2010), ‚ÄòThe Bank‚Äôs money market framework‚Äô, Bank of England Quarterly Bulletin, Vol. 50, No. 4, pages 292‚Äì301.  Congdon, T (1992), Reflections on monetarism, Clarendon Press.  Congdon, T (2005), ‚ÄòMoney and asset prices in boom and bust‚Äô, Institute of Economic Affairs, Hobart Paper No. 152.  Disyatat, P (2008), ‚ÄòMonetary policy implementation: misconceptions and their consequences‚Äô, BIS Working Paper No. 269.  Farag, M, Harland, D and Nixon, D (2013), ‚ÄòBank capital and liquidity‚Äô, Bank of England Quarterly Bulletin, Vol. 53, No. 3, pages 201‚Äì15.  Friedman, M (1963), Inflation:  causes and consequences, Asia Publishing House.  Haldane, A (2009), ‚ÄòWhy banks failed the stress test‚Äô, available at www.bankofengland.co.uk/archive/documents/historicpubs/ speeches/2009/speech374.pdf.  Hancock, M (2005), ‚ÄòDivisia money‚Äô, Bank of England Quarterly Bulletin, Spring, pages 39‚Äì46.  Bridges, J, Rossiter, N and Thomas, R (2011), ‚ÄòUnderstanding the recent weakness in broad money growth‚Äô, Bank of England Quarterly Bulletin, Vol. 51, No. 1, pages 22‚Äì35.  Hills, S, Thomas, R and Dimsdale, N (2010), ‚ÄòThe UK recession in context ‚Äî what do three centuries of data tell us?‚Äô, Bank of England Quarterly Bulletin, Vol. 50, No. 4, pages 277‚Äì91.  Bridges, J and Thomas, R (2012), ‚ÄòThe impact of QE on the UK economy ‚Äî some supportive monetarist arithmetic‚Äô, Bank of England Working Paper No. 442.  Brigden, A and Mizen, P (2004), ‚ÄòMoney, credit and investment in the UK industrial and commercial companies sector‚Äô, The Manchester School, Vol. 72, No. 1, pages 72‚Äì79.  Burgess, S and Janssen, N (2007), ‚ÄòProposals to modify the measurement of broad money in the United Kingdom:  a user consultation‚Äô, Bank of England Quarterly Bulletin, Vol. 47, No. 3, pages 402‚Äì14.  Butt, N, Domit, S, Kirkham, L, McLeay, M and Thomas, R (2012), ‚ÄòWhat can the money data tell us about the impact of QE?‚Äô, Bank of England Quarterly Bulletin, Vol. 52, No. 4, pages 321‚Äì31.  Button, R, Pezzini, S and Rossiter, N (2010), ‚ÄòUnderstanding the price of new lending to households‚Äô, Bank of England Quarterly Bulletin, Vol. 50, No. 3, pages 172‚Äì82.  Capie, F and Webber, A (1985), A monetary history of the United Kingdom, 1870‚Äì1982, Vol. 1, Routledge.  Carpenter, S and Demiralp, S (2012), ‚ÄòMoney, reserves, and the transmission of monetary policy:  does the money multiplier exist?‚Äô, Journal of Macroeconomics, Vol. 34, No. 1, pages 59‚Äì75.  Howells, P (1995), ‚ÄòThe demand for endogenous money‚Äô, Journal of Post Keynesian Economics, Vol. 18, No. 1, pages 89‚Äì106.  Jackson, C and Sim, M (2013), ‚ÄòRecent developments in the sterling overnight money market‚Äô, Bank of England Quarterly Bulletin, Vol. 53, No. 3, pages 223‚Äì32.  Joyce, M, Tong, M and Woods, R (2011), ‚ÄòThe United Kingdom‚Äôs quantitative easing policy:  design, operation and impact‚Äô, Bank of England Quarterly Bulletin, Vol. 51, No. 3, pages 200‚Äì12.  Kaldor, N and Trevithick, J (1981), ‚ÄòA Keynesian perspective on money‚Äô, Lloyds Bank Review, January, pages 1‚Äì19.  Kamath, K, Reinold, K, Nielsen, M and Radia, A (2011), ‚ÄòThe financial position of British households:  evidence from the 2011 NMG Consulting survey‚Äô, Bank of England Quarterly Bulletin, Vol. 51, No. 4, pages 305‚Äì18.  Kydland, F and Prescott, E (1990), ‚ÄòBusiness cycles:  real facts and a monetary myth‚Äô, Federal Reserve Bank of Minneapolis Quarterly Review, Vol. 14, No. 2, pages 3‚Äì18.  Laidler, D (1984), ‚ÄòThe buffer stock notion in monetary economics‚Äô, The Economic Journal, Vol. 94, Supplement:  Conference Papers, pages 17‚Äì34.  Topical articles  Money creation in the modern economy  27  Laidler, D and Robson, W (1995), ‚ÄòEndogenous buffer-stock money‚Äô, Credit, interest rate spreads and the monetary policy transmission mechanism, Session 3, conference on The Transmission of Monetary Policy held at the Bank of Canada in November 1994.  Miles, D (2012), ‚ÄòAsset prices, saving and the wider effects of monetary policy‚Äô, available at www.bankofengland.co.uk/ publications/Documents/speeches/2012/speech549.pdf.  Mitchell, B R (1988), British historical statistics, Cambridge University Press.  Moore, B (1988), Horizontalists and verticalists:  the macroeconomics of credit money, Cambridge University Press.  Palley, T (1996), Post Keynesian economics:  debt, distribution and the macro economy, Macmillan.  Sefton, J and Weale, M (1995), Reconciliation of National Income and Expenditure:  balanced estimates of national income for the United Kingdom, 1920‚Äì1990, Cambridge University Press.  Singh, M (2013), ‚ÄòCollateral and monetary policy‚Äô, IMF Working Paper No. 13/186.  Solomou, S N and Weale, M (1991), ‚ÄòBalanced estimates of UK GDP 1870‚Äì1913‚Äô, Explorations in Economic History, Vol. 28, No. 1, pages 54‚Äì63.  Thomas, R (1997a), ‚ÄòThe demand for M4:  a sectoral analysis, Part 1 ‚Äî the personal sector‚Äô, Bank of England Working Paper No. 61.  Thomas, R (1997b), ‚ÄòThe demand for M4:  a sectoral analysis, Part 2 ‚Äî the corporate sector‚Äô, Bank of England Working Paper No. 62.  Tobin, J (1963), ‚ÄòCommercial banks as creators of ‚Äòmoney‚Äô‚Äô, Cowles Foundation Discussion Papers No. 159.  Tucker, P (2009), ‚ÄòThe debate on financial system resilience: macroprudential instruments‚Äô, available at www.bankofengland.co.uk/archive/Documents/historicpubs/ speeches/2009/speech407.pdf.  Tucker, P, Hall, S and Pattani, A (2013), ‚ÄòMacroprudential policy at the Bank of England‚Äô, Bank of England Quarterly Bulletin, Vol. 53, No. 3, pages 192‚Äì200.  "
https://news.ycombinator.com/rss,"‚ÄòExcuuuuse me, Princess ‚Äô: An oral history of The Legend of Zelda cartoon",https://www.polygon.com/zelda/23540526/legend-of-zelda-cartoon-oral-history-zeldathon,Comments,"

Share this story




Share this on Facebook





Share this on Twitter








Share
All sharing options






Share
All sharing options for:
‚ÄòExcuuuuse me, Princess!‚Äô: An oral history of The Legend of Zelda cartoon












Reddit







Pocket









Flipboard





Email









This story is part of a group of stories called 





    In 2023, Polygon is embarking on a Zeldathon. Join us on our journey through The Legend of Zelda series, from the original 1986 game to the release of The Legend of Zelda: Tears of the Kingdom, and beyond.
  


The world knows The Legend of Zelda‚Äôs Link as the brave hero of Hyrule ‚Äî a young warrior of few words. Link is a master with his bow and an excellent swordsman. But back in 1989, when The Legend of Zelda cartoon first aired, all Link wanted was a smooch. A kiss from Zelda, to be exact ‚Äî but he‚Äôs not exactly picky, and unlike the laconic hero of the games, he would not shut up about it. The hero of Hyrule is still tasked with defending the Triforce of Wisdom from Ganon‚Äôs grasp on the TV show, but that‚Äôs secondary to his insistence on a little kiss. The show‚Äôs bizarre portrayal of Link ‚Äî especially his constant begging of ‚ÄúExcuse me, Princess!‚Äù ‚Äî has made The Legend of Zelda cartoon a hilarious head-scratcher to this day. 




In 2023, Polygon is embarking on a Zeldathon. Join us on our journey through The Legend of Zelda series, from the original 1986 game to the release of The Legend of Zelda: Tears of the Kingdom, and beyond.



Back in 1989, The Legend of Zelda aired in 15-minute episodes every Friday during The Super Mario Bros. Super Show!, a mix of live-action and animated segments based on Nintendo games. Once a week, The Legend of Zelda replaced the Super Mario Bros. show, which featured animated segments of Mario and Luigi but, more memorably, the wacky, iconic live-action performances of WWF wrestler Lou Albano as Mario and The Jeffersons‚Äô Danny Wells as Luigi, who welcomed fans of the show with the catchphrase, ‚ÄúHey there, paisanos.‚Äù
Clearly, Super Mario Bros. was the main event for the Nintendo-themed TV block. It ran for 52 episodes compared to The Legend of Zelda‚Äôs 13. But for the writers of the Zelda cartoon, that was a boon: They had very little oversight and direction beyond character designs, a franchise ‚Äúbible‚Äù provided by Nintendo, and the original game, also called The Legend of Zelda, and its sequel, Zelda 2: The Adventure of Link. As they were not video game players themselves, the writers did their research and decided to go in a different direction ‚Äî one that‚Äôs more focused on story than gameplay. There were elements of the games, like sound effects and visuals, but the show mostly has Zelda and Link posted up in Hyrule castle defending the Triforce of Wisdom from Ganon while trying to acquire the Triforce of Power from the evil wizard himself. (The Triforces talk, by the way.)
Between the mischief that Zelda, Link, and fairy friend Spryte get into, The Legend of Zelda relied heavily on the relationship between Zelda and Link. Zelda, donning pink pants and purple thigh-high boots, more often plays the hero to Link‚Äôs bumbling teenage angst.
What we get from the short-lived ordeal is a charming and absurd rendition of a beloved (and often quiet and unvoiced) franchise.
 









Image: DiC Entertainment/Nintendo



From pixels to the small screen
Most of the Super Mario Bros. Super Show‚Äôs budget was tied up in the main part of the show ‚Äî the Super Mario Bros. show that led the time slot. When Super Mario. Bros Super Show was canceled, The Legend of Zelda was shut down alongside it. But for the show‚Äôs short run, writers said they had little interference from Nintendo, which just wanted more eyes on its game properties ‚Äî especially a new one like The Legend of Zelda. It was the first time ‚Äî and still one of the rare times ‚Äî that Link and Zelda got their own voice actor performances, and probably not the ones fans expected. 
Rather than simply recreating the video game, The Legend of Zelda‚Äôs writers positioned the show more as a mix of action, comedy, and drama, taking specific inspiration from Cybill Shepherd‚Äôs and Bruce Willis‚Äô ‚Äô80s show Moonlighting. Writers wanted Zelda and Link‚Äôs relationship to mirror Shepherd‚Äôs and Willis‚Äô rapport as Maddie and David on the detective show ‚Äî the same angry sexual tension, but goofier and lighter for the kid-friendly cartoon TV show. 
 











Bob Forward Story editor and writer, The Legend of Zelda
The Legend of Zelda was going to be a small addendum to the Super Mario Bros. Super Show, which was the actual star of the time slot. DiC needed somebody who could handle it on their own without a lot of supervision. After we had the initial discussion, they supplied me with a VHS tape of [a playthrough of] the game itself, since I wasn‚Äôt actually a person who played video games ‚Äî not that I had any objection. I just hadn‚Äôt really done it. They had a playthrough of the game that my sons were fascinated by. That was my research for it.
I don‚Äôt know if anyone cares about this, but the playthrough VHS tape that they supplied me with I guess had been played by one of the new Charlie‚Äôs Angels. I think it was Tonya Roberts. I guess she was a gamer when she was younger.

 









Image: DiC Entertainment/Nintendo



 











Reed Shelly Story editor and writer, Super Mario Bros. Super Show
The project originated as a concept by Andy Heyward as Super Mario Bros. Power Hour, a one hour-long animation block that would have featured series based on a number of intellectual properties. Concept art was produced for adaptations of Super Mario Bros., The Legend of Zelda, Metroid, Castlevania, Double Dragon, and California Games. With the exception of Mario and Zelda, none of these additional adaptations were ultimately produced.

 











John Grusd Director, The Legend of Zelda
Nintendo wanted us to base the show on the new game [Zelda 2: The Adventure of Link], because, you know, it‚Äôs great marketing. What they did was give me the Japanese version of the game, because it wasn‚Äôt out here yet. I didn‚Äôt know anything about the game when I started. I‚Äôd never played them. I wasn‚Äôt a gamer or anything. That‚Äôs how I learned how the characters move, the sound effects, the music. I got to be able to do the games all the way through pretty quickly, as a matter of fact, because I knew all the shortcuts. I could get through both of them in less than two minutes, probably. It‚Äôs pretty fast.

 











Phil Harnage Writer, The Legend of Zelda
It was a fun little show. And I say little, because they tacked it on to Super Mario. It really should have been a stand-alone show. It was very limiting for what the writers could do. I worked on the bible and wrote a couple of episodes. When you write the bible, you hand it off to somebody else, but occasionally you get to write a script. That‚Äôs the fun part. It was a fun show to write for because of the tension between Link and the princess. We modeled it after Moonlighting. We tried to capture that, and I think we did. Maybe over the top a little bit, but that‚Äôs what we were shooting for. We could have come up with a lot more shows. That was the sad part, that we only got to do one season. 


 









Image: Eve Forward



 











Eve Forward Writer, The Legend of Zelda
My brother somehow ended up suggesting I try writing an episode, and I was able to turn out a couple of scripts that, with his editing, ended up getting used. I was about 16-17 at the time. The only direction I had was the show bible, which outlined the basic characters and sorts of stories they were looking for. I didn‚Äôt have a Nintendo, so I rented one, and the game, and tried to play it, but I didn‚Äôt get very far. But the basic relationships were all established in the show bible; Ganon bad guy, Zelda tough girl, Link charming scamp, Triforce MacGuffin, etc.
I did play Dungeons & Dragons though, at the time, and some of that feel made it into the show. [The seventh episode] ‚ÄúDoppelganger‚Äù was based on a cursed mirror in D&D. Well, the monsters in Zelda were all based on things from the Nintendo game; same with the weapons, like Link‚Äôs boomerang. But in D&D of course you‚Äôre always fighting monsters and imagining how cool your character looks doing it, so a lot of the various swashbuckling stuff I liked to put in was based on things that had happened in our D&D games. I always thought of Link as more of a rogue than a fighter.

 











Bob Forward
We had a schedule we had to put the scripts through, and I think it was two a week. That wasn‚Äôt hard ‚Äî I worked on shows we had to do five a week, so two a week was just fine. Eve and I were just writing them on our own. We even had my mom pitch a story. She wrote something that we ended up having to do a lot of work on, but it wasn‚Äôt a bad initial concept. [Bob and Eve‚Äôs mom, Marsha Forward, had her script adapted as The Legend of Zelda‚Äôs 11th episode, ‚ÄúFairies in the Spring.‚Äù]
I wrote a bible for my own purposes, something that just outlined who all the characters were and what they wanted. Robby London [DiC executive] wanted to have some signature lines, and Moonlighting had just come out, or was very popular. Robby London came up with the idea of the line, ‚ÄúExcuuuuse me, Princess,‚Äù which is inspired by the Moonlighting relationship and a snarky line from a Steve Martin routine. I‚Äôll be honest, what I liked about Robby is that he would make quick decisions. As much as I was giving him a hard time about it, I put [that line] into the show way more than it was really necessary. But it turned out to be OK, even though people made fun of it. People remembered it, so I guess he was right. I have to admit, it caught on.

 









Image: DiC Entertainment/Nintendo




No one had ever heard Link or Zelda speak
People were certainly familiar with Link and Zelda by the time The Legend of Zelda cartoon was released ‚Äî The Legend of Zelda and Zelda 2: The Adventure of Link had been out for some time and already were popular. But characters were composed of just a few dozen pixels, and they weren‚Äôt voiced. It gave the TV writers lots of room to mess around; the show existed outside of the games, with Link just hanging around Zelda and her father‚Äôs castle, defending the Triforce from Ganon every once in a while. 
The show had to be largely carried by Link and Zelda‚Äôs personalities, plus the few other characters who appeared: Spryte, a fairy, and the two talking Triforce pieces (Wisdom and Power). So, the writers made those few characters big. Ganon is merely an annoyance to Link, whose more pressing problem is convincing Zelda to give him a kiss.
 











Bob Forward
We very much made it up as we went along. The other nice thing was that everybody was so concentrated on the Mario brothers that they completely left us alone, which is always my favorite way of working. You know, as long as we hit the page count and got the scripts in on time, nobody was looking. 
Link always wanted a kiss. That was one of Robby‚Äôs inventions. I thought it worked out. I was down for it. I kept expecting people to tell us we couldn‚Äôt do it. But apparently it worked.

 











Jonathan Potts Voice actor, Link
I pictured Link as being a teenager who was like the ultimate teenage boy, who was like a puppy. If you can imagine what a puppy would be [like] ‚Äî running around, peeing on the carpet, and overreacting ‚Äî everything was dramatic. I remember wanting to do that. I wasn‚Äôt a teenager then; I was well into my 20s when I did the part. I had to be that youthful, goofy teenage boy who acts before he thinks.

 











Cynthia Preston Voice actor, Zelda
You start reading something and you just have instincts ‚Äî all of your experience, and all of the movies you‚Äôve done, and all of the classes you‚Äôve taken, and that feeds into how to start molding a character. What does this character want? What do they want from this character? I don‚Äôt think I was playing Zelda as a teenager. She was an independent woman ‚Äî a young woman, but she was independent. She didn‚Äôt need a hero to save her, and that was so cool.
The show certainly wasn‚Äôt ahead of its time, but nonetheless it was a cool aspect that it wasn‚Äôt playing a damsel in distress.

 











Phil Harnage
We didn‚Äôt want a Disney princess. We‚Äôre not going to be selling princess dresses to six-year-olds. So yeah, she was an action hero in her own right, and that was kind of unique. But the writers didn‚Äôt come up with [Zelda wearing pants] ‚Äî that was something the artists came up with, and Nintendo loved it.
It was ahead of its time in some ways, but wasn‚Äôt always. Zelda was a good role model for girls. She was confident and took charge. She did want what she wanted, but was also very responsible. And Link was irresponsible. He was out there conniving: ‚ÄúHow am I gonna get her to kiss me?‚Äù There‚Äôs fun in that. That‚Äôs where the Moonlighting model really worked.

 











Jonathan Potts
The scripts weren‚Äôt complex. There weren‚Äôt a lot of deep things going on. It was all right there, sort of obvious. So [direction] usually came down to technical things ‚Äî more energy. 

 











Cynthia Preston
There was this time a director wanted me to laugh more as Zelda. I was trying, but laughing is harder than crying to do naturally. Shockingly, he mooned me and I fell over laughing. I really have the feeling I didn‚Äôt get the right laugh, but it was damn funny.

 









Image: DiC Entertainment/Nintendo



A talking Triforce?
Writers said Nintendo didn‚Äôt want them coming up with new characters and backstories, so they worked with what they had. That‚Äôs where the Triforce pieces came in ‚Äî the show couldn‚Äôt only be Zelda, Link, Ganon, and Spryte. There was the Triforce. Why not make it talk? Successful or not, the Wisdom piece of the Triforce did have a role in the show: Moving the story forward and explaining the situation.
 











Bob Forward
Link and Zelda wanted the Triforce of Power, and Ganon wanted the Triforce of Wisdom, so [in] half the shows Link and Zelda would be the ones to instigate the action as opposed to just hanging around and waiting for Ganon to start something and trying to reestablish the status quo.

 











Phil Harnage
The whole Triforce thing, it came out of the game and everything, but I don‚Äôt know ‚Äî it was hard to figure out. What does that mean? The Triforce? What do you have to do with it to make it work? I wasn‚Äôt really happy with that. I thought it would be much more fun to have them fighting over who‚Äôs going to control the land. But [the Triforce] was from the game, and you had to do it for the gamers. 
The more things talk, the more explanatory it can be. You‚Äôre like, Why did this happen? And the Triforce can tell you, you know? It‚Äôs magic. In a magical world, you have to set the rules, of course. But you set the rules yourself.

 









Image: DiC Entertainment/Nintendo



A sword fighter in a show without fighting
For a TV show about a game with a hero who hits things with swords, The Legend of Zelda has surprisingly little sword fighting. The Legend of Zelda was a kids‚Äô TV show, and that meant it had to  follow the network‚Äôs standards ‚Äî so characters couldn‚Äôt die. Link and Zelda still have weapons, of course, but they don‚Äôt seem deadly. Link‚Äôs sword shoots out magic bullets that stun enemies, and Zelda often uses a magic bow that uses magic instead of arrows.
 











John Grusd
Link has a sword, but can he actually use it to chop somebody‚Äôs head off? He can‚Äôt do what he does in the game. Nintendo wants us to do what they do in the game, but the standards and practices at the network say no. We can‚Äôt kill someone on children‚Äôs TV.

 











Phil Harnage
Magic brings a whole different ambiance to a cartoon, because it‚Äôs something you can do that‚Äôs not repeatable by kids. You can shoot a lightning bolt and turn someone into toast. And the toast gets up and walks away. You just have to be careful ‚Äî you can‚Äôt do everything you want to do. You can‚Äôt do anything that could be copied by a child. You don‚Äôt want kids sword fighting.

 











Bob Forward
Link‚Äôs sword could fire like a ranged weapon. Actually hitting people with swords was questionable. It wasn‚Äôt something they wanted to do back then. It was easier to just shoot zaps from the sword. We also had to establish that nobody was dying, so there was the jar of evil or something, where everyone hit by zaps were sent to and got put into storage for a while. We had to downplay a lot of things.

 









Image: DiC Entertainment/Nintendo



One and done
While Super Mario Bros. Super Show had tons of episodes, The Legend of Zelda has only one season. That‚Äôs the way of TV cartoons ‚Äî things get canceled and people move on. The Legend of Zelda itself has gone on to be one of Nintendo‚Äôs most successful properties, but the TV show is still a small part of that legacy.
 











Reed Shelly
The show feels like a time capsule to me. It‚Äôs such a different world now and so different for kids. The shows were made for a different era.
It was an incredible creative playground. We had to deliver 52 episodes at a rate of four a week [for the Super Mario. Bros Super Show]. We had live action, animation, and an action sequence set to a well known song. It was an amazing production circus to be a part of.
With Andy Heyward and Haim Saban executive producing and running the shows, we were allowed to have a ton of fun. All we had to do was make millions of kids laugh.

 











Eve Forward
I‚Äôve no idea what the reception to the show was. This was in the days before internet; you couldn‚Äôt just log in and see your work torn apart in real time. My own feeling is that the Super Mario Bros. show wasn‚Äôt very good, especially the live-action bits, and that Zelda was the best part of it, but y‚Äôknow, it was a cartoon, for kids. We weren‚Äôt trying to make Citizen Kane or something. But of course it was a huge thrill for me to see my work on television!

 











Phil Harnage
Part of the reason [the show was canceled] is that it wasn‚Äôt its own show ‚Äî it was part of the Mario Bros. show. It was tied to it, and they didn‚Äôt want to renew The Mario Bros., and Zelda got shuffled off. History, at that point. I wish I had done more. We could have come up with a lot more shows. That was the sad part, that we only got to do one season.
I think the show holds up pretty well after all these years. They‚Äôre all on YouTube. [Ed. note: And Amazon Prime Video!] I don‚Äôt know if you know this, but we don‚Äôt get residuals.
Everybody wishes that Link and Zelda had gone on to bigger and better things [with the TV show], but they didn‚Äôt. You have these regrets about every show you do. Sometimes you wish you could have done more, that you could do more, but there were certain things you had to do to please the network.
We got a lot of good feedback from kids, and even older kids who knew the video game. They would watch the show out of curiosity and get sucked in. We had a few letters saying, ‚ÄúOh, please don‚Äôt cancel it!‚Äù But getting a few letters isn‚Äôt enough to convince the network. They‚Äôre the boss, because they funded the things. DiC, the studio I worked for at the time ‚Äî they were known for finding the current properties they could exploit. They were purely in the business to make money, like all the studios.

 











Jonathan Potts
I‚Äôm always surprised at how much notoriety it has. I don‚Äôt think it was a hit at the time, because then we would have done more. We did it years ago, and it was one season with 13 episodes. It was a one-and-done sort of thing. It had its time, and it just keeps growing. I get letters from all over the world. 
I was teaching voice classes at Second City, and the class would be people in their 30s, and the engineer would look at me like, Go ahead, tell them. And I‚Äôd say, ‚ÄúYou know, I was the voice of Link in The Legend of Zelda,‚Äù and inevitably, three or four people would be like‚Ä¶ I became a celebrity. I can‚Äôt believe that. It was just a gig years ago.
They would be so starstruck, which is a joke, because I‚Äôm not a star. But they‚Äôd get ‚Ä¶ [imitates expression of amazement]

 











Cynthia Preston
It came up [at a party] that I was the voice of Zelda in the cartoon, and [people] were so stunned. They rolled up their shirt sleeves, and they both have the Triforce tattooed on their arms. I‚Äôve been at pitch sessions and somebody will find out that I‚Äôm the voice of Zelda, and the reaction is astounding. People love it so much. 

 











Reed Shelly
I remember on my first trip to Redmond and the Nintendo headquarters, they had a couple of hundred ‚Äúgame counselors‚Äù in a call center at computers giving tips to gamers calling in. It cost, as I remember, something like 99 cents a minute for players to get game tips. When a group got to go on their lunch break, they raced each other to play the newest arcade console game in the cafeteria. I remember thinking, ‚ÄúThis computer gaming thing is gonna be big...‚Äù 




"
https://news.ycombinator.com/rss,Twitter API Page,https://developer.twitter.com/apitools,Comments,"




Twitter / Error















This page is down
I scream. You scream. We all scream... for us to fix this page. We‚Äôll stop making jokes and get things up and running soon.
Retry




Home
Status
Terms of Service
Privacy Policy
Cookie Policy
Imprint
Ads info
¬© Twitter ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ






"
https://news.ycombinator.com/rss,1991: A server-side web framework written in Forth,https://www.1-9-9-1.com/,Comments,"


World Wild Web

                        The year is 1991. The World Wide Web has just seen public release. 1991 looks to ease your interactions with the new web using cutting edge programming techniques in Forth (well, Gforth).
                    


Logging In

                        Getting started in 1991 is easy.
                    

                        All you need to do is include 1991.fs into your Forth source file. Next, you can define your public routes using the /1991 word. Once your routes are all layed out, start the server using 1991:.
                    

\ app.fs
\ Load 1991.
include 1991.fs

\ Define our route handlers.
: handle-/ ( -- addr u )
    \ Any string returned by the handler
    \ will be output to the browser.
    s"" Hello, 1991."" ;

\ Set up our routes.
/1991 / handle-/

\ Start the server on port 8080.
8080 1991:

You can run the server using gforth app.fs.
Logging In II: Logging In, Deeper
Route Wildcards (Fuzzy Routing / URL Mapping)

                        If you want to specify that some part of a route is a wildcard (accepts any value), then you can wrap some named value in <chevrons>. 1991 will accept any URL that matches your wildcard pattern, setting the internal value of whatever you place between the chevrons to whatever is actually requested.
                    

                        In the example below, <uid> specifies that we're willing to accept any (non-empty) value in its place which we'd like to access using the name uid.
                    

\ wildcards.fs
\ Load 1991.
include 1991.fs

\ Define our route handler.
: handle-wildcard-route ( -- addr u )
    s"" contents of the route request: "" get-query-string s+ ;

\ Set up our route.
/1991 /users/<uid> handle-wildcard-route

\ We can set up multiple wildcards too (must be slash-separated).
/1991 /users/<uid>/posts/<pid> handle-wildcard-route

\ Start server on port 8080.
8080 1991:


                         All wildcards are treated similar to query string arguments. As such, wildcards can be retrieved using get-query-string.
                    

                        In the example above, visiting http://localhost:8080/users/urlysses will result in the following query string: uid=urlysses.
                    File Serving

                        Use a public/ directory to act as a basic fileserver.
                        Whenever a requested URL doesn't resolve through the registered routes, 1991 will attempt to find the requested route within your specified public directory.
                    

\ public.fs
\ Load 1991.
include 1991.fs

\ Specify the location of our public directory.
\ Anything in the public/ directory within the
\ same dir as this source file will resolve.
\ You can change ""public"" to anything you want
\ as long as it matches your directory name.
sourcedir s"" public"" s+ set-public-path

\ We can set mimetypes using the `filetype:` word.
\ In the case below, we want .mp4 files to be served
\ with the content-type video/mp4.
s"" video/mp4"" filetype: mp4

\ Start the server on port 8080.
8080 1991:


                        In the above example, If we have a file public/my-video.mp4, then it will be available through http://localhost:8080/my-video.mp4.
                    
Views

1991 offers basic templating through views.
                    

                        In order to get started, you should specify the views/ path. Notice the trailing slash, which differs from how we define public.
                    

                        Once you've specified your views/ directory, you can write views/ files to it. This can be any kind of file, honestly. The benefit offered by views/ is the ability to use basic templating. You can write any valid Forth code within opening (<$ ) and closing ( $>) tags. Additionally, you can use the import word to import other views into your view.
                    

\ views.fs
\ Load 1991.
include 1991.fs

\ Specify the location of our views directory.
sourcedir s"" views/"" s+ set-view-path

\ Define some words we'll use within
\ our view.
: page-title ( -- addr u )
    s"" Dynamic page title"" ;
: ten-lines ( -- )
    10 0 do
        s"" line "" i s>d <# #s #> s+
        s"" <br>"" s+
        $type
    loop ;

\ Use render-view to output the contents
\ of a file in the views/ directory.
: handle-/
    s"" v-index.html"" render-view ;

/1991 / handle-/

\ Start the server on port 8080.
8080 1991:


\ views/index.html
<!DOCTYPE html>
<html>
    <head>
        <meta charset=""utf-8"">
        <title><$ page-title $type $></title>
    </head>
    <body>
        <$ ten-lines $>
        <$ s"" imported-view.html"" import $>
    </body>
</html>


\ views/imported-view.html
It's possible to import view files from within other view files. This is from <code>views/imported-view.html</code>



Wait, what?
Why is 1991: post-fix when /1991 is pre-fix?

                        Forth is a (mostly) post-fix notation language. So, for example, you'd write two plus two as 2 2 +. This is the language's natural and immediate notation. Along those lines, 1991: is an immediate word‚Äî‚Äîrunning it results in immediate action. As such, we use Forth's post-fix notation to set the port and start the server immediately. Alternately, /1991 doesn't exactly have immediate effect per se. All it does is tell 1991 that any request to /path should be handled by path-handler. As such, we opt to write non-immediate code using pre-fix notation.
                    
You're using Gforth, which came out in 1992. Also, it's 2017.
Okay. But Fredric Jameson establishes that in postmodernism we have experienced a weakening sense of historisity such that what is, what was, and what will be all exist as presents in time. 1970, 1991, 1992, and 2017 all happen simultaneously. Hence developers working on new projects while still coding in decades-old text editors. They write the future in the past and are made present in so doing.


"
https://news.ycombinator.com/rss,NASA‚Äôs Double Asteroid Redirection Test Is a Smashing Success,https://eos.org/articles/nasas-double-asteroid-redirection-test-is-a-smashing-success,Comments,"

Posted inNews 
			NASA‚Äôs Double Asteroid Redirection Test Is a Smashing Success		

			The mission, focused on the Didymos-Dimorphos binary asteroid system, proved that an asteroid‚Äôs orbit can be altered by kinetic impactor technology.		




by
Katherine Kornei 
12 January 202312 January 2023 
Share this:Print 



 This illustration of NASA‚Äôs Double Asteroid Redirection Test (DART) spacecraft and the Italian Space Agency‚Äôs LICIACube depicts them just prior to impact at the Didymos binary system on 26 September 2022. Credit: NASA/Johns Hopkins APL/Steve Gribben





Rocks from space have walloped Earth for eons, and it‚Äôs only a matter of time until our planet lands yet again in the crosshairs of a very large asteroid. But unlike other forms of life‚Äîhere‚Äôs looking at you, dinosaurs‚Äîhumans have a fighting chance of altering our cosmic destiny. At AGU‚Äôs Fall Meeting 2022 held in December, researchers presented a slate of new results from NASA‚Äôs Double Asteroid Redirection Test (DART) mission, the first demonstration of asteroid deflection.
Peering at an Orbit
DART‚Äôs target, the Didymos-Dimorphos asteroid system, was first discovered in the mid-1990s. But astronomers back then spotted only its larger member, Didymos, which is roughly 800 meters (half a mile) in diameter. It wasn‚Äôt until 2003 that scientists realized that a much smaller body, dubbed Dimorphos, was also present. Dimorphos is about one fifth the size of Didymos, and its orbit takes it in front of and behind Didymos as seen from Earth. That‚Äôs serendipitous, because by monitoring how the brightness of the Didymos-Dimorphos asteroid system varies over time, scientists were able to precisely determine how long it took Dimorphos to complete an orbit: 11 hours and 55 minutes.
‚ÄúWe needed to understand the Didymos-Dimorphos system before we changed it.‚Äù
‚ÄúWe needed to understand the Didymos-Dimorphos system before we changed it,‚Äù said Cristina Thomas, a planetary scientist at Northern Arizona University in Flagstaff, at AGU‚Äôs Fall Meeting 2022.
 



This newsletter rocks.
Get the most fascinating science news stories of the week in your inbox every Friday.

Sign up now



The primary goals of the DART mission were simple, at least in concept: Hit Dimorphos with the roughly 570-kilogram (half-ton) DART spacecraft to alter the orbital period of Dimorphos around Didymos significantly and measure that change and characterize the physics of the impact. If successful, it would be the first demonstration of deflecting an asteroid using so-called kinetic impactor technology. (In 2005, another NASA mission, Deep Impact, tested kinetic impactor technology with a comet.)
On 23 November 2021, a Falcon 9 rocket lifted off from California‚Äôs Vandenberg Space Force Base. By then, the SpaceX-designed rocket had notched more than 100 successful launches, but for members of the DART mission, the event was anything but ordinary: Nestled within the rocket‚Äôs nose cone was the spacecraft they‚Äôd spent well over a decade designing, building, and testing.
The launch went smoothly, and DART soon entered into orbit around the Sun. For roughly 10 months, the spacecraft largely tracked the orbit of Earth, essentially waiting to catch up to the Didymos-Dimorphos asteroid system, which orbits the Sun between Earth and Mars. ‚ÄúWe stayed close to Earth the entire time and just caught up with the Didymos system at its closest approach to Earth,‚Äù said Elena Adams, DART mission systems engineer at the Johns Hopkins University Applied Physics Laboratory in Laurel, Md.
Approaching the Unknown
It was only around July of 2022 that DART‚Äôs onboard camera‚Äîthe Didymos Reconnaissance and Asteroid Camera for Optical navigation (DRACO)‚Äîcaught its first glimpse of Didymos. But Dimorphos wouldn‚Äôt come into view until much, much later: Just an hour before impact, at a distance of roughly 25,000 kilometers, the tiny moonlet was still a mere two pixels across in DRACO images.
‚ÄúWe didn‚Äôt see Dimorphos until late in the game,‚Äù said Adams. To prepare for the uncertainties of impacting a body they knew virtually nothing about, DART team members ran thousands of Monte Carlo simulations beforehand in which they varied the moonlet‚Äôs size, shape, albedo, and a slew of other parameters.
The DART spacecraft successfully impacted Dimorphos on 26 September 2022. The event was recorded by a cadre of Earth-based telescopes and also the Light Italian Cubesat for Imaging of Asteroids (LICIACube), a briefcase-sized spacecraft carrying two cameras that launched with DART and was released from the spacecraft 15 days prior to impact.
A Serendipitous Boost
Researchers had calculated that the impact, which occurred roughly head-on, would shorten Dimorphos‚Äôs orbital period by just under 10 minutes. That was assuming the simplest case of no ejecta being produced, said Andy Cheng, DART investigation team lead at the Johns Hopkins University Applied Physics Laboratory, at a press conference.
‚ÄúThe amount of momentum that you put in the target is exactly equal to the momentum that the spacecraft came in with.‚Äù But if ejecta flies off the asteroid after impact, physics dictates that the asteroid can get an extra boost, said Cheng. ‚ÄúYou end up with a bigger deflection.‚Äù
‚ÄúIf you‚Äôre trying to save the Earth, that makes a big difference.‚Äù
That‚Äôs good news when it comes to pushing a potentially harmful space rock out of the way, said Cheng. ‚ÄúIf you‚Äôre trying to save the Earth, that makes a big difference.‚Äù
And ejecta there was, in spades‚Äîon the basis of detailed follow-up observations of the Didymos-Dimorphos system, scientists discovered that Dimorphos is now traveling around Didymos once every 11 hours and 22 minutes. That‚Äôs a full 33 minutes shorter than its original orbital period, a finding that implied that a substantial amount of ejecta was produced. Imagery obtained from ground- and space-based telescopes has borne that out‚Äîa plume of debris tens of thousands of kilometers long currently stretches out from Dimorphos. Researchers have estimated that at least a million kilograms (1,100 U.S. tons) of material were blasted off the asteroid by the impact. That‚Äôs enough debris to fill several rail cars, said Andy Rivkin, DART investigation team lead at the Johns Hopkins University Applied Physics Laboratory, at a press conference at the Fall Meeting.
Follow the Debris
Interestingly, the ejecta shed by Dimorphos has remained in distinctly more plumelike configurations than the debris shed by comet 9P/Tempel 1 when NASA‚Äôs Deep Impact spacecraft intentionally crashed into it in 2005. ‚ÄúThe Dimorphos ejecta has a lot of morphological features,‚Äù said Jian-Yang Li, a planetary scientist at the Planetary Science Institute in Fairfax County, Virginia, and a member of the DART team, at the Fall Meeting.
The reason is probably the different compositions and surface features of the two bodies, he said. Tempel 1 is rich in volatiles and fine-grained dust; Dimorphos‚Äôs surface, on the other hand, is littered with boulders. Scientists plan to continue to monitor Dimorphos‚Äôs debris plume through at least March.
The DART mission has also enabled scientists to investigate a fundamental question about the Didymos-Dimorphos asteroid system: Do the two asteroids have the same composition? It‚Äôs a common assumption when it comes to binary asteroids, but it‚Äôs never been confirmed. Thomas, leader of the DART Observations Working Group, presented new results on the subject at a press conference at the Fall Meeting. She shared near-infrared spectra of the binary asteroid system that astronomers had collected both before and after impact using a NASA telescope in Hawaii.
Observations obtained prior to impact (when the overwhelming majority of the sunlight reflected off the asteroid system came from Didymos) and after impact (when the debris shed by Dimorphos was responsible for more than two thirds of the reflected light) revealed very similar spectra, with characteristic dips at wavelengths of 1 and 2 micrometers in both cases. That‚Äôs strong evidence that the two asteroids have similar compositions, said Thomas.
Scientists aren‚Äôt yet finished with Didymos and Dimorphos: In 2024, researchers involved in the European Space Agency‚Äôs Hera mission plan to launch a spacecraft to the system to further characterize the asteroids‚Äîincluding accurately measuring the mass of Dimorphos‚Äîand to study the crater created by the DART impact.
‚ÄîKatherine Kornei (@KatherineKornei), Science Writer
Citation: Kornei, K. (2023), NASA‚Äôs Double Asteroid Redirection Test is a smashing success, Eos, 104, https://doi.org/10.1029/2023EO230010. Published on 12 January 2023.
Text ¬© 2023. The authors.¬†CC BY-NC-ND 3.0Except where otherwise noted, images are subject to copyright. Any reuse without express permission from the copyright owner is prohibited.
RelatedAre We Prepared for an Asteroid Headed Straight to Earth?NASA's New Asteroid Sampler Will Illuminate Solar System's HistoryExploring Planetary Breadcrumbs One Asteroid at a Time 

Tagged: #AGU22,¬†asteroids,¬†NASA,¬†orbits & rotations,¬†solar system,¬†Space & Planets 









"
https://news.ycombinator.com/rss,Janet Malcolm on the Stand,https://www.nplusonemag.com/online-only/book-review/malcolm-on-the-stand/,Comments,"         Malcolm on the Stand | Online Only | n+1 | Max Abelson                                      Sign InYour AccountHomeMagazineOnline OnlyBookstoreEventsDonateSubscribeFull
 NavigationSign In to n+1    Forgot Password Subscribe NowCloseSearch   Online Only February 3, 2023A discussion of The Feeling SonnetsEventsEugene Ostashevsky and Genya Turovskaya in conversation  January 6, 2023Film ReviewAn Entire Society Exists Within MeMark Krotov  December 24, 2022Online OnlyCouscous and ChickenNicholas Hamburger   December 23, 2022Online OnlyA Strike DiaryKyle McCarthy   Book ReviewMax AbelsonMalcolm on the StandShe is cutting, wary, funny, and wise. Her style is what I wish I had instead of the chipper inner voice I‚Äôm stuck with. Nothing in Malcolm‚Äôs writing is dull or amiss unless she‚Äôs quoting somebody else. Her lines put me in mind of the painter Agnes Martin‚Äîeverything so even and tight.To read Malcolm is to be moved by the clarity of her journalism‚Äîand warned, again and again, that the form is no good    January 10, 2023  Tags Reading, Writing, and Publishing Reviews  Share and Save Twitter Facebook Google
 Plus    Instapaper Email NewsletterGet n+1 in your inbox. Email Address    Janet Malcolm: The Last Interview and Other Conversations. Introduction by Katie Roiphe. Melville House, 2022.In the few times Janet Malcolm let other reporters interview her, she did what she could to keep herself safe.‚ÄúDoing this interview by email gives me a chance to think of answers to your questions,‚Äù Malcolm wrote to the Believer in 2004. ‚ÄúIf we did it in person, I might just look at you in blank helplessness.‚Äù She invited a Paris Review interviewer over to her Gramercy Park apartment seven years later, but didn‚Äôt answer the questions until typing on her desktop Mac.Malcolm, who died in 2021 at 86, was as attuned as anyone to the dangers‚Äîmalice, betrayal, misunderstanding‚Äîof a tape recorder clicking on. The monster in her masterpiece, The Journalist and the Murderer, isn‚Äôt the man convicted of killing his family but the bestselling author he took to court for publishing a tell-all; what haunts The Silent Woman, her book about Sylvia Plath, isn‚Äôt the poet‚Äôs suicide or Ted Hughes but the couple‚Äôs biographers. To read Malcolm‚Äôs decades of work for the New Yorker is to be moved by the clarity of her journalism‚Äîand warned, again and again, that the form is no good.There are few reporters you‚Äôd rather see on the other side‚Äîthe wrong end‚Äîof a Q&A. That‚Äôs where we find her in Janet Malcolm: The Last Interview and Other Conversations, a compilation of her exchanges with critic Nan Goldberg for Salon in 2001, novelist Daphne Beal in the Believer, Canadian radio host Eleanor Wachtel in 2008, writer Katie Roiphe in the Paris Review, and the New York Times Book Review in 2019. At their best, the transcripts channel and help explain Malcolm‚Äôs mesmerizing journalism, only the tables are turned. Reading the interviews has the perverse quality of seeing a judge on trial or your analyst in therapy.Often, though, it‚Äôs a polite book‚Äîone in Melville House‚Äôs series of ‚Äúlast‚Äù interviews with interesting people‚Äîthat chooses the wrong times to go soft. Malcolm, who knew the subjects of journalism are always ‚Äúastonished when they see the flash of the knife,‚Äù doesn‚Äôt even get nicked. The friction that‚Äôs missing here is what electrifies not just Malcolm‚Äôs writing but the record of what happened when she was actually put on the hot seat. When, decades ago, the star of one of her books sued her for libel, taking her to the Supreme Court and then to two trials, Malcolm‚Äîin front of a jury‚Äîgave the moral accounting these interviews avoid.What makes Malcolm‚Äôs reporting unusual, besides the trouble it caused her, is how much fun it is to be in her company on the page. She is cutting, wary, funny, and wise. Her style is what I wish I had instead of the chipper inner voice I‚Äôm stuck with. Nothing in Malcolm‚Äôs writing is dull or amiss unless she‚Äôs quoting somebody else. Her lines put me in mind of the painter Agnes Martin‚Äîeverything so even and tight.I like the way Beal puts it in the introduction to the Believer interview: ‚ÄúWhat grabs and re-grabs the reader in her writing is its deft commingling of sleuthing and contemplation,‚Äù she writes. ‚ÄúReading Malcolm, one has the sensation of being in the presence of a mind constantly in action on several levels, mediating between external reality (one most often consisting of facts that are at odds with one another) and her own consciousness.‚ÄùFor reporters, Malcolm offers even more than just a guidebook to craft. She‚Äôs a tuning fork whose pitch tells the rest of us when we‚Äôve fallen flat or drifted sharp. That‚Äôs because of the clarity of her writing‚Äî‚Äúvanity, hypocrisy, pomposity, inanity, mediocrity‚Äù is in The Journalist and the Murderer with ‚Äútenderness, sensitivity, judgment, warmth‚Äù as well as ‚Äúambiguity, obscurity, doubt, disappointment, compromise, and accommodation‚Äù‚Äîand how in touch it is with what‚Äôs going on, how it comes together, and why it sometimes falls apart. When I read Malcolm I‚Äôm like Roiphe greeting her for their Paris Review interview: ‚ÄúAround her it is hard not to feel large, flashy, blowsy, theatrical, reckless.‚ÄùMalcolm was born Jana Klara Wienerov√° in Prague in July 1934. Five years later, after Hitler had marched through the city, her family escaped on one of the last civilian ships to America from Europe before the war. Her father became a successful New York psychiatrist‚Äîin these interviews she swears she ‚Äúpaid little attention to my father‚Äôs work‚Äù and that psychoanalysis ‚Äúhas had curiously little influence‚Äù on her style, but one can‚Äôt quite believe her. At the University of Michigan, she wrote for the newspaper, edited its humor magazine, and met her first husband. They both went on to write for the New Yorker, and after he died she married her editor, whose stepfather‚Äôs yeast fortune helped fund the magazine. It wasn‚Äôt until she gave up cigarettes in the late¬†‚Äô70s that Malcolm did her first long piece of journalism: ‚ÄúI figured that by the time I finished the reporting I would be ready to try writing without smoking.‚ÄùIf you want Malcolm at her most rabbinical, there‚Äôs Reading Chekhov, her underrated and atmospheric meditation on the Russian genius from 2001. For momentum that crime writers would kill for, she has Iphigenia in Forest Hills, a 2011 thriller about an Orthodox Jewish woman on trial for murder. Her stories about psychotherapy have the sweet swing of sportswriting, and her work about the law, too, is riveting and deep. Malcolm‚Äôs ear and eye‚Äîand unusual sense of structure‚Äîare most dazzling in her writing about art, especially ‚ÄúForty-one False Starts,‚Äù a portrait of the painter David Salle that begins again and again, and ‚ÄúA Girl of the Zeitgeist,‚Äù a profile of the editor Ingrid Sischy that waits and waits to find her. The shapes of the pieces convey so much about their subjects that a reader can‚Äôt help but feel both are also about the machinery of journalism.There are different forms of Malcolm on the page. The observant and sometimes cold journalist who asks her subjects unsettlingly short questions isn‚Äôt quite the same figure as the ingenious narrator whose essayistic contemplation radiates generosity. Then there‚Äôs the small and sometimes anxious woman who emerges to play key plot roles, especially in Iphigenia.The Malcolm we encounter in The Last Interview shares the self-awareness, briskness, clarity, and humor she possesses elsewhere: ‚ÄúI walk fast and am impatient. I get bored easily,‚Äù Malcolm tells the Paris Review. ‚ÄúI often get stuck. Then I get sleepy and have to lie down. Or I make myself leave the house‚Äîwalking sometimes produces a solution. The problem is usually one of logic or point of view. I keep regular morning hours. The first hour is the most productive one.‚Äù Worrywart writers will find much to love in Malcolm‚Äôs description of herself: ‚ÄúThe machinery works slowly and erratically and I am always a little nervous about it, though by now I‚Äôm pretty used to it,‚Äù she emails. ‚ÄúI guess I trust it more.‚ÄùBut where Malcolm the journalist is unsparing and direct, Malcolm the interviewee is ultimately vague and evasive. ‚ÄúWhat‚Äôs true? Is it possible to know what‚Äôs true?‚Äù Goldberg asks in the Salon interview. Malcolm answers: ‚ÄúI‚Äôd love to hear you talk about it rather than me.‚Äù‚ÄúWhy do you think the subject of betrayal is something that‚Äôs your subject?‚Äù Wachtel asks her on the radio. ‚ÄúThat‚Äôs very interesting,‚Äù Malcolm says. ‚ÄúYou‚Äôre kind of putting me on the spot.‚ÄùInterviews aren‚Äôt the same as sworn testimony, but they rhyme. They use questions to flatter, badger, and trap witnesses who, in turn, evade when they can and admit things they don‚Äôt want to. Reporting and the law both rely on evidence and discovery, asking for honesty and promising fairness in exchange. They offer just about the best systems we have for hearing arguments, measuring doubt, rendering judgment, and appealing verdicts‚Äîexcept, maybe, for psychoanalysis. All three approaches use confrontation to turn ambiguity into clarity, but only one can punish an outburst or lie by locking the speaker away.The long and famous case against Malcolm began in 1984, after she published a New Yorker profile about a lawsuit from a star scholar named Jeffrey Moussaieff Masson against the ‚Äã‚ÄãSigmund Freud Archives. It was also a vibrant portrait of a charming and ambitious heel. Masson accused Malcolm in the Washington Post of misquoting him ‚Äúany number of times.‚Äù (The newspaper reporter, a young David Remnick, wrote that Malcolm was away on vacation in Italy and couldn‚Äôt be reached for comment.) When Malcolm expanded the piece into a book, In the Freud Archives, Masson complained again, in a letter to the New York Times. The response she sent offered to play the tapes of their conversations for Times editors ‚Äúwhenever they have 40 or 50 short hours to spare.‚Äù Masson sued her for libel soon after.His case began terribly. Many of the quotes he had denied saying turned out to be on her tape. Others really were missing, though, and Malcolm had an unusual story: she had tripped over her recorder‚Äôs cord before a morning interview, took notes instead, typed them up, and misplaced the originals. But a judge decided Malcolm had reasonably interpreted whatever Masson had actually said and threw out the suit. Just after Malcolm published The Journalist and the Murderer, the Supreme Court agreed to hear Masson‚Äôs appeal. Anthony Kennedy, joined by six other justices, wrote that a misquote has to hit the reader‚Äôs mind differently than the right one would to cross the line into libel. It would be up to a jury to decide if Malcolm‚Äôs writing and Masson‚Äôs words shared ‚Äúthe substance, the gist, the sting.‚Äù (When the Times asked for comment, Malcolm was back in Italy.)Inside a federal court in California in May 1993, Malcolm took the stand. Masson‚Äôs lawyer, a former quarterback and Air Force navigator named Charles O. Morgan Jr., asked Malcolm about a key monologue at the Berkeley restaurant Chez Panisse. ‚ÄúYou reported in the article that the entire statement was made by Mr. Masson at lunch,‚Äù Morgan asked.1‚ÄúYes,‚Äù Malcolm said.‚ÄúAnd that is not true.‚Äù‚ÄúThat‚Äôs right,‚Äù Malcolm said. She had compressed conversations over seven months into one monologue, she told the jury, using them like ‚Äúsketches incorporated into one painting.‚Äù Anything else, she testified, would be foolish: ‚ÄúI do not want to write the exact words, I do not want to write a transcript,‚Äù she went on. ‚ÄúThis thing called speech is sloppy, redundant, repetitious, full of uhs and ahs.‚Äù When that line surfaces in the introduction to The Last Interview, Roiphe cites it admiringly, praising Malcolm for improving on the ‚Äúcasualness and mediocrity of expression‚Äù by ‚Äútrimming and shaping.‚Äù Morgan, the attorney, argued otherwise.‚ÄúDo you call that rearranging events?‚Äù he asked.‚ÄúI don‚Äôt know what that means.‚Äù‚ÄúDo you call it creating a conversation?‚Äù‚ÄúI wouldn‚Äôt put it that way, no.‚ÄùBefore the trial ended, Malcolm testified that the chaos and contradiction of speech had forced her hand: ‚ÄúHe‚Äôs trying to tell too many things at the same time. You had to work hard to get the story straight because he was all over the place.‚ÄùJurors decided for Masson. But they couldn‚Äôt agree on damages, so the judge announced they‚Äôd start the whole thing over. In one of her most gripping pieces, published in the New York Review of Books months before her death, Malcolm recalls the makeover she gave herself with a speech coach before returning to the stand. Back in the courthouse, she gave ‚Äúa long speech about the monologue technique that Morgan kept interrupting but was unable to stop,‚Äù Malcolm writes. ‚ÄúI went relentlessly on and on. I talked about the difference between the full and compelling account of his rise and fall in the Freud Archives that Masson gives in the article and his wandering incomplete speech in the restaurant. I spoke of the months of interviews out of which, bit by bit, the monologue was formed. I concluded by saying, ‚ÄòI have taken this round-about way of answering your question, Mr. Morgan, because I wanted the jury to know how I work, and what we‚Äôre talking about here in talking about this monologue.‚Äô‚ÄùAfter one kind of Malcolm monologue about another, the jury dismissed concerns about three of the five quotes in question. But jurors decided the two others‚Äîabout the sterility of psychoanalysis and Masson‚Äôs bosses‚Äîwere false, and that the latter qualified as defamation. They also decided, though, that Malcolm hadn‚Äôt been reckless enough to cross the legal line of libel. She won and wept.What happened a year later gets more attention in the book of her interviews than her testimony. ‚ÄúI was in my country house, and there was something red on the floor, and I picked it up, this red notebook. My granddaughter had seen something red in the bookcase and pulled it out, and there were the notes‚Äù‚Äîthe ones she took after tripping over the recorder‚Äôs cord. ‚ÄúI felt like I was going to faint.‚Äù If only she had found them earlier, she tells Wachtel, ‚Äúthe whole thing could‚Äôve been avoided.‚Äù She emails the story to the Believer, too: ‚ÄúThe jury had decided to believe me anyway. But if the notebook hadn‚Äôt got misplaced, there would have been no lawsuit.‚ÄùThe fact is that Malcolm‚Äôs accounts, as she liked to write about other people, ‚Äúdon‚Äôt add up.‚Äù The missing red notebook had three of the quotes in question, but not the two that had bothered the jury. ‚ÄúDo we ourselves add up?‚Äù Goldberg asks Malcolm in the Salon interview.‚ÄúNo,‚Äù Malcolm answers. ‚ÄúOf course we don‚Äôt.‚ÄùThat someone so thoughtful about the moral perils of journalism could be messy enough to collage scenes together‚Äîthen blithe enough to testify it was all for the best and deluded enough to say a missing notebook explained it all‚Äîis a paradox that we Malcolm fans have to live with. It‚Äôs also, of course, a version of what her work was warning about. Whether her shortcomings bring her journalism to life or do something more like undermine it is the kind of thing you would want to ask her, right before running out of the room.Roiphe‚Äôs introduction to The Last Interview avoids Malcolm‚Äôs mysteries and messes. It‚Äôs less about the morals or machinery of her journalism than what it was like to be her bud. ‚ÄúWhen a friend texted me that Janet Malcolm had died, I experienced more than the usual amount of disbelief,‚Äù it begins. ‚ÄúThis is one of the conversations I wish I‚Äôd had with Janet herself at Choshi,‚Äù Roiphe writes later, ‚Äúthe now-closed sushi place she favored around the corner from her house. I know she would have had thoughts on it.‚Äù If you‚Äôre in a generous mood, you can read the intro as a kind of pun on Malcolm‚Äôs interest in the ‚ÄúI‚Äù who narrates literary nonfiction: ‚ÄúI wonder,‚Äù ‚ÄúI confessed,‚Äù ‚ÄúI suggested,‚Äù ‚ÄúI fixate,‚Äù I understand,‚Äù ‚ÄúI hear,‚Äù Roiphe writes, and then ‚ÄúI love‚Äù three times in a row.It all comes to a crescendo with a sort of Freudian slip. ‚ÄúI have always used her writing to teach confidence,‚Äù she writes, meaning to refer to Malcolm‚Äôs authoritativeness but channeling the line that follows The Journalist and the Murderer‚Äôs famous opening: ‚ÄúHe is a kind of confidence man, preying on people‚Äôs vanity, ignorance, or loneliness, gaining their trust and betraying them without remorse.‚ÄùThe first of the interviews, with Salon, opens with the kind of reportorial antagonism I found myself missing as the book went on: If journalists are murderers, Goldberg asks, why was Malcolm speaking to one? Malcolm twists away by complimenting the question and pointing out she used to avoid reporters entirely. ‚ÄúWhen the book came out and people wanted to ask me questions, I said, ‚ÄòWell, read the book.‚Äù‚ÄúI did,‚Äù Goldberg answers, standing her ground. ‚ÄúThat‚Äôs why I‚Äôm asking.‚Äù Instead of saying why she agreed to the interview, Malcolm explains why she shouldn‚Äôt have: ‚ÄúI‚Äôm just not very good at it. I often have no answers to the questions; I think of the answers later.‚ÄùIt‚Äôs a moment when the tension and confrontation that the book mostly suppresses manages to leak out, but not the only one. In the Believer, when Beal dares to ask if getting sued changed her approach with subjects, Malcolm shows her teeth: ‚ÄúUntil this moment you were the first interviewer who did not bring Jeffrey Masson into the discussion. I guess that isn‚Äôt possible after all.‚Äù You again get the sense that the nastiness of journalism was only fun for Malcolm to consider when it was someone else‚Äôs.Beal backs off: ‚ÄúSorry to be so tiresome,‚Äù she writes back. ‚ÄúJust like the rest.‚Äù The exchange ends soon after, but, following a line break, an italicized note says Malcolm read the transcript and sent this in an email: ‚ÄúI read the interview in the way one looks at photographs of oneself, and, except for one place, I thought I came out looking okay. But the exception may be the most interesting part of the interview.‚Äù It‚Äôs Masson. ‚ÄúUntil that moment the atmosphere of the interview is friendly and collegial, almost conspiratorial. Now it turns icy.‚Äù Malcolm goes on: ‚ÄúWhat is most interesting about this moment in our interview is the illustration it offers of a subject‚Äôs feeling of betrayal when he or she realizes that the journalist is writing his or her own story.‚ÄùIf a goal of Malcolm‚Äôs journalism was to measure the distance between the stories that reporters shape and the ones subjects tell about themselves, this book makes the mistake of letting her maintain almost complete control of the tape‚Äîor, in the case of Roiphe‚Äôs Paris Review interview, the notebook. ‚ÄúI want to talk about that moment in our meeting at my apartment last week, when I left the room to find a book and suggested that while I was away you might want to take notes about the living room for the descriptive opening of this interview,‚Äù Malcolm writes in an email that‚Äôs quoted in the interview‚Äôs introduction. ‚ÄúYou obediently took out a notebook, and gave me a rather stricken look, as if I had asked you to do something faintly embarrassing.‚Äù Malcolm fills it in for her in the interview: ‚ÄúMy living room has an oakwood floor, Persian carpets, floor-to-ceiling bookcases, a large ficus and large fern, a fireplace with a group of photographs and drawings over it, a glass-top coffee table with a bowl of dried pomegranates on it, and sofas and chairs covered in off-white linen.‚ÄùThat‚Äôs not to say deference is so terrible. Even though the ‚ÄúBy the Book‚Äù feature in the Times only asks Malcolm about reading, she looks around her bookshelves and spots ‚Äúpetulant desperation,‚Äù ‚Äúwild terrain,‚Äù ‚Äúthe eye of eternity,‚Äù and on top of her night table ‚Äúa box of Kleenex, a two-year-old Garnet Hill catalog and a cough drop on it.‚ÄùBut you find yourself yearning for confrontation and catharsis. Did she, in her later writing, stick with the collage method she defended on the stand, or was it sublimated into the delicate collages she started making‚Äîand exhibiting at art galleries‚Äîfrom handwriting, typewritten papers and book pages?Salon asks this: ‚ÄúDo you see any relationship between your collages and your writing?‚Äù‚ÄúI think so,‚Äù Malcolm answers. ‚ÄúI like to think about my work as kind of collage-like.‚Äù There isn‚Äôt a followup, because their interview, right there, is over.Roiphe ends her essay with herself, so I get to end mine with me.When I was hired as a newspaper reporter in 2006, taking over the New York Observer‚Äôs weekly column on the city‚Äôs most expensive real estate, I was introduced to business journalism at the peak of what turned out to be a bubble. I wrote about buyers and sellers who played a role in inflating and then bursting it: ‚ÄúMoney just doesn‚Äôt mean anything,‚Äù one of the city‚Äôs high-end brokers told me not long before Bear Stearns collapsed and the era came to an end. I switched beats in 2009 to report on the landscape of Wall Street‚Äôs culture, covering some of the same people, only how they made their money instead of where they spent it.I fell in love with Malcolm‚Äôs journalism because it seemed to me at the time to be a map to the crises I would have to navigate. I imagined that the tensions her books describe only multiply if interview subjects are rich and powerful. Higher stakes, I figured, only complicate access and relationships. Reading these interviews makes me think I was wrong. They are reminders that even the subjects who best understand what‚Äôs happening can‚Äôt fully explain themselves to outsiders, and that no journalist knows how to get them to reveal it all.What is there to do about it? ‚ÄúPerhaps the way to minimize one‚Äôs feeling that one has not been as straightforward with the subject as one should have been,‚Äù she tells the Believer, ‚Äúis to be a little more straightforward.‚Äù Sometimes I talk to my subjects about the trouble Malcolm picked up on, and the trouble she got into. And sometimes I can feel the acknowledgement make us both relax, at least for a little bit.But Malcolm wouldn‚Äôt want anybody to get too comfortable. When the Believer asks if subjects occupy a different place in her mind at the end of writing than they had during interviews, she writes back that she isn‚Äôt sure she understands the question. Beal tries again: ‚ÄúHow do you make the switch from supplicant or equal interviewer to authority writer?‚Äù‚ÄúYes, it is a problem,‚Äù Malcolm answers, ‚Äúand no, it can‚Äôt be resolved.‚ÄùQuotes from the trials come from coverage in the New York Times, Village Voice and Washington Post.¬†‚Ü© If you like this article, please subscribe or leave a tax-deductible tip below to support n+1.    The Burglaries Were Never the Story Related Articles  Issue 2 HappinessKeith Gessen replies: The genital flag?Issue 2LettersThe Editors   Issue 5 Decivilizing ProcessIt has lately become clear that nothing burdens a life like an email account.Issue 5Against EmailThe Editors   Issue 39 Take CareI can tell you only what I found helpful.Issue 39Baby YeahAnthony Veasna So   Issue 6 MainstreamCanons in daily life just demarcate the books you can count on other people feeling comfortable about in conversation.Issue 6The Spirit of RevivalThe Editors More by this Author August 20, 2021‚ÄúListen to the last half-hour of ‚ÄòDark Star‚Äô in a darkened room and see if you feel remotely secure.‚ÄùOnline OnlyIn the Dead ArchivesMax Abelson  n+1n+1 is a print and digital magazine of literature, culture, and politics published three times a year. We also post new online-only work several times each week and publish books expanding on the interests of the magazine.MagazineCurrent IssueRenew SubscriptionSubscribeGift SubscriptionsYour AccountBack IssuesBookstoresLibrariesAdvertiseRSSAbout n+1AboutFoundationEventsContactBack to TopCopyright ¬© 2023 n+1 Foundation, Inc.Terms & Conditions | Privacy Policy                                        "
https://news.ycombinator.com/rss,CircleCI says hackers stole encryption keys and customers‚Äô source code,https://techcrunch.com/2023/01/14/circleci-hackers-stole-customer-source-code/,Comments,"










 


CircleCI says hackers stole encryption keys and customers' secrets ‚Ä¢ TechCrunch















































































 
 



 













CircleCI says hackers stole encryption keys and customers‚Äô secrets




			Zack Whittaker		

@zackwhittaker
 / 
						23 hours		







CircleCi, a software company whose products are popular with developers and software engineers, confirmed that some customers‚Äô data was stolen in a data breach last month.
The company said in a detailed blog post on Friday that it identified the intruder‚Äôs initial point of access as an employee‚Äôs laptop that was compromised with malware, allowing the theft of session tokens used to keep the employee logged in to certain applications, even though their access was protected with two-factor authentication.
The company took the blame for the compromise, calling it a ‚Äúsystems failure,‚Äù adding that its antivirus software failed to detect the token-stealing malware on the employee‚Äôs laptop.
Session tokens allow a user to stay logged in without having to keep re-entering their password or re-authorizing using two-factor authentication each time. But a stolen session token allows an intruder to gain the same access as the account holder without needing their password or two-factor code. As such, it can be difficult to differentiate between a session token of the account owner, or a hacker who stole the token.
CircleCi said the theft of the session token allowed the cybercriminals to impersonate the employee and gain access to some of the company‚Äôs production systems, which store customer data.
‚ÄúBecause the targeted employee had privileges to generate production access tokens as part of the employee‚Äôs regular duties, the unauthorized third party was able to access and exfiltrate data from a subset of databases and stores, including customer environment variables, tokens, and keys,‚Äù said Rob Zuber, the company‚Äôs chief technology officer. Zuber said the intruders had access from December 16 through January 4.
Zuber said that while customer data was encrypted, the cybercriminals also obtained the encryption keys able to decrypt customer data. ‚ÄúWe encourage customers who have yet to take action to do so in order to prevent unauthorized access to third-party systems and stores,‚Äù Zuber added.
Several customers have already informed CircleCi of unauthorized access to their systems, Zuber said.
The post-mortem comes days after the company warned customers to rotate ‚Äúany and all secrets‚Äù stored in its platform, fearing that hackers had stolen its customers‚Äô code and other sensitive secrets used for access to other applications and services.
Zuber said that CircleCi employees who retain access to production systems ‚Äúhave added additional step-up authentication steps and controls,‚Äù which should prevent a repeat-incident, likely by way of using hardware security keys.
The initial point of access ‚Äî the token-stealing on an employee‚Äôs laptop ‚Äî bears some resemblance to how the password manager giant LastPass was hacked, which also involved an intruder targeting an employee‚Äôs device, though it‚Äôs not known if the two incidents are linked. LastPass confirmed in December that its customers‚Äô encrypted password vaults were stolen in an earlier breach. LastPass said the intruders had initially compromised an employee‚Äôs device and account access, allowing them to break into LastPass‚Äô internal developer environment.
Updated headline to better reflect the customer data that was taken.















 
 


"
