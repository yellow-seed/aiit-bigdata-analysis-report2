url,title,link,summary,content
https://news.ycombinator.com/rss,I'm Shadow Banned by DuckDuckGo (and Bing),https://daverupert.com/2023/01/shadow-banned-by-duckduckgo-and-bing/,Comments,"




I'm Shadow Banned by DuckDuckGo (and Bing)


January 14, 2023




It came to my attention that my site does not appear on DuckDuckGo search results. Even when searching for “daverupert.com” directly. After some digging, DuckDuckGo used to get their site index from Yandex, but now gets their site index from Bing and sure enough… I didn’t appear on Bing either.
First of all… rude. I’m the one person I know who actually uses Bing and I started using DuckDuckGo on my Mac… and they have the audacity —nay, the cowardice!— to shadow ban me and my contributions to the Web!? I —a southern gentleman— take the highest offense at this slighting and misconstruing of my character. I do declare.
SEO isn’t one of my top objectives with this site, so initially I dismissed it. But that nerdsnipe shot a signal flare up in my brain that spun into mystery I needed to solve. I mean… there can be money from blogging. Surely I’ve built some clout for my blog over the years… right?

I have been blogging, almost weekly, for over a decade…
I co-host a somewhat successful web development podcast…
I’ve been back linked from popular blogs like CSS-Tricks…
I’ve been on hacker news a handful of times…
And probably most important… I show up on Google!

Why on earth would Bing not index my site at all? To solve this, I took the first step and signed up for Bing Webmaster Tools to try to know what Bing knows about my site and sure enough: zero clicks, zero impressions, and zero indexed pages for my site. Awful.

The one clue I have to go off are some “Errors” according to Bing’s Crawler. 100% of those errors are “missing meta description”. That doesn’t seem like an SEO dealbreaker to me (I get a 91 on Lighthouse SEO), but does Bing super care about meta descriptions? Doesn’t seem like I should have 0 out of 418 pages in my sitemap.xml though.
One “out there” reason I can think is that I use Amazon Affiliate links on my Bookshelf and my /Uses page and that triggers a shadow ban? I could see how that appears spammy and I question the ethics of Amazon links sometimes myself, but what would I do without those $ones of dollars that I make each year!? I keep it around as a money carrot incentive to motivate me to update the bookshelf, but perhaps it’s time I retire that monetization avenue.
Anyways, a mystery is afoot… let the investigation begin! I will post a follow up if I ever solve this.



"
https://news.ycombinator.com/rss,SLT – A Common Lisp Language Plugin for Jetbrains IDE Lineup,https://github.com/Enerccio/SLT,Comments,"








Enerccio

/

SLT

Public




 

Notifications



 

Fork
    0




 


          Star
 26
  









        SLT is an IDE Plugin for Itellij/Jetbrains IDE lineup implementing support for Common Lisp via SBCL and Slime/Swank 
      
License





     Apache-2.0 license
    






26
          stars
 



0
          forks
 



 


          Star

  





 

Notifications












Code







Issues
4






Pull requests
0






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







Enerccio/SLT









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











master





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








3
branches





1
tag







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit









Peter Vanusanik

fixed wrong import on StringUtils, tested building distributions




        …
      




        4681185
      

Jan 15, 2023





fixed wrong import on StringUtils, tested building distributions


4681185



Git stats







19

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.run



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









gradle/wrapper



init



Jan 6, 2023









src



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









.gitignore



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









CodeOfConduct.md



rename file



Jan 14, 2023









LICENSE.txt



readme and such



Jan 14, 2023









README.md



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









build-distributions.sh



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









build.gradle.kts



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









gradle.properties



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









gradlew



init



Jan 6, 2023









gradlew.bat



init



Jan 6, 2023









settings.gradle.kts



init



Jan 6, 2023




    View code
 















SLT - A Common Lisp Language Plugin for Jetbrains IDE lineup
Requirements
Getting started
Compiling source
Planned features / goals
License





README.md




SLT - A Common Lisp Language Plugin for Jetbrains IDE lineup



THIS PLUGIN IS EXPERIMENTAL and can crash at any time! Please report all bugs!
This plugin is providing support for Common Lisp for JetBrains IDEs.
Using modified SLIME/Swank protocol to commmunicate with SBCL providing
IDE capabilities for Common Lisp.

Requirements

Intellij based IDE - tested on Intellij Idea Community/Ultimate but should workd on all major IDEs
Steel Bank Common Lisp installed
Quicklisp

Getting started
Download plugin for your IDE from releases and install it via file.
To find out which release applies to you check this table:



Jetbrains IDE Variant
Plugin name pattern




CLion
slt-version-signed-CL.zip


GoLand
slt-version-signed-GO.zip


Intellij Community
slt-version-signed-IC.zip


Intellij Ultimate
slt-version-signed-IU.zip


PyCharm
slt-version-signed-PY.zip


PyCharm Community
slt-version-signed-PC.zip


Rider
slt-version-signed-RD.zip



PhpStorm is coming when I read how to build it correctly since just swapping
the type does not work.
Compiling source
Clone the repository and change gradle.properties for your IDE.
Then use gradle to build the plugin.
You can also open this as a project in Intellij Idea.
Planned features / goals

 Upload to marketplace when it has enough features
 REPL
 Interactive debugging
 Walkable debugger without actions
 Breakpoints
 Documentation
 Macro expand in documentation
 Find function by symbol name
 Search for symbols
 Back references
 Refactoring
 List of quicklisp installed packages / ASDF packages
 List of modified top level forms that are yet to be evaluated

License
This project is licensed under Apache License v2.









About

      SLT is an IDE Plugin for Itellij/Jetbrains IDE lineup implementing support for Common Lisp via SBCL and Slime/Swank 
    
Topics



  lisp


  integrated-development-environment


  jetbrains


  common-lisp


  sbcl


  intellij-plugin


  jetbrains-plugin



Resources





      Readme
 
License





     Apache-2.0 license
    



Stars





26
    stars

Watchers





3
    watching

Forks





0
    forks







    Releases





1
tags







    Packages 0


        No packages published 











Languages













Java
91.4%







Common Lisp
4.4%







Lex
3.3%







Other
0.9%











"
https://news.ycombinator.com/rss,Ask HN: How do you trust that your personal machine is not compromised?,https://news.ycombinator.com/item?id=34388866,Comments,"

Ask HN: How do you trust that your personal machine is not compromised? | Hacker News

Hacker News
new | past | comments | ask | show | jobs | submit 
login




 Ask HN: How do you trust that your personal machine is not compromised?
44 points by coderatlarge 1 hour ago  | hide | past | favorite | 29 comments 

""Compromised"" meaning that malware hasn't been installed or that it's not being accessed by malicious third parties.  This could be at the BIOS, firmware, OS, app or any other other level. 
 
  
 
gnfargbl 31 minutes ago  
             | next [–] 

Here's a short, fairly practical guide that you might find helpful: https://www.ncsc.gov.uk/files/Cyber-Essentials-Requirements-.... It is aimed mostly at small businesses, but I find a lot of the guidance to be pretty relevant to my personal IT.My even shorter (and incomplete) summary of the document would be: configure your router and firewall; remove default passwords and crapware from your devices; use a lock screen; don't run as root; use a password manager and decent passwords; enable 2FA everywhere you can; enable anti-malware if your OS has it built it; don't run software from untrusted sources; patch regularly.There are also other controls that you can choose to impose on yourself. For example, I require full-disk encryption, and I will only use mobile devices which get regular updates. Would be interested in hearing other things that HN'ers do to limit risk.
 
reply



  
 
amelius 16 minutes ago  
             | parent | next [–] 

Do you lock your computer every time you leave your desk?And do you always check for keylogger thumbdrives and such?
 
reply



  
 
Semaphor 5 minutes ago  
             | root | parent | next [–] 

For me: No, and no. But as that would require someone breaking into my apartment, I don’t worry too much.
 
reply



  
 
k3liutZu 5 minutes ago  
             | root | parent | prev | next [–] 

Yes (I don't bother when I'm working from home)I haven't used a use usb stick in +10 years.
 
reply



  
 
greggyb 14 minutes ago  
             | root | parent | prev | next [–] 

Yes. Why wouldn't you?
 
reply



  
 
NikolaNovak 22 minutes ago  
             | prev | next [–] 

Great question. I don't anymore. Decades ago when I had a 286 and knew what each file did and what all the software was, and threats were limited and crude, I had good confidence of controlling my machine. Today, when my laptop has millions of files and each website - even hacker news - could inject something malicious and my surface is so broad (browsers applications extensions libraries everything) and virtually anything I do involves network connections... I just don't have the confidence.FWIW, I try to segregate my machines for different categories of behaviour - this laptop is for work, this one is for photos and personal documents, this one is for porn, this one is if I want to try something. But even still my trust in e. G. software vlan on my router and access controls on my NAS etc are limited in this day and age.I feel today it's not about striving for zero risk (for 99.99 of people) , but picking the ratio of overhead and risk you're ok with. And backups. (bonus question - how to make backups safe in age of encrypting ransom ware).
 
reply



  
 
jl6 9 minutes ago  
             | parent | next [–] 

On the backup question, this is one reason why I have a set of backups that are physically disconnected and not automated.
 
reply



  
 
h2odragon 42 minutes ago  
             | prev | next [–] 

You really can't, anymore. You can watch traffic and hope that anything nasty isn't communicating with the outside world, but then there's all sorts of side channels that you may not know to watch.At some point you just have to admit there's limits to privacy and work with them. You paper journal could be stolen and read / rewritten too, yaknow? It's not a new problem, its just in a new context.
 
reply



  
 
ramraj07 3 minutes ago  
             | prev | next [–] 

There are two levels here: compromised by some national agency vs. compromised by anyone else.For the former, I don’t assume anything especially since I’m not an American citizen. I still believe with some certainty that my iPhone is safe from the government but not 100%
 
reply



  
 
adriancr 25 minutes ago  
             | prev | next [–] 

Just some generic things that should help avoid or clean up after a compromise.- clean reinstall every month, just pick a new flavor of Linux to try out. (also helps ensure I have proper backups and scripts for setting up environment)- Dev work I usually do in docker containers, easy to set up/nuke environments.- Open source router with open source bios (apu2), firewall on it, usually reinstall once in a while.- Spin up VMs via scripts for anything else. (games - windows VM with passthrough GPU for example)- automatic updates everywhere.
 
reply



  
 
867-5309 22 minutes ago  
             | parent | next [–] 

>Spin up VMs via scripts for .. gamesthis is not sustainable. you do this once and then pray nothing breaks!
 
reply



  
 
adriancr 20 minutes ago  
             | root | parent | next [–] 

I just have a clone of a clean windows VM.If something breaks or I get bored, nuke the active one and start clone, update it and make another backup, then reinstall games again.On the other hand, gpu pass-through breaks once in a while and is annoying to fix.
 
reply



  
 
albntomat0 6 minutes ago  
             | prev | next [–] 

The biggest thing is being deliberate about your threat model.  Who would want to get onto your systems, and how much do they care about you in particular?From there, take appropriate actions.  For the vast, vast majority of us, that means using good passwords, updating software, and not running weird things from the internet.If you’re worried about 0 click RCE in Chrome/Windows/iOS, you either should be getting better advice from folks outside of HN, or are being unrealistic about who is coming after you.
 
reply



  
 
lifthrasiir 36 minutes ago  
             | prev | next [–] 

I'm reasonably sure that my personal machine is less compromised than the average, but I can't and will never be able to ensure that it is not compromised because I have no way to know everything the machine trying to do. This remains true even when you have an entirely free and directly inspectable hardware; you simply have no knowledge and time to verify everything. Just keep a reasonable amount of precaution and skepticism.
 
reply



  
 
jl6 30 minutes ago  
             | prev | next [–] 

I don’t have ultimate trust in any software or hardware, but I get to “good enough” by deciding which providers I trust:* Software: Canonical, Google, Microsoft, Valve, Oracle, Dropbox. I install software from their official repos. Anything 3rd-party/unofficial/experimental/GitHub goes in a VM.* Hardware: I built my main PC from mainstream commodity components. I have no way of knowing if there are secret backdoors but I consider it unlikely.I’m also privileged enough to not be a “person of interest” so don’t feel the need to take any extraordinary precautions.Yes, I’m aware of VM escapes. Yes, I’ve read Reflections on Trusting Trust. I choose to trust regardless because life’s too short for paranoia. As Frank Drebin said:“You take a chance getting up in the morning, crossing the street, or sticking your face in a fan.”
 
reply



  
 
rhn_mk1 17 minutes ago  
             | parent | next [–] 

What about publicly known backdoors in your hardware?https://www.techrepublic.com/article/is-the-intel-management...There is hardware that doesn't contain those at least, but it doesn't break power records.
 
reply



  
 
codetrotter 38 minutes ago  
             | prev | next [–] 

Noone has drained my crypto from my wallets yet.So either my personal machine is not compromised, or they think the amount of crypto in the wallets is too low.Jokes on them though, cause I am moving my crypto to a hardware wallet eventually
 
reply



  
 
progval 14 minutes ago  
             | parent | next [–] 

Joke's on you, you just told them they should hurry up before you do ;)
 
reply



  
 
tluyben2 31 minutes ago  
             | parent | prev | next [–] 

Quite an interesting honeypot really.
 
reply



  
 
mimimi31 23 minutes ago  
             | root | parent | next [–] 

More like a canary I think.
 
reply



  
 
adg001 25 minutes ago  
             | prev | next [–] 

The reality is that you cannot trust that your machines are not compromised.The only option we are left with is to operate under the assumption that, indeed, our machines are permanently compromised.
 
reply



  
 
rekrsiv 4 minutes ago  
             | prev | next [–] 

You don't. Treat your personal machine(s) as compromised by default and take it from there.
 
reply



  
 
PaulHoule 35 minutes ago  
             | prev | next [–] 

Reminds me of the time I was watching a creepypasta horror movie about some guy who gets strange phone calls and my phone rang.I think this guy had gotten my phone number from my HN profile and he thought I might be able to help him.  He thought his android phone was infected by malware and he knew who did it.  I told him the people who repair cell phones at the mall could do a system reset on his phone…. Unless he was dealing with state-level actors in which case it might be an advanced persistent threat and it might be permanent.
 
reply



  
 
treebeard901 43 minutes ago  
             | prev | next [–] 

You should assume all devices are compromised
 
reply



  
 
wadayano 34 minutes ago  
             | parent | next [–] 

*compromisable
 
reply



  
 
baobabKoodaa 40 minutes ago  
             | parent | prev | next [–] 

Not helpful
 
reply



  
 
twaw 14 minutes ago  
             | root | parent | next [–] 

Why not? It still possible to communicate securely using compromised devices and networks.
 
reply



  
 
cube2222 11 minutes ago  
             | prev | next [–] 

I try to follow what others already mentioned, but still, for any personal high-security stuff I use a device whose OS  puts strong limits on apps, like an iPad.
 
reply



  
 
crims0n 22 minutes ago  
             | prev [–] 

Keep it air gapped, only way to be sure!Only half kidding, unfortunately.
 
reply







Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
Search:  


"
https://news.ycombinator.com/rss,Ubuntu 22.04 LTS servers and phased apt updates,https://utcc.utoronto.ca/~cks/space/blog/linux/Ubuntu2204ServerPhasedUpdates,Comments,"
 
 Chris's Wiki :: blog/linux/Ubuntu2204ServerPhasedUpdates 






Chris Siebenmann ::
CSpace »
       blog »
       linux »
       Ubuntu2204ServerPhasedUpdates
Welcome, guest.




Ubuntu 22.04 LTS servers and phased apt updates
January 13, 2023

I was working on getting one of our 22.04 LTS servers up to date,
even for packages we normally hold, when I hit a mystery and
posted about it on the Fediverse:
Why does apt on this 22.04 Ubuntu machine want to hold back a bunch of
package updates even with '--with-new-pkgs --ignore-hold'? Who knows,
it won't tell me why it doesn't like any or all of:
open-vm-tools openssh-client openssh-server openssh-sftp-server
osinfo-db python3-software-properties software-properties-common
(Apt is not my favorite package manager for many reasons, this among
them.)

Steve suggested that it was Ubuntu's ""Phased Update"" system, which is what it turned
out to be. This set me off to do some investigations, and it turns
out that phased (apt) updates explain some other anomalies we've
seen with package updates on our Ubuntu 22.04 machines.
The basic idea of phased updates is explained in the ""Phasing""
section of Ubuntu's page on Stable Release Updates (SRUs); it's a
progressive rollout of the package to more and more of the system
base. Ubuntu introduced phased updates in 2013 (cf) but initially they weren't
directly supported by apt, only by the desktop upgrade programs.
Ubuntu 21.04 added apt support for phased updates and
Ubuntu 22.04 LTS is thus the first LTS version to subject servers
to phased updates. More explanations of phased updates are in this
askubuntu answer, which includes
one way to work around them.
(Note that as far as I know and have seen, security updates are not
released as phased updates; if it's a security update, everyone
gets it right away. Phased updates are only used for regular,
non-security updates.)
Unfortunately apt (or apt-get) won't tell you if an update is being
held back because of phasing. This user-hostile apt issue is tracked
in Ubuntu bug #1988819 and
you should add yourself as someone it affects if this is relevant
to you. Ubuntu has a web page on what updates are currently in
phased release,
although packages are removed from this page once they reach 100%.
Having reached 100%, such a package is no longer a phased update,
which will become relevant soon. If you can't see a reason for a
package to be held back, it's probably a phased update but you can
check the page
to be sure.
(As covered in the ""Phasing"" section, packages
normally move forward through the phased rollout every six hours,
so you can have a package held back on some server in the morning
and then be not-held in the afternoon. This is great fun for
troubleshooting why a given server didn't get a particular update.)
Your place in a phased update is randomized across both different
servers and different packages. If you have a fleet of servers,
they will get each phased update at different times, and the order
won't be consistent from package to package. This explains an anomaly
we've been seeing in our package updates for some time, where
different 22.04 servers would get updates at different times without
any consistent pattern.
The phased update related apt settings available and some of the
technical details are mostly explained in this askubuntu answer. If you want to opt out of phased
updates entirely, you have two options; you can have your servers
install all phased updates right away (basically putting you at the
0% start line), or you can skip all phased updates and only install
such packages when they reach 100% and stop being considered phased
updates at all. Unfortunately, as of 22.04 there's no explicit
option to set your servers to have a particular order within all
updates (so that you can have, for example, a 'canary' server that
always installs updates at 0% or 10%, ahead of the rest of the
fleet).
For any given package update, machines are randomized based on the
contents of /etc/machine-id, which
can be overridden for apt by setting APT::Machine-ID to a 32 hex
digit value of your choice (the current version of apt appears to
only use the machine ID for phased updates).  If you set this to
the same value across your fleet, your fleet will update in sync
(although not at a predictable point in the phase process); you can
also set subsets of your fleet to different shared values so that
the groups will update at different times.  The assignment of a
particular machine to a point in the phased rollout is done through
a relatively straightforward approach; the package name, version,
and machine ID are all combined into a seed for a random number
generator, and then the random number generator is used to produce
a 0 to 100 value, which is your position in the phased rollout. The
inclusion of the package name and version means that a given machine
ID will be at different positions in the phased update for different
packages. All of this turns out to be officially documented in the
""Phased Updates"" section of apt_preferences(5),
although not in much detail.
(There is a somewhat different mechanism for desktop updates, covered
in the previously mentioned askubuntu answer.)
As far as I can see from looking at the current apt source code, apt doesn't log anything
at any verbosity if it holds a package back because the package is
a phased update and your machine doesn't qualify for it yet. The
fact that a package was a phased update the last time apt looked
may possibly be recorded in /var/log/apt/eipp.log.xz, but documentation
on this file is sparse.
Now that I've looked at all of this and read about APT::Machine-ID,
we'll probably set it to a single value across all of our fleet
because we find different machines getting updates at different
times to be confusing and annoying (and it potentially complicates
troubleshooting problems that are reported to us, since we normally
assume that all 22.04 machines have the same version of things like
OpenSSH). If we could directly control the position within a phased
rollout we'd probably set up some canary machines, but since we
can't I don't think there's a strong reason to have more than one
machine-id group of machines.
(We could set some very important machines to only get updates when
packages reach 100% and stop being phased updates, but Ubuntu has
a good record of not blowing things up with eg OpenSSH updates.)

(4 comments.)
Written on 13 January 2023. 

     «   A browser tweak for system administrators doing (web) network debugging    
    Your server BMCs can need to be rebooted every so often   »     



 These are my WanderingThoughts 
(About the blog)
Full index of entries 
Recent comments
This is part of CSpace, and is written by ChrisSiebenmann. 
Mastodon: @cks 
Twitter: @thatcks
* * *
Categories: links, linux, programming, python, snark, solaris, spam, sysadmin, tech, unix, web 
Also: (Sub)topics
This is a DWiki. 
GettingAround 
(Help)
 
 Search:  



 Page tools: View Source, Add Comment. 

Search: 

Login: 
Password: 


 

Atom Syndication: Recent Comments.
 Last modified: Fri Jan 13 22:56:18 2023 
This dinky wiki is brought to you by the Insane Hackers
Guild, Python sub-branch.


"
https://news.ycombinator.com/rss,Single-file scripts that download their dependencies,https://dbohdan.com/scripts-with-dependencies,Comments,"



Single-file scripts that download their dependencies · DBohdan.com











Toggle navigation




dbohdan



Home





 




Homescripts-with-dependencies





Single-file scripts that download their dependencies
An ideal distributable script is fully contained in a single file. It runs on any compatible operating system with an appropriate language runtime. It is plain text, and you can copy and paste it. It does not require mucking about with a package manager, or several, to run. It does not conflict with other scripts’ packages or require managing a project environment to avoid such conflicts.
The classic way to get around all of these issues with scripts is to limit yourself to using the scripting language’s standard library. However, programmers writing scripts don’t want to; they want to use libraries that do not come with the language by default. Some scripting languages, runtimes, and environments resolve this conflict by offering a means to download and cache a script’s dependencies with just declarations in the script itself. This page lists such languages, runtimes, and environments. If you know more, drop me a line.
Contents



Anything with a Nix package


D


Groovy


JavaScript (Deno)


Kotlin (kscript)


Racket (Scripty)


Scala (Ammonite)



Anything with a Nix package
The Nix package manager can act as a #! interpreter and start another program with a list of dependencies available to it.
#! /usr/bin/env nix-shell
#! nix-shell -i python3 -p python3
print(""Hello, world!"".rjust(20, ""-""))
D
D’s official package manager DUB supports single-file packages.
#! /usr/bin/env dub
/+ dub.sdl:
name ""foo""
+/
import std.range : padLeft;
import std.stdio : writeln;
void main() {
    writeln(padLeft(""Hello, world!"", '-', 20));
}
Groovy
Groovy comes with an embedded JAR dependency manager.
#! /usr/bin/env groovy
@Grab(group='org.apache.commons', module='commons-lang3', version='3.12.0')
import org.apache.commons.lang3.StringUtils
println StringUtils.leftPad('Hello, world!', 20, '-')
JavaScript (Deno)
Deno downloads dependencies like a browser. Deno 1.28 and later can also import from NPM packages. Current versions of Deno require you to pass a run argument to deno. One way to accomplish this from a script is with a form of “exec magic”. Here the magic is modified from a comment by Rafał Pocztarski.
#! /bin/sh
"":"" //#; exec /usr/bin/env deno run ""$0"" ""$@""
import leftPad from ""npm:left-pad"";
console.log(leftPad(""Hello, world!"", 20, ""-""));
On Linux systems with recent GNU env(1) and on FreeBSD you can replace the magic with env -S.
#! /usr/bin/env -S deno run
import leftPad from ""npm:left-pad"";
console.log(leftPad(""Hello, world!"", 20, ""-""));
Kotlin (kscript)
kscript is an unofficial scripting tool for Kotlin that understands several comment-based directives, including one for dependencies.
#! /usr/bin/env kscript
//DEPS org.apache.commons:commons-lang3:3.12.0
import org.apache.commons.lang3.StringUtils
println(StringUtils.leftPad(""Hello, world!"", 20, ""-""))
Racket (Scripty)
Scripty interactively prompts you to install the missing dependencies for a script in any Racket language.
#! /usr/bin/env racket
#lang scripty
#:dependencies '(""base"" ""typed-racket-lib"" ""left-pad"")
------------------------------------------
#lang typed/racket/base
(require left-pad/typed)
(displayln (left-pad ""Hello, world!"" 20 ""-""))
Scala (Ammonite)
The scripting environment in Ammonite lets you import Ivy dependencies.
#! /usr/bin/env amm
import $ivy.`org.apache.commons:commons-lang3:3.12.0`,
  org.apache.commons.lang3.StringUtils
println(StringUtils.leftPad(""Hello, world!"", 20, ""-""))

Tags: list, programming.
 
 
 
 


Copyright 2013–2022 D. Bohdan.




 


"
https://news.ycombinator.com/rss,"The Fourier Transform, explained in one sentence",https://blog.revolutionanalytics.com/2014/01/the-fourier-transform-explained-in-one-sentence.html,Comments,"


































The Fourier Transform, explained in one sentence (Revolutions)













Revolutions

			Milestones in AI, Machine Learning, Data Science, and visualization with R and Python since 2008
		









« Forecasting By Combining Expert Opinion |
	Main
	| Predictive Models in R Clustered By Tag Similarity »



January 03, 2014


The Fourier Transform, explained in one sentence


If, like me, you struggled to understand the Fourier Transformation when you first learned about it, this succinct one-sentence colour-coded explanation from Stuart Riffle probably comes several years too late:

Stuart provides a more detailed explanation here. This is the formula for the Discrete Fourier Transform, which converts sampled signals (like a digital sound recording) into the frequency domain (what tones are represented in the sound, and at what energies?). It's the mathematical engine behind a lot of the technology you use today, including mp3 files, file compression, and even how your old AM radio stays in tune.
The daunting formula involves imaginary numbers and complex summations, but Stuart's idea is simple. Imagine an enormous speaker, mounted on a pole, playing a repeating sound. The speaker is so large, you can see the cone move back and forth with the sound. Mark a point on the cone, and now rotate the pole. Trace the point from an above-ground view, if the resulting squiggly curve is off-center, then there is frequency corresponding the pole's rotational frequency is represented in the sound. This animated illustration (click to see it in action) illustrates the process:

The upper signal is make up of three frequencies (""notes""), but only the bottom-right squiggle is generated by a rotational frequency matching one of the component frequencies of the signal.
By the way, no-one uses that formula to actually calculate the Discrete Fourier Transform — use the Fast Fourier Transform instead, as implemented by the fft function in R. As the name suggests, it's much faster.
AltDevBlog: Understanding the Fourier Transform (note: updated link 20 Oct 2015 with active mirror)





Posted by David Smith at 13:30 in R, random  | Permalink











Comments

 You can follow this conversation by subscribing to the comment feed for this post.





Very interesting article, thank you. Please take a moment to rephrase the following key statement, if you would: ""...then there is frequency corresponding the pole's rotational frequency is represented in the sound.""


		Posted by:
		C. Griffith |
		January 04, 2014 at 10:09




polar form e^iθ is equal to the rectangular form cosθ+isinθ and corresponds to the coordinates (cosθ,sinθ) such that 
e^i0    =  1 = (1,0)
e^iτ/4  =  i = (0,1)
e^iτ/2  = -1 = (-1,0)
e^iτ3/4 = -i = (0,-1)
e^iτ    =  1 = (1,0)


		Posted by:
		MasterG |
		January 04, 2014 at 16:33




May I suggest a  minor exception to your claim about FFT: most modern languages, R included, use some variation of the ""pure"" 2^N Cooley-Tukey FFT algorithm as appropriate to support factors of 3, 5, etc. in the length of the dataset, and even default to the ""raw"" DFT for other data lengths (unless specifically suppressed by the user).   
And, of course, the FFT is in fact that equation, just with gobs of like terms grouped together. :-)


		Posted by:
		Carl Witthoft |
		January 06, 2014 at 08:40











	The comments to this entry are closed.







Information


About this blog
Comments Policy
About Categories
About the Authors
Local R User Group Directory
Tips on Starting an R User Group





Search Revolutions Blog






















Got comments or suggestions for the blog editor? 
Email David Smith.
    




 Follow David on Twitter: @revodavid





Get this blog via email with 




Categories


academia (41)
advanced tips (218)
AI (62)
airoundups (20)
announcements (201)
applications (288)
beginner tips (106)
big data (272)
courses (60)
current events (126)
data science (227)
developer tips (90)
events (280)
finance (126)
government (25)
graphics (378)
high-performance computing (115)
life sciences (35)
Microsoft (314)
mlops (4)
open source (78)
other industry (58)
packages (388)
popularity (54)
predictive analytics (163)
profiles (15)
python (69)
R (2442)
R is Hot (8)
random (464)
reviews (22)
Revolution (422)
Rmedia (136)
roundups (121)
sports (55)
statistics (297)
user groups (127)


See More




R links


R on AzureDeveloper's guide and documentation
Find R packagesCRAN package directory at MRAN
Download Microsoft R OpenFree, high-performance R
R Project siteInformation about the R project






Recommended Sites


@RLangTipDaily tips on using R
FlowingDataModern data visualization
Probability and statistics blogMonte Carlo simulations in R
R BloggersDaily news and tutorials about R, contributed by R bloggers worldwide.
R Project group on analyticbridge.comCommunity and discussion forum
Statistical Modeling, Causal Inference, and Social ScienceAndrew Gelman's statistics blog






Archives


January 2023
August 2022
September 2021
July 2021
June 2021
April 2021
March 2021
February 2021
January 2021
December 2020





 Subscribe to this blog's feed




​
    













 








"
https://news.ycombinator.com/rss,Go FOSS: Information is power,https://gofoss.net/,Comments,"          gofoss.net      Home      Get started      Get started     Protect your digital freedom         Browse privately      Browse privately     Free your browser     Firefox     Tor Browser     VPN         Speak freely      Speak freely     Keep conversations private     Encrypted messages     Encrypted emails         Store safely      Store safely     Secure your data     Safe passwords     Backups     Encrypted files         Stay mobile & free      Stay mobile & free     Free your phone     FOSS apps     CalyxOS     LineageOS for microG         Unlock your computer      Unlock your computer     Free your computer     Ubuntu     Ubuntu apps         Own your cloud      Own your cloud     Free your cloud     Fediverse     Alternative cloud providers     Server hosting     Basic server security     Advanced server security     Secure access     Cloud storage     Photo gallery     Contacts, calendars & tasks     Media streaming     Server backups         About      About     Big Tech threatens privacy     The project     The team     Thanks     Contributing     Roadmap     Disclaimer         Origins      Origins     Hackers, 1984     The GNU Manifesto, 1985     The Techno-Revolution, 1986     The Conscience of a Hacker, 1986     The Crypto Anarchist Manifesto, 1988     A Cypherpunk's Manifesto, 1993     A Declaration of the Independence of Cyberspace, 1996     A Cyberpunk Manifesto, 1997     The Cathedral & The Bazaar, 1999     The dotCommunist Manifesto, 2003     A Hacker Manifesto, 2004     The Maker's Bill of Rights, 2006     Guerilla Open Access Manifesto, 2008     The Apache Way, 2009     Repair Manifesto, 2009     The Cult of Done Manifesto, 2009     Self-Repair Manifesto, 2010     The Hardware Hacker Manifesto, 2010     An Anonymous Manifesto, 2011     The Declaration of the Independence of the people of the Internet, 2012                          Back to top  "
https://news.ycombinator.com/rss,Constrain – Interactive figures using declarative constraint solving,https://github.com/andrewcmyers/constrain,Comments,"








andrewcmyers

/

constrain

Public




 

Notifications



 

Fork
    2




 


          Star
 50
  









        Responsive, animated figures in JavaScript/HTML canvases
      





andrewcmyers.github.io/constrain







50
          stars
 



2
          forks
 



 


          Star

  





 

Notifications












Code







Issues
18






Pull requests
0






Discussions







Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Discussions
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







andrewcmyers/constrain









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











master





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








7
branches





6
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




andrewcmyers

Fix backprop for Smooth/Linear.




        …
      




        130cac8
      

Jan 15, 2023





Fix backprop for Smooth/Linear.

Remove unnecessary tree constraints.
Add necessary constraints to example.

130cac8



Git stats







516

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








doc



doc improvement



Jan 13, 2023









examples



Fix backprop for Smooth/Linear.



Jan 15, 2023









images



Update image



Jun 27, 2022









reveal.js @ 9430a98



latest



Jul 25, 2019









tests



un/refix test



Jan 15, 2023









.gitignore



Reorg/clean up repo



Dec 25, 2020









.gitmodules



change to my version of Reveal



Jul 18, 2019









README.md



tree example



Jan 13, 2023









constrain-graph.js



argument checking



May 11, 2022









constrain-mathjax.js



formatting



Jul 27, 2022









constrain-pdf.js



Monkey-patch broken jsPDF calls



Apr 9, 2022









constrain-ps.js



Implement missing methods for PS rendering



Apr 9, 2022









constrain-reveal.js



Clean up comment



Apr 19, 2021









constrain-slide.css



More refactoring of Reveal mods



Aug 21, 2019









constrain-trees.js



Fix backprop for Smooth/Linear.



Jan 15, 2023









constrain.js



Fix backprop for Smooth/Linear.



Jan 15, 2023









numeric-1.2.6.js



execute bit should not be on



Jul 22, 2019




    View code
 















Constrain - a JS (ES6) library for animated, interactive figures, based on declarative constraint solving
Demos
Requirements





README.md




Constrain - a JS (ES6) library for animated, interactive figures, based on declarative constraint solving


Responsive, animated figures embedded in web pages
Figures implemented declaratively with time-dependent constraints on graphical objects
Integrates with Reveal.js presentations
GitHub repository
Reference manual
A short talk about Constrain

Demos
Using constraints to compute the Golden Ratio (Drag the diamond!)
A short talk about Constrain, using Reveal
Cornell University course notes using Constrain for embedded figures: CS 2112,
CS 4120/lexer generation,
CS 4120/bottom-up parsing
Interactive Pythagorean Theorem
Interactively computing centers of a triangle
TeX-style text formatting
Simple template page for using Constrain
Animated trees
Requirements

ES6-capable web browser

Tested on Chrome, Opera, Brave, Firefox, Safari (runs best on the first three)
Does not work on Internet Explorer or Opera Mini


Numeric.js version 1.2.6 (included)










About

      Responsive, animated figures in JavaScript/HTML canvases
    





andrewcmyers.github.io/constrain


Topics



  constraints


  web-pages


  animated-figures


  embedded-figures



Resources





      Readme
 


Stars





50
    stars

Watchers





4
    watching

Forks





2
    forks







    Releases
      5







Release 0.3.0

          Latest
 
Apr 21, 2021

 

        + 4 releases







    Packages 0


        No packages published 





Languages












JavaScript
95.1%







HTML
4.8%







CSS
0.1%











"
https://news.ycombinator.com/rss,Flightcontrol (YC W22) is hiring Developer Advocate (remote/fulltime),https://jobs.flightcontrol.dev/developer-advocate,Comments,"🛫Developer Advocate at a Calm, Ambitious DevTools Startup https://www.flightcontrol.dev Flightcontrol is a 4 person devtool startup cofounded by the creator of Blitz.js. We’re not the macho, overworked team that “startup” might bring to mind. We’re intentionally building the most life-giving and fulfilling company possible, and we want you to join us! :) Photos from our last retreat in Italy — contrary to appearances, we also did real work :)Flightcontrol is transforming the app deployment landscapeFlightcontrol provides the deployment experience of a Platform-as-a-Service but without the limitations.Traditionally, users had to choose between a PaaS like Heroku, which is easy to use but is a closed-box with limitations and restrictions, or AWS which gives you full power and control but is a royal pain to set up. But with Flightcontrol they get the best of both worlds: great developer experience and full control and scalabilityOur product is so compelling that most of our users are migrating existing applications from Heroku, Render, Railway, Vercel and from custom AWS setups.Before you cringe at our current design, know that we are currently working with Overnice to completely overhaul our brand design, marketing site, and UI/UX. So rest assured, you’ll soon have world-class brand materials to work with :)Since launching in January, over 350 users have deployed 30,000+ timesUsers of all sizes love the product, from solo indie hackers to enterprisesWe went through Y Combinator in W22 and have raised $3.5M.We’re default alive — on track to become profitable this yearIntrigued? Read more about our company here.Deployments Per Month Meet Our Team of 4Brandon Bayer, Cofounder and CEO. Dayton, Ohio. You might know me as the creator of Blitz.js. Although highly technical, my strengths are product design and marketing. My superpower is simplicity. My top values that define everything I do are excellence, equality, inclusion, and freedom. Outside of work I love traveling, flying airplanes and helicopters, and rock climbing. My intention is to build the best company to work for in the world. I’m here to support you and help make your dreams come true.Mina Abadir, Cofounder and CTO. Toronto, Canada. Mina is the technical genius that brings our core product features to life. He’s deeply authentic and caring, loves to laugh, and greatly enjoys a good video game. His superpower is empathy.Blake Bayer, Junior Software Engineer. Dayton, Ohio. Formerly a nurse, Blake joined at the end of April. This is his first job in tech, and he continually impresses everyone on the team with his ability to learn and implement complex things quickly. He loves rock climbing and learning new things.Camila Rondinini, Senior Backend Software Engineer. Spain. Since joining this past June, she is already an extremely foundational part of our team, having designed and shipped some of our most important features. She’s an incredible engineer and has made a massive impact on our engineering culture.You? 😉 We need you to help us grow through awareness & educationAs our first devrel hire, this is your chance to really shine and help propel Flightcontrol into one of the most loved developer companies of this decade. This is a critical role in shaping FlightcontrolYour high level goal is to grow “top of funnel” traffic through awareness and education, with written content as the foundation.We are building a product-led growth flywheel. Your primary role is feeding the flywheel via new top of funnel users. Your secondary role is improving the flywheel through education and documentation.So far Brandon has been doing all the devrel related tasks. You’ll work closely with him to figure out where and how to invest your efforts. We don’t yet have the perfect content strategy. So we’ll be experimenting to see what works and what doesn’t.In time, we want to be known as the cloud education company. The place where all devs turn to become maestros of the cloud. We believe that most engineers would love working with native cloud providers like AWS if only it was more approachable and more easily understood. Our vision is to empower an entirely new generation of engineers through our product and through our educational content.Essential Responsibilities Deeply understand our product, its strengths and weaknessesCreate and distribute compelling technical content, including documentation, documentation, guides, demos, and videosWe care about quality over quantityIdentify and work on collaborations and integrations with other companies and projectsExample: how to use Flightcontrol preview environments to run isolated Cypress e2e testsSecondary ResponsibilitiesHelp with customer success. As a small team, we all share this responsibility. Helping with this is one of the best ways to understand nitty-gritty details of our product, product improvement ideas, and documentation ideas.A customer recently said that we “have amazing support and developer success” and that it provides a tremendous amount of value to themPossibly, but not required: represent Flightcontrol at eventsIdentifying relevant events for Flightcontrol and organizing our participation (meetups, conferences, hackathons, workshops, etc.),Ideally participating in 4-8 of these events per year, as a speaker or sponsorRequirements1 year full-time devrel experience2 years full-time software engineering experience and are comfortable with fullstackSome working knowledge of some basic AWS services like EC2, S3, RDS, LambdaTeacher — can explain complex things in as simple a manner as possibleWriter — great at writing technical content, ideally for 1+ yearsGrit — can ship content consistently over time through thick and thinEmpathy — can learn and understand what’s important to developers and engineering organizationsCredible — produces content that acknowledges the trade-offs and complexities of the real worldCan overlap with 10a-noon US Eastern time (EST) . You can work from anywhere in the world, but we have our company wide meetings in the 10a-12p EST time range.Nice to haveExperience in the AWS or cloud spaceGreat at creating videosConference speaking experienceGreat knowledge of the application hosting/deployment ecosystemAble to work with basic demos in several programming languagesExperience with our stack: Typescript, React, Next.js You Are Someone WhoIs Kind. We are a team that seeks to work really well together by building deep relationships. We have each other’s backs. We care about and check in on each other, and we enjoy being together. We have company retreats 2-3 times per year for a week at a time.Is Collaborative. We all work closely together to design and develop the best product possible. We want someone who is humble but will bring your own ideas on how to be more excellent.Takes Ownership. We offer significant equity because we want you to think at a higher level than just your daily tasks. We want you to help us shape the business. We need someone who loves to dig in and do what it takes to figure things out. And we want someone who is good at turning vague ideas into magnificence. Has a Growth Mindset. It matters more where you are going than where you are today. We’re looking for someone who loves to grow, improve, and learn new things. Your Typical Week at FlightcontrolOn Monday, depending on your timezone, you’ll start your morning or afternoon with a coffee chat where everyone is together for causal conversation. After that, you’ll join our Flightcontrol planning session with the entire Flightcontrol product team.Tuesday is usually meeting free, so you’ll be focused on your work.On Wednesday you’ll have your weekly 1 on 1 with Brandon, the CEO. This is your time to ask for what you want, bring up issues, ask hard questions, and give and receive feedback. Brandon takes feedback very seriously and is quick to make needed changes. Thursday and Friday are your time for deep work.Aside from being available 9a-12p EST, your work hours are flexible and up to you. Some of us work a standard 9-5 type of deal while others have varying schedules.You’ll collaborate with Brandon as much as is needed.Since we’re a startup, the journey from idea to building to shipping to growing is certainly a bit of a roller coaster. But we're all on the roller coaster together, learning and iterating as quickly as we can. As long as we stick to our values and show up for each other with curiosity, compassion, and collaboration, we can likely overcome just about anything together.Every month we have a tech-debt cleanup day. Every other month we have a company hackathon. You Can Grow With UsWe want you to grow with us as much as you desire. As we scale, you’ll be able to grow into almost any role you can imagine. Want to become a team lead? We’ll help train you. Want to become a manager? We’ll make it happen. Want to be an executive? Let’s figure that out. We want you to be with us as long as you are extremely happy. If we get to place were you aren’t happy, we’ll do everything we can to help you find a place where you are. Our Code of ExcellenceGo above and beyond. We’re not here to half-way do anything. If we’re going to do something, we’re going to do a stellar job.Tell the truth even when it hurts. We don’t tell white lies, and we don’t deceive. Even when it costs.Take care of you and yours first, work second. Nothing matters more than family and close relationships. We never sacrifice them for work.Treat people better than they deserve. Kindness and generosity guides how we treat everyone, including teammates and customers.Give and receive feedback. Feedback is essential for growth. We highly value giving and receiving informal, constructive feedback between all members of the team, and then taking prompt action on that feedback.Have a life outside work. It can be anything, hobbies, side projects, reading, etc. As long as you have something and work isn’t all you live for.Eradicate stress. Stress is a killer, and we work to eliminate it through any means, including systems, exercise, and meditation.Nothing is impossible. We believe we can create any future we imagine, and we lean into solving the things that seem impossible.Build a legacy. We are here to do our very best work. Work that will inspire generations for years to come. Salary & Benefits32 Hour Work Week - More and more companies are finding that people accomplish the same amount of work in 32 hours as in 40 hours.Salary: $110k — $145k USD (same as our engineering roles)0.75% — 1% Equity Stock Options. You’ll be a $20+ millionaire if our growth continues like it isMinimum 4 Weeks PTO - It's critical to have good work life balance, so you must take at least 4 weeks PTO each year.Fully RemoteHealth Insurance Fully Paid For401k - We’re still working out the details on this, but will get it nailed down asap if it’s important to youMenstrual Leave - There's no use trying to be productive when you are suffering. Take the day(s) off as PTO, no explanation needed.Unlimited Sick Leave - If you are feeling crappy, you aren't going to be doing your best work. So rest, get better, then come back energized.2+ In-Person Company Retreats Per YearOpen Source - We are passionate about open-source and encourage you to contribute on company time to anything that will benefit the company.Equipment - We'll make sure you have all the equipment you need to have an ergonomic, productive environment, including a standing desk and external monitors.Conferences - We're a big fan of in-person conference experiences, and encourage you to speak at and attend them. We'll fully pay for you to attend 2 conferences per year.Education - Budget for books or courses that are at least tangentially related to your work.  🔥Please apply here 👉 https://airtable.com/shrPet5euUinQ0uP4 👈
Our process:You submit the application45 minute zoom with Brandon, CEO45 minute technical interview with Mina, CTONo LeetCode garbage — we’ll offer you a range of options so you can choose a style that you’ll do best at45 minute technical interview with Camila1 hour zoom with Brandon, CEO — a deep dive on your experience, devrel strategy, tactics, and information architectureAnother short call with Brandon for both of us to ask and answer questions in preparation for making an offer 📣If you’d like to hear about future job openings, sign up here. "
https://news.ycombinator.com/rss,"Porth, It's Like Forth but in Python",https://gitlab.com/tsoding/porth,Comments,"






P



porth






Project ID: 30419193








Star
250






1,189 Commits

1 Branch

0 Tags

16.1 MB Project Storage








Concatenative Programming Language for Computers


Read more
























Find file




Select Archive Format




Download source code


zip
tar.gz
tar.bz2
tar









Clone






Clone with SSH










Clone with HTTPS











Open in your IDE



Visual Studio Code (SSH)




Visual Studio Code (HTTPS)




IntelliJ IDEA (SSH)




IntelliJ IDEA (HTTPS)







Copy HTTPS clone URL





Copy SSH clone URLgit@gitlab.com:tsoding/porth.git


Copy HTTPS clone URLhttps://gitlab.com/tsoding/porth.git








README

MIT License

CONTRIBUTING





"
https://news.ycombinator.com/rss,I analyzed shuffling in a million games of MtG Arena (2020),https://old.reddit.com/r/MagicArena/comments/b21u3n/i_analyzed_shuffling_in_a_million_games/,Comments,"



Too Many Requests



whoa there, pardner!
we're sorry, but you appear to be a bot and we've seen too many requests
from you lately. we enforce a hard speed limit on requests that appear to come
from bots to prevent abuse.
if you are not a bot but are spoofing one via your browser's user agent
string: please change your user agent string to avoid seeing this message
again.
please wait 1 second(s) and try again.
as a reminder to developers, we recommend that clients make no
    more than one
    request every two seconds to avoid seeing this message.


"
https://news.ycombinator.com/rss,Faster than the filesystem (2021),https://www.sqlite.org/fasterthanfs.html,Comments,"




35% Faster Than The Filesystem









Small. Fast. Reliable.Choose any three.



Home
Menu
About
Documentation
Download
License
Support
Purchase

Search




About
Documentation
Download
Support
Purchase





Search Documentation
Search Changelog










35% Faster Than The Filesystem



►
Table Of Contents

1. Summary
1.1. Caveats
1.2. Related Studies
2. How These Measurements Are Made
2.1. Read Performance Measurements
2.2. Write Performance Measurements
2.3. Variations
3. General Findings
4. Additional Notes
4.1. Compiling And Testing on Android




1. Summary
SQLite reads and writes small blobs (for example, thumbnail images)
35% faster¹ than the same blobs
can be read from or written to individual files on disk using
fread() or fwrite().

Furthermore, a single SQLite database holding
10-kilobyte blobs uses about 20% less disk space than
storing the blobs in individual files.

The performance difference arises (we believe) because when
working from an SQLite database, the open() and close() system calls
are invoked only once, whereas
open() and close() are invoked once for each blob
when using blobs stored in individual files.  It appears that the
overhead of calling open() and close() is greater than the overhead
of using the database.  The size reduction arises from the fact that
individual files are padded out to the next multiple of the filesystem
block size, whereas the blobs are packed more tightly into an SQLite
database.


The measurements in this article were made during the week of 2017-06-05
using a version of SQLite in between 3.19.2 and 3.20.0.  You may expect
future versions of SQLite to perform even better.

1.1. Caveats


¹The 35% figure above is approximate.  Actual timings vary
depending on hardware, operating system, and the
details of the experiment, and due to random performance fluctuations
on real-world hardware.  See the text below for more detail.
Try the experiments yourself.  Report significant deviations on
the SQLite forum.


The 35% figure is based on running tests on every machine
that the author has easily at hand.
Some reviewers of this article report that SQLite has higher 
latency than direct I/O on their systems.  We do not yet understand
the difference.  We also see indications that SQLite does not
perform as well as direct I/O when experiments are run using
a cold filesystem cache.


So let your take-away be this: read/write latency for
SQLite is competitive with read/write latency of individual files on
disk.  Often SQLite is faster.  Sometimes SQLite is almost
as fast.  Either way, this article disproves the common
assumption that a relational database must be slower than direct
filesystem I/O.

1.2. Related Studies

Jim Gray
and others studied the read performance of BLOBs
versus file I/O for Microsoft SQL Server and found that reading BLOBs 
out of the 
database was faster for BLOB sizes less than between 250KiB and 1MiB.
(Paper).
In that study, the database still stores the filename of the content even
if the content is held in a separate file.  So the database is consulted
for every BLOB, even if it is only to extract the filename.  In this
article, the key for the BLOB is the filename, so no preliminary database
access is required.  Because the database is never used at all when
reading content from individual files in this article, the threshold
at which direct file I/O becomes faster is smaller than it is in Gray's
paper.


The Internal Versus External BLOBs article on this website is an
earlier investigation (circa 2011) that uses the same approach as the
Jim Gray paper — storing the blob filenames as entries in the
database — but for SQLite instead of SQL Server.



2. How These Measurements Are Made
I/O performance is measured using the
kvtest.c program
from the SQLite source tree.
To compile this test program, first gather the kvtest.c source file
into a directory with the SQLite amalgamation source
files ""sqlite3.c"" and ""sqlite3.h"".  Then on unix, run a command like
the following:

gcc -Os -I. -DSQLITE_DIRECT_OVERFLOW_READ \
  kvtest.c sqlite3.c -o kvtest -ldl -lpthread

Or on Windows with MSVC:

cl -I. -DSQLITE_DIRECT_OVERFLOW_READ kvtest.c sqlite3.c

Instructions for compiling for Android
are shown below.


Use the resulting ""kvtest"" program to
generate a test database with 100,000 random uncompressible
blobs, each with a random
size between 8,000 and 12,000 bytes
using a command like this:

./kvtest init test1.db --count 100k --size 10k --variance 2k


If desired, you can verify the new database by running this command:

./kvtest stat test1.db


Next, make copies of all the blobs into individual files in a directory
using a command like this:

./kvtest export test1.db test1.dir


At this point, you can measure the amount of disk space used by
the test1.db database and the space used by the test1.dir directory
and all of its content.  On a standard Ubuntu Linux desktop, the
database file will be 1,024,512,000 bytes in size and the test1.dir
directory will use 1,228,800,000 bytes of space (according to ""du -k""),
about 20% more than the database.


The ""test1.dir"" directory created above puts all the blobs into a single
folder.  It was conjectured that some operating systems would perform 
poorly when a single directory contains 100,000 objects.  To test this,
the kvtest program can also store the blobs in a hierarchy of folders with no
more than 100 files and/or subdirectories per folder.  The alternative
on-disk representation of the blobs can be created using the --tree
command-line option to the ""export"" command, like this:

./kvtest export test1.db test1.tree --tree


The test1.dir directory will contain 100,000 files
with names like ""000000"", ""000001"", ""000002"" and so forth but the
test1.tree directory will contain the same files in subdirectories like
""00/00/00"", ""00/00/01"", and so on.  The test1.dir and test1.test
directories take up approximately the same amount of space, though
test1.test is very slightly larger due to the extra directory entries.


All of the experiments that follow operate the same with either 
""test1.dir"" or ""test1.tree"".  Very little performance difference is
measured in either case, regardless of operating system.


Measure the performance for reading blobs from the database and from
individual files using these commands:

./kvtest run test1.db --count 100k --blob-api
./kvtest run test1.dir --count 100k --blob-api
./kvtest run test1.tree --count 100k --blob-api


Depending on your hardware and operating system, you should see that reads 
from the test1.db database file are about 35% faster than reads from 
individual files in the test1.dir or test1.tree folders.  Results can vary
significantly from one run to the next due to caching, so it is advisable
to run tests multiple times and take an average or a worst case or a best
case, depending on your requirements.

The --blob-api option on the database read test causes kvtest to use
the sqlite3_blob_read() feature of SQLite to load the content of the
blobs, rather than running pure SQL statements.  This helps SQLite to run
a little faster on read tests.  You can omit that option to compare the
performance of SQLite running SQL statements.
In that case, the SQLite still out-performs direct reads, though
by not as much as when using sqlite3_blob_read().
The --blob-api option is ignored for tests that read from individual disk
files.


Measure write performance by adding the --update option.  This causes
the blobs are overwritten in place with another random blob of
exactly the same size.

./kvtest run test1.db --count 100k --update
./kvtest run test1.dir --count 100k --update
./kvtest run test1.tree --count 100k --update


The writing test above is not completely fair, since SQLite is doing
power-safe transactions whereas the direct-to-disk writing is not.
To put the tests on a more equal footing, add either the --nosync
option to the SQLite writes to disable calling fsync() or
FlushFileBuffers() to force content to disk, or using the --fsync option
for the direct-to-disk tests to force them to invoke fsync() or
FlushFileBuffers() when updating disk files.


By default, kvtest runs the database I/O measurements all within
a single transaction.  Use the --multitrans option to run each blob
read or write in a separate transaction.  The --multitrans option makes
SQLite much slower, and uncompetitive with direct disk I/O.  This
option proves, yet again, that to get the most performance out of
SQLite, you should group as much database interaction as possible within
a single transaction.


There are many other testing options, which can be seen by running
the command:

./kvtest help

2.1. Read Performance Measurements
The chart below shows data collected using 
kvtest.c on five different
systems:


Win7: A circa-2009 Dell Inspiron laptop, Pentium dual-core
    at 2.30GHz, 4GiB RAM, Windows7.
Win10: A 2016 Lenovo YOGA 910, Intel i7-7500 at 2.70GHz,
    16GiB RAM, Windows10.
Mac: A 2015 MacBook Pro, 3.1GHz intel Core i7, 16GiB RAM,
    MacOS 10.12.5
Ubuntu: Desktop built from Intel i7-4770K at 3.50GHz, 32GiB RAM,
    Ubuntu 16.04.2 LTS
Android: Galaxy S3, ARMv7, 2GiB RAM

All machines use SSD except Win7 which has a
hard-drive. The test database is 100K blobs with sizes uniformly
distributed between 8K and 12K, for a total of about 1 gigabyte
of content.  The database page size
is 4KiB.  The -DSQLITE_DIRECT_OVERFLOW_READ compile-time option was
used for all of these tests.
Tests were run multiple times.
The first run was used to warm up the cache and its timings were discarded.


The chart below shows average time to read a blob directly from the
filesystem versus the time needed to read the same blob from the SQLite 
database.
The actual timings vary considerably from one system to another 
(the Ubuntu desktop is much
faster than the Galaxy S3 phone, for example).  
This chart shows the ratio of the
times needed to read blobs from a file divided by the time needed to
from the database.  The left-most column in the chart is the normalized
time to read from the database, for reference.


In this chart, an SQL statement (""SELECT v FROM kv WHERE k=?1"") 
is prepared once.  Then for each blob, the blob key value is bound 
to the ?1 parameter and the statement is evaluated to extract the
blob content.


The chart shows that on Windows10, content can be read from the SQLite
database about 5 times faster than it can be read directly from disk.
On Android, SQLite is only about 35% faster than reading from disk.






Chart 1:  SQLite read latency relative to direct filesystem reads.
100K blobs, avg 10KB each, random order using SQL


The performance can be improved slightly by bypassing the SQL layer
and reading the blob content directly using the
sqlite3_blob_read() interface, as shown in the next chart:






Chart 2:  SQLite read latency relative to direct filesystem reads.
100K blobs, avg size 10KB, random order
using sqlite3_blob_read().


Further performance improves can be made by using the
memory-mapped I/O feature of SQLite.  In the next chart, the
entire 1GB database file is memory mapped and blobs are read
(in random order) using the sqlite3_blob_read() interface.
With these optimizations, SQLite is twice as fast as Android
or MacOS-X and over 10 times faster than Windows.






Chart 3:  SQLite read latency relative to direct filesystem reads.
100K blobs, avg size 10KB, random order
using sqlite3_blob_read() from a memory-mapped database.


The third chart shows that reading blob content out of SQLite can be
twice as fast as reading from individual files on disk for Mac and
Android, and an amazing ten times faster for Windows.

2.2. Write Performance Measurements

Writes are slower.
On all systems, using both direct I/O and SQLite, write performance is
between 5 and 15 times slower than reads.


Write performance measurements were made by replacing (overwriting)
an entire blob with a different blob.  All of the blobs in these
experiment are random and incompressible.  Because writes are so much
slower than reads, only 10,000 of the 100,000 blobs in the database
are replaced.  The blobs to be replaced are selected at random and
are in no particular order.


The direct-to-disk writes are accomplished using fopen()/fwrite()/fclose().
By default, and in all the results shown below, the OS filesystem buffers are
never flushed to persistent storage using fsync() or
FlushFileBuffers().  In other words, there is no attempt to make the
direct-to-disk writes transactional or power-safe.
We found that invoking fsync() or FlushFileBuffers() on each file
written causes direct-to-disk storage
to be about 10 times or more slower than writes to SQLite.


The next chart compares SQLite database updates in WAL mode
against raw direct-to-disk overwrites of separate files on disk.
The PRAGMA synchronous setting is NORMAL.
All database writes are in a single transaction.
The timer for the database writes is stopped after the transaction
commits, but before a checkpoint is run.
Note that the SQLite writes, unlike the direct-to-disk writes,
are transactional and power-safe, though because the synchronous
setting is NORMAL instead of FULL, the transactions are not durable.






Chart 4:  SQLite write latency relative to direct filesystem writes.
10K blobs, avg size 10KB, random order,
WAL mode with synchronous NORMAL,
exclusive of checkpoint time


The android performance numbers for the write experiments are omitted
because the performance tests on the Galaxy S3 are so random.  Two
consecutive runs of the exact same experiment would give wildly different
times.  And, to be fair, the performance of SQLite on android is slightly
slower than writing directly to disk.


The next chart shows the performance of SQLite versus direct-to-disk
when transactions are disabled (PRAGMA journal_mode=OFF)
and PRAGMA synchronous is set to OFF.  These settings put SQLite on an
equal footing with direct-to-disk writes, which is to say they make the
data prone to corruption due to system crashes and power failures.






Chart 5:  SQLite write latency relative to direct filesystem writes.
10K blobs, avg size 10KB, random order,
journaling disabled, synchronous OFF.


In all of the write tests, it is important to disable anti-virus software
prior to running the direct-to-disk performance tests.  We found that
anti-virus software slows down direct-to-disk by an order of magnitude
whereas it impacts SQLite writes very little.  This is probably due to the
fact that direct-to-disk changes thousands of separate files which all need
to be checked by anti-virus, whereas SQLite writes only changes the single
database file.

2.3. Variations
The -DSQLITE_DIRECT_OVERFLOW_READ compile-time option causes SQLite
to bypass its page cache when reading content from overflow pages.  This
helps database reads of 10K blobs run a little faster, but not all that much
faster.  SQLite still holds a speed advantage over direct filesystem reads
without the SQLITE_DIRECT_OVERFLOW_READ compile-time option.

Other compile-time options such as using -O3 instead of -Os or
using -DSQLITE_THREADSAFE=0 and/or some of the other
recommended compile-time options might help SQLite to run even faster
relative to direct filesystem reads.

The size of the blobs in the test data affects performance.
The filesystem will generally be faster for larger blobs, since
the overhead of open() and close() is amortized over more bytes of I/O,
whereas the database will be more efficient in both speed and space
as the average blob size decreases.


3. General Findings


SQLite is competitive with, and usually faster than, blobs stored in
separate files on disk, for both reading and writing.


SQLite is much faster than direct writes to disk on Windows
when anti-virus protection is turned on.  Since anti-virus software
is and should be on by default in Windows, that means that SQLite
is generally much faster than direct disk writes on Windows.


Reading is about an order of magnitude faster than writing, for all
systems and for both SQLite and direct-to-disk I/O.


I/O performance varies widely depending on operating system and hardware.
Make your own measurements before drawing conclusions.


Some other SQL database engines advise developers to store blobs in separate
files and then store the filename in the database.  In that case, where
the database must first be consulted to find the filename before opening
and reading the file, simply storing the entire blob in the database
gives much faster read and write performance with SQLite.
See the Internal Versus External BLOBs article for more information.

4. Additional Notes

4.1. Compiling And Testing on Android

The kvtest program is compiled and run on Android as follows.
First install the Android SDK and NDK.  Then prepare a script
named ""android-gcc"" that looks approximately like this:

#!/bin/sh
#
NDK=/home/drh/Android/Sdk/ndk-bundle
SYSROOT=$NDK/platforms/android-16/arch-arm
ABIN=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin
GCC=$ABIN/arm-linux-androideabi-gcc
$GCC --sysroot=$SYSROOT -fPIC -pie $*

Make that script executable and put it on your $PATH.  Then
compile the kvtest program as follows:

android-gcc -Os -I. kvtest.c sqlite3.c -o kvtest-android

Next, move the resulting kvtest-android executable to the Android
device:

adb push kvtest-android /data/local/tmp

Finally use ""adb shell"" to get a shell prompt on the Android device,
cd into the /data/local/tmp directory, and begin running the tests
as with any other unix host.
This page last modified on  2021-03-01 12:55:48 UTC 
"
https://news.ycombinator.com/rss,The Inner Beauty of Basic Electronics,https://spectrum.ieee.org/open-circuits,Comments,"The Inner Beauty of Basic Electronics - IEEE SpectrumIEEE.orgIEEE Xplore Digital LibraryIEEE StandardsMore SitesSign InJoin IEEEThe Inner Beauty of Basic ElectronicsShareFOR THE TECHNOLOGY INSIDERSearch: Explore by topicAerospaceArtificial IntelligenceBiomedicalComputingConsumer ElectronicsEnergyHistory of TechnologyRoboticsSemiconductorsSensorsTelecommunicationsTransportationIEEE SpectrumFOR THE TECHNOLOGY INSIDERTopicsAerospaceArtificial IntelligenceBiomedicalComputingConsumer ElectronicsEnergyHistory of TechnologyRoboticsSemiconductorsSensorsTelecommunicationsTransportationSectionsFeaturesNewsOpinionCareersDIYThe Big PictureEngineering ResourcesMoreSpecial ReportsCollectionsExplainersPodcastsVideosNewslettersTop Programming LanguagesRobots GuideFor IEEE MembersCurrent IssueMagazine ArchiveThe InstituteTI ArchiveFor IEEE MembersCurrent IssueMagazine ArchiveThe InstituteTI ArchiveIEEE SpectrumAbout UsContact UsReprints & PermissionsAdvertisingFollow IEEE SpectrumSupport IEEE SpectrumIEEE Spectrum is the flagship publication of the IEEE — the world’s largest professional organization devoted to engineering and applied sciences. Our articles, podcasts, and infographics inform our readers about developments in technology, engineering, and science.Join IEEESubscribeAbout IEEEContact & SupportAccessibilityNondiscrimination PolicyTermsIEEE Privacy Policy© Copyright 2023 IEEE — All rights reserved. A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
                        view privacy policy
                    
                    accept & close
                Enjoy more free content and benefits by creating an accountSaving articles to read later requires an IEEE Spectrum accountThe Institute content is only available for membersDownloading full PDF issues is exclusive for IEEE MembersAccess to Spectrum's Digital Edition is exclusive for IEEE MembersFollowing topics is a feature exclusive for IEEE MembersAdding your response to an article requires an IEEE Spectrum accountCreate an account to access more content and features on IEEE Spectrum, including the ability to save articles to read later, download Spectrum Collections, and participate in conversations with readers and editors. For more exclusive content and features, consider Joining IEEE.Join the world’s largest professional organization devoted to engineering and applied sciences and get access to all of Spectrum’s articles, archives, PDF downloads, and other benefits. Learn more →CREATE AN ACCOUNTSIGN INJOIN IEEESIGN INCloseAccess Thousands of Articles — Completely FreeCreate an account and get exclusive content and features: Save articles, download collections, and talk to tech insiders — all free! For full access and benefits, join IEEE as a paying member.CREATE AN ACCOUNTSIGN INConsumer ElectronicsTopicMagazineTypeFeature
        The Inner Beauty of Basic Electronics
    Open Circuits showcases the surprising complexity of passive componentsEric SchlaepferWindell H. Oskay20h5 min readBlueEric Schlaepfer was trying to fix a broken piece of test equipment when he came across the cause of the problem—a troubled tantalum capacitor. The component had somehow shorted out, and he wanted to know why. So he polished it down for a look inside. He never found the source of the short, but he and his collaborator, Windell H. Oskay, discovered something even better: a breathtaking hidden world inside electronics. What followed were hours and hours of polishing, cleaning, and photography that resulted in Open Circuits: The Inner Beauty of Electronic Components (No Starch Press, 2022), an excerpt of which follows. As the authors write, everything about these components is deliberately designed to meet specific technical needs, but that design leads to “accidental beauty: the emergent aesthetics of things you were never expected to see.”

	From a book that spans the wide world of electronics, what we at 
	IEEE Spectrum found surprisingly compelling were the insides of things we don’t spend much time thinking about, passive components. Transistors, LEDs, and other semiconductors may be where the action is, but the simple physics of resistors, capacitors, and inductors have their own sort of splendor. 
            
                High-Stability Film Resistor
            
            
        All photos by Eric Schlaepfer & Windell H. OskayThis high-stability film resistor, about 4 millimeters in diameter, is made in much the same way as its inexpensive carbon-film cousin, but with exacting precision. A ceramic rod is coated with a fine layer of resistive film (thin metal, metal oxide, or carbon) and then a perfectly uniform helical groove is machined into the film.Instead of coating the resistor with an epoxy, it’s hermetically sealed in a lustrous little glass envelope. This makes the resistor more robust, ideal for specialized cases such as precision reference instrumentation, where long-term stability of the resistor is critical. The glass envelope provides better isolation against moisture and other environmental changes than standard coatings like epoxy.
            
                15-Turn Trimmer Potentiometer
            
            
        It takes 15 rotations of an adjustment screw to move a 15-turn trimmer potentiometer from one end of its resistive range to the other. Circuits that need to be adjusted with fine resolution control use this type of trimmer pot instead of the single-turn variety.The resistive element in this trimmer is a strip of cermet—a composite of ceramic and metal—silk-screened on a white ceramic substrate. Screen-printed metal links each end of the strip to the connecting wires. It’s a flattened, linear version of the horseshoe-shaped resistive element in single-turn trimmers.Turning the adjustment screw moves a plastic slider along a track. The wiper is a spring finger, a spring-loaded metal contact, attached to the slider. It makes contact between a metal strip and the selected point on the strip of resistive film.
            
                Ceramic Disc Capacitor
            
            
        Capacitors are fundamental electronic components that store energy in the form of static electricity. They’re used in countless ways, including for bulk energy storage, to smooth out electronic signals, and as computer memory cells. The simplest capacitor consists of two parallel metal plates with a gap between them, but capacitors can take many forms so long as there are two conductive surfaces, called electrodes, separated by an insulator.A ceramic disc capacitor is a low-cost capacitor that is frequently found in appliances and toys. Its insulator is a ceramic disc, and its two parallel plates are extremely thin metal coatings that are evaporated or sputtered onto the disc’s outer surfaces. Connecting wires are attached using solder, and the whole assembly is dipped into a porous coating material that dries hard and protects the capacitor from damage.
            
                Film Capacitor
            
            
        Film capacitors are frequently found in high-quality audio equipment, such as headphone amplifiers, record players, graphic equalizers, and radio tuners. Their key feature is that the dielectric material is a plastic film, such as polyester or polypropylene.The metal electrodes of this film capacitor are vacuum-deposited on the surfaces of long strips of plastic film. After the leads are attached, the films are rolled up and dipped into an epoxy that binds the assembly together. Then the completed assembly is dipped in a tough outer coating and marked with its value.Other types of film capacitors are made by stacking flat layers of metallized plastic film, rather than rolling up layers of film.
            
                Dipped Tantalum Capacitor
            
            
        At the core of this capacitor is a porous pellet of tantalum metal. The pellet is made from tantalum powder and sintered, or compressed at a high temperature, into a dense, spongelike solid.Just like a kitchen sponge, the resulting pellet has a high surface area per unit volume. The pellet is then anodized, creating an insulating oxide layer with an equally high surface area. This process packs a lot of capacitance into a compact device, using spongelike geometry rather than the stacked or rolled layers that most other capacitors use.The device’s positive terminal, or anode, is connected directly to the tantalum metal. The negative terminal, or cathode, is formed by a thin layer of conductive manganese dioxide coating the pellet.
            
                Axial Inductor
            
            
        Inductors are fundamental electronic components that store energy in the form of a magnetic field. They’re used, for example, in some types of power supplies to convert between voltages by alternately storing and releasing energy. This energy-efficient design helps maximize the battery life of cellphones and other portable electronics.Inductors typically consist of a coil of insulated wire wrapped around a core of magnetic material like iron or ferrite, a ceramic filled with iron oxide. Current flowing around the core produces a magnetic field that acts as a sort of flywheel for current, smoothing out changes in the current as it flows through the inductor.This axial inductor has a number of turns of varnished copper wire wrapped around a ferrite form and soldered to copper leads on its two ends. It has several layers of protection: a clear varnish over the windings, a light-green coating around the solder joints, and a striking green outer coating to protect the whole component and provide a surface for the colorful stripes that indicate its inductance value.
            
                Power Supply Transformer
            
            
        This transformer has multiple sets of windings and is used in a power supply to create multiple output AC voltages from a single AC input such as a wall outlet.The small wires nearer the center are “high impedance” turns of magnet wire. These windings carry a higher voltage but a lower current. They’re protected by several layers of tape, a copper-foil electrostatic shield, and more tape.The outer “low impedance” windings are made with thicker insulated wire and fewer turns. They handle a lower voltage but a higher current.All of the windings are wrapped around a black plastic bobbin. Two pieces of ferrite ceramic are bonded together to form the magnetic core at the heart of the transformer.From Your Site ArticlesDell Tried to Hide Bad Capacitors Problem 2003-2005 ›Hands On - IEEE Spectrum ›Watch: Laser Origami Makes Inductors ›Related Articles Around the WebOpen Circuits: The Inner Beauty of Electronic Components: Oskay ... ›Open Circuits | No Starch Press ›Open Circuits ›passive componentsArt of Electronicsresistorscapacitorsinductorsbooks{""imageShortcodeIds"":[]}Eric SchlaepferEric Schlaepfer runs the popular engineering Twitter account @TubeTimeUS, where he posts cross-section photos, shares his retrocomputing and reverse engineering projects, investigates engineering accidents, and even features the occasional vacuum tube or two. He is coauthor of Open Circuits: The Inner Beauty of Electronic Components (No Starch Press, 2022).Windell H. OskayWindell H. Oskay is the cofounder of Evil Mad Scientist Laboratories, where he designs robots for a living. He is coauthor of Open Circuits: The Inner Beauty of Electronic Components (No Starch Press, 2022).The Conversation (0)
        Video Friday: Robots at Night
    13 Jan 20233 min readAerospaceTopicTypeRoboticsNews
        Relativity Space Aims for Orbit
    13 Jan 20234 min readConsumer ElectronicsTopicTypeNews
        Paper Batteries, Blue Quantum Dots, and Other Enabling Technologies from CES 2023
    12 Jan 20233 min readThe InstituteTopicArticleTypeHistory of Technology
        How This Record Company Engineer Invented the CT Scanner
    The machine, made to image the human brain, won him a Nobel PrizeJoanna GoodrichJoanna Goodrich is the associate editor of The Institute, covering the work and accomplishments of IEEE members and IEEE and technology-related events. She has a master's degree in health communications from Rutgers University, in New Brunswick, N.J.12 Jan 20234 min readResearch engineer Godfrey Hounsfield invented the CT scanner to create three-dimensional brain images.
        PA Images/Getty Images
    ieee historyieee tech historyhistory of technologyct scannermedical devicesieee milestonetype:tiThe inspiration for computed tomography (CT) came from a chance conversation that research engineer Godfrey Hounsfield had with a doctor while on vacation in the 1960s. The physician complained that X-ray images of the brain were too grainy and only two-dimensional.Hounsfield worked at Electrical and Musical Industry in Hayes, England. Best known for producing and selling Beatles records, EMI also developed electronic equipment. Keep Reading ↓Show lessConsumer ElectronicsTopicTypeComputingSponsored Article
        Building the Future of Smart Home Security
    Engineers must invent new technology to enhance security products’ abilitiesNate WilfertNate Wilfert is Vice President of Software Engineering at SimpliSafe.22 Mar 20224 min readIn this article, SimpliSafe’s VP of Software Engineering discusses his team’s focus on creating a safer future through enhanced technology.
        SimpliSafe
    smart homeiotconnected homesecuritysimplisafeThis is a sponsored article brought to you by SimpliSafe.It’s nearly impossible to find a household today that doesn’t have at least one connected smart home device installed. From video doorbells to robot vacuums, automated lighting, and voice assistants, smart home technology has invaded consumers’ homes and shows no sign of disappearing anytime soon. Indeed, according to a study conducted by consulting firm Parks Associates, smart home device adoption has increased by more than 64 percent in the past two years, with 23 percent of households owning three or more smart home devices. This is particularly true for devices that provide security with 38 percent of Americans owning a home security product. This percentage is likely to increase as 7 in 10 homebuyers claimed that safety and security was the primary reason, after convenience, that they would be seeking out smart homes, according to a report published by Security.org last year.As the demand for smart home security grows, it’s pertinent that the engineers who build the products and services that keep millions of customers safe continue to experiment with new technologies that could enhance overall security and accessibility. At SimpliSafe, an award-winning home security company based in Boston, Mass., it is the pursuit of industry-leading protection that drives the entire organization to continue innovating.In this article, Nate Wilfert, VP of Software Engineering at SimpliSafe, discusses the complex puzzles his team is solving on a daily basis—such as applying artificial intelligence (AI) technology into cameras and building load-balancing solutions to handle server traffic—to push forward the company’s mission to make every home secure and advance the home security industry as a whole.Keep Reading ↓Show less
        Trending Stories
    The most-read stories on IEEE Spectrum right nowThe InstituteTopicArticleTypeHistory of Technology
        How This Record Company Engineer Invented the CT Scanner
    12 Jan 20234 min readAerospaceTopicTypeRoboticsNews
        Relativity Space Aims for Orbit
    13 Jan 20234 min readConsumer ElectronicsTopicTypeNews
        Paper Batteries, Blue Quantum Dots, and Other Enabling Technologies from CES 2023
    12 Jan 20233 min readConsumer ElectronicsTopicTypeNews
        CES 2023’s Four Wildest—and Catchiest—Gadgets
    11 Jan 20233 min readThe InstituteTopicTypeOpinionTelecommunications
        Examining the Impact of 6G Telecommunications on Society
    10 Jan 20233 min readSensorsTopicArtificial IntelligenceTypeNews
        Spray-on Smart Skin Reads Typing and Hand Gestures
    11 Jan 20233 min readTelecommunicationsTopicMagazineTypeFeature
        How Police Exploited the Capitol Riot’s Digital Records
    06 Jan 202311 min readConsumer ElectronicsTopicTypeNewsTransportation
        The Best Tech of CES 2023
    09 Jan 20236 min read"
https://news.ycombinator.com/rss,Running KDE Plasma on RISC-V VisionFive-2,https://cordlandwehr.wordpress.com/2023/01/14/running-plasma-on-visionfive-2/,Comments,"


Running Plasma on VisionFive-2 

New year, new RISC-V Yocto blog post \o/ When I wrote my last post, I did really not expect my brand new VisionFive-2 board to find its way to me so soon… But well, a week ago it was suddenly there. While unpacking I shortly pondered over my made plans to prepare a Plasma Bigscreen RaspberryPi 4 demo board for this year’s FOSDEM.
Obvious conclusion: “Screw it! Let’s do the demo on the VisionFive-2!” — And there we are:
After some initial bumpy steps to boot up a first self-compiled U-boot and Kernel (If you unbox a new board, you need to do a bootloader and firmware update first! Otherwise it will not boot the latest VisionFive Kernel) it was surprisingly easy to prepare Yocto to build a core-image-minimal that really boots the whole way up.
Unfortunately after these first happy hours, the last week was full of handling the horrors of closed-source binary drivers for the GPU. Even though Imagination promised to provide an open source driver at some time, right now there is only the solution to use the closed source PVR driver. After quite a lot of trying, guessing and and comparing the boot and init sequences of the reference image to the dark screen in front of me, I came up with:

a new visionfive2-graphics Yocto package for the closed source driver blobs
a fork of Mesa that uses a very heavy patch set for the PVR driver adaptions; all patches are taken from the VisionFive 2 buildroot configurations
and a couple of configs for making the system start with doing an initial modeset

The result right now:

VisionFive-2 device with Plasma-Bigscreen (KWin running via Wayland), SD card image built via Yocto, KDE software via KDE’s Yocto layers, Kernel and U-Boot being the latest fork versions from StarFive
Actually, the full UI even feels much smoother than on my RPi4, which is quite cool. I am not sure where I will end in about 3 weeks with some more debugging and patching. But I am very confident that you can see a working RISC-V board with onboard GPU and running Plasma Shell, when you visit the KDE stall at FOSDEM in February 😉
For people who are interested in Yocto, here is the WIP patch set: https://github.com/riscv/meta-riscv/pull/382
Share this:TwitterFacebookLike this:Like Loading...

Related
 

Posted on January 14, 2023January 14, 2023Author cordlandwehrCategories KDE, YoctoTags KDE 



Leave a Reply Cancel reply


Enter your comment here...




Fill in your details below or click an icon to log in:







 



 



 






 
 


Email (required) (Address never made public)



Name (required)



Website
















			You are commenting using your WordPress.com account.			
				( Log Out / 
				Change )
			
















			You are commenting using your Twitter account.			
				( Log Out / 
				Change )
			
















			You are commenting using your Facebook account.			
				( Log Out / 
				Change )
			






Cancel
Connecting to %s




 Notify me of new comments via email. Notify me of new posts via email.
 



Δ 



Post navigation
Previous Previous post: Getting a First Picture on my Nezha RISC-V Board

"
https://news.ycombinator.com/rss,The Bibites: Artificial Life Simulation,https://leocaussan.itch.io/the-bibites,Comments,"The Bibites by The BibitesFollow The BibitesFollowFollowing The BibitesFollowingAdd To CollectionCollectionCommentsDevlogRelated gamesRelatedThe BibitesA downloadable project for Windows, macOS, and LinuxDownload NowName your own priceWelcome everyone! 
This is The Bibites  
A simulation where you are able to watch evolution happen before your very eyes! 
Each bibite (the small critters you see on the screen) starts off with an empty brain (they do nothing) and pretty basic genes (they all look alike). 
Through random mutations, one can be spawned with a brain connection that will link two neurons and might trigger a behavior, like going forward, which will allow them to eat food, and then reproduce with the energy gained.
You have reproduction, mutations, and natural selection, which leads to ... 

With time, this develops into complex behaviors, like following pheromone trails to hunt other bibites, or stockpiling food in a specific area of the map. 

Present Features
VisionProcedural Sprites (generating a custom sprite for each bibite from their genes)
Self-awareness (state, health, energy, etc.)Pheromones (producing and sensing)Grabbing and Throwing stuff (pellets and other bibites)Materials and Digestion SimulationRealistic Energy System
The simulation is also interactive, allowing you to YEET bibites and pellets around. You can selectively kill bibites, feed them, force the laying of eggs, and so much more.
It's also highly customizable, allowing you to test a nearly infinite number of scenarios. How will they evolve if there is no drag (no friction)? What about if moving is extremely energy-costly? It's your job to test it all, I sure can't do it by myself.
I'LL STATE CLEARLY THAT THIS IS THE REGULAR VERSION. I TRIED TO DISABLE ""name your own price"" AND SET IT TO 0.00$ BUT IT DON'T SEEM TO WORK...I ENCOURAGE YOU TO DOWNLOAD THIS FOR FREE, ONLY PAY SOMETHING IF YOU WANT TO THROW MONEY AT ME FOR NO OTHER REASON THAN TO SUPPORT THIS PROJECT. The best way to do so is to subscribe to my Patreon to provide me with reliable support and have access to the alpha updates as I develop them: 
Become a Patron to get alpha updates! 

Follow the development and see additional content on Youtube

Follow me on Twitter to see... whatever I do there

Join the subreddit community

Upcoming features 
Module-based systems for unbounded evolution and incredible performancesBiomes (environmental simulation)Evolving ecosystems (the plants/food evolves too)Rocks (Movable objects)And much more!

After trying it out, please give me some feedback

Or report bugs
More informationUpdated 28 days agoStatusIn developmentPlatformsWindows, macOS, LinuxRatingRated 4.6 out of 5 stars(69 total ratings)AuthorThe BibitesGenreSimulationMade withUnityTags2D, artificial-intelligence, evolution, interactive, Life Simulation, Pixel Art, Procedural Generation, Sandbox, UnityAverage sessionA few hoursLanguagesEnglishInputsKeyboard, MouseMultiplayerLocal multiplayerPlayer countSingleplayerLinksYouTube, Patreon, Twitter, CommunityDownloadDownload NowName your own priceClick download now to get access to the following files:The Bibites 0.4.2 - Windows 64x.zip 30 MB  The Bibites 0.4.2 - Linux.zip 41 MB  The Bibites 0.4.2 - Mac Universal.zip 36 MB  The Bibites 0.4.2 - Windows 32x.zip 27 MB  The Bibites 0.5.0 - Linux.zip 84 MB  The Bibites 0.5.0 - Windows 32x.zip 70 MB  The Bibites 0.5.0 - Mac Universal.zip 98 MB  The Bibites 0.5.0 - Windows 64x.zip 73 MB  Development logThe Bibites 0.5.0: Modernity and Progress 28 days agoThe Bibites  v0.4.2: Balance and stability Jun 19, 2022The Bibites  v0.4.1 Mar 28, 2022The Bibites 0.3.0 : Artificial Life With Herding and Viruses Jun 25, 2021It's official, this is launch 🚀🚀🚀! Full-time on The Bibites May 20, 2021Roadmap for the future of the project Ep.3 Procedural Sprites! Jan 24, 2021Roadmap for the future of the project Ep.2 Modules! Jan 11, 2021Roadmap of the future of the Project Ep.1 Dec 28, 2020View all postsCommentsLog in with itch.io to leave a comment.Viewing most recent comments 1 to 40 of 112 · Next page · Last page Waterloo057 hours agoso dowload the game... from where do i enter to it?Reply Davket00520 hours ago!!!Reply Davket00520 hours agohow the hell do I download itReply Victoria_the_cool4 days agoi cant figure out how to save my progressReply Victoria_the_cool4 days agoyou should probably implement in-game save filesReply Riptides_storm2 days agohit settings top right, save gameleads to a menu where u can name the save file for 0.5 btwReply JoeKing295 days agoWill you add android version? (if it is possible)Reply TheSmartBanana5 days agoIs there an opotion that allows you to paint or upload your own bit parts like texture packs in minecraft?Reply Filipcucumer18 days agoIf i have a problem how do i report it?Reply R0fael25 days ago(+1)It's the best simulator of lifeReply The Bibites25 days ago(+1)Thanks!Reply R0fael23 days ago (1 edit) Can you fix 0 fps when you have more than 100 bibitesReply johnnysmith10 days agoprobably a hardware issue (bad computer)Reply R0fael7 days agono, it can run windows 11Reply Riptides_storm2 days agothat dosn't really determine how good you computer is.Reply coryedora28 days ago(+1)(-3)how I download the game?Reply Crknite!26 days ago(+10)By completing elementary school.Reply Nikki_Devil31 days ago(+2)I wanted to know, will the Linux version also be compiled to Arm64 processors ? I'd really like to use this on my server but as of now I can't and am stuck with my 15yo 32bit pc :,)Reply EKKN38 days ago(+5)Great game, recommended.Even though there are bugs and uncompleted features, it is a decent and very interesting game.Good luck on developing the game!Reply tosety56 days ago(+5)(-1)running on Ubuntu 20 I get a grey screen and cursor, but nothing elseReply Methisa53 days ago (1 edit) (+4)(-1)same for me on steamdeck running steam os. Happened on v 3.0 and 4.2Reply geomagas27 days ago(+1)Same here!Reply Friday_13th56 days ago(+1)I think we rly need some multi core optimizations. Program struggles a lot when there is a high birth rate bibite developedReply baulerbonduc63 days ago(+2)(-1)hey this don't work on linux.please fix if you canReply raktul89 days ago(+1)I like the o & g binding to find oldest and highest generation respectfully, but would love to add more search features/ toggles between multiple bit bits of the same generation. Not sure how to support the development(as in offering my own time/skills to learn and implement)Reply Garyizcool103 days ago(+2)I got to play it once but now its not letting me go on. not sure if its my computer or some sort of glitch but I'm getting a new computer today so we will see if it works then :). if this has happened to anyone else and they know how to solve it could you please help?Reply ThemonstousBibiteengineer111 days ago(-2)how download simReply Skyper111 days ago(+1)Is there a way to save the simulation?Reply Victoria_the_cool2 days agoi hope he adds in-game save filesReply Magnet Boi117 days ago(+5)When will neural netork editor be available?Reply bloodytomb122 days ago(-1)my own thing is how do i use neural network editorReply Phoenix_185128 days ago(+2)How do you download this on linux/chromebook? It will be very helpful if someone can tell me.Reply sssemil130 days ago(+4)To fix the blue screen on Linux, run with the following parameter: -force-vulkan. Cheers.Reply HotNoob130 days ago(+3)
./'The Bibites.x86_64' -force-vulkanWorks! thx.Reply jinnturtle130 days ago (1 edit) (+5)Stuck on a dark blue screen immediately after running the executable, nothing seems to change even if I let it sit there for a while.OS: distro is ArchLinux running on kernell v5.18.15GPU: Nvidia GeForce 1060s ; driver version 515.57CPU: Intel i5-9400FGame version in question: The Bibites 0.4.2The game/sim looks quite interesting from what I've seen and read of it, well done!Reply juega331131 days ago(+1)Is this going to be on Android at some point?Reply Fiddeou131 days ago(+4)I'm on mac Mojave. I open the game, and when loading it just stops at 50%Reply damiantyler8a58 days ago(+2)same, why does this happen?Reply ViyWolf56 days ago(+3)You need to put the file into the applications folder.Reply IIDisruptII133 days ago (1 edit) (+4)(-1)Linux version is broken.
After the unity flash screen goes away it get's stuck on a dark blue screen.
Nothing at all, gotta alt f4 or tab out to close.I'm on Ubuntu 22.04.1 LTS x86_64, I hope this get's fixed the game looks super dope.Reply TeDe3152 days ago (1 edit) (+2)Add bodyplans and abylity to change them!!!!!!!! PLSReply Morado161 days ago(+3)I'm having difficulties on macOS, the game hangs on 50%; any suggestion?Reply sunusl157 days ago(+1)I am having the same issueReply RottenLynx165 days ago(+3)The game does not work on linux. There's just a dark blue screen after the unity splash screen.Reply someguyplaysitchgames132 days ago(+1)yeah same hereReply R333999174 days ago(+2)make an android version pleaseReply lilyhavok179 days ago(+1)As a fan of Framsticks, Artificial LIfe ENvironment, and AL:RE this is definitely on my watchlist. I love everything so far, and it runs quite well. Do you plan to allow creating Bibites through genetic programming?Reply String Studios184 days agoWhere source code?Reply helsy185 days ago(+1)wont load :( stuck on 50% permanentlyReply neccarus180 days ago(+1)Had this happen. Moved it to another folder location and it worked. Seems like it was a permissions errorReply helsy180 days ago(+1)i'll try it out! :D thanks for the replyReply Daevan189 days ago(+1)I tried to download it, sadly my Mac says it can't search for malware and the software needs to be updated to do that. Reply Robotex4193 days ago (2 edits) (+4)some of my bibites evolved to ""herd"" with individual prey, chasing it to eat its meat once it dies. I kind of think that the herding node is too advanced and powerful (it seems to completely overpower things like ""pellet concentration angle""), it could have an internal neural network which can also evolve and change, or a better option would be to make it an input node like pellet concentration angle which would simply cause the bibite to accelerate towards its herd (but only when moving slower) if connected to the accelerate node, or turn towards its herd, if connected to the rotate node, this option could also come with making all input/output nodes also being able to be modifier nodes, allowing for strands like """"pellet concentration angle""-""herding""-""rotate"""" which would cause the bibite to turn towards pellets by a value altered by how close/far said pellet is from the herdReply Robotex4193 days ago(+3)Here's my idea to make bibites able to be just a bit better: memory, for example,  if they see a pellet somewhere but pass by it (and no longer see it). They could, with this adaptation, remember where it is anyway, acting as though they can see it even if it's out of view. A memorized thing would no longer need to be sensed to trigger something like ""pellet concentration angle"". This would all work through a ""commit to memory"" node, which would save the bibites location, direction, and everything it senses to a single memory slot, a ""bite"" if you will, but it would not need any extra nodes to call this info, instead, it would be called by the normal sensing nodes if nothing else is found. there would be a ""memory"" stat, which by default would be 0, and would control the number of ""bites"" a bibite can remember, and once a bibite fills its memory older memories are deleted. There could also be a node that would update/replace a memory slot, and another that would delete a memory slot. This memory system would save many splendid bibites from a lonely fate in the void.Reply Jognh199 days ago(+1)I have a glitch with the latest windows version where no matter how much energy my bibites are getting they have a minimum energy loss. If they end up deep in the negative of energy consumption they still lose energy faster, but there seems to be a point (around 0.1 e/s) where it just doesnt lose less or gain any and all my bibites just dieReply EnchantedAxolotl204 days ago (1 edit) (+1)my strong bibites keep throwing themselves into the void :( they are kinda dumb, but i love this game!Reply Kammcorder205 days ago(+3)i am having the hardest time modding the game, can you please make the guide more easy to understand?Reply Lrapava205 days ago(+2)Fix Linux version plzReplyViewing most recent comments 1 to 40 of 112 · Next page · Last pageitch.io·View all by The Bibites·Report·Embed·Updated  28 days agoGames › Simulation › Free"
https://news.ycombinator.com/rss,VToonify: Controllable high-resolution portrait video style transfer,https://github.com/williamyang1991/VToonify,Comments,"








williamyang1991

/

VToonify

Public




 

Notifications



 

Fork
    238




 


          Star
 2.2k
  









        [SIGGRAPH Asia 2022] VToonify: Controllable High-Resolution Portrait Video Style Transfer
      
License





     View license
    






2.2k
          stars
 



238
          forks
 



 


          Star

  





 

Notifications












Code







Issues
7






Pull requests
1






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







williamyang1991/VToonify









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





0
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




williamyang1991

Update train_vtoonify_d.py




        …
      




        cf993aa
      

Nov 15, 2022





Update train_vtoonify_d.py


cf993aa



Git stats







166

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








checkpoint



Update README.md



Sep 12, 2022









data



Add files via upload



Oct 3, 2022









environment



Add files via upload



Sep 14, 2022









model



Update align_all_parallel.py



Oct 2, 2022









notebooks



使用 Colaboratory 创建



Oct 7, 2022









output



Update readme.md



Sep 12, 2022









LICENSE.md



Update LICENSE.md



Sep 14, 2022









README.md



Update README.md



Oct 13, 2022









smooth_parsing_map.py



Add files via upload



Sep 12, 2022









style_transfer.py



Add files via upload



Sep 12, 2022









train_vtoonify_d.py



Update train_vtoonify_d.py



Nov 15, 2022









train_vtoonify_t.py



Update train_vtoonify_t.py



Sep 16, 2022









util.py



Update util.py



Oct 1, 2022









vtoonify_model.py



Update vtoonify_model.py



Oct 4, 2022




    View code
 


















VToonify - Official PyTorch Implementation
Updates
Web Demo
Installation
(1) Inference for Image/Video Toonification
Inference Notebook
Pre-trained Models
Style Transfer with VToonify-D
Style Transfer with VToonify-T
(2) Training VToonify
Train VToonify-D
Train VToonify-T
(3) Results
Citation
Acknowledgments





README.md




VToonify - Official PyTorch Implementation





overview.mp4





This repository provides the official PyTorch implementation for the following paper:
VToonify: Controllable High-Resolution Portrait Video Style Transfer
Shuai Yang, Liming Jiang, Ziwei Liu and Chen Change Loy
In ACM TOG (Proceedings of SIGGRAPH Asia), 2022.
Project Page | Paper | Supplementary Video | Input Data and Video Results 




Abstract: Generating high-quality artistic portrait videos is an important and desirable task in computer graphics and vision.
Although a series of successful portrait image toonification models built upon the powerful StyleGAN have been proposed,
these image-oriented methods have obvious limitations when applied to videos, such as the fixed frame size, the requirement of face alignment, missing non-facial details and temporal inconsistency.
In this work, we investigate the challenging controllable high-resolution portrait video style transfer by introducing a novel VToonify framework.
Specifically, VToonify leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details. The resulting fully convolutional architecture accepts non-aligned faces in videos of variable size as input, contributing to complete face regions with natural motions in the output.
Our framework is compatible with existing StyleGAN-based image toonification models to extend them to video toonification, and inherits appealing features of these models for flexible style control on color and intensity.
This work presents two instantiations of VToonify built upon Toonify and DualStyleGAN for collection-based and exemplar-based portrait video style transfer, respectively.
Extensive experimental results demonstrate the effectiveness of our proposed VToonify framework over existing methods in generating high-quality and temporally-coherent artistic portrait videos with flexible style controls.

Features:
High-Resolution Video (>1024, support unaligned faces) | Data-Friendly (no real training data) | Style Control

Updates

[10/2022] Integrate Gradio interface into Colab notebook. Enjoy the web demo!
[10/2022] Integrated to 🤗 Hugging Face. Enjoy the web demo!
[09/2022] Input videos and video results are released.
[09/2022] Paper is released.
[09/2022] Code is released.
[09/2022] This website is created.

Web Demo
Integrated into Huggingface Spaces 🤗 using Gradio. Try out the Web Demo 
Installation
Clone this repo:
git clone https://github.com/williamyang1991/VToonify.git
cd VToonify
Dependencies:
We have tested on:

CUDA 10.1
PyTorch 1.7.0
Pillow 8.3.1; Matplotlib 3.3.4; opencv-python 4.5.3; Faiss 1.7.1; tqdm 4.61.2; Ninja 1.10.2

All dependencies for defining the environment are provided in environment/vtoonify_env.yaml.
We recommend running this repository using Anaconda (you may need to modify vtoonify_env.yaml to install PyTorch that matches your own CUDA version following https://pytorch.org/):
conda env create -f ./environment/vtoonify_env.yaml
If you have a problem regarding the cpp extention (fused and upfirdn2d), or no GPU is available, you may refer to CPU compatible version.

(1) Inference for Image/Video Toonification
Inference Notebook

To help users get started, we provide a Jupyter notebook found in ./notebooks/inference_playground.ipynb that allows one to visualize the performance of VToonify.
The notebook will download the necessary pretrained models and run inference on the images found in ./data/.
Pre-trained Models
Pre-trained models can be downloaded from Google Drive, Baidu Cloud (access code: sigg) or Hugging Face:


BackboneModelDescription


DualStyleGANcartoonpre-trained VToonify-D models and 317 cartoon style codes


caricaturepre-trained VToonify-D models and 199 caricature style codes


arcanepre-trained VToonify-D models and 100 arcane style codes


comicpre-trained VToonify-D models and 101 comic style codes


pixarpre-trained VToonify-D models and 122 pixar style codes


illustrationpre-trained VToonify-D models and 156 illustration style codes


Toonifycartoonpre-trained VToonify-T model


caricaturepre-trained VToonify-T model


arcanepre-trained VToonify-T model


comicpre-trained VToonify-T model


pixarpre-trained VToonify-T model


Supporting model 


encoder.ptPixel2style2pixel encoder to map real faces into Z+ space of StyleGAN


faceparsing.pthBiSeNet for face parsing from face-parsing.PyTorch


The downloaded models are suggested to be arranged in this folder structure.
The VToonify-D models are named with suffixes to indicate the settings, where

_sXXX: supports only one fixed style with XXX the index of this style.

_s without XXX means the model supports examplar-based style transfer


_dXXX: supports only a fixed style degree of XXX.

_d without XXX means the model supports style degrees ranging from 0 to 1


_c: supports color transfer.

Style Transfer with VToonify-D
✔ A quick start HERE
Transfer a default cartoon style onto a default face image ./data/077436.jpg:
python style_transfer.py --scale_image
The results are saved in the folder ./output/, where 077436_input.jpg is the rescaled input image to fit VToonify (this image can serve as the input without --scale_image) and 077436_vtoonify_d.jpg is the result.

Specify the content image and the model, control the style with the following options:

--content: path to the target face image or video
--style_id: the index of the style image (find the mapping between index and the style image here).
--style_degree (default: 0.5): adjust the degree of style.
--color_transfer(default: False): perform color transfer if loading a VToonify-Dsdc model.
--ckpt: path of the VToonify-D model. By default, a VToonify-Dsd trained on cartoon style is loaded.
--exstyle_path: path of the extrinsic style code. By default, codes in the same directory as --ckpt are loaded.
--scale_image: rescale the input image/video to fit VToonify (highly recommend).
--padding (default: 200, 200, 200, 200): left, right, top, bottom paddings to the eye center.

Here is an example of arcane style transfer:
python style_transfer.py --content ./data/038648.jpg \
       --scale_image --style_id 77 --style_degree 0.5 \
       --ckpt ./checkpoint/vtoonify_d_arcane/vtoonify_s_d.pt \
       --padding 600 600 600 600     # use large padding to avoid cropping the image

Specify --video to perform video toonification:
python style_transfer.py --scale_image --content ./data/YOUR_VIDEO.mp4 --video
The above style control options (--style_id, --style_degree, --color_transfer) also work for videos.
Style Transfer with VToonify-T
Specify --backbone as ''toonify'' to load and use a VToonify-T model.
python style_transfer.py --content ./data/038648.jpg \
       --scale_image --backbone toonify \
       --ckpt ./checkpoint/vtoonify_t_arcane/vtoonify.pt \
       --padding 600 600 600 600     # use large padding to avoid cropping the image

In VToonify-T, --style_id, --style_degree, --color_transfer, --exstyle_path are not used.
As with VToonify-D, specify --video to perform video toonification.

(2) Training VToonify
Download the supporting models to the ./checkpoint/ folder and arrange them in this folder structure:



Model
Description




stylegan2-ffhq-config-f.pt
StyleGAN model trained on FFHQ taken from rosinality


encoder.pt
Pixel2style2pixel encoder that embeds FFHQ images into StyleGAN2 Z+ latent code


faceparsing.pth
BiSeNet for face parsing from face-parsing.PyTorch


directions.npy
Editing vectors taken from LowRankGAN for editing face attributes


Toonify | DualStyleGAN
pre-trained stylegan-based toonification models



To customize your own style, you may need to train a new Toonify/DualStyleGAN model following here.
Train VToonify-D
Given the supporting models arranged in the default folder structure, we can simply pre-train the encoder and train the whole VToonify-D by running
# for pre-training the encoder
python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_d.py \
       --iter ITERATIONS --stylegan_path DUALSTYLEGAN_PATH --exstyle_path EXSTYLE_CODE_PATH \
       --batch BATCH_SIZE --name SAVE_NAME --pretrain
# for training VToonify-D given the pre-trained encoder
python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_d.py \
       --iter ITERATIONS --stylegan_path DUALSTYLEGAN_PATH --exstyle_path EXSTYLE_CODE_PATH \
       --batch BATCH_SIZE --name SAVE_NAME                  # + ADDITIONAL STYLE CONTROL OPTIONS
The models and the intermediate results are saved in ./checkpoint/SAVE_NAME/ and ./log/SAVE_NAME/, respectively.
VToonify-D provides the following STYLE CONTROL OPTIONS:

--fix_degree: if specified, model is trained with a fixed style degree (no degree adjustment)
--fix_style: if specified, model is trained with a fixed style image (no examplar-based style transfer)
--fix_color: if specified, model is trained with color preservation (no color transfer)
--style_id: the index of the style image (find the mapping between index and the style image here).
--style_degree (default: 0.5): the degree of style.

Here is an example to reproduce the VToonify-Dsd on Cartoon style and the VToonify-D specialized for a mild toonification on the 26th cartoon style:
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_d.py \
       --iter 30000 --stylegan_path ./checkpoint/cartoon/generator.pt --exstyle_path ./checkpoint/cartoon/refined_exstyle_code.npy \
       --batch 1 --name vtoonify_d_cartoon --pretrain      
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_d.py \
       --iter 2000 --stylegan_path ./checkpoint/cartoon/generator.pt --exstyle_path ./checkpoint/cartoon/refined_exstyle_code.npy \
       --batch 4 --name vtoonify_d_cartoon --fix_color 
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_d.py \
       --iter 2000 --stylegan_path ./checkpoint/cartoon/generator.pt --exstyle_path ./checkpoint/cartoon/refined_exstyle_code.npy \
       --batch 4 --name vtoonify_d_cartoon --fix_color --fix_degree --style_degree 0.5 --fix_style --style_id 26
Note that the pre-trained encoder is shared by different STYLE CONTROL OPTIONS. VToonify-D only needs to pre-train the encoder once for each DualStyleGAN model.
Eight GPUs are not necessary, one can train the model with a single GPU with larger --iter.
Tips: [how to find an ideal model] we can first train a versatile model VToonify-Dsd,
and navigate around different styles and degrees. After finding the ideal setting, we can then train the model specialized in that setting for high-quality stylization.
Train VToonify-T
The training of VToonify-T is similar to VToonify-D,
# for pre-training the encoder
python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_t.py \
       --iter ITERATIONS --finetunegan_path FINETUNED_MODEL_PATH \
       --batch BATCH_SIZE --name SAVE_NAME --pretrain       # + ADDITIONAL STYLE CONTROL OPTION
# for training VToonify-T given the pre-trained encoder
python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_t.py \
       --iter ITERATIONS --finetunegan_path FINETUNED_MODEL_PATH \
       --batch BATCH_SIZE --name SAVE_NAME                  # + ADDITIONAL STYLE CONTROL OPTION
VToonify-T only has one STYLE CONTROL OPTION:

--weight (default: 1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0): 18 numbers indicate how the 18 layers of the ffhq stylegan model and the finetuned model are blended to obtain the final Toonify model. Here is the --weight we use in the paper for different styles. Please refer to toonify for the details.

Here is an example to reproduce the VToonify-T model on Arcane style:
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_t.py \
       --iter 30000 --finetunegan_path ./checkpoint/arcane/finetune-000600.pt \
       --batch 1 --name vtoonify_t_arcane --pretrain --weight 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1 1 1 1 1 1 1 1 1 1 1
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_t.py \
       --iter 2000 --finetunegan_path ./checkpoint/arcane/finetune-000600.pt \
       --batch 4 --name vtoonify_t_arcane --weight 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1 1 1 1 1 1 1 1 1 1 1

(3) Results
Our framework is compatible with existing StyleGAN-based image toonification models to extend them to video toonification, and inherits their appealing features for flexible style control. With DualStyleGAN as the backbone, our VToonify is able to transfer the style of various reference images and adjust the style degree in one model.





joint.style.and.degree.control.mp4





Here are the color interpolated results of VToonify-D and VToonify-Dc on Arcane, Pixar and Comic styles.





styles.mp4





Citation
If you find this work useful for your research, please consider citing our paper:
@article{yang2022Vtoonify,
  title={VToonify: Controllable High-Resolution Portrait Video Style Transfer},
  author={Yang, Shuai and Jiang, Liming and Liu, Ziwei and Loy, Chen Change},
  journal={ACM Transactions on Graphics (TOG)},
  volume={41},
  number={6},
  articleno={203},
  pages={1--15},
  year={2022},
  publisher={ACM New York, NY, USA},
  doi={10.1145/3550454.3555437},
}
Acknowledgments
The code is mainly developed based on stylegan2-pytorch, pixel2style2pixel and DualStyleGAN.









About

      [SIGGRAPH Asia 2022] VToonify: Controllable High-Resolution Portrait Video Style Transfer
    
Topics



  style-transfer


  face


  siggraph-asia


  stylegan2


  toonify


  video-style-transfer



Resources





      Readme
 
License





     View license
    



Stars





2.2k
    stars

Watchers





54
    watching

Forks





238
    forks







    Releases

No releases published






    Packages 0


        No packages published 







    Contributors 4





 



 



 



 







Languages












Jupyter Notebook
91.0%







Python
8.3%







Other
0.7%











"
https://news.ycombinator.com/rss,ZSWatch – Open-source Zephyr-based smartwatch,https://github.com/jakkra/ZSWatch,Comments,"








jakkra

/

ZSWatch

Public




 

Notifications



 

Fork
    5




 


          Star
 204
  









        ZSWatch - the Open Source Zephyr™ based Smartwatch, including both HW and FW.
      
License





     MIT license
    






204
          stars
 



5
          forks
 



 


          Star

  





 

Notifications












Code







Issues
1






Pull requests
1






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







jakkra/ZSWatch









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





0
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




jakkra

Merge remote-tracking branch 'origin/main' into main




        …
      




        b0bbf79
      

Jan 14, 2023





Merge remote-tracking branch 'origin/main' into main


b0bbf79



Git stats







123

                      commits
                    







Files

Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github


 


 









CAD


 


 









ZSWatch-kicad


 


 









app


 


 









schematic


 


 









.gitignore


 


 









LICENCE


 


 









README.md


 


 




    View code
 



















ZSWatch
Hardware Features in ZSWatch v1
Upcoming Hardware features in ZSWatch v2
Charger/Dock
Enclosure/Casing
Software Features
Larger not yet implemented SW Features and TODOs
Android phone communication
PCB
ZSWatch in action
Writing apps for the Application Manager
Dock





README.md





ZSWatch


  The ZSWatch v1



Smartwatch built from scratch, both hardware and software. Built on the Zephyr™ Project RTOS, hence the name ZSWatch - Zephyr Smartwatch.

Hardware Features in ZSWatch v1

nRF52833 BLE chip (u-blox ANNA-B402 module).
1.28"" 240x240 IPS TFT Circular Display with GC9A01 driver.
Accelerometer for step counting etc. (LIS2DS12TR).
Pulse oximetry and heartrate using (MAX30101EFD)).
Vibration motor with haptics driver to give better vibration control (DRV2603RUNT).
External 8MB flash (MX25R6435FZNIL0).
Battery charger and battery supervisor (MAX1811ESA+ datasheet, TLV840MAPL3).
3 buttons for navigation (prev/next/enter)
220 mAh Li-Po battery.
Sapphire Crystal Glass to protect the display.

Upcoming Hardware features in ZSWatch v2

nRF5340 BLE chip (u-blox NORA-B10 module)
Touch screen with same size and features as v1
8MB external flash will probably be removed due to larger size of u-blox NORA-B10 vs. ANNA-B402.
Find another way to dock the clock for charging and programming, maybe can find some connector similar to what smartwatches normally have.

Charger/Dock
Basic pogo-pin dock that connects the power and SWD pins to the bottom of the watch.
Enclosure/Casing
3D printed casing with 3D printed buttons. Does it's job, but for revision v2 of the watch I'll probably do something CNC'd for nicer looks.
Software Features

Bluetooth LE communications with GadgetBridge Android app.
Also support Bluetooth Direction Finding so the watch can act as a tag and is trackable using any u-blox AoA antenna board
Watchface that shows:

Standard stuff as time, date, battery
Weather
Step count
Number unread notifications
Heart rate (not implemented yet however)


Pop-up notifications
Setting menu system, with easy extendability
Application picker and app concept

Music control app
Settings app
etc.


Step counting

Larger not yet implemented SW Features and TODOs

Heart rate, right now only samples the raw data, but no heart rate is calculated from it.
Proper BLE pairing, currently removed due to flash constraints (fixed by nRF5340 upgrade).
Watchface should also be an application.
Refactoring of main.c, should have way less logic, utlize Zephyr architecture more.

Android phone communication
Fortunately there is a great Android app called GadgetBridge which handles everything needed on the phone side, such as notifications management, music control and so much more... The ZSWatch right now pretends to be one of the supported Smart Watches in Gadgetbridge, following the same API as it does. In future there may be a point adding native support, we'll see.
PCB
A 4 layer board which measures 36mm in diameter designed in KiCad.








ZSWatch in action



Music control
Accelerometer for step count and tap detection




 object-fit=""cover""



Notifications from phone (Gmail here)
Settings







Writing apps for the Application Manager
Check out the sample application for the general app design. The main idea is each app have an <app_name>_app.c file which registers the app, chooses icon and drives the logic for the app. Then there should be one or more files named for example <app_name>_ui.c containing pure LVGL code with no dependencies to Zephyr or the watch software. The idea is that this UI code should be runnable in a LVGL simulator to speed up development of UI, however right now that's not set up yet. The <app_name>_app.c will do all logic and call functions in <app_name>_ui.c to update the UI accordingly.
Each application needs to have a way to close itself, for example a button, and then through callback tell the application_manager.c to close the app:
When user clicks an app in the app picker:

application_manager.c deletes it's UI elements and calls the application_start_fn.
<app_name>_app.c will do necessary init and then call the <app_name>_ui.c to draw the app UI.
User can now navigate arund and the application and do whatever.

When user for example presses a close button in the application:

Typically a callback from the UI code in <app_name>_ui.c will call <app_name>_app.c to tell that user requested to close the app. <app_name>_app.c will notify application_manager.c that it want to close itself. application_manager.c will then call <app_name>_app.c application_stop_fn and <app_name>_app.c will tell UI to close then do necessary de-init and return.
application_manager.c will now draw the app picker again.

The application manager can also at any time close a running application by calling it's application_stop_fn.
Dock
Very basic, will be re-worked for next watch revision v2.












About

      ZSWatch - the Open Source Zephyr™ based Smartwatch, including both HW and FW.
    
Topics



  bluetooth


  ble


  smartwatch


  zephyr


  nrf52


  lvgl


  angle-of-arrival


  nrf-connect


  nordic-semiconductor


  nrf53


  nrf-connect-sdk


  zswatch



Resources





      Readme
 
License





     MIT license
    



Stars





204
    stars

Watchers





5
    watching

Forks





5
    forks







    Releases

No releases published




Languages











C
99.9%







Other
0.1%











"
https://news.ycombinator.com/rss,Four thousand weeks,https://leebyron.com/4000/,Comments,"



















Four Thousand Weeks










FourThousandWeeks


          A tribute to
          the book by
          Oliver Burkeman, an exploration of time management in the
          face of human finitude, and addressing the anxiety of “getting
          everything done.”
        

          To begin, enter when were you born
          This site uses no cookies nor saves your information




Scroll on...




We live our livesweek by week

















Yet a week feels frustratingly limited

        The pressure to be more productive and fit ever-increasing quantities of
        activity into a stubbornly non-increasing quantity of time leads to
        productivity anxiety, shriveled attention spans, and burn-out.
      


And there are alarmingly few of them

        You would feel less anxious about wasting an evening doom-scrolling if
        you had an infinite amount of them. Somehow either doing too much or too
        little can create the sense of wasting time.
      

        Despite all this activity we sense there are important and fulfilling
        ways we could be spending our time, even if we can’t say exactly what
        they are. Yet, we systematically spend our time doing other things to
        get by instead.
      


          I (like many others) felt a wrongness in the world. Life, I knew, was
          supposed to be more joyful than this, more real, more meaningful. We
          were not supposed to hate Mondays and live for the weekends and
          holidays. We were not supposed to have to raise our hands to be
          allowed to pee.
        
Charles Eisenstien




The average human life is only four thousand weeks


        Scientists estimate that life, in some form, will persist for another
        1.5 billion years or more, until the intensifying heat of the sun
        condemns the last organism to death.
      

        But you? Assuming you live to be eighty, you’ll have had about four
        thousand weeks. The rare few lucky enough to become a centenarian will
        see only five thousand.
      
That’s absurdly, terrifyingly, insultingly short.




          You have lived  of them so far
        


        You likely have many more weeks ahead of you. The psychologist
        Erik Erikson suggests that at this phase of life you focus
        on the virtues of competence and fidelity. Allow yourself failures in
        the spirit of discovering and developing your personal identity and
        priorities so that your future weeks can be lived well with intention
        and purpose.
      

        That’s a significant amount of the weeks you’ll see. The psychologist
        Erik Erikson suggests that at this phase of life you focus
        on the virtue of love. Share yourself more intimately with others and
        invest in happy relationships so that your future weeks can be lived
        well with companionship and purpose.
      

        That’s likely a majority of the weeks you’ll see. The psychologist
        Erik Erikson suggests that at this phase of life you focus
        on the virtue of care. Spend your weeks “making your mark” by
        intentionally nurturing things that will outlast you, raising children,
        mentoring others, becoming involved in your community and organizations,
        and creating positive change that benefits others.
      

        You’re likely well aware of your own finitude having lived the large
        majority of the weeks you’ll see. The psychologist
        Erik Erikson suggests that at this phase of life you focus
        on the virtue of wisdom. Accept and appreciate your accomplishments so
        far as a life well lived. Continue to nurture things that will outlast
        you and mentor others, spend your weeks intentionally on your true
        priorities, and appreciate novelty in the mundane.
      

        You’re no doubt well aware of your own finitude as one of the lucky ones
        to live well past four thousand weeks. The psychologist
        Erik Erikson suggests that at this phase of life you focus
        on the virtue of wisdom. Accept and appreciate your accomplishments so
        far as a life well lived. Spend every remaining week intentionally on
        your true priorities and appreciate novelty in the mundane.
      


Productivity is a trap

        There are numerous techniques, products, and services to squeeze the
        most productivity from your week. The problem isn’t that these don’t
        work, it’s that they do work. And yet paradoxically you only
        feel busier, more anxious, and somehow emptier as a result.
      

        The day will never arrive when you finally have everything under
        control—when the flood of emails has been contained; when your to-do
        lists have stopped getting longer; when you’re meeting all your
        obligations at work and in your home life; when nobody’s angry with you
        for missing a deadline or dropping the ball; and when the fully
        optimized person you’ve become can turn, at long last, to the things
        life is really supposed to be about.
      

        Let’s start by admitting defeat: none of this is ever going to happen.
      


          Time feels like an unstoppable conveyer belt, bringing us new tasks as
          fast as we can dispatch the old ones; and becoming “more productive”
          just seems to cause the belt to speed up.
        
Edward T. Hall



Adopt a limit-embracing attitude

        If you truly don’t have time for everything you want to do, or feel you
        ought to do, or that others are badgering you to do, then, well, you
        don’t have time—no matter how grave the consequences of failing to do it
        all might prove to be. So, technically, it’s irrational to feel troubled
        by an overwhelming to-do list. You’ll do what you can, you won’t do what
        you can’t, and the tyrannical inner voice insisting that you must do
        everything is simply mistaken.
      

        We rarely stop to consider things so rationally, though, because that
        would mean confronting the painful truth of our limitations.
      

        Surrender to the reality that things just take the time they take, and
        that you can’t quiet your anxieties by working faster, because it isn’t
        within your power to force reality’s pace as much as you feel you need
        to, and because the faster you go, the faster you’ll feel you need to
        go.
      


          Which of us truly lives on twenty-four hours a day? Which of us is not
          saying: “I shall alter that when I have a little more time?” We never
          shall have any more time. We have, and we have always had, all the
          time there is.
        
Arnold Bennett



How you spend your time is a choice

        We are forced to accept that there will always be too much to do; that
        you can’t make the world run at your preferred speed and so there are
        tough choices to be made: which balls to let drop, which people to
        disappoint, which cherished ambitions to abandon, which roles to fail
        at.
      

        Once you truly understand that you’re guaranteed to miss out on almost
        every experience the world has to offer, the fact that there are so many
        you still haven’t experienced stops feeling like a problem. Instead, you
        get to focus on fully enjoying the tiny slice of experiences you
        actually do have time for. Digging in to a challenging project that
        can’t be hurried becomes not a trigger for stressful emotions but a
        bracing act of choice.
      


The importance of rest

        A real risk of doing too much is finding your work time, in attempt to
        be productive, encroaching on an evening’s rest. Rest as it turns
        out—whether in the evening, over a weekend, or a long vacation—is
        critical for productive creative work. Its absence can lead to stress,
        burnout, and counterintuitively overall poor performance despite the
        extra hours worked.
      

        Though why should vacations or lazy mornings need defending in terms of
        improved work performance? Enjoying leisure for its own sake—which is
        the whole point of leisure—should not feel as though you’re failing at
        life. Leisure is not merely an opportunity for recovery and
        replenishment for the purposes of further work, but for its intrinsic
        satisfactions.
      


          I have to die. If it is now, well then I die now; if later, then now I
          will take my lunch, since the hour for lunch has arrived - and dying I
          will tend to later.
        
Epictetus



The loneliness of temporal sovereignty

        Other human beings are always impinging on your time in countless
        frustrating ways. In an ideal world the only person making decisions
        about your time is you. However this comes at a cost that’s not worth
        paying.
      

        It’s good to have plenty of time, but having all the time in the world
        isn’t much use if you’re forced to experience it all on your own. To do
        countless important things with time: to socialize, go on dates, raise
        children, launch businesses, start movements; it has to be synchronized
        with other people. In fact, having large amounts of time but no
        opportunity to use it collaboratively can be actively unpleasant.
      

        We treat our time as something to hoard, when it’s better approached as
        something to share. Even if that means surrendering some of your power
        to decide exactly what you do with it and when.
      


          However, the two things must be mingled and varied, solitude and
          joining a crowd: the one will make us long for people and the other
          for ourselves, and each will be a remedy for the other; solitude will
          cure our distaste for a crowd, and a crowd will cure our boredom with
          solitude.
        
Seneca




        Ten tools for embracing finitude
      


1.
Adopt a fixed volume approach to productivity

        Tough choices are inevitable; focus on making them consciously and well.
      

        Keep two to-do lists: an “open” one for everything on your plate,
        doubtlessly nightmarishly long, and “closed” with a fixed number of
        entries, only moving tasks onto it when previous ones have been
        completed.
      

        You’ll never get through all the tasks on the open list, but you were
        never going to in any case. The choice to leave them there is hard, but
        time spent on them is time not spent on the things you chose to focus
        on.
      

        Establish pre-determined time boundaries on your work, and make
        decisions in light of those limits. If your primary goal is to do what’s
        required to be finished by 5:30 you’ll be aware of the constraints on
        your time and motivated to use it wisely.
      


2.
Serialize,serialize,serialize

        Focus on one big project at a time, and see it to completion before
        moving onto the next.
      

        It’s alluring to try to alleviate the anxiety of having too many
        responsibilities or ambitions by getting started on them all at once,
        but you’ll make little progress that way. Instead, train yourself to get
        incrementally better at tolerating that anxiety by consciously
        postponing everything you possibly can except for one thing.
      

        Soon the satisfaction of completing important projects will make that
        anxiety feel worthwhile, and as you complete them you’ll have less to be
        anxious about anyway.
      


3.
Strategic underachievement

        Simply because your time is finite, you’ll inevitably underachieve at
        something. When you can’t do it all, you can feel ashamed and give up.
        When you decide in advance what to fail at, you remove the sting of
        shame.
      

        Nominate in advance whole areas of life in which you won’t expect
        excellence from yourself. Instead focus that time more effectively, and
        you won’t be surprised when you fail at what you planned to fail at all
        along.
      


4.
Celebrate wins

        The to-do list will never be finished. Inbox zero will inevitably
        refill. There’s an unhelpful assumption that you begin each morning with
        a productivity debt that you must pay off with hard work to achieve a
        zero-balance by evening.
      

        Keep a “done” list which starts empty and fills up over the day. You
        could have spent the day doing nothing remotely constructive, and look
        what you did instead! Lower the bar for what gets to count as an
        accomplishment; small wins accrue.
      


5.
Consolidate care

        The attention economy demands urgency, bringing a litany of demands for
        your care every day. Consciously choose your battles in industry,
        charity, activism, and politics.
      

        To make a real difference, you must focus your finite capacity for care.
      


6.
Embrace boring & single-purpose technology

        Modern digital devices offer distraction to a place where painful human
        limitations do not apply; you need never feel bored or constrained in
        your freedom of action—which isn’t the case when it comes to work that
        matters.
      

        Combat this by making your devices boring. Remove apps that distract
        (even consider Slack or Email). Switch your screen to grayscale. Use
        time-limiting reminders.
      

        Choose single-purpose devices like an e-reader where it’s tedious and
        awkward to do anything but read. If distracting apps are only a swipe
        away they’ll prove impossible to resist when the first twinge of boredom
        or difficulty of focus arises.
      


7.
Seek novelty in the mundane

        The fewer weeks we have left the faster we seem to lose them. The
        likeliest explanation for this phenomenon is that our brains encode the
        passing of time on the basis of how much information we process in any
        given interval.
      

        Cramming your life with novel experiences does work, but can also lead
        to existential overwhelm and is also impractical, especially if you have
        a job or children.
      

        Alternatively pay more attention to every moment no matter how mundane.
        Plunge into the life you already have with twice the intensity and your
        life will feel twice as full and will be remembered as lasting twice as
        long. Meditation, going on unplanned walks, photography, journaling,
        anything that draws your attention more fully to the present.
      


8.
Be a researcher in relationships

        When presented with a challenging or boring moment with another person,
        deliberately adopt an attitude of curiosity in which your goal isn’t to
        achieve any particular outcome or explain your position but to figure
        out who this human being is who we’re with.
      

        This curiosity is well suited to the unpredictability of life with
        others because it can be satisfied by their behaving in ways you like or
        dislike whereas the stance of demanding a certain result is frustrated
        each time things fail to go your way.
      


9.
Cultivate instantaneous generosity

        Whenever a generous impulse arises your mind: to give money, to check in
        on a friend, send an email praising someone’s work, act on that impulse
        right away. If you put it off for whatever reason, you’ll likely not get
        back to it. The only acts of generosity that count are the ones you’re
        actually making.
      

        People are social creatures, and generous action reliably makes us feel
        much happier.
      


10.
Practice doing nothing

        When it comes to the challenge of using your four thousand weeks well,
        the capacity to do nothing is indispensable. If you can’t bear the
        discomfort of not acting you’re far more likely to make poor choices
        with your time simply to feel as if you’re acting. Calm down, gain
        autonomy over your choices, and make better ones.
      
Do nothing meditation

Set a timer, even for only five minutes.
Sit in a chair and then stop trying to do anything.

          Every time you notice you’re doing something, including thinking or
          focusing on your breathing, stop doing it.
        

          If you notice you’re criticizing yourself inwardly for doing things
          well… that’s a thought too so stop doing that.
        
Keep on stopping until the timer goes off.





        Thanks for reading this tribute to
        Four Thousand Weeks, by Oliver Burkeman.
      

        This page is comprised of themes and excerpts from the book. If you’ve
        scrolled this far you should absolutely read it in its entirety.
      

        Set in
        Playfair 2.0
        by Claus Eggers


        Made and open-sourced by
        Lee Byron
        with ♥ in San Francisco.
      
✌︎



"
https://news.ycombinator.com/rss,Use.GPU Goes Trad,https://acko.net/blog/use-gpu-goes-trad/,Comments,"



Use.GPU Goes Trad — Acko.net































Hackery, Math & Design
Steven Wittens i













Home







Home






January 14, 2023
Use.GPU Goes Trad


Old is new again




I've released a new version of Use.GPU, my experimental reactive/declarative WebGPU framework, now at version 0.8.
My goal is to make GPU rendering easier and more sane. I do this by applying the lessons and patterns learned from the React world, and basically turning them all up to 11, sometimes 12. This is done via my own Live run-time, which is like a martian React on steroids.
The previous 0.7 release was themed around compute, where I applied my shader linker to a few challenging use cases. It hopefully made it clear that Use.GPU is very good at things that traditional engines are kinda bad at.
In comparison, 0.8 will seem banal, because the theme was to fill the gaps and bring some traditional conveniences, like:

Scenes and nodes with matrices
Meshes with instancing
Shadow maps for lighting
Visibility culling for geometry






These were absent mostly because I didn't really need them, and they didn't seem like they'd push the architecture in novel directions. That's changed however, because there's one major refactor underpinning it all: the previously standard forward renderer is now entirely swappable. There is a shiny deferred-style renderer to showcase this ability, where lights are rendered separately, using a g-buffer with stenciling.
This new rendering pipeline is entirely component-driven, and fully dogfooded. There is no core renderer per-se: the way draws are realized depends purely on the components being used. It effectively realizes that most elusive of graphics grails, which established engines have had difficulty delivering on: a data-driven, scriptable render pipeline, that mortals can hopefully use.





Root of the App



Deep inside the tree


I've spent countless words on Use.GPU's effect-based architecture in prior posts, which I won't recap. Rather, I'll just summarize the one big trick: it's structured entirely as if it needs to produce only 1 frame. Then in order to be interactive, and animate, it selectively rewinds parts of the program, and reactively re-runs them. If it sounds crazy, that's because it is. And yet it works.
So the key point isn't the feature list above, but rather, how it does so. It continues to prove that this way of coding can pay off big. It has all the benefits of immediate-mode UI, with none of the downsides, and tons of extensibility. And there are some surprises along the way.
Real Reactivity
You might think: isn't this a solved problem? There are plenty of JS 3D engines. Hasn't React-Three-Fiber (R3F) shown how to make that declarative? And aren't these just web versions of what native engines like Unreal and Unity already do well, and better?
My answer is no, but it might not be clear why. Let me give an example from my current job.







My client needs a specialized 3D editing tool. In gaming terms you might think of it as a level design tool, except the levels are real buildings. The details don't really matter, only that they need a custom 3D editing UI. I've been using Three.js and R3F for it, because that's what works today and what other people know.
Three.js might seem like a great choice for the job: it has a 3D scene, editing controls and so on. But, my scene is not the source of truth, it's the output of a process. The actual source of truth being live-edited is another tree that sits before it. So I need to solve a two-way synchronization problem between both. This requires careful reasoning about state changes.





Change handlers in Three.js and R3F


Sadly, the way Three.js responds to changes is ill-defined. As is common, its objects have ""dirty"" flags. They are resolved and cleared when the scene is re-rendered. But this is not an iron rule: many methods do trigger a local refresh on the spot. Worse, certain properties have an invisible setter, which immediately triggers a ""change"" event when you assign a new value to it. This also causes derived state to update and cascade, and will be broadcast to any code that might be listening.
The coding principle applied here is ""better safe than sorry"". Each of these triggers was only added to fix a particular stale data bug, so their effects are incomplete, creating two big problems. Problem 1 is a mix of old and new state... but problem 2 is you can only make it worse, by adding even more pre-emptive partial updates, sprinkled around everywhere.
These ""change"" events are oblivious to the reason for the change, and this is actually key: if a change was caused by a user interaction, the rest of the app needs to respond to it. But if the change was computed from something else, then you explicitly don't want anything earlier to respond to it, because it would just create an endless cycle, which you need to detect and halt.


R3F introduces a declarative model on top, but can't fundamentally fix this. In fact it adds a few new problems of it own in trying to bridge the two worlds. The details are boring and too specific to dig into, but let's just say it took me a while to realize why my objects were moving around whenever I did a hot-reload, because the second render is not at all the same as the first.
Yet this is exactly what one-way data flow in reactive frameworks is meant to address. It creates a fundamental distinction between the two directions: cascading down (derived state) vs cascading up (user interactions). Instead of routing both through the same mutable objects, it creates a one-way reverse-path too, triggered only in specific circumstances, so that cause and effect are always unambigious, and cycles are impossible.
Three.js is good for classic 3D. But if you're trying to build applications with R3F it feels fragile, like there's something fundamentally wrong with it, that they'll never be able to fix. The big lesson is this: for code to be truly declarative, changes must not be allowed to travel backwards. They must also be resolved consistently, in one big pass. Otherwise it leads to endless bug whack-a-mole.
What reactivity really does is take cache invalidation, said to be the hardest problem, and turn the problem itself into the solution. You never invalidate a cache without immediately refreshing it, and you make that the sole way to cause anything to happen at all. Crazy, and yet it works.
When I tell people this, they often say ""well, it might work well for your domain, but it couldn't possibly work for mine."" And then I show them how to do it.


Figuring out which way your cube map points:just gfx programmer things.

And... Scene
One of the cool consequences of this architecture is that even the most traditional of constructs can suddenly bring neat, Lispy surprises.
The new scene system is a great example. Contrary to most other engines, it's actually entirely optional. But that's not the surprising part.
Normally you just have a tree where nodes contain other nodes, which eventually contain meshes, like this:
<Scene>
  <Node matrix={...}>
    <Mesh>
    <Mesh>
  <Node matrix={...}>
    <Mesh>
    <Node matrix={...}>
      <Mesh>
      <Mesh>


It's a way to compose matrices: they cascade and combine from parent to child. The 3D engine is then built to efficiently traverse and render this structure.
But what it ultimately does is define a transform for every mesh: a function vec3 => vec3 that maps one vertex position to another. So if you squint, <Mesh> is really just a marker for a place where you stop composing matrices and pass a composed matrix transform to something else.
Hence Use.GPU's equivalent, <Primitive>, could actually be called <Unscene>. What it does is escape from the scene model, mirroring the Lisp pattern of quote-unquote. A chain of <Node> parents is just a domain-specific-language (DSL) to produce a TransformContext with a shader function, one that applies a single combined matrix transform.
In turn, <Mesh> just becomes a combination of <Primitive> and a <FaceLayer>, i.e. triangle geometry that uses the transform. It all composes cleanly.
So if you just put meshes inside the scene tree, it works exactly like a traditional 3D engine. But if you put, say, a polar coordinate plot in there from the plot package, which is not a matrix transform, inside a primitive, then it will still compose cleanly. It will combine the transforms into a new shader function, and apply it to whatever's inside. You can unscene and scene repeatedly, because it's just exiting and re-entering a DSL.
In 3D this is complicated by the fact that tangents and normals transform differently from vertices. But, this was already addressed in 0.7 by pairing each transform with a differential function, and using shader fu to compose it. So this all just keeps working.
Another neat thing is how this works with instancing. There is now an <Instances> component, which is exactly like <Mesh>, except that it gives you a dynamic <Instance> to copy/paste via a render prop:
<Instances
   mesh={mesh}
   render={(Instance) => (<>
     <Instance position={[1, 2, 3]} />
     <Instance position={[3, 4, 5]} />
   </>)
 />


As you might expect, it will gather the transforms of all instances, stuff all of them into a single buffer, and then render them all with a single draw call. The neat part is this: you can still wrap individual <Instance> components in as many <Node> levels as you like. Because all <Instance> does is pass its matrix transform back up the tree to the parent it belongs to.





This is done using Live captures, which are React context providers in reverse. It doesn't violate one-way data flow, because captures will only run after all the children have finished running. Captures already worked previously, the semantics were just extended and formalized in 0.8 to allow this to compose with other reduction mechanisms.


But there's more. Not only can you wrap <Instance> in <Node>, you can also wrap either of them in <Animate>, which is Use.GPU's keyframe animator, entirely unchanged since 0.7:









<Instances
  mesh={mesh}
  render={(Instance) => (

    <Animate
      prop=""rotation""
      keyframes={ROTATION_KEYFRAMES}
      loop
      ease=""cosine""
    >
      <Node>
        {seq(20).map(i => (
          <Animate
            prop=""position""
            keyframes={POSITION_KEYFRAMES}
            loop
            delay={-i * 2}
            ease=""linear""
          >
            <Instance
              rotation={[
                Math.random()*360,
                Math.random()*360,
                Math.random()*360,
              ]}
              scale={[0.2, 0.2, 0.2]}
            />
          </Animate>
        ))}
      </Node>
    </Animate>

  )}
/>


The scene DSL and the instancing DSL and the animation DSL all compose directly, with nothing up my sleeve. Each of these <Components> are still just ordinary functions. On the inside they look like constructors with all the other code missing. There is zero special casing going on here, and none of them are explicitly walking the tree to reach each other. The only one doing that is the reactive run-time... and all it does is enforce one-way data flow by calling functions, gathering results and busting caches in tree order. Because a capture is a long-distance yeet.
Personally I find this pretty magical. It's not as efficient as a hand-rolled scene graph with instancing and built-in animation, but in terms of coding lift it's literally O(0) instead of OO. I needed to add zero lines of code to any of the 3 sub-systems, in order to combine them into one spinning whole.
The entire scene + instancing package clocks in at about 300 lines and that's including empties and generous formatting. I don't need to architect the rest of the framework around a base Object3D class that everything has to inherit from either, which is a-ok in my book.
This architecture will never reach Unreal or Unity levels of hundreds of thousands of draw calls, but then, it's not meant to do that. It embraces the idea of a unique shader for every draw call, and then walks that back if and when it's useful. The prototype map package for example does this, and can draw a whole 3D vector globe in 2 draw calls: fill and stroke. Adding labels would make it 3. And it's not static: it's doing the usual quad-tree of LOD'd mercator map tiles.










Multi-Pass
Next up, the modular renderer passes. Architecturally and reactively-speaking, there isn't much here. This was mainly an exercise in slicing apart the existing glue.
The key thing to grok is that in Use.GPU, the <Pass> component does not correspond to a literal GPU render pass. Rather, it's a virtual, logical render pass. It represents all the work needed to draw some geometry to a screen or off-screen buffer, in its fully shaded form. This seems like a useful abstraction, because it cleanly separates the nitty gritty rendering from later compositing (e.g. overlays).
For the forward renderer, this means first rendering a few shadow maps, and possibly rendering a picking buffer for interaction. For the deferred renderer, this involves rendering the g-buffer, stencils, lights, and so on.
My goal was for the toggle between the two to be as simple as replacing a <ForwardRenderer> with a <DeferredRenderer>... but also to have both of those be flexible enough that you could potentially add on, say, SSAO, or bloom, or a Space Engine-style black hole, as an afterthought. And each <Pass> can have its own renderer, rather than shoehorning everything into one big engine.
Neatly, that's mostly what it is now. The basic principle rests on three pillars.



Deferred rendering


First, there are a few different rendering modes, by default solid vs shaded vs ui. These define what kind of information is needed at every pixel, i.e. the classic varying attributes. But they have no opinion on where the data comes from or what it's used for: that's defined by the geometry layer being rendered. It renders a <Virtual> draw call, which it gives e.g. a getVertex and getFragment shader function with a particular signature for that mode. These functions are not complete shaders, just the core functions, which are linked into a stub. There are a few standard 'tropes' used here, not just these two.
Second, there are a few different rendering buckets, like opaque, transparent, shadow, picking and debug. These are used to group draws into. Different GPU render passes then pick and choose from that. opaque and transparent are drawn to the screen, while shadow is drawn repeatedly into all the shadow maps. This includes sorting front-to-back and back-to-front, as well as culling.
Finally, there's the renderer itself (forward vs deferred), and its associated pass components (e.g. <ColorPass>, <ShadowPass>, <PickingPass>, and so on). The renderer decides how to translate a particular ""mode + bucket"" combination into a concrete draw call, by lowering it into render components (e.g. <ShadedRender>). The pass components decide which buffer to actually render stuff to, and how. So the renderer itself doesn't actually render, it merely spawns and delegates to other components that do.


The forward path works mostly the same as before, only the culling and shadow maps are new... but it's now split up into all its logical parts. And I verified this design by adding the deferred renderer, which is a lot more convoluted, but still needs to do some forward rendering.
It works like a treat, and they use all the same lighting shaders. You can extend any of the 3 pillars just by replacing or injecting a new component. And you don't need to fork either renderer to do so: you can just pick and choose à la carte by selectively overriding or extending its ""mode + bucket"" mapping table, or injecting a new actual render pass.










To really put a bow on top, I upgraded the Use.GPU inspector so that you can directly view any render target in a RenderDoc-like way. This will auto-apply useful colorization shaders, e.g. to visualize depth. This is itself implemented as a Use.GPU Live canvas, sitting inside the HTML-based inspector, sitting on top of Live, which makes this a Live-in-React-in-Live scenario.
For shits and giggles, you can also inspect the inspector's canvas, recursively, ad infinitum. Useful for debugging the debugger:







There are still of course some limitations. If, for example, you wanted to add a new light type, or add support for volumetric lights, you'd have to reach in more deeply to make that happen: the resulting code needs to be tightly optimized, because it runs per pixel and per light. But if you do, you're still going to be able to reuse 90% of the existing components as-is.
I do want a more comprehensive set of light types (e.g. line and area), I just didn't get around to it. Same goes for motion vectors and TXAA. However, with WebGPU finally nearing public release, maybe people will actually help out. Hint hint.







Port of a Reaction Diffusion system by Felix Woitzel.



A Clusterfuck of Textures
A final thing to talk about is 2D image effects and how they work. Or rather, the way they don't work. It seems simple, but in practice it's kind of ludicrous.
If you'd asked me a year ago, I'd have thought a very clean, composable post-effects pipeline was entirely within reach, with a unified API that mostly papered over the difference between compute and render. Given that I can link together all sorts of crazy shaders, this ought to be doable.
Well, I did upgrade the built-in fullscreen conveniences a bit, so that it's now easier to make e.g. a reaction diffusion sim like this (full code):



The devil here is in the details. If you want to process 2D images on a GPU, you basically have several choices:

Use a compute shader or render shader?
Which pixel format do you use?
Are you sampling one flat image or a MIP pyramid of pre-scaled copies?
Are you sampling color images, or depth/stencil images?
Use hardware filtering or emulate filtering in software?

The big problem is that there is no single approach that can handle all cases. Each has its own quirks. To give you a concrete example: if you wrote a float16 reaction-diffusion sim, and then decided you actually needed float32, you'd probably have to rewrite all your shaders, because float16 is always renderable and hardware filterable, but float32 is not.
Use.GPU has a pretty nice set of Compute/Stage/Kernel components, which are elegant on the outside; but they require you to write pretty gnarly shader code to actually use them. On the other side are the RenderToTexture/Pass/FullScreen components which conceptually do the same thing, and have much nicer shader code, but which don't work for a lot of scenarios. All of them can be broken by doing something seemingly obvious, that just isn't natively supported and difficult to check ahead of time.
Even just producing universal code to display any possible texture type on screen becomes a careful exercise in code-generation. If you're familiar with the history of these features, it's understandable how it got to this point, but nevertheless, the resulting API is abysmal to use, and is a never-ending show of surprise pitfalls.
Here's a non-exhaustive list of quirks:

Render shaders are the simplest, but can only be used to write those pixel formats that are ""renderable"".
Compute shaders must be dispatched in groups of N, even if the image size is not a multiple of N. You have to manually trim off the excess threads.
Hardware filtering only works on some formats, and some filtering functions only work in render shaders.
Hardware filtering (fast) uses [0..1] UV float coordinates, software emulation in a shader (slow) uses [0..N] XY uint coordinates.
Reading and writing from/to the same render texture is not allowed, you have to bounce between a read and write buffer.
Depth+stencil images have their own types and have an additional notion of ""aspect"" to select one or both.
Certain texture functions cannot be called conditionally, i.e. inside an if.
Copying from one texture to another doesn't work between certain formats and aspects.

My strategy so far has been to try and stick to native WGSL semantics as much as possible, meaning the shader code you do write gets inserted pretty much verbatim. But if you wanted to paper over all these differences, you'd have to invent a whole new shader dialect. This is a huge effort which I have not bothered with. As a result, compute vs render pretty much have to remain separate universes, even when they're doing 95% the same thing. There is also no easy way to explain to users which one they ought to use.
While it's unrealistic to expect GPU makers to support every possible format and feature on a fast path, there is little reason why they can't just pretend a little bit more. If a texture format isn't hardware filterable, somebody will have to emulate that in a shader, so it may as well be done once, properly, instead of in hundreds of other hand-rolled implementations.
If there is one overarching theme in this space, it's that limitations and quirks continue to be offloaded directly onto application developers, often with barely a shrug. To make matters worse, the ""next gen"" APIs like Metal and Vulkan, which WebGPU inherits from, do not improve this. They want you to become an expert at their own kind of busywork, instead of getting on with your own.
I can understand if the WebGPU designers have looked at the resulting venn-diagram of poorly supported features, and have had to pick their battles. But there's a few absurdities hidden in the API, and many non-obvious limitations, where the API spec suggests you can do a lot more than you actually can. It's a very mixed bag all things considered, and in certain parts, plain retarded. Ask me about minimum binding size. No wait, don't.
* * *
Most promising is that as Use.GPU grows to do more, I'm not touching extremely large parts of it. This to me is the sign of good architecture. I also continue to focus on specific use cases to validate it all, because that's the only way I know how to do it well.
There are some very interesting goodies lurking inside too. To give you an example... that R3F client app I mentioned at the start. It leverages Use.GPU's state package to implement a universal undo/redo system in 130 lines. A JS patcher is very handy to wrangle the WebGPU API's deep argument style, but it can do a lot more.
One more thing. As a side project to get away from the core architecting, I made a viewer for levels for Dark Engine games, i.e. Thief 1 (1998), System Shock 2 (1999) and Thief 2 (2000). I want to answer a question I've had for ages: how would those light-driven games have looked, if we'd had better lighting tech back then? So it actually relights the levels. It's still a work in progress, and so far I've only done slow-ass offline CPU bakes with it, using a BSP-tree based raytracer. But it works like a treat.









I basically don't have to do any heavy lifting if I want to draw something, be it normal geometry, in-place data/debug viz, or zoomable overlays. Integrating old-school lightmaps takes about 10 lines of shader code and 10 lines of JS, and the rest is off-the-shelf Use.GPU. I can spend my cycles working on the problem I actually want to be working on. That to me is the real value proposition here.
I've noticed that when you present people with refined code that is extremely simple, they often just do not believe you, or even themselves. They assume that the only way you're able to juggle many different concerns is through galaxy brain integration gymnastics. It's really quite funny. They go looking for the complexity, and they can't find it, so they assume they're missing something really vital. The realization that it's simply not there can take a very long time to sink in.
Visit usegpu.live for more and to view demos in a WebGPU capable browser.






Compute  Data Flow  GPU  Latest  Use.GPU

January 14, 2023











Home







Home









Subscribe










About
© 2003–2023







This article contains graphics made with WebGL, which your browser does not seem to support.
  Try Google Chrome or Mozilla Firefox.
  
  ×



"
https://news.ycombinator.com/rss,DragonFlyBSD's HAMMER2 File-System Being Ported to NetBSD,https://www.phoronix.com/news/NetBSD-HAMMER2-Port,Comments,"







DragonFlyBSD's HAMMER2 File-System Being Ported To NetBSD - Phoronix






































 













Articles & Reviews
News Archive
Forums
Premium  Categories
Computers Display Drivers Graphics Cards Linux Gaming Memory Motherboards Processors Software Storage Operating Systems Peripherals Close








Articles & Reviews


News Archive


Forums


Premium
 
Contact


 Categories


Computers Display Drivers Graphics Cards Linux Gaming Memory Motherboards Processors Software Storage Operating Systems Peripherals 






























Show Your Support:  This site is primarily supported by advertisements. Ads are what have allowed this site to be maintained on a daily basis for the past 18+ years. We do our best to ensure only clean, relevant ads are shown, when any nasty ads are detected, we work to remove them ASAP. If you would like to view the site without ads while still supporting our work, please consider our ad-free Phoronix Premium.
DragonFlyBSD's HAMMER2 File-System Being Ported To NetBSD
Written by Michael Larabel in BSD on 11 January 2023 at 06:26 AM EST. 21 Comments


NetBSD continues using the FFS file-system by default while it's offered ZFS support that has been slowly improving -- in NetBSD-CURRENT is the ability to use ZFS as the root file-system if first booting to FFS, for example.  There may be another modern file-system option soon with an effort underway to port DragonFlyBSD's HAMMER2 over to NetBSD. HAMMER2 has been built-up over the past decade by Matthew Dillon and other DragonFlyBSD developers. HAMMER2 has been the default and working rather well with recent releases of DragonFlyBSD while now there is a port underway to try to get the file-system working good for NetBSD.HAMMER2 on DragonFlyBSD. NetBSD developer  Tomohiro Kusumi has started working on a HAMMER2 port to NetBSD, who had also worked on porting HAMMER2 to FreeBSD as another exercise. This port is intended to be built against recent NetBSD code, initially is only read-only support but write support will be tackled once the read support has stabilized.  More details on this still very early stage port of HAMMER2 to NetBSD can be found via this GitHub repository. It will be interesting to see how the HAMMER2 port to NetBSD goes and if eventually could become a viable file-system option for NetBSD installations.









21 Comments 




Tweet







Related News
DragonFlyBSD 6.4 Released With Many FixesNetBSD 10 Beta Brings Much Improved Performance, Long Overdue Hardware SupportFreeBSD 12.4 Released With Various Fixes & ImprovementsTrying Out The BSDs On The Intel Core i9 13900K ""Raptor Lake""FreeBSD Re-Introduces WireGuard Support Into Its KernelFreeBSD 12.4-BETA1 Released, Q3-2022 Status Report Issued
 






About The Author

Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via Twitter, LinkedIn, or contacted via MichaelLarabel.com.


Popular News This Week
DragonFlyBSD's HAMMER2 File-System Being Ported To NetBSDOpenZFS Lands A Very Nice Performance OptimizationA Developer Hopes To Restore GCC's Java Front-EndOBS Studio 29 Released With AV1 Encode Additions, Upward Compression FilterUbuntu's Real-Time Kernel Approaching GA StatusLinux Preparing To Disable Drivers For Microsoft's RNDIS ProtocolLinux 4.9.337 Released To End Out The 2016 LTS SeriesWine 8.0-rc3 Released With 28 Known Bug Fixes
 













Latest Linux News
Removing Some Old Arm Drivers & Board/Machine Code To Lighten The Kernel By 154k Lines


Linux 6.3 To Support Making Use Of Intel's New LKGS Instruction (Part Of FRED)


Linux 6.3 Will Better Handle Missing AMD Radeon Firmware / Unsupported Hardware


Basic OpenGL ES Compute Shader Support Begins Working For The Apple GPU Linux Driver


GNU Binutils 2.40 Released With AMD Zen 4 & Upcoming Intel Instructions, Zstd Support


MSI PRO Z690-A WiFi DDR5 Support Upstreamed To Coreboot


Intel Posts Linux Patches For Linear Address Space Separation (LASS)


AMD ROCm 5.4.2 Released As Another Small Update To The Compute Stack


KDE This Week: ""Pretty Juicy In The Eye Candy Department""


Linux Developers Eye Orphaning The JFS File-System










Show Your Support, Go Premium
Phoronix Premium allows ad-free access to the site, multi-page articles on a single page, and other features while supporting this site's continued operations.


Latest Featured Articles
Setting Up Intel 4th Gen Xeon Scalable ""Sapphire Rapids"" For Accelerator Use


AMD Radeon vs. Intel Arc Graphics With Linux 6.2 + Mesa 23.0


Intel Xeon Platinum 8490H ""Sapphire Rapids"" Performance Benchmarks


Intel Launches 4th Gen Xeon Scalable ""Sapphire Rapids"", Xeon CPU Max Series


AMD Ryzen 5 7600 / Ryzen 7 7700 / Ryzen 9 7900 Linux Performance







 


Support Phoronix
The mission at Phoronix since 2004 has centered around enriching the Linux hardware experience. In addition to supporting our site through advertisements, you can help by subscribing to Phoronix Premium. You can also contribute to Phoronix through a PayPal tip or tip via Stripe.











Phoronix Media


Contact
Michael Larabel
OpenBenchmarking.org



Phoronix Premium


Support Phoronix
While Having Ad-Free Browsing,
Single-Page Article Viewing



Share


Facebook
Twitter







Legal Disclaimer, Privacy Policy, Cookies | Contact
Copyright © 2004 - 2023 by Phoronix Media.
All trademarks used are properties of their respective owners. All rights reserved.







"
https://news.ycombinator.com/rss,The joy of sets,https://www.prospectmagazine.co.uk/arts-and-books/the-joy-of-sets,Comments,"
403 Forbidden

403 Forbidden


"
https://news.ycombinator.com/rss,Server BMCs can need to be rebooted every so often,https://utcc.utoronto.ca/~cks/space/blog/sysadmin/BMCsCanNeedRebooting,Comments,"
 
 Chris's Wiki :: blog/sysadmin/BMCsCanNeedRebooting 






Chris Siebenmann ::
CSpace »
       blog »
       sysadmin »
       BMCsCanNeedRebooting
Welcome, guest.




Your server BMCs can need to be rebooted every so often
January 14, 2023

Over on the Fediverse I said:
A sysadmin tip: if your BMC/IPMI
is doing weird things, restart (reboot) it. Server BMCs are little
computers running ancient versions of Linux with software that's
probably terribly written and they stay running forever, which means
all sorts of opportunities for slow bugs. Reboot away!
This is brought to you by the BMC with a KVM-over-IP that wouldn't
accept '2' entered on the (virtual) keyboard in any way or form. Until
I rebooted the BMC. 
PS: Our IP addresses have 2s in them.

(This probably isn't the only weird BMC glitch we've experienced,
but it's the first one where I tried rebooting the BMC and that
fixed it.)
A number of people shared additional stories in the replies, and I
especially 'liked' @frederic@chaos.social's:
Same for IPMI hardware sensors: Thought the motherboard was damaged
because half the sensors were reported as ""n/a"".  Rebooting magically
fixed this. ðŸ™ˆ

This happens for more or less the reasons I mentioned above. BMCs
naturally accumulate very large uptimes because they don't normally
reboot when your server reboots; if you don't do anything special,
your BMC will normally stay up for as long as the server has power.
In many places this can amount to years of uptime, and it's a rare
set of software that can stand up to that even if you don't use
them much. Server vendors typically don't want you to think about
this, and I don't believe 'BMC uptime' is generally exposed anywhere.
(Routinely querying the BMC's sensor readings via IPMI may actually
make this worse, since then the BMC's software is active to answer
those queries. I should probably make our metrics system notice when a server decreases the
number of IPMI metrics it exposes without a reboot.)
Modern BMCs can generally reboot themselves without rebooting their
host (the actual server), although you may want to test this to be
sure since apparently some vendors can do that differently.
PS: How I encountered this is that I was reinstalling a server using
KVM-over-IP, and I hit the portion of the base Ubuntu 22.04 install
when I had to enter the subnet and various associated IP addresses.
Our network has a '2' in it, so all of that failed. Helpfully, the
KVM-over-IP software had a virtual keyboard so I could see it wasn't
just some browser weirdness intercepting a '2' from my real keyboard;
even the virtual keyboard's '2' key wouldn't get through to the
Ubuntu 22.04 installer running on the server being reinstalled. Since
rebooting the BMC didn't reboot the host, I could verify that rebooting
the BMC alone fixed the problem; when the BMC rebooted, my KVM-over-IP
session could now enter all digits.
(I'm glad that it occurred to me to reboot the BMC, instead of just
grumble and go down to the machine room to do the install with the
physical console.)

(One comment.)
Written on 14 January 2023. 

     «   Ubuntu 22.04 LTS servers and phased apt updates    
  



 These are my WanderingThoughts 
(About the blog)
Full index of entries 
Recent comments
This is part of CSpace, and is written by ChrisSiebenmann. 
Mastodon: @cks 
Twitter: @thatcks
* * *
Categories: links, linux, programming, python, snark, solaris, spam, sysadmin, tech, unix, web 
Also: (Sub)topics
This is a DWiki. 
GettingAround 
(Help)
 
 Search:  



 Page tools: View Source, Add Comment. 

Search: 

Login: 
Password: 


 

Atom Syndication: Recent Comments.
 Last modified: Sat Jan 14 22:02:49 2023 
This dinky wiki is brought to you by the Insane Hackers
Guild, Python sub-branch.


"
https://news.ycombinator.com/rss,Bayesian statistics and machine learning: How do they differ?,https://statmodeling.stat.columbia.edu/2023/01/14/bayesian-statistics-and-machine-learning-how-do-they-differ/,Comments,"
403 Forbidden

403 Forbidden
nginx


"
https://news.ycombinator.com/rss,Money creation in the modern economy (2014) [pdf],https://www.bankofengland.co.uk/-/media/boe/files/quarterly-bulletin/2014/money-creation-in-the-modern-economy.pdf?la=en&hash=9A8788FD44A62D8BB927123544205CE476E01654,Comments,"14  Quarterly Bulletin  2014 Q1  Money creation in the modern economy  By Michael McLeay, Amar Radia and Ryland Thomas of the Bank’s Monetary Analysis Directorate.(1)    This article explains how the majority of money in the modern economy is created by commercial  banks making loans.    Money creation in practice differs from some popular misconceptions — banks do not act simply as intermediaries, lending out deposits that savers place with them, and nor do they ‘multiply up’ central bank money to create new loans and deposits.    The amount of money created in the economy ultimately depends on the monetary policy of the central bank.  In normal times, this is carried out by setting interest rates.  The central bank can also affect the amount of money directly through purchasing assets or ‘quantitative easing’.  Overview  In the modern economy, most money takes the form of bank deposits.  But how those bank deposits are created is often misunderstood:  the principal way is through commercial banks making loans.  Whenever a bank makes a loan, it simultaneously creates a matching deposit in the borrower’s bank account, thereby creating new money.  The reality of how money is created today differs from the description found in some economics textbooks:    Rather than banks receiving deposits when households save and then lending them out, bank lending creates deposits.    In normal times, the central bank does not fix the amount  of money in circulation, nor is central bank money ‘multiplied up’ into more loans and deposits.  Although commercial banks create money through lending, they cannot do so freely without limit.  Banks are limited in how much they can lend if they are to remain profitable in a competitive banking system.  Prudential regulation also acts as a constraint on banks’ activities in order to maintain the resilience of the financial system.  And the households and companies who receive the money created by new lending may take actions that affect the stock of money — they could quickly ‘destroy’ money by using it to repay their existing debt, for instance.  Monetary policy acts as the ultimate limit on money creation.  The Bank of England aims to make sure the amount of money creation in the economy is consistent with  low and stable inflation.  In normal times, the Bank of England implements monetary policy by setting the interest rate on central bank reserves.  This then influences a range of interest rates in the economy, including those on bank loans.  In exceptional circumstances, when interest rates are at theireffective lower bound, money creation and spending in the economy may still be too low to be consistent with the central bank’s monetary policy objectives.  One possible response is to undertake a series of asset purchases, or‘quantitative easing’ (QE).  QE is intended to boost theamount of money in the economy directly by purchasingassets, mainly from non-bank financial companies. QE initially increases the amount of bank deposits those companies hold (in place of the assets they sell).  Those companies will then wish to rebalance their portfolios ofassets by buying higher-yielding assets, raising the price ofthose assets and stimulating spending in the economy. As a by-product of QE, new central bank reserves arecreated.  But these are not an important part of thetransmission mechanism.  This article explains how, just as innormal times, these reserves cannot be multiplied into moreloans and deposits and how these reserves do not represent‘free money’ for banks. Click here for a short video filmed in the Bank’s gold vaults that discusses some of the key topics from this article.  (1)  The authors would like to thank Lewis Kirkham for his help in producing this article.  Topical articles  Money creation in the modern economy  15  Introduction  ‘Money in the modern economy:  an introduction’, a companion piece to this article, provides an overview of what is meant by money and the different types of money that exist in a modern economy, briefly touching upon how each type of money is created.  This article explores money creation in the modern economy in more detail.  The article begins by outlining two common misconceptions about money creation, and explaining how, in the modern economy, money is largely created by commercial banks making loans.(1)  The article then discusses the limits to the banking system’s ability to create money and the important role for central bank policies in ensuring that credit and money growth are consistent with monetary and financial stability in the economy.  The final section discusses the role of money in the monetary transmission mechanism during periods of quantitative easing (QE), and dispels some myths surrounding money creation and QE.  A short video explains some of the key topics covered in this article.(2)  Two misconceptions about money creation  The vast majority of money held by the public takes the form of bank deposits.  But where the stock of bank deposits comes from is often misunderstood.  One common misconception is that banks act simply as intermediaries, lending out the deposits that savers place with them.  In this view deposits are typically ‘created’ by the saving decisions of households, and banks then ‘lend out’ those existing deposits to borrowers, for example to companies looking to finance investment or individuals wanting to purchase houses.  In fact, when households choose to save more money in bank accounts, those deposits come simply at the expense of deposits that would have otherwise gone to companies in payment for goods and services.  Saving does not by itself increase the deposits or ‘funds available’ for banks to lend. Indeed, viewing banks simply as intermediaries ignores the fact that, in reality in the modern economy, commercial banks are the creators of deposit money.  This article explains how, rather than banks lending out deposits that are placed with them, the act of lending creates deposits — the reverse of the sequence typically described in textbooks.(3)  Another common misconception is that the central bank determines the quantity of loans and deposits in the economy by controlling the quantity of central bank money — the so-called ‘money multiplier’ approach.  In that view, central banks implement monetary policy by choosing a quantity of reserves.  And, because there is assumed to be a constant ratio of broad money to base money, these reserves are then ‘multiplied up’ to a much greater change in bank  loans and deposits.  For the theory to hold, the amount of reserves must be a binding constraint on lending, and the central bank must directly determine the amount of reserves. While the money multiplier theory can be a useful way of introducing money and banking in economic textbooks, it is not an accurate description of how money is created in reality. Rather than controlling the quantity of reserves, central banks today typically implement monetary policy by setting the price of reserves — that is, interest rates.  In reality, neither are reserves a binding constraint on lending, nor does the central bank fix the amount of reserves that are available.  As with the relationship between deposits and loans, the relationship between reserves and loans typically operates in the reverse way to that described in some economics textbooks.  Banks first decide how much to lend depending on the profitable lending opportunities available to them — which will, crucially, depend on the interest rate set by the Bank of England.  It is these lending decisions that determine how many bank deposits are created by the banking system.  The amount of bank deposits in turn influences how much central bank money banks want to hold in reserve (to meet withdrawals by the public, make payments to other banks, or meet regulatory liquidity requirements), which is then, in normal times, supplied on demand by the Bank of England.  The rest of this article discusses these practices in more detail.  Money creation in reality  Lending creates deposits — broad money determination at the aggregate level As explained in ‘Money in the modern economy:  an introduction’, broad money is a measure of the total amount of money held by households and companies in the economy. Broad money is made up of bank deposits — which are essentially IOUs from commercial banks to households and companies — and currency — mostly IOUs from the central bank.(4)(5)  Of the two types of broad money, bank deposits make up the vast majority — 97% of the amount currently in circulation.(6)  And in the modern economy, those bank deposits are mostly created by commercial banks themselves.  (1)  Throughout this article, ‘banks’ and ‘commercial banks’ are used to refer to banks and  building societies together.  (2)  See www.youtube.com/watch?v=CvRAqR2pAgw. (3)  There is a long literature that does recognise the ‘endogenous’ nature of money  creation in practice.  See, for example, Moore (1988), Howells (1995) and Palley (1996).  (4)  The definition of broad money used by the Bank of England, M4ex, also includes a  wider range of bank liabilities than regular deposits;  see Burgess and Janssen (2007) for more details.  For simplicity, this article describes all of these liabilities as deposits. A box later in this article provides details about a range of popular monetary aggregates in the United Kingdom.  (5)  Around 6% of the currency in circulation is made up of coins, which are produced by The Royal Mint.  Of the banknotes that circulate in the UK economy, some are issued by some Scottish and Northern Irish commercial banks, although these are fully matched by Bank of England money held at the Bank.  (6)  As of December 2013.  16  Quarterly Bulletin  2014 Q1  Commercial banks create money, in the form of bank deposits, by making new loans.  When a bank makes a loan, for example to someone taking out a mortgage to buy a house, it does not typically do so by giving them thousands of pounds worth of banknotes.  Instead, it credits their bank account with a bank deposit of the size of the mortgage.  At that moment, new money is created.  For this reason, some economists have referred to bank deposits as ‘fountain pen money’, created at the stroke of bankers’ pens when they approve loans.(1)  This process is illustrated in Figure 1, which shows how new lending affects the balance sheets of different sectors of the economy (similar balance sheet diagrams are introduced in ‘Money in the modern economy:  an introduction’).  As shown in the third row of Figure 1, the new deposits increase the assets of the consumer (here taken to represent households and companies) — the extra red bars — and the new loan increases their liabilities — the extra white bars.  New broad money has been created.  Similarly, both sides of the commercial banking sector’s balance sheet increase as new money and loans are created.  It is important to note that although the simplified diagram of Figure 1 shows the amount of new money created as being identical to the amount of new lending, in practice there will be several factors that may subsequently cause the amount of deposits to be different from the amount of lending.  These are discussed in detail in the next section.  While new broad money has been created on the consumer’s balance sheet, the first row of Figure 1 shows that this is without — in the first instance, at least — any change in the amount of central bank money or ‘base money’.  As discussed earlier, the higher stock of deposits may mean that banks want, or are required, to hold more central bank money in order to meet withdrawals by the public or make payments to other banks.  And reserves are, in normal times, supplied ‘on demand’ by the Bank of England to commercial banks in exchange for other assets on their balance sheets.  In no way does the aggregate quantity of reserves directly constrain the amount of bank lending or deposit creation.  This description of money creation contrasts with the notion that banks can only lend out pre-existing money, outlined in the previous section.  Bank deposits are simply a record of how much the bank itself owes its customers.  So they are a liability of the bank, not an asset that could be lent out.  A related misconception is that banks can lend out their reserves. Reserves can only be lent between banks, since consumers do not have access to reserves accounts at the Bank of England.(2)  Other ways of creating and destroying deposits Just as taking out a new loan creates money, the repayment of bank loans destroys money.(3)  For example, suppose a consumer has spent money in the supermarket throughout the month by using a credit card.  Each purchase made using the  Figure 1  Money creation by the aggregate banking sector making additional loans(a)  Before loans are made  After loans are made  Assets  Liabilities  Assets  Liabilities  Central bank(b)  Non-money  Reserves  Currency  Base money  Non-money  Reserves  Currency  Base money  Commercial banks(c)  Assets  Liabilities  New loans  New deposits  Assets  Liabilities  Reserves  Deposits  Reserves  Deposits  Currency  Currency  Consumers(d)  Assets  Liabilities  New deposits  New loans  Assets  Liabilities  Broad money  Broad money  Deposits  Non-money  Deposits  Non-money  Currency  Currency  (a)  Balance sheets are highly stylised for ease of exposition:  the quantities of each type of  money shown do not correspond to the quantities actually held on each sector’s balance sheet.  (b)  Central bank balance sheet only shows base money liabilities and the corresponding assets. In practice the central bank holds other non-money liabilities.  Its non-monetary assets are mostly made up of government debt.  Although that government debt is actually held by the Bank of England Asset Purchase Facility, so does not appear directly on the balance sheet. (c)  Commercial banks’ balance sheets only show money assets and liabilities before any loans  are made.  (d)  Consumers represent the private sector of households and companies.  Balance sheet only shows broad money assets and corresponding liabilities — real assets such as the house being transacted are not shown.  Consumers’ non-money liabilities include existing secured and unsecured loans.  credit card will have increased the outstanding loans on the consumer’s balance sheet and the deposits on the supermarket’s balance sheet (in a similar way to that shown in Figure 1).  If the consumer were then to pay their credit card  (1)  Fountain pen money is discussed in Tobin (1963), who mentions it in the context of making an argument that banks cannot create unlimited amounts of money in practice.  (2)  Part of the confusion may stem from some economists’ use of the term ‘reserves’ when referring to ‘excess reserves’ — balances held above those required by regulatory reserve requirements.  In this context, ‘lending out reserves’ could be a shorthand way of describing the process of increasing lending and deposits until the bank reaches its maximum ratio.  As there are no reserve requirements in the United Kingdom the process is less relevant for UK banks.  (3)  The fall in bank lending in the United Kingdom since 2008 is an important reason why  the growth of money in the economy has been so much lower than in the years leading up to the crisis, as discussed in Bridges, Rossiter and Thomas (2011) and Butt et al (2012).  Topical articles  Money creation in the modern economy  17  bill in full at the end of the month, its bank would reduce the amount of deposits in the consumer’s account by the value of the credit card bill, thus destroying all of the newly created money.  Banks making loans and consumers repaying them are the most significant ways in which bank deposits are created and destroyed in the modern economy.  But they are far from the only ways.  Deposit creation or destruction will also occur any time the banking sector (including the central bank) buys or sells existing assets from or to consumers, or, more often, from companies or the government.  Banks buying and selling government bonds is one particularly important way in which the purchase or sale of existing assets by banks creates and destroys money.  Banks often buy and hold government bonds as part of their portfolio of liquid assets that can be sold on quickly for central bank money if, for example, depositors want to withdraw currency in large amounts.(1)  When banks purchase government bonds from the non-bank private sector they credit the sellers with bank deposits.(2)  And, as discussed later in this article, central bank asset purchases, known as quantitative easing (QE), have similar implications for money creation.  Money can also be destroyed through the issuance of long-term debt and equity instruments by banks.  In addition to deposits, banks hold other liabilities on their balance sheets. Banks manage their liabilities to ensure that they have at least some capital and longer-term debt liabilities to mitigate certain risks and meet regulatory requirements.  Because these ‘non-deposit’ liabilities represent longer-term investments in the banking system by households and companies, they cannot be exchanged for currency as easily as bank deposits, and therefore increase the resilience of the bank.  When banks issue these longer-term debt and equity instruments to non-bank financial companies, those companies pay for them with bank deposits.  That reduces the amount of deposit, or money, liabilities on the banking sector’s balance sheet and increases their non-deposit liabilities.(3)  Buying and selling of existing assets and issuing longer-term liabilities may lead to a gap between lending and deposits in a closed economy.  Additionally, in an open economy such as the United Kingdom, deposits can pass from domestic residents to overseas residents, or sterling deposits could be converted into foreign currency deposits.  These transactions do not destroy money per se, but overseas residents’ deposits and foreign currency deposits are not always counted as part of a country’s money supply.  Limits to broad money creation Although commercial banks create money through their lending behaviour, they cannot in practice do so without limit. In particular, the price of loans — that is, the interest rate (plus  any fees) charged by banks — determines the amount that households and companies will want to borrow.  A number of factors influence the price of new lending, not least the monetary policy of the Bank of England, which affects the level of various interest rates in the economy.  The limits to money creation by the banking system were discussed in a paper by Nobel Prize winning economist James Tobin and this topic has recently been the subject of debate among a number of economic commentators and bloggers.(4)  In the modern economy there are three main sets of constraints that restrict the amount of money that banks can create.  (i)  Banks themselves face limits on how much they can  lend.  In particular:    Market forces constrain lending because individual  banks have to be able to lend profitably in a competitive market.    Lending is also constrained because banks have to take  steps to mitigate the risks associated with making additional loans.    Regulatory policy acts as a constraint on banks’  activities in order to mitigate a build-up of risks that could pose a threat to the stability of the financial system.  (ii)  Money creation is also constrained by the behaviour of the money holders — households and businesses. Households and companies who receive the newly created money might respond by undertaking transactions that immediately destroy it, for example by repaying outstanding loans.  (iii)  The ultimate constraint on money creation is monetary policy.  By influencing the level of interest rates in the economy, the Bank of England’s monetary policy affects how much households and companies want to borrow. This occurs both directly, through influencing the loan rates charged by banks, but also indirectly through the overall effect of monetary policy on economic activity in  (1)  It is for this reason that holdings of some government bonds are counted towards meeting prudential liquidity requirements, as described in more detail by Farag, Harland and Nixon (2013).  (2)  In a balance sheet diagram such as Figure 1, a purchase of government bonds from consumers by banks would be represented by a change in the composition of consumers’ assets from government bonds to deposits and an increase in both deposits and government bonds on the commercial banks’ balance sheet.  (3)  Commercial banks’ purchases of government bonds and their issuance of long-term  debt and equity have both been important influences on broad money growth during the financial crisis as discussed in Bridges, Rossiter and Thomas (2011) and Butt et al (2012).  (4)  Tobin (1963) argued that banks do not possess a ‘widow’s cruse’, referring to a biblical story (earlier referenced in economics by John Maynard Keynes) in which a widow is able to miraculously refill a cruse (a pot or jar) of oil during a famine.  Tobin was arguing that there were limits to how many loans could be automatically matched by deposits.  18  Quarterly Bulletin  2014 Q1  the economy.  As a result, the Bank of England is able to ensure that money growth is consistent with its objective of low and stable inflation.  make many such loans every day.  So if a given bank financed all of its new loans in this way, it would soon run out of reserves.  The remainder of this section explains how each of these mechanisms work in practice.  (i) Limits on how much banks can lend Market forces facing individual banks Figure 1 showed how, for the aggregate banking sector, loans are initially created with matching deposits.  But that does not mean that any given individual bank can freely lend and create money without limit.  That is because banks have to be able to lend profitably in a competitive market, and ensure that they adequately manage the risks associated with making loans.  Banks receive interest payments on their assets, such as loans, but they also generally have to pay interest on their liabilities, such as savings accounts.  A bank’s business model relies on receiving a higher interest rate on the loans (or other assets) than the rate it pays out on its deposits (or other liabilities). Interest rates on both banks’ assets and liabilities depend on the policy rate set by the Bank of England, which acts as the ultimate constraint on money creation.  The commercial bank uses the difference, or spread, between the expected return on their assets and liabilities to cover its operating costs and to make profits.(1)  In order to make extra loans, an individual bank will typically have to lower its loan rates relative to its competitors to induce households and companies to borrow more.  And once it has made the loan it may well ‘lose’ the deposits it has created to those competing banks.  Both of these factors affect the profitability of making a loan for an individual bank and influence how much borrowing takes place.  For example, suppose an individual bank lowers the rate it charges on its loans, and that attracts a household to take out a mortgage to buy a house.  The moment the mortgage loan is made, the household’s account is credited with new deposits. And once they purchase the house, they pass their new deposits on to the house seller.  This situation is shown in the first row of Figure 2.  The buyer is left with a new asset in the form of a house and a new liability in the form of a new loan. The seller is left with money in the form of bank deposits instead of a house.  It is more likely than not that the seller’s account will be with a different bank to the buyer’s.  So when the transaction takes place, the new deposits will be transferred to the seller’s bank, as shown in the second row of Figure 2.  The buyer’s bank would then have fewer deposits than assets.  In the first instance, the buyer’s bank settles with the seller’s bank by transferring reserves.  But that would leave the buyer’s bank with fewer reserves and more loans relative to its deposits than before.  This is likely to be problematic for the bank since it would increase the risk that it would not be able to meet all of its likely outflows.  And, in practice, banks  Banks therefore try to attract or retain additional liabilities to accompany their new loans.  In practice other banks would also be making new loans and creating new deposits, so one way they can do this is to try and attract some of those newly created deposits.  In a competitive banking sector, that may involve increasing the rate they offer to households on their savings accounts.  By attracting new deposits, the bank can increase its lending without running down its reserves, as shown in the third row of Figure 2.  Alternatively, a bank can borrow from other banks or attract other forms of liabilities, at least temporarily.  But whether through deposits or other liabilities, the bank would need to make sure it was attracting and retaining some kind of funds in order to keep expanding lending.  And the cost of that needs to be measured against the interest the bank expects to earn on the loans it is making, which in turn depends on the level of Bank Rate set by the Bank of England.  For example, if a bank continued to attract new borrowers and increase lending by reducing mortgage rates, and sought to attract new deposits by increasing the rates it was paying on its customers’ deposits, it might soon find it unprofitable to keep expanding its lending.  Competition for loans and deposits, and the desire to make a profit, therefore limit money creation by banks.  Managing the risks associated with making loans Banks also need to manage the risks associated with making new loans.  One way in which they do this is by making sure that they attract relatively stable deposits to match their new loans, that is, deposits that are unlikely or unable to be withdrawn in large amounts.  This can act as an additional limit to how much banks can lend.  For example, if all of the deposits that a bank held were in the form of instant access accounts, such as current accounts, then the bank might run the risk of lots of these deposits being withdrawn in a short period of time.  Because banks tend to lend for periods of many months or years, the bank may not be able to repay all of those deposits — it would face a great deal of liquidity risk. In order to reduce liquidity risk, banks try to make sure that some of their deposits are fixed for a certain period of time, or term.(2)  Consumers are likely to require compensation for the inconvenience of holding longer-term deposits, however, so these are likely to be more costly for banks, limiting the amount of lending banks wish to do.  And as discussed earlier, if banks guard against liquidity risk by issuing long-term liabilities, this may destroy money directly when companies pay for them using deposits.  (1)  See Button, Pezzini and Rossiter (2010) for an explanation of how banks price new  loans.  (2)  Banks also guard against liquidity risk by holding liquid assets (including reserves and currency), which either can be used directly to cover outflows, or if not can quickly and cheaply be converted into assets that can.  Although if banks purchase liquid assets such as government bonds from non-banks, this could create further deposits.  Topical articles  Money creation in the modern economy  19  Figure 2  Money creation for an individual bank making an additional loan(a)  Changes to the balance sheets of the house buyer and seller  House buyer  House seller  House buyer  House seller  House buyer  House seller  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Non-money (house)  GovernmentDeposits debt Currency  Deposits  Currency  Non-money  New deposit  New loan  Non-money (house)  Non-money (house)  New loan  New deposit  Non-money  Non-money  Non-money  Deposits  Currency  Non-money  GovernmentDeposits debt Currency  Deposits  Currency  Non-money  GovernmentDeposits debt Currency  Balance sheets before the loan is made.  The house buyer takes out a mortgage…  …and uses its new deposits to pay the house seller.  Changes to the balance sheets of the house buyer and seller’s banks  Buyer’s bank  Seller’s bank  Buyer’s bank  Seller’s bank  Buyer’s bank  Seller’s bank  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  New loan  New deposit  Transferred reserves  New deposit  Reserves  Deposits  Reserves  Deposits  Reserves  Deposits  Reserves  Deposits  Currency  Currency  Currency  Currency  Balance sheets before the loan is made.  The mortgage lender creates new deposits…  New loan  Reserves  Currency  Deposits  Reserves  Currency  Deposits  …which are transferred to the seller’s bank, along with reserves, which the buyer’s bank uses to settle the transaction.  But settling all transactions in this way would be unsustainable: •  The buyer’s bank would have fewer reserves to meet its possible  outfows, for example from deposit withdrawals.  •  And if it made many new loans it would eventually run out  of reserves.  Buyer’s bank  Seller’s bank  Assets  Liabilities  Assets  Liabilities  New loan  New deposit  Reserves  Reserves  Currency  Deposits  Reserves  Currency  Deposits  So the buyer’s bank will in practice seek to attract or retain new deposits (and reserves) — in the example shown here, from the seller’s bank — to accompany their new loans.  (a)  Balance sheets are highly stylised for ease of exposition:  the quantities of each type of money shown do not correspond to the quantities actually held on each sector’s balance sheet.  Individual banks’ lending is also limited by considerations of credit risk.  This is the risk to the bank of lending to borrowers who turn out to be unable to repay their loans.  In part, banks can guard against credit risk by having sufficient capital to absorb any unexpected losses on their loans.  But since loans will always involve some risk to banks of incurring losses, the cost of these losses will be taken into account when pricing loans.  When a bank makes a loan, the interest rate it charges will typically include compensation for the average level of credit losses the bank expects to suffer.  The size of this component of the interest rate will be larger when banks estimate that they will suffer higher losses, for example when lending to mortgagors with a high loan to value ratio.  As banks expand lending, their average expected loss per loan is likely to increase, making those loans less profitable.  This further limits the amount of lending banks can profitably do, and the money they can therefore create.  Market forces do not always lead individual banks to sufficiently protect themselves against liquidity and credit risks.  Because of this, prudential regulation aims to ensure that banks do not take excessive risks when making new loans, including via requirements for banks’ capital and liquidity positions.  These requirements can therefore act as an additional brake on how much money commercial banks create by lending.  The prudential regulatory framework, along with more detail on capital and liquidity, is described in Farag, Harland and Nixon (2013).  So far this section has considered the case of an individual bank making additional loans by offering competitive interest rates — both on its loans and deposits.  But if all banks simultaneously decide to try to do more lending, money growth may not be limited in quite the same way.  Although an individual bank may lose deposits to other banks, it would itself be likely to gain some deposits as a result of the other banks making loans.     20  Quarterly Bulletin  2014 Q1  There are a number of reasons why many banks may choose to increase their lending markedly at the same time.  For example, the profitability of lending at given interest rates could increase because of a general improvement in economic conditions.  Alternatively, banks may decide to lend more if they perceive the risks associated with making loans to households and companies to have fallen.  This sort of development is sometimes argued to be one of the reasons why bank lending expanded so much in the lead up to the financial crisis.(1)  But if that perception of a less risky environment were unwarranted, the result could be a more fragile financial system.(2)  One of the responses to the crisis in the United Kingdom has been the creation of a macroprudential authority, the Financial Policy Committee, to identify, monitor and take action to reduce or remove risks which threaten the resilience of the financial system as a whole.(3)  (ii) Constraints arising from the response of households and companies In addition to the range of constraints facing banks that act to limit money creation, the behaviour of households and companies in response to money creation by the banking sector can also be important, as argued by Tobin.  The behaviour of the non-bank private sector influences the ultimate impact that credit creation by the banking sector has on the stock of money because more (or less) money may be created than they wish to hold relative to other assets (such as property or shares).  As the households and companies who take out loans do so because they want to spend more, they will quickly pass that money on to others as they do so.  How those households and companies then respond will determine the stock of money in the economy, and potentially have implications for spending and inflation.  There are two main possibilities for what could happen to newly created deposits.  First, as suggested by Tobin, the money may quickly be destroyed if the households or companies receiving the money after the loan is spent wish to use it to repay their own outstanding bank loans.  This is sometimes referred to as the ‘reflux theory’.(4) For example, a first-time house buyer may take out a mortgage to purchase a house from an elderly person who, in turn, repays their existing mortgage and moves in with their family.  As discussed earlier, repaying bank loans destroys money just as making loans creates it.  So, in this case, the balance sheet of consumers in the economy would be returned to the position it was in before the loan was made.  in the economy.(5)  Instead, the money may initially pass to households or companies with positive holdings of financial assets:  the elderly person may have already paid off their mortgage, or a company receiving money as a payment may already have sufficient liquid assets to cover possible outgoings.  They may then be left holding more money than they desire, and attempt to reduce their ‘excess’ money holdings by increasing their spending on goods and services. (In the case of a company it may instead buy other, higher-yielding, assets.)  These two scenarios for what happens to newly created money — being quickly destroyed or being passed on via spending — have very different implications for economic activity.  In the latter, the money may continue to be passed between different households and companies each of whom may, in turn, increase their spending.  This process — sometimes referred to as the ‘hot potato’ effect — can lead, other things equal, to increased inflationary pressure on the economy.(6)  In contrast, if the money is quickly destroyed as in the former scenario, there need be no further effects on the economy.  This section has so far discussed how the actions of banks, households and companies can affect the amount of money in the economy, and therefore inflationary pressure.  But the ultimate determinant of monetary conditions in the economy is the monetary policy of the central bank.  (iii) Monetary policy — the ultimate constraint on money creation One of the Bank of England’s primary objectives is to ensure monetary stability by keeping consumer price inflation on track to meet the 2% target set by the Government.  And, as discussed in the box on pages 22–23, over some periods of time, various measures of money have grown at a similar rate to nominal spending, which determines inflationary pressure in the economy in the medium term.  So setting monetary policy appropriately to meet the inflation target should ultimately ensure a stable rate of credit and money creation consistent with meeting that target.  This section explains the relationship between monetary policy and different types of money.  In normal times, the Monetary Policy Committee (MPC), like most of its equivalents in other countries, implements monetary policy by setting short-term interest rates, specifically by setting the interest rate paid on central bank reserves held by commercial banks.  It is able to do so because  The second possible outcome is that the extra money creation by banks can lead to more spending in the economy.  For newly created money to be destroyed, it needs to pass to households and companies with existing loans who want to repay them.  But this will not always be the case, since asset and debt holdings tend to vary considerably across individuals  (1)  See, for example, Haldane (2009). (2)  Tucker (2009) discusses the possibility of such ‘risk illusion’ in the financial system. (3)  Tucker, Hall and Pattani (2013) describe the new powers for macroprudential policymaking in the United Kingdom in the wake of the recent financial crisis.  (4)  See Kaldor and Trevithick (1981). (5)  See Kamath et al (2011). (6)  This mechanism is explained in more detail in papers including Laidler (1984),  Congdon (1992, 2005), Howells (1995), Laidler and Robson (1995), Bridges, Rossiter and Thomas (2011) and Bridges and Thomas (2012).  Topical articles  Money creation in the modern economy  21  of the Bank’s position as the monopoly provider of central bank money in the United Kingdom.  And it is because there is demand for central bank money — the ultimate means of settlement for banks, the creators of broad money — that the price of reserves has a meaningful impact on other interest rates in the economy.  The interest rate that commercial banks can obtain on money placed at the central bank influences the rate at which they are willing to lend on similar terms in sterling money markets — the markets in which the Bank and commercial banks lend to each other and other financial institutions.  The exact details of how the Bank uses its money market operations to implement monetary policy has varied over time, and central bank operating procedures today differ somewhat from country to country, as discussed in Clews, Salmon and Weeken (2010).(1)  Changes in interbank interest rates then feed through to a wider range of interest rates in different markets and at different maturities, including the interest rates that banks charge borrowers for loans and offer savers for deposits.(2)  By influencing the price of credit in this way, monetary policy affects the creation of broad money.  This description of the relationship between monetary policy and money differs from the description in many introductory textbooks, where central banks determine the quantity of broad money via a ‘money multiplier’ by actively varying the quantity of reserves.(3)  In that view, central banks implement monetary policy by choosing the quantity of reserves.  And, because there is assumed to be a stable ratio of broad money to base money, these reserves are then ‘multiplied up’ to a much greater change in bank deposits as banks increase lending and deposits.  Neither step in that story represents an accurate description of the relationship between money and monetary policy in the modern economy.  Central banks do not typically choose a quantity  of reserves to bring about the desired short-term interest rate.(4)  Rather, they focus on prices — setting interest rates.(5)  The Bank of England controls interest rates by supplying and remunerating reserves at its chosen policy rate.  The supply of both reserves and currency (which together make up base money) is determined by banks’ demand for reserves both for the settlement of payments and to meet demand for currency from their customers — demand that the central bank typically accommodates.  This demand for base money is therefore more likely to be a consequence rather than a cause of banks making loans and creating broad money.  This is because banks’ decisions to extend credit are based on the availability of profitable lending opportunities at any given point in time.  The profitability of making a loan will depend on a number of factors, as discussed earlier.  One of these is the cost of funds that banks face, which is closely related to the interest rate paid on reserves, the policy rate.  In contrast, the quantity of reserves already in the system does not constrain the creation of broad money through the act of lending.(6)  This leg of the money multiplier is sometimes motivated by appealing to central bank reserve requirements, whereby banks are obliged to hold a minimum amount of reserves equal to a fixed proportion of their holdings of deposits.  But reserve requirements are not an important aspect of monetary policy frameworks in most advanced economies today.(7)  A looser stance of monetary policy is likely to increase the stock of broad money by reducing loan rates and increasing the volume of loans.  And a larger stock of broad money, accompanied by an increased level of spending in the economy, may cause banks and customers to demand more reserves and currency.(8)  So, in reality, the theory of the money multiplier operates in the reverse way to that normally described.  QE — creating broad money directly with monetary policy  The previous section discussed how monetary policy can be seen as the ultimate limit to money creation by commercial banks.  But commercial banks could alternatively create too little money to be consistent with the economy meeting the inflation target.  In normal times, the MPC can respond by lowering the policy rate to encourage more lending and hence more money creation.  But, in response to the financial crisis, the MPC cut Bank Rate to 0.5% — the so-called effective lower bound.  Once short-term interest rates reach the effective lower bound, it is not possible for the central bank to provide further stimulus to the economy by lowering the rate at which reserves are remunerated.(9)  One possible way of providing further monetary stimulus to the economy is through a programme of asset purchases (QE).  Like reductions in Bank  (1)  The framework for the Bank’s operations in the sterling money markets is set out in  the Bank’s ‘Red Book’, available at www.bankofengland.co.uk/markets/Documents/money/publications/redbook.pdf. Recent developments in sterling money markets are discussed by Jackson and Sim (2013).  (2)  Bank of England (1999) discusses the transmission mechanism of monetary policy in  more detail.  (3)  Benes and Kumhof (2012) discuss the money multiplier myth in more detail. (4)  As discussed by Disyatat (2008). (5)  Bindseil (2004) provides a detailed account of how monetary policy implementation  works through short-term interest rates.  (6)  Carpenter and Demiralp (2012) show that changes in quantities of reserves are  unrelated to changes in quantities of loans in the United States.  (7)  The Bank of England currently has no formal reserve requirements, for example. (It does require banks to hold a proportion of non-interest bearing ‘cash ratio deposits’ with the Bank for a subset of their liabilities.  But the function of these cash ratio deposits is non-operational.  Their sole purpose is to provide income for the Bank.)  Bernanke (2007) discusses how reserve requirements now present less of a constraint than in the past in the United States.  (8)  Kydland and Prescott (1990) found that broad money aggregates led the cycle, while  base money aggregates tended to lag the cycle slightly.  (9)  If the central bank were to lower interest rates significantly below zero, banks could swap their bank reserves into currency, which would pay a higher interest rate (of zero, or slightly less after taking into account the costs of storing currency).  Or put another way, the demand for central bank reserves would disappear, so the central bank could no longer influence the economy by changing the price of those reserves.  22  Quarterly Bulletin  2014 Q1  The information content of different types of money and monetary aggregates  One of the Bank of England’s primary objectives is to ensure monetary stability by keeping inflation on track to meet the Government’s 2% target.  Milton Friedman (1963) famously argued that ‘inflation is always and everywhere a monetary phenomenon’.  So changes in the money supply may contain valuable information about spending and inflationary pressure in the economy.  Since money is essential for buying goods and services, it is likely to contain corroborative information about the current level of nominal spending in the economy.  It may also provide incremental information about future movements in nominal spending, and so can be a useful indicator of future inflationary pressure.  Finally, the behaviour of money may help to reveal the nature of the monetary transmission mechanism, especially when monetary policy is operated through ‘quantitative easing’ (QE).  In practice, a key difficulty is assessing which measures of money are the appropriate ones to look at for each of the different purposes.  The Bank currently constructs a number of monetary aggregates and publishes a range of data that allow to be created, summarised in Table 1.  Chart A shows some long-run historical time series of the growth of monetary aggregates compared with that of nominal spending in the economy.(1)  Given the various changes in the UK monetary regime over the past 150 years, it is unlikely that a single monetary indicator perfectly captures both the corroborative and incremental information in money.  The UK financial sector has also undergone various structural changes that need to be taken into account when considering the underlying link between money and spending.  For example, during periods when the financial sector has grown relative to the rest of the economy (such as in the early 1980s and the 2000s), broad money has tended to grow persistently faster than nominal spending.  Narrower measures of money, such as notes and coin and sight deposits (accounts that can be withdrawn immediately without penalty) are, in principle, better corroborative indicators of spending, as these are likely to be the types of money used to carry out the majority of transactions in goods and services in the economy.  The sum of notes and coin and sight deposits held by the non-bank private sector is sometimes known as zero maturity money or ‘MZM’.(2)  Broader measures of money might be more appropriate as incremental indicators of future spending and more revealing about the nature of the transmission mechanism.  M2, for example, additionally includes household time deposits such as savings accounts.(3)  And M4 is an even broader measure, including all sight and time deposits held by non-financial companies and non-bank financial companies.  The main article describes how QE works by first increasing the deposits of financial companies.  As these companies rebalance their  portfolios, asset prices are likely to increase and, with a lag, lead to an increase in households’ and companies’ spending.  So monitoring broad money has been an important part of assessing the effectiveness of QE.(4)  A number of econometric studies have suggested that sectoral movements in broad money may also provide valuable incremental information about spending in the economy.(5)  For example, non-financial companies’ deposits appear to be a leading indicator of business investment in the economy. One can also try and weight different types of narrow and broad money together using some metric of how much each type of money is used in transactions — known as a Divisia index.(6)  In practice, the interest paid on a given type of money is typically used as a weighting metric.  That is because individuals and companies are only likely to hold money which earns a low interest rate relative to other financial instruments if it compensates them by providing greater transactions services.  Identifying the appropriate measurement of money has been complicated by the continued development of the financial sector.  This has both expanded the range of instruments that might serve as money and the range of financial institutions that borrow from and deposit with the traditional banking system.  For example, sale and repurchase agreements (known as repos) — where a company agrees to buy a security from a bank with agreement to sell it back later — are currently included in M4 since the claim held on the bank can be thought of as a secured deposit.  In addition, some economists have argued that a range of instruments that provide collateral for various types of borrowing and lending could also be included in a broader measure of money.(7)  Moreover, many of the non-bank institutions that hold deposits mainly intermediate between banks themselves.  The deposits of these institutions, known as ‘intermediate other financial corporations’ (IOFCs), are likely to reflect activities within the banking system that are not directly related to spending in the economy.(8)  For this reason, the Bank’s headline measure of broad money is M4ex, which excludes IOFC deposits.  (1)  These series involve splicing together current Bank of England data with historic data  on monetary aggregates.  A spreadsheet of the data is available at www.bankofengland.co.uk/publications/Documents/quarterlybulletin/2014/ longrunmoneydata.xls.  (2)  A narrower measure known as non-interest bearing M1 can also be constructed.  This measure has become a less useful aggregate as most sight deposits now pay some form of interest.  For example, during the financial crisis when interest rates fell close to zero, the growth of non-interest bearing M1 picked up markedly as the relative cost of holding a non-interest bearing deposit fell sharply compared to an interest-bearing one.  Focusing on M1 would have given a misleading signal about the growth of nominal spending in the economy.  (3)  M2 contains the non-bank private sector’s holdings of notes and coin plus ‘retail’  deposits which are deposits that pay an advertised interest rate.  Those will largely be deposits held by households but will also apply to some corporate deposits.  (4)  See Bridges, Rossiter and Thomas (2011) and Butt et al (2012). (5)  See, for example, Astley and Haldane (1995), Thomas (1997a, b) and Brigden and  Mizen (2004).  (6)  See Hancock (2005), for example. (7)  See, for example, Singh (2013). (8)  See Burgess and Janssen (2007) and  www.bankofengland.co.uk/statistics/Pages/iadb/notesiadb/m4adjusted.aspx for more detail.  Topical articles  Money creation in the modern economy  23  Table 1  Popular monetary aggregates that can be constructed from available UK data(a)  Name  Definition  Description(b)  Availability  Notes and coin  M0  Notes and coin in circulation outside the Bank of England.  Notes and coin plus central bank reserves.  The narrowest measure of money and used as an indicator of cash-based transactions.  1870–present(c)  Historically the base measure of money used in money multiplier calculations.  Often used as an approximate measure of the size of the Bank of England’s balance sheet. No longer published by the Bank of England but can be reconstructed.(d)  1870–present(c)  Non-interest bearing M1  Notes and coin plus non-interest bearing sight deposits held by the non-bank private sector.  An indicator of transactions in goods and services in the economy, less useful now since most sight deposits pay some form of interest.  1921–present(c)  Not published by the Bank of England but can be constructed from published components.  MZM  Notes and coin plus all sight deposits held by the non-bank private sector.  M2 or retail M4  Notes and coin plus all retail deposits (including retail time deposits) held by the non-bank private sector.  Notes and coin plus all sight and time deposits held with banks (excluding building societies) by the non-bank private sector.  Notes and coin, deposits, certificates of deposit, repos and securities with a maturity of less than five years held by the non-bank private sector.  M3  M4  M4ex  Divisia  An indicator of transactions in goods and services in the economy.  1977–present  Not published by the Bank of England but can be constructed from published components. The Bank also produces a measure based on an ECB definition of M1.  A broader measure of money than MZM encompassing all retail deposits.  The key additions are household time deposits and some corporate retail time deposits.  1982–present  Published by the Bank of England.  The Bank also produces a measure based on an ECB definition of M2.  Up until 1987 the headline broad monetary aggregate constructed by the Bank of England.  1870–1990(c)  The Bank also produces a measure based on an ECB definition of M3.  Up until 2007 the headline broad monetary aggregate constructed by the Bank of England.  1963–present  M4 excluding the deposits of IOFCs.  Since 2007 the headline broad monetary aggregate constructed by the Bank of England.  1997–present  A weighted sum of different types of money.  Aims to weight the component assets of broad money according to the transactions services they provide.(e)  1977–present  (a)  All definitions refer to sterling instruments only.  Some of the definitions in this table were changed at various points in time.  For example the original M3 aggregate included public sector deposits and thesector’s holdings of deposits in foreign currency.  A more comprehensive history of the development of UK monetary aggregates can be found at www.bankofengland.co.uk/statistics/Documents/ms/articl  non-bank private es/art2jul03.pdf.  (b)  Published by the Bank of England unless otherwise stated. (c)  This series uses the data constructed by Capie and Webber (1985). (d)  Data on M0 were discontinued following reforms to the Bank of England’s money market operations in 2006.  See www.bankofengland.co.uk/statistics/Documents/ms/articles/artjun06.pdf for more detai(e)  The Divisia indices for other financial corporations and for the non-bank private sector were discontinued in 2013.  See www.bankofengland.co.uk/statistics/Documents/ms/articles/art1aug13.pdf for more  ls. details.  Chart A  Different monetary aggregates and nominal spending  Notes and coin(a) Non-interest bearing M1(b)  MZM(c) M2(d) Divisia M3/M4/M4ex(e)  Nominal GDP(f)  Percentage changes on a year earlier  1870  80  90  1900  10  20  30  40  50  60  70  80  90  2000 10  60  50  40  30  20  10 + 0 – 10  20  30  Sources:  Bank of England, Capie and Webber (1985), Mitchell (1988), ONS, Sefton and Weale (1995), Solomou and Weale (1991) and Bank calculations.  All series seasonally adjusted and break-adjusted where possible.  Historical data seasonally adjusted using X12.  (a)  1969 Q2 to 2013 Q4 — notes and coin in circulation.  1870 Q1 to 1969 Q2 — M0 from Capie and Webber (1985). (b)  1977 Q1 to 2013 Q4 — notes and coin held by the non-bank and building society private sector plus non-interest bearing deposits.  Prior to 2008 Q1, excludes deposits with  building societies.  1963 Q1 to 1977 Q1 — historical M1 data from Bank of England Quarterly Bulletins.  1921 Q4 to 1963 Q1 — Capie and Webber (1985). (c)  Notes and coin held by the non-bank and building society private sector plus total sight deposits.  Prior to 1998 Q4 excludes deposits with building societies. (d)  Notes and coin and retail deposits held by the non-bank and building society private sector. (e)  1997 Q4 to 2013 Q4 — M4 excluding intermediate OFCs.  1963 Q1 to 1997 Q4 — M4.  1870 Q2 to 1963 Q1 — M3 from Capie and Webber (1985). (f)  Composite estimate of nominal GDP at market prices.  See appendix of Hills, Thomas and Dimsdale (2010) for details.  24  Quarterly Bulletin  2014 Q1  Rate, asset purchases are a way in which the MPC can loosen the stance of monetary policy in order to stimulate economic activity and meet its inflation target.  But the role of money in the two policies is not the same.  QE involves a shift in the focus of monetary policy to the quantity of money:  the central bank purchases a quantity of assets, financed by the creation of broad money and a corresponding increase in the amount of central bank reserves. The sellers of the assets will be left holding the newly created deposits in place of government bonds.  They will be likely to be holding more money than they would like, relative to other assets that they wish to hold.  They will therefore want to rebalance their portfolios, for example by using the new deposits to buy higher-yielding assets such as bonds and shares issued by companies — leading to the ‘hot potato’ effect discussed earlier.  This will raise the value of those assets and lower the cost to companies of raising funds in these markets.  That, in turn, should lead to higher spending in the economy.(1)  The way in which QE works therefore differs from two common misconceptions about central bank asset purchases:  that QE involves giving banks ‘free money’;  and that the key aim of QE is to increase bank lending by providing more reserves to the banking system, as might be described by the money multiplier theory.  This section explains the relationship between money and QE and dispels these misconceptions.  The link between QE and quantities of money QE has a direct effect on the quantities of both base and broad money because of the way in which the Bank carries out its asset purchases.  The policy aims to buy assets, government bonds, mainly from non-bank financial companies, such as pension funds or insurance companies.  Consider, for example, the purchase of £1 billion of government bonds from a pension fund.  One way in which the Bank could carry out the purchase would be to print £1 billion of banknotes and swap these directly with the pension fund.  But transacting in such large quantities of banknotes is impractical.  These sorts of transactions are therefore carried out using electronic forms of money.  As the pension fund does not hold a reserves account with the Bank of England, the commercial bank with whom they hold a bank account is used as an intermediary.  The pension fund’s bank credits the pension fund’s account with £1 billion of deposits in exchange for the government bonds.  This is shown in the first panel of Figure 3.  The Bank of England finances its purchase by crediting reserves to the pension fund’s bank — it gives the commercial bank an IOU (second row).  The commercial bank’s balance sheet expands:  new deposit liabilities are matched with an asset in the form of new reserves (third row).  Figure 3  Impact of QE on balance sheets(a)  Before asset purchase  After asset purchase  Pension fund  Assets  Liabilities  Assets  Liabilities  Government debt  Other  Deposits  Other  Central bank(b)  Assets  Liabilities  Assets  Liabilities  Government debt  Reserves  Other assets  Reserves  Other assets  Commercial bank  Assets  Liabilities  Assets  Liabilities  Reserves  Deposits Reserves  Deposits (a)  Balance sheets are highly stylised for ease of exposition:  quantities of assets and liabilities shown do not correspond to the quantities actually held by those sectors.  The figure only shows assets and liabilities relevant to the transaction.  (b)  Government debt is actually purchased by the Bank of England’s Asset Purchase Facility using a loan from the Bank of England, so does not actually appear directly on the Bank’s official consolidated balance sheet.  Two misconceptions about how QE works Why the extra reserves are not ‘free money’ for banks While the central bank’s asset purchases involve — and affect — commercial banks’ balance sheets, the primary role of those banks is as an intermediary to facilitate the transaction between the central bank and the pension fund.  The additional reserves shown in Figure 3 are simply a by-product of this transaction.  It is sometimes argued that, because they are assets held by commercial banks that earn interest, these reserves represent ‘free money’ for banks.  While banks do earn interest on the newly created reserves, QE also creates an accompanying liability for the bank in the form of the pension fund’s deposit, which the bank will itself typically have to pay interest on.  In other words, QE leaves banks with both a new IOU from the central bank but also a new, equally sized IOU to consumers (in this case, the pension fund), and the interest rates on both of these depend on Bank Rate.  Why the extra reserves are not multiplied up into new loans and broad money As discussed earlier, the transmission mechanism of QE relies on the effects of the newly created broad — rather than base — money.  The start of that transmission is the creation of  (1)  The ways in which QE affects the economy are covered in more detail in Benford et al (2009), Joyce, Tong and Woods (2011) and Bowdler and Radia (2012).  The role of money more specifically is described in Bridges, Rossiter and Thomas (2011), Bridges and Thomas (2012) and Butt et al (2012).  Topical articles  Money creation in the modern economy  25  bank deposits on the asset holder’s balance sheet in the place of government debt (Figure 3, first row).  Importantly, the reserves created in the banking sector (Figure 3, third row) do not play a central role.  This is because, as explained earlier, banks cannot directly lend out reserves.  Reserves are an IOU from the central bank to commercial banks.  Those banks can use them to make payments to each other, but they cannot ‘lend’ them on to consumers in the economy, who do not hold reserves accounts.  When banks make additional loans they are matched by extra deposits — the amount of reserves does not change.  Moreover, the new reserves are not mechanically multiplied up into new loans and new deposits as predicted by the money multiplier theory.  QE boosts broad money without directly leading to, or requiring, an increase in lending.  While the first leg of the money multiplier theory does hold during QE — the monetary stance mechanically determines the quantity of reserves — the newly created reserves do not, by themselves, meaningfully change the incentives for the banks to create new broad money by lending.  It is possible that QE might indirectly affect the incentives facing banks to make new loans, for example by reducing their funding costs, or by increasing the quantity of credit by boosting activity.(1)  But equally, QE could lead to companies repaying bank credit, if they were to issue more bonds or equity and use those funds  to repay bank loans.  On balance, it is therefore possible for QE to increase or to reduce the amount of bank lending in the economy.  However these channels were not expected to be key parts of its transmission:  instead, QE works by circumventing the banking sector, aiming to increase private sector spending directly.(2)  Conclusion  This article has discussed how money is created in the modern economy.  Most of the money in circulation is created, not by the printing presses of the Bank of England, but by the commercial banks themselves:  banks create money whenever they lend to someone in the economy or buy an asset from consumers.  And in contrast to descriptions found in some textbooks, the Bank of England does not directly control the quantity of either base or broad money.  The Bank of England is nevertheless still able to influence the amount of money in the economy.  It does so in normal times by setting monetary policy — through the interest rate that it pays on reserves held by commercial banks with the Bank of England.  More recently, though, with Bank Rate constrained by the effective lower bound, the Bank of England’s asset purchase programme has sought to raise the quantity of broad money in circulation. This in turn affects the prices and quantities of a range of assets in the economy, including money.  (1)  A similar mechanism whereby QE could increase bank lending by enabling banks to  attract more stable funding is discussed in Miles (2012).  (2)  These channels, along with the effect of QE on bank lending more broadly, are  discussed in detail in a box in Butt et al (2012).  26  Quarterly Bulletin  2014 Q1  References  Astley, M and Haldane, A (1995), ‘Money as an indicator’, Bank of England Working Paper No. 35.  Bank of England (1999), ‘The transmission mechanism of monetary policy’, available at www.bankofengland.co.uk/publications/ Documents/other/monetary/montrans.pdf.  Benes, J and Kumhof, M (2012), ‘The Chicago Plan revisited’, IMF Working Paper No. 12/202.  Benford, J, Berry, S, Nikolov, K, Robson, M and Young, C (2009), ‘Quantitative easing’, Bank of England Quarterly Bulletin, Vol. 49, No. 2, pages 90–100.  Bernanke, B (2007), ‘The financial accelerator and the credit channel’, speech at a conference on The Credit Channel of Monetary Policy in the Twenty-first Century, Federal Reserve Bank of Atlanta.  Bindseil, U (2004), ‘The operational target of monetary policy and the rise and fall of the reserve position doctrine’, ECB Working Paper No. 372.  Bowdler, C and Radia, A (2012), ‘Unconventional monetary policy: the assessment’, Oxford Review of Economic Policy, Vol. 28, No. 4, pages 603–21.  Clews, R, Salmon, C and Weeken, O (2010), ‘The Bank’s money market framework’, Bank of England Quarterly Bulletin, Vol. 50, No. 4, pages 292–301.  Congdon, T (1992), Reflections on monetarism, Clarendon Press.  Congdon, T (2005), ‘Money and asset prices in boom and bust’, Institute of Economic Affairs, Hobart Paper No. 152.  Disyatat, P (2008), ‘Monetary policy implementation: misconceptions and their consequences’, BIS Working Paper No. 269.  Farag, M, Harland, D and Nixon, D (2013), ‘Bank capital and liquidity’, Bank of England Quarterly Bulletin, Vol. 53, No. 3, pages 201–15.  Friedman, M (1963), Inflation:  causes and consequences, Asia Publishing House.  Haldane, A (2009), ‘Why banks failed the stress test’, available at www.bankofengland.co.uk/archive/documents/historicpubs/ speeches/2009/speech374.pdf.  Hancock, M (2005), ‘Divisia money’, Bank of England Quarterly Bulletin, Spring, pages 39–46.  Bridges, J, Rossiter, N and Thomas, R (2011), ‘Understanding the recent weakness in broad money growth’, Bank of England Quarterly Bulletin, Vol. 51, No. 1, pages 22–35.  Hills, S, Thomas, R and Dimsdale, N (2010), ‘The UK recession in context — what do three centuries of data tell us?’, Bank of England Quarterly Bulletin, Vol. 50, No. 4, pages 277–91.  Bridges, J and Thomas, R (2012), ‘The impact of QE on the UK economy — some supportive monetarist arithmetic’, Bank of England Working Paper No. 442.  Brigden, A and Mizen, P (2004), ‘Money, credit and investment in the UK industrial and commercial companies sector’, The Manchester School, Vol. 72, No. 1, pages 72–79.  Burgess, S and Janssen, N (2007), ‘Proposals to modify the measurement of broad money in the United Kingdom:  a user consultation’, Bank of England Quarterly Bulletin, Vol. 47, No. 3, pages 402–14.  Butt, N, Domit, S, Kirkham, L, McLeay, M and Thomas, R (2012), ‘What can the money data tell us about the impact of QE?’, Bank of England Quarterly Bulletin, Vol. 52, No. 4, pages 321–31.  Button, R, Pezzini, S and Rossiter, N (2010), ‘Understanding the price of new lending to households’, Bank of England Quarterly Bulletin, Vol. 50, No. 3, pages 172–82.  Capie, F and Webber, A (1985), A monetary history of the United Kingdom, 1870–1982, Vol. 1, Routledge.  Carpenter, S and Demiralp, S (2012), ‘Money, reserves, and the transmission of monetary policy:  does the money multiplier exist?’, Journal of Macroeconomics, Vol. 34, No. 1, pages 59–75.  Howells, P (1995), ‘The demand for endogenous money’, Journal of Post Keynesian Economics, Vol. 18, No. 1, pages 89–106.  Jackson, C and Sim, M (2013), ‘Recent developments in the sterling overnight money market’, Bank of England Quarterly Bulletin, Vol. 53, No. 3, pages 223–32.  Joyce, M, Tong, M and Woods, R (2011), ‘The United Kingdom’s quantitative easing policy:  design, operation and impact’, Bank of England Quarterly Bulletin, Vol. 51, No. 3, pages 200–12.  Kaldor, N and Trevithick, J (1981), ‘A Keynesian perspective on money’, Lloyds Bank Review, January, pages 1–19.  Kamath, K, Reinold, K, Nielsen, M and Radia, A (2011), ‘The financial position of British households:  evidence from the 2011 NMG Consulting survey’, Bank of England Quarterly Bulletin, Vol. 51, No. 4, pages 305–18.  Kydland, F and Prescott, E (1990), ‘Business cycles:  real facts and a monetary myth’, Federal Reserve Bank of Minneapolis Quarterly Review, Vol. 14, No. 2, pages 3–18.  Laidler, D (1984), ‘The buffer stock notion in monetary economics’, The Economic Journal, Vol. 94, Supplement:  Conference Papers, pages 17–34.  Topical articles  Money creation in the modern economy  27  Laidler, D and Robson, W (1995), ‘Endogenous buffer-stock money’, Credit, interest rate spreads and the monetary policy transmission mechanism, Session 3, conference on The Transmission of Monetary Policy held at the Bank of Canada in November 1994.  Miles, D (2012), ‘Asset prices, saving and the wider effects of monetary policy’, available at www.bankofengland.co.uk/ publications/Documents/speeches/2012/speech549.pdf.  Mitchell, B R (1988), British historical statistics, Cambridge University Press.  Moore, B (1988), Horizontalists and verticalists:  the macroeconomics of credit money, Cambridge University Press.  Palley, T (1996), Post Keynesian economics:  debt, distribution and the macro economy, Macmillan.  Sefton, J and Weale, M (1995), Reconciliation of National Income and Expenditure:  balanced estimates of national income for the United Kingdom, 1920–1990, Cambridge University Press.  Singh, M (2013), ‘Collateral and monetary policy’, IMF Working Paper No. 13/186.  Solomou, S N and Weale, M (1991), ‘Balanced estimates of UK GDP 1870–1913’, Explorations in Economic History, Vol. 28, No. 1, pages 54–63.  Thomas, R (1997a), ‘The demand for M4:  a sectoral analysis, Part 1 — the personal sector’, Bank of England Working Paper No. 61.  Thomas, R (1997b), ‘The demand for M4:  a sectoral analysis, Part 2 — the corporate sector’, Bank of England Working Paper No. 62.  Tobin, J (1963), ‘Commercial banks as creators of ‘money’’, Cowles Foundation Discussion Papers No. 159.  Tucker, P (2009), ‘The debate on financial system resilience: macroprudential instruments’, available at www.bankofengland.co.uk/archive/Documents/historicpubs/ speeches/2009/speech407.pdf.  Tucker, P, Hall, S and Pattani, A (2013), ‘Macroprudential policy at the Bank of England’, Bank of England Quarterly Bulletin, Vol. 53, No. 3, pages 192–200.  "
https://news.ycombinator.com/rss,"‘Excuuuuse me, Princess ’: An oral history of The Legend of Zelda cartoon",https://www.polygon.com/zelda/23540526/legend-of-zelda-cartoon-oral-history-zeldathon,Comments,"

Share this story




Share this on Facebook





Share this on Twitter








Share
All sharing options






Share
All sharing options for:
‘Excuuuuse me, Princess!’: An oral history of The Legend of Zelda cartoon












Reddit







Pocket









Flipboard





Email









This story is part of a group of stories called 





    In 2023, Polygon is embarking on a Zeldathon. Join us on our journey through The Legend of Zelda series, from the original 1986 game to the release of The Legend of Zelda: Tears of the Kingdom, and beyond.
  


The world knows The Legend of Zelda’s Link as the brave hero of Hyrule — a young warrior of few words. Link is a master with his bow and an excellent swordsman. But back in 1989, when The Legend of Zelda cartoon first aired, all Link wanted was a smooch. A kiss from Zelda, to be exact — but he’s not exactly picky, and unlike the laconic hero of the games, he would not shut up about it. The hero of Hyrule is still tasked with defending the Triforce of Wisdom from Ganon’s grasp on the TV show, but that’s secondary to his insistence on a little kiss. The show’s bizarre portrayal of Link — especially his constant begging of “Excuse me, Princess!” — has made The Legend of Zelda cartoon a hilarious head-scratcher to this day. 




In 2023, Polygon is embarking on a Zeldathon. Join us on our journey through The Legend of Zelda series, from the original 1986 game to the release of The Legend of Zelda: Tears of the Kingdom, and beyond.



Back in 1989, The Legend of Zelda aired in 15-minute episodes every Friday during The Super Mario Bros. Super Show!, a mix of live-action and animated segments based on Nintendo games. Once a week, The Legend of Zelda replaced the Super Mario Bros. show, which featured animated segments of Mario and Luigi but, more memorably, the wacky, iconic live-action performances of WWF wrestler Lou Albano as Mario and The Jeffersons’ Danny Wells as Luigi, who welcomed fans of the show with the catchphrase, “Hey there, paisanos.”
Clearly, Super Mario Bros. was the main event for the Nintendo-themed TV block. It ran for 52 episodes compared to The Legend of Zelda’s 13. But for the writers of the Zelda cartoon, that was a boon: They had very little oversight and direction beyond character designs, a franchise “bible” provided by Nintendo, and the original game, also called The Legend of Zelda, and its sequel, Zelda 2: The Adventure of Link. As they were not video game players themselves, the writers did their research and decided to go in a different direction — one that’s more focused on story than gameplay. There were elements of the games, like sound effects and visuals, but the show mostly has Zelda and Link posted up in Hyrule castle defending the Triforce of Wisdom from Ganon while trying to acquire the Triforce of Power from the evil wizard himself. (The Triforces talk, by the way.)
Between the mischief that Zelda, Link, and fairy friend Spryte get into, The Legend of Zelda relied heavily on the relationship between Zelda and Link. Zelda, donning pink pants and purple thigh-high boots, more often plays the hero to Link’s bumbling teenage angst.
What we get from the short-lived ordeal is a charming and absurd rendition of a beloved (and often quiet and unvoiced) franchise.
 









Image: DiC Entertainment/Nintendo



From pixels to the small screen
Most of the Super Mario Bros. Super Show’s budget was tied up in the main part of the show — the Super Mario Bros. show that led the time slot. When Super Mario. Bros Super Show was canceled, The Legend of Zelda was shut down alongside it. But for the show’s short run, writers said they had little interference from Nintendo, which just wanted more eyes on its game properties — especially a new one like The Legend of Zelda. It was the first time — and still one of the rare times — that Link and Zelda got their own voice actor performances, and probably not the ones fans expected. 
Rather than simply recreating the video game, The Legend of Zelda’s writers positioned the show more as a mix of action, comedy, and drama, taking specific inspiration from Cybill Shepherd’s and Bruce Willis’ ’80s show Moonlighting. Writers wanted Zelda and Link’s relationship to mirror Shepherd’s and Willis’ rapport as Maddie and David on the detective show — the same angry sexual tension, but goofier and lighter for the kid-friendly cartoon TV show. 
 











Bob Forward Story editor and writer, The Legend of Zelda
The Legend of Zelda was going to be a small addendum to the Super Mario Bros. Super Show, which was the actual star of the time slot. DiC needed somebody who could handle it on their own without a lot of supervision. After we had the initial discussion, they supplied me with a VHS tape of [a playthrough of] the game itself, since I wasn’t actually a person who played video games — not that I had any objection. I just hadn’t really done it. They had a playthrough of the game that my sons were fascinated by. That was my research for it.
I don’t know if anyone cares about this, but the playthrough VHS tape that they supplied me with I guess had been played by one of the new Charlie’s Angels. I think it was Tonya Roberts. I guess she was a gamer when she was younger.

 









Image: DiC Entertainment/Nintendo



 











Reed Shelly Story editor and writer, Super Mario Bros. Super Show
The project originated as a concept by Andy Heyward as Super Mario Bros. Power Hour, a one hour-long animation block that would have featured series based on a number of intellectual properties. Concept art was produced for adaptations of Super Mario Bros., The Legend of Zelda, Metroid, Castlevania, Double Dragon, and California Games. With the exception of Mario and Zelda, none of these additional adaptations were ultimately produced.

 











John Grusd Director, The Legend of Zelda
Nintendo wanted us to base the show on the new game [Zelda 2: The Adventure of Link], because, you know, it’s great marketing. What they did was give me the Japanese version of the game, because it wasn’t out here yet. I didn’t know anything about the game when I started. I’d never played them. I wasn’t a gamer or anything. That’s how I learned how the characters move, the sound effects, the music. I got to be able to do the games all the way through pretty quickly, as a matter of fact, because I knew all the shortcuts. I could get through both of them in less than two minutes, probably. It’s pretty fast.

 











Phil Harnage Writer, The Legend of Zelda
It was a fun little show. And I say little, because they tacked it on to Super Mario. It really should have been a stand-alone show. It was very limiting for what the writers could do. I worked on the bible and wrote a couple of episodes. When you write the bible, you hand it off to somebody else, but occasionally you get to write a script. That’s the fun part. It was a fun show to write for because of the tension between Link and the princess. We modeled it after Moonlighting. We tried to capture that, and I think we did. Maybe over the top a little bit, but that’s what we were shooting for. We could have come up with a lot more shows. That was the sad part, that we only got to do one season. 


 









Image: Eve Forward



 











Eve Forward Writer, The Legend of Zelda
My brother somehow ended up suggesting I try writing an episode, and I was able to turn out a couple of scripts that, with his editing, ended up getting used. I was about 16-17 at the time. The only direction I had was the show bible, which outlined the basic characters and sorts of stories they were looking for. I didn’t have a Nintendo, so I rented one, and the game, and tried to play it, but I didn’t get very far. But the basic relationships were all established in the show bible; Ganon bad guy, Zelda tough girl, Link charming scamp, Triforce MacGuffin, etc.
I did play Dungeons & Dragons though, at the time, and some of that feel made it into the show. [The seventh episode] “Doppelganger” was based on a cursed mirror in D&D. Well, the monsters in Zelda were all based on things from the Nintendo game; same with the weapons, like Link’s boomerang. But in D&D of course you’re always fighting monsters and imagining how cool your character looks doing it, so a lot of the various swashbuckling stuff I liked to put in was based on things that had happened in our D&D games. I always thought of Link as more of a rogue than a fighter.

 











Bob Forward
We had a schedule we had to put the scripts through, and I think it was two a week. That wasn’t hard — I worked on shows we had to do five a week, so two a week was just fine. Eve and I were just writing them on our own. We even had my mom pitch a story. She wrote something that we ended up having to do a lot of work on, but it wasn’t a bad initial concept. [Bob and Eve’s mom, Marsha Forward, had her script adapted as The Legend of Zelda’s 11th episode, “Fairies in the Spring.”]
I wrote a bible for my own purposes, something that just outlined who all the characters were and what they wanted. Robby London [DiC executive] wanted to have some signature lines, and Moonlighting had just come out, or was very popular. Robby London came up with the idea of the line, “Excuuuuse me, Princess,” which is inspired by the Moonlighting relationship and a snarky line from a Steve Martin routine. I’ll be honest, what I liked about Robby is that he would make quick decisions. As much as I was giving him a hard time about it, I put [that line] into the show way more than it was really necessary. But it turned out to be OK, even though people made fun of it. People remembered it, so I guess he was right. I have to admit, it caught on.

 









Image: DiC Entertainment/Nintendo




No one had ever heard Link or Zelda speak
People were certainly familiar with Link and Zelda by the time The Legend of Zelda cartoon was released — The Legend of Zelda and Zelda 2: The Adventure of Link had been out for some time and already were popular. But characters were composed of just a few dozen pixels, and they weren’t voiced. It gave the TV writers lots of room to mess around; the show existed outside of the games, with Link just hanging around Zelda and her father’s castle, defending the Triforce from Ganon every once in a while. 
The show had to be largely carried by Link and Zelda’s personalities, plus the few other characters who appeared: Spryte, a fairy, and the two talking Triforce pieces (Wisdom and Power). So, the writers made those few characters big. Ganon is merely an annoyance to Link, whose more pressing problem is convincing Zelda to give him a kiss.
 











Bob Forward
We very much made it up as we went along. The other nice thing was that everybody was so concentrated on the Mario brothers that they completely left us alone, which is always my favorite way of working. You know, as long as we hit the page count and got the scripts in on time, nobody was looking. 
Link always wanted a kiss. That was one of Robby’s inventions. I thought it worked out. I was down for it. I kept expecting people to tell us we couldn’t do it. But apparently it worked.

 











Jonathan Potts Voice actor, Link
I pictured Link as being a teenager who was like the ultimate teenage boy, who was like a puppy. If you can imagine what a puppy would be [like] — running around, peeing on the carpet, and overreacting — everything was dramatic. I remember wanting to do that. I wasn’t a teenager then; I was well into my 20s when I did the part. I had to be that youthful, goofy teenage boy who acts before he thinks.

 











Cynthia Preston Voice actor, Zelda
You start reading something and you just have instincts — all of your experience, and all of the movies you’ve done, and all of the classes you’ve taken, and that feeds into how to start molding a character. What does this character want? What do they want from this character? I don’t think I was playing Zelda as a teenager. She was an independent woman — a young woman, but she was independent. She didn’t need a hero to save her, and that was so cool.
The show certainly wasn’t ahead of its time, but nonetheless it was a cool aspect that it wasn’t playing a damsel in distress.

 











Phil Harnage
We didn’t want a Disney princess. We’re not going to be selling princess dresses to six-year-olds. So yeah, she was an action hero in her own right, and that was kind of unique. But the writers didn’t come up with [Zelda wearing pants] — that was something the artists came up with, and Nintendo loved it.
It was ahead of its time in some ways, but wasn’t always. Zelda was a good role model for girls. She was confident and took charge. She did want what she wanted, but was also very responsible. And Link was irresponsible. He was out there conniving: “How am I gonna get her to kiss me?” There’s fun in that. That’s where the Moonlighting model really worked.

 











Jonathan Potts
The scripts weren’t complex. There weren’t a lot of deep things going on. It was all right there, sort of obvious. So [direction] usually came down to technical things — more energy. 

 











Cynthia Preston
There was this time a director wanted me to laugh more as Zelda. I was trying, but laughing is harder than crying to do naturally. Shockingly, he mooned me and I fell over laughing. I really have the feeling I didn’t get the right laugh, but it was damn funny.

 









Image: DiC Entertainment/Nintendo



A talking Triforce?
Writers said Nintendo didn’t want them coming up with new characters and backstories, so they worked with what they had. That’s where the Triforce pieces came in — the show couldn’t only be Zelda, Link, Ganon, and Spryte. There was the Triforce. Why not make it talk? Successful or not, the Wisdom piece of the Triforce did have a role in the show: Moving the story forward and explaining the situation.
 











Bob Forward
Link and Zelda wanted the Triforce of Power, and Ganon wanted the Triforce of Wisdom, so [in] half the shows Link and Zelda would be the ones to instigate the action as opposed to just hanging around and waiting for Ganon to start something and trying to reestablish the status quo.

 











Phil Harnage
The whole Triforce thing, it came out of the game and everything, but I don’t know — it was hard to figure out. What does that mean? The Triforce? What do you have to do with it to make it work? I wasn’t really happy with that. I thought it would be much more fun to have them fighting over who’s going to control the land. But [the Triforce] was from the game, and you had to do it for the gamers. 
The more things talk, the more explanatory it can be. You’re like, Why did this happen? And the Triforce can tell you, you know? It’s magic. In a magical world, you have to set the rules, of course. But you set the rules yourself.

 









Image: DiC Entertainment/Nintendo



A sword fighter in a show without fighting
For a TV show about a game with a hero who hits things with swords, The Legend of Zelda has surprisingly little sword fighting. The Legend of Zelda was a kids’ TV show, and that meant it had to  follow the network’s standards — so characters couldn’t die. Link and Zelda still have weapons, of course, but they don’t seem deadly. Link’s sword shoots out magic bullets that stun enemies, and Zelda often uses a magic bow that uses magic instead of arrows.
 











John Grusd
Link has a sword, but can he actually use it to chop somebody’s head off? He can’t do what he does in the game. Nintendo wants us to do what they do in the game, but the standards and practices at the network say no. We can’t kill someone on children’s TV.

 











Phil Harnage
Magic brings a whole different ambiance to a cartoon, because it’s something you can do that’s not repeatable by kids. You can shoot a lightning bolt and turn someone into toast. And the toast gets up and walks away. You just have to be careful — you can’t do everything you want to do. You can’t do anything that could be copied by a child. You don’t want kids sword fighting.

 











Bob Forward
Link’s sword could fire like a ranged weapon. Actually hitting people with swords was questionable. It wasn’t something they wanted to do back then. It was easier to just shoot zaps from the sword. We also had to establish that nobody was dying, so there was the jar of evil or something, where everyone hit by zaps were sent to and got put into storage for a while. We had to downplay a lot of things.

 









Image: DiC Entertainment/Nintendo



One and done
While Super Mario Bros. Super Show had tons of episodes, The Legend of Zelda has only one season. That’s the way of TV cartoons — things get canceled and people move on. The Legend of Zelda itself has gone on to be one of Nintendo’s most successful properties, but the TV show is still a small part of that legacy.
 











Reed Shelly
The show feels like a time capsule to me. It’s such a different world now and so different for kids. The shows were made for a different era.
It was an incredible creative playground. We had to deliver 52 episodes at a rate of four a week [for the Super Mario. Bros Super Show]. We had live action, animation, and an action sequence set to a well known song. It was an amazing production circus to be a part of.
With Andy Heyward and Haim Saban executive producing and running the shows, we were allowed to have a ton of fun. All we had to do was make millions of kids laugh.

 











Eve Forward
I’ve no idea what the reception to the show was. This was in the days before internet; you couldn’t just log in and see your work torn apart in real time. My own feeling is that the Super Mario Bros. show wasn’t very good, especially the live-action bits, and that Zelda was the best part of it, but y’know, it was a cartoon, for kids. We weren’t trying to make Citizen Kane or something. But of course it was a huge thrill for me to see my work on television!

 











Phil Harnage
Part of the reason [the show was canceled] is that it wasn’t its own show — it was part of the Mario Bros. show. It was tied to it, and they didn’t want to renew The Mario Bros., and Zelda got shuffled off. History, at that point. I wish I had done more. We could have come up with a lot more shows. That was the sad part, that we only got to do one season.
I think the show holds up pretty well after all these years. They’re all on YouTube. [Ed. note: And Amazon Prime Video!] I don’t know if you know this, but we don’t get residuals.
Everybody wishes that Link and Zelda had gone on to bigger and better things [with the TV show], but they didn’t. You have these regrets about every show you do. Sometimes you wish you could have done more, that you could do more, but there were certain things you had to do to please the network.
We got a lot of good feedback from kids, and even older kids who knew the video game. They would watch the show out of curiosity and get sucked in. We had a few letters saying, “Oh, please don’t cancel it!” But getting a few letters isn’t enough to convince the network. They’re the boss, because they funded the things. DiC, the studio I worked for at the time — they were known for finding the current properties they could exploit. They were purely in the business to make money, like all the studios.

 











Jonathan Potts
I’m always surprised at how much notoriety it has. I don’t think it was a hit at the time, because then we would have done more. We did it years ago, and it was one season with 13 episodes. It was a one-and-done sort of thing. It had its time, and it just keeps growing. I get letters from all over the world. 
I was teaching voice classes at Second City, and the class would be people in their 30s, and the engineer would look at me like, Go ahead, tell them. And I’d say, “You know, I was the voice of Link in The Legend of Zelda,” and inevitably, three or four people would be like… I became a celebrity. I can’t believe that. It was just a gig years ago.
They would be so starstruck, which is a joke, because I’m not a star. But they’d get … [imitates expression of amazement]

 











Cynthia Preston
It came up [at a party] that I was the voice of Zelda in the cartoon, and [people] were so stunned. They rolled up their shirt sleeves, and they both have the Triforce tattooed on their arms. I’ve been at pitch sessions and somebody will find out that I’m the voice of Zelda, and the reaction is astounding. People love it so much. 

 











Reed Shelly
I remember on my first trip to Redmond and the Nintendo headquarters, they had a couple of hundred “game counselors” in a call center at computers giving tips to gamers calling in. It cost, as I remember, something like 99 cents a minute for players to get game tips. When a group got to go on their lunch break, they raced each other to play the newest arcade console game in the cafeteria. I remember thinking, “This computer gaming thing is gonna be big...” 




"
https://news.ycombinator.com/rss,Twitter API Page,https://developer.twitter.com/apitools,Comments,"




Twitter / Error















This page is down
I scream. You scream. We all scream... for us to fix this page. We’ll stop making jokes and get things up and running soon.
Retry




Home
Status
Terms of Service
Privacy Policy
Cookie Policy
Imprint
Ads info
© Twitter     






"
https://news.ycombinator.com/rss,1991: A server-side web framework written in Forth,https://www.1-9-9-1.com/,Comments,"


World Wild Web

                        The year is 1991. The World Wide Web has just seen public release. 1991 looks to ease your interactions with the new web using cutting edge programming techniques in Forth (well, Gforth).
                    


Logging In

                        Getting started in 1991 is easy.
                    

                        All you need to do is include 1991.fs into your Forth source file. Next, you can define your public routes using the /1991 word. Once your routes are all layed out, start the server using 1991:.
                    

\ app.fs
\ Load 1991.
include 1991.fs

\ Define our route handlers.
: handle-/ ( -- addr u )
    \ Any string returned by the handler
    \ will be output to the browser.
    s"" Hello, 1991."" ;

\ Set up our routes.
/1991 / handle-/

\ Start the server on port 8080.
8080 1991:

You can run the server using gforth app.fs.
Logging In II: Logging In, Deeper
Route Wildcards (Fuzzy Routing / URL Mapping)

                        If you want to specify that some part of a route is a wildcard (accepts any value), then you can wrap some named value in <chevrons>. 1991 will accept any URL that matches your wildcard pattern, setting the internal value of whatever you place between the chevrons to whatever is actually requested.
                    

                        In the example below, <uid> specifies that we're willing to accept any (non-empty) value in its place which we'd like to access using the name uid.
                    

\ wildcards.fs
\ Load 1991.
include 1991.fs

\ Define our route handler.
: handle-wildcard-route ( -- addr u )
    s"" contents of the route request: "" get-query-string s+ ;

\ Set up our route.
/1991 /users/<uid> handle-wildcard-route

\ We can set up multiple wildcards too (must be slash-separated).
/1991 /users/<uid>/posts/<pid> handle-wildcard-route

\ Start server on port 8080.
8080 1991:


                         All wildcards are treated similar to query string arguments. As such, wildcards can be retrieved using get-query-string.
                    

                        In the example above, visiting http://localhost:8080/users/urlysses will result in the following query string: uid=urlysses.
                    File Serving

                        Use a public/ directory to act as a basic fileserver.
                        Whenever a requested URL doesn't resolve through the registered routes, 1991 will attempt to find the requested route within your specified public directory.
                    

\ public.fs
\ Load 1991.
include 1991.fs

\ Specify the location of our public directory.
\ Anything in the public/ directory within the
\ same dir as this source file will resolve.
\ You can change ""public"" to anything you want
\ as long as it matches your directory name.
sourcedir s"" public"" s+ set-public-path

\ We can set mimetypes using the `filetype:` word.
\ In the case below, we want .mp4 files to be served
\ with the content-type video/mp4.
s"" video/mp4"" filetype: mp4

\ Start the server on port 8080.
8080 1991:


                        In the above example, If we have a file public/my-video.mp4, then it will be available through http://localhost:8080/my-video.mp4.
                    
Views

1991 offers basic templating through views.
                    

                        In order to get started, you should specify the views/ path. Notice the trailing slash, which differs from how we define public.
                    

                        Once you've specified your views/ directory, you can write views/ files to it. This can be any kind of file, honestly. The benefit offered by views/ is the ability to use basic templating. You can write any valid Forth code within opening (<$ ) and closing ( $>) tags. Additionally, you can use the import word to import other views into your view.
                    

\ views.fs
\ Load 1991.
include 1991.fs

\ Specify the location of our views directory.
sourcedir s"" views/"" s+ set-view-path

\ Define some words we'll use within
\ our view.
: page-title ( -- addr u )
    s"" Dynamic page title"" ;
: ten-lines ( -- )
    10 0 do
        s"" line "" i s>d <# #s #> s+
        s"" <br>"" s+
        $type
    loop ;

\ Use render-view to output the contents
\ of a file in the views/ directory.
: handle-/
    s"" v-index.html"" render-view ;

/1991 / handle-/

\ Start the server on port 8080.
8080 1991:


\ views/index.html
<!DOCTYPE html>
<html>
    <head>
        <meta charset=""utf-8"">
        <title><$ page-title $type $></title>
    </head>
    <body>
        <$ ten-lines $>
        <$ s"" imported-view.html"" import $>
    </body>
</html>


\ views/imported-view.html
It's possible to import view files from within other view files. This is from <code>views/imported-view.html</code>



Wait, what?
Why is 1991: post-fix when /1991 is pre-fix?

                        Forth is a (mostly) post-fix notation language. So, for example, you'd write two plus two as 2 2 +. This is the language's natural and immediate notation. Along those lines, 1991: is an immediate word——running it results in immediate action. As such, we use Forth's post-fix notation to set the port and start the server immediately. Alternately, /1991 doesn't exactly have immediate effect per se. All it does is tell 1991 that any request to /path should be handled by path-handler. As such, we opt to write non-immediate code using pre-fix notation.
                    
You're using Gforth, which came out in 1992. Also, it's 2017.
Okay. But Fredric Jameson establishes that in postmodernism we have experienced a weakening sense of historisity such that what is, what was, and what will be all exist as presents in time. 1970, 1991, 1992, and 2017 all happen simultaneously. Hence developers working on new projects while still coding in decades-old text editors. They write the future in the past and are made present in so doing.


"
https://news.ycombinator.com/rss,NASA’s Double Asteroid Redirection Test Is a Smashing Success,https://eos.org/articles/nasas-double-asteroid-redirection-test-is-a-smashing-success,Comments,"

Posted inNews 
			NASA’s Double Asteroid Redirection Test Is a Smashing Success		

			The mission, focused on the Didymos-Dimorphos binary asteroid system, proved that an asteroid’s orbit can be altered by kinetic impactor technology.		




by
Katherine Kornei 
12 January 202312 January 2023 
Share this:Print 



 This illustration of NASA’s Double Asteroid Redirection Test (DART) spacecraft and the Italian Space Agency’s LICIACube depicts them just prior to impact at the Didymos binary system on 26 September 2022. Credit: NASA/Johns Hopkins APL/Steve Gribben





Rocks from space have walloped Earth for eons, and it’s only a matter of time until our planet lands yet again in the crosshairs of a very large asteroid. But unlike other forms of life—here’s looking at you, dinosaurs—humans have a fighting chance of altering our cosmic destiny. At AGU’s Fall Meeting 2022 held in December, researchers presented a slate of new results from NASA’s Double Asteroid Redirection Test (DART) mission, the first demonstration of asteroid deflection.
Peering at an Orbit
DART’s target, the Didymos-Dimorphos asteroid system, was first discovered in the mid-1990s. But astronomers back then spotted only its larger member, Didymos, which is roughly 800 meters (half a mile) in diameter. It wasn’t until 2003 that scientists realized that a much smaller body, dubbed Dimorphos, was also present. Dimorphos is about one fifth the size of Didymos, and its orbit takes it in front of and behind Didymos as seen from Earth. That’s serendipitous, because by monitoring how the brightness of the Didymos-Dimorphos asteroid system varies over time, scientists were able to precisely determine how long it took Dimorphos to complete an orbit: 11 hours and 55 minutes.
“We needed to understand the Didymos-Dimorphos system before we changed it.”
“We needed to understand the Didymos-Dimorphos system before we changed it,” said Cristina Thomas, a planetary scientist at Northern Arizona University in Flagstaff, at AGU’s Fall Meeting 2022.
 



This newsletter rocks.
Get the most fascinating science news stories of the week in your inbox every Friday.

Sign up now



The primary goals of the DART mission were simple, at least in concept: Hit Dimorphos with the roughly 570-kilogram (half-ton) DART spacecraft to alter the orbital period of Dimorphos around Didymos significantly and measure that change and characterize the physics of the impact. If successful, it would be the first demonstration of deflecting an asteroid using so-called kinetic impactor technology. (In 2005, another NASA mission, Deep Impact, tested kinetic impactor technology with a comet.)
On 23 November 2021, a Falcon 9 rocket lifted off from California’s Vandenberg Space Force Base. By then, the SpaceX-designed rocket had notched more than 100 successful launches, but for members of the DART mission, the event was anything but ordinary: Nestled within the rocket’s nose cone was the spacecraft they’d spent well over a decade designing, building, and testing.
The launch went smoothly, and DART soon entered into orbit around the Sun. For roughly 10 months, the spacecraft largely tracked the orbit of Earth, essentially waiting to catch up to the Didymos-Dimorphos asteroid system, which orbits the Sun between Earth and Mars. “We stayed close to Earth the entire time and just caught up with the Didymos system at its closest approach to Earth,” said Elena Adams, DART mission systems engineer at the Johns Hopkins University Applied Physics Laboratory in Laurel, Md.
Approaching the Unknown
It was only around July of 2022 that DART’s onboard camera—the Didymos Reconnaissance and Asteroid Camera for Optical navigation (DRACO)—caught its first glimpse of Didymos. But Dimorphos wouldn’t come into view until much, much later: Just an hour before impact, at a distance of roughly 25,000 kilometers, the tiny moonlet was still a mere two pixels across in DRACO images.
“We didn’t see Dimorphos until late in the game,” said Adams. To prepare for the uncertainties of impacting a body they knew virtually nothing about, DART team members ran thousands of Monte Carlo simulations beforehand in which they varied the moonlet’s size, shape, albedo, and a slew of other parameters.
The DART spacecraft successfully impacted Dimorphos on 26 September 2022. The event was recorded by a cadre of Earth-based telescopes and also the Light Italian Cubesat for Imaging of Asteroids (LICIACube), a briefcase-sized spacecraft carrying two cameras that launched with DART and was released from the spacecraft 15 days prior to impact.
A Serendipitous Boost
Researchers had calculated that the impact, which occurred roughly head-on, would shorten Dimorphos’s orbital period by just under 10 minutes. That was assuming the simplest case of no ejecta being produced, said Andy Cheng, DART investigation team lead at the Johns Hopkins University Applied Physics Laboratory, at a press conference.
“The amount of momentum that you put in the target is exactly equal to the momentum that the spacecraft came in with.” But if ejecta flies off the asteroid after impact, physics dictates that the asteroid can get an extra boost, said Cheng. “You end up with a bigger deflection.”
“If you’re trying to save the Earth, that makes a big difference.”
That’s good news when it comes to pushing a potentially harmful space rock out of the way, said Cheng. “If you’re trying to save the Earth, that makes a big difference.”
And ejecta there was, in spades—on the basis of detailed follow-up observations of the Didymos-Dimorphos system, scientists discovered that Dimorphos is now traveling around Didymos once every 11 hours and 22 minutes. That’s a full 33 minutes shorter than its original orbital period, a finding that implied that a substantial amount of ejecta was produced. Imagery obtained from ground- and space-based telescopes has borne that out—a plume of debris tens of thousands of kilometers long currently stretches out from Dimorphos. Researchers have estimated that at least a million kilograms (1,100 U.S. tons) of material were blasted off the asteroid by the impact. That’s enough debris to fill several rail cars, said Andy Rivkin, DART investigation team lead at the Johns Hopkins University Applied Physics Laboratory, at a press conference at the Fall Meeting.
Follow the Debris
Interestingly, the ejecta shed by Dimorphos has remained in distinctly more plumelike configurations than the debris shed by comet 9P/Tempel 1 when NASA’s Deep Impact spacecraft intentionally crashed into it in 2005. “The Dimorphos ejecta has a lot of morphological features,” said Jian-Yang Li, a planetary scientist at the Planetary Science Institute in Fairfax County, Virginia, and a member of the DART team, at the Fall Meeting.
The reason is probably the different compositions and surface features of the two bodies, he said. Tempel 1 is rich in volatiles and fine-grained dust; Dimorphos’s surface, on the other hand, is littered with boulders. Scientists plan to continue to monitor Dimorphos’s debris plume through at least March.
The DART mission has also enabled scientists to investigate a fundamental question about the Didymos-Dimorphos asteroid system: Do the two asteroids have the same composition? It’s a common assumption when it comes to binary asteroids, but it’s never been confirmed. Thomas, leader of the DART Observations Working Group, presented new results on the subject at a press conference at the Fall Meeting. She shared near-infrared spectra of the binary asteroid system that astronomers had collected both before and after impact using a NASA telescope in Hawaii.
Observations obtained prior to impact (when the overwhelming majority of the sunlight reflected off the asteroid system came from Didymos) and after impact (when the debris shed by Dimorphos was responsible for more than two thirds of the reflected light) revealed very similar spectra, with characteristic dips at wavelengths of 1 and 2 micrometers in both cases. That’s strong evidence that the two asteroids have similar compositions, said Thomas.
Scientists aren’t yet finished with Didymos and Dimorphos: In 2024, researchers involved in the European Space Agency’s Hera mission plan to launch a spacecraft to the system to further characterize the asteroids—including accurately measuring the mass of Dimorphos—and to study the crater created by the DART impact.
—Katherine Kornei (@KatherineKornei), Science Writer
Citation: Kornei, K. (2023), NASA’s Double Asteroid Redirection Test is a smashing success, Eos, 104, https://doi.org/10.1029/2023EO230010. Published on 12 January 2023.
Text © 2023. The authors. CC BY-NC-ND 3.0Except where otherwise noted, images are subject to copyright. Any reuse without express permission from the copyright owner is prohibited.
RelatedAre We Prepared for an Asteroid Headed Straight to Earth?NASA's New Asteroid Sampler Will Illuminate Solar System's HistoryExploring Planetary Breadcrumbs One Asteroid at a Time 

Tagged: #AGU22, asteroids, NASA, orbits & rotations, solar system, Space & Planets 









"
https://news.ycombinator.com/rss,Janet Malcolm on the Stand,https://www.nplusonemag.com/online-only/book-review/malcolm-on-the-stand/,Comments,"         Malcolm on the Stand | Online Only | n+1 | Max Abelson                                      Sign InYour AccountHomeMagazineOnline OnlyBookstoreEventsDonateSubscribeFull
 NavigationSign In to n+1    Forgot Password Subscribe NowCloseSearch   Online Only February 3, 2023A discussion of The Feeling SonnetsEventsEugene Ostashevsky and Genya Turovskaya in conversation  January 6, 2023Film ReviewAn Entire Society Exists Within MeMark Krotov  December 24, 2022Online OnlyCouscous and ChickenNicholas Hamburger   December 23, 2022Online OnlyA Strike DiaryKyle McCarthy   Book ReviewMax AbelsonMalcolm on the StandShe is cutting, wary, funny, and wise. Her style is what I wish I had instead of the chipper inner voice I’m stuck with. Nothing in Malcolm’s writing is dull or amiss unless she’s quoting somebody else. Her lines put me in mind of the painter Agnes Martin—everything so even and tight.To read Malcolm is to be moved by the clarity of her journalism—and warned, again and again, that the form is no good    January 10, 2023  Tags Reading, Writing, and Publishing Reviews  Share and Save Twitter Facebook Google
 Plus    Instapaper Email NewsletterGet n+1 in your inbox. Email Address    Janet Malcolm: The Last Interview and Other Conversations. Introduction by Katie Roiphe. Melville House, 2022.In the few times Janet Malcolm let other reporters interview her, she did what she could to keep herself safe.“Doing this interview by email gives me a chance to think of answers to your questions,” Malcolm wrote to the Believer in 2004. “If we did it in person, I might just look at you in blank helplessness.” She invited a Paris Review interviewer over to her Gramercy Park apartment seven years later, but didn’t answer the questions until typing on her desktop Mac.Malcolm, who died in 2021 at 86, was as attuned as anyone to the dangers—malice, betrayal, misunderstanding—of a tape recorder clicking on. The monster in her masterpiece, The Journalist and the Murderer, isn’t the man convicted of killing his family but the bestselling author he took to court for publishing a tell-all; what haunts The Silent Woman, her book about Sylvia Plath, isn’t the poet’s suicide or Ted Hughes but the couple’s biographers. To read Malcolm’s decades of work for the New Yorker is to be moved by the clarity of her journalism—and warned, again and again, that the form is no good.There are few reporters you’d rather see on the other side—the wrong end—of a Q&A. That’s where we find her in Janet Malcolm: The Last Interview and Other Conversations, a compilation of her exchanges with critic Nan Goldberg for Salon in 2001, novelist Daphne Beal in the Believer, Canadian radio host Eleanor Wachtel in 2008, writer Katie Roiphe in the Paris Review, and the New York Times Book Review in 2019. At their best, the transcripts channel and help explain Malcolm’s mesmerizing journalism, only the tables are turned. Reading the interviews has the perverse quality of seeing a judge on trial or your analyst in therapy.Often, though, it’s a polite book—one in Melville House’s series of “last” interviews with interesting people—that chooses the wrong times to go soft. Malcolm, who knew the subjects of journalism are always “astonished when they see the flash of the knife,” doesn’t even get nicked. The friction that’s missing here is what electrifies not just Malcolm’s writing but the record of what happened when she was actually put on the hot seat. When, decades ago, the star of one of her books sued her for libel, taking her to the Supreme Court and then to two trials, Malcolm—in front of a jury—gave the moral accounting these interviews avoid.What makes Malcolm’s reporting unusual, besides the trouble it caused her, is how much fun it is to be in her company on the page. She is cutting, wary, funny, and wise. Her style is what I wish I had instead of the chipper inner voice I’m stuck with. Nothing in Malcolm’s writing is dull or amiss unless she’s quoting somebody else. Her lines put me in mind of the painter Agnes Martin—everything so even and tight.I like the way Beal puts it in the introduction to the Believer interview: “What grabs and re-grabs the reader in her writing is its deft commingling of sleuthing and contemplation,” she writes. “Reading Malcolm, one has the sensation of being in the presence of a mind constantly in action on several levels, mediating between external reality (one most often consisting of facts that are at odds with one another) and her own consciousness.”For reporters, Malcolm offers even more than just a guidebook to craft. She’s a tuning fork whose pitch tells the rest of us when we’ve fallen flat or drifted sharp. That’s because of the clarity of her writing—“vanity, hypocrisy, pomposity, inanity, mediocrity” is in The Journalist and the Murderer with “tenderness, sensitivity, judgment, warmth” as well as “ambiguity, obscurity, doubt, disappointment, compromise, and accommodation”—and how in touch it is with what’s going on, how it comes together, and why it sometimes falls apart. When I read Malcolm I’m like Roiphe greeting her for their Paris Review interview: “Around her it is hard not to feel large, flashy, blowsy, theatrical, reckless.”Malcolm was born Jana Klara Wienerová in Prague in July 1934. Five years later, after Hitler had marched through the city, her family escaped on one of the last civilian ships to America from Europe before the war. Her father became a successful New York psychiatrist—in these interviews she swears she “paid little attention to my father’s work” and that psychoanalysis “has had curiously little influence” on her style, but one can’t quite believe her. At the University of Michigan, she wrote for the newspaper, edited its humor magazine, and met her first husband. They both went on to write for the New Yorker, and after he died she married her editor, whose stepfather’s yeast fortune helped fund the magazine. It wasn’t until she gave up cigarettes in the late ’70s that Malcolm did her first long piece of journalism: “I figured that by the time I finished the reporting I would be ready to try writing without smoking.”If you want Malcolm at her most rabbinical, there’s Reading Chekhov, her underrated and atmospheric meditation on the Russian genius from 2001. For momentum that crime writers would kill for, she has Iphigenia in Forest Hills, a 2011 thriller about an Orthodox Jewish woman on trial for murder. Her stories about psychotherapy have the sweet swing of sportswriting, and her work about the law, too, is riveting and deep. Malcolm’s ear and eye—and unusual sense of structure—are most dazzling in her writing about art, especially “Forty-one False Starts,” a portrait of the painter David Salle that begins again and again, and “A Girl of the Zeitgeist,” a profile of the editor Ingrid Sischy that waits and waits to find her. The shapes of the pieces convey so much about their subjects that a reader can’t help but feel both are also about the machinery of journalism.There are different forms of Malcolm on the page. The observant and sometimes cold journalist who asks her subjects unsettlingly short questions isn’t quite the same figure as the ingenious narrator whose essayistic contemplation radiates generosity. Then there’s the small and sometimes anxious woman who emerges to play key plot roles, especially in Iphigenia.The Malcolm we encounter in The Last Interview shares the self-awareness, briskness, clarity, and humor she possesses elsewhere: “I walk fast and am impatient. I get bored easily,” Malcolm tells the Paris Review. “I often get stuck. Then I get sleepy and have to lie down. Or I make myself leave the house—walking sometimes produces a solution. The problem is usually one of logic or point of view. I keep regular morning hours. The first hour is the most productive one.” Worrywart writers will find much to love in Malcolm’s description of herself: “The machinery works slowly and erratically and I am always a little nervous about it, though by now I’m pretty used to it,” she emails. “I guess I trust it more.”But where Malcolm the journalist is unsparing and direct, Malcolm the interviewee is ultimately vague and evasive. “What’s true? Is it possible to know what’s true?” Goldberg asks in the Salon interview. Malcolm answers: “I’d love to hear you talk about it rather than me.”“Why do you think the subject of betrayal is something that’s your subject?” Wachtel asks her on the radio. “That’s very interesting,” Malcolm says. “You’re kind of putting me on the spot.”Interviews aren’t the same as sworn testimony, but they rhyme. They use questions to flatter, badger, and trap witnesses who, in turn, evade when they can and admit things they don’t want to. Reporting and the law both rely on evidence and discovery, asking for honesty and promising fairness in exchange. They offer just about the best systems we have for hearing arguments, measuring doubt, rendering judgment, and appealing verdicts—except, maybe, for psychoanalysis. All three approaches use confrontation to turn ambiguity into clarity, but only one can punish an outburst or lie by locking the speaker away.The long and famous case against Malcolm began in 1984, after she published a New Yorker profile about a lawsuit from a star scholar named Jeffrey Moussaieff Masson against the ​​Sigmund Freud Archives. It was also a vibrant portrait of a charming and ambitious heel. Masson accused Malcolm in the Washington Post of misquoting him “any number of times.” (The newspaper reporter, a young David Remnick, wrote that Malcolm was away on vacation in Italy and couldn’t be reached for comment.) When Malcolm expanded the piece into a book, In the Freud Archives, Masson complained again, in a letter to the New York Times. The response she sent offered to play the tapes of their conversations for Times editors “whenever they have 40 or 50 short hours to spare.” Masson sued her for libel soon after.His case began terribly. Many of the quotes he had denied saying turned out to be on her tape. Others really were missing, though, and Malcolm had an unusual story: she had tripped over her recorder’s cord before a morning interview, took notes instead, typed them up, and misplaced the originals. But a judge decided Malcolm had reasonably interpreted whatever Masson had actually said and threw out the suit. Just after Malcolm published The Journalist and the Murderer, the Supreme Court agreed to hear Masson’s appeal. Anthony Kennedy, joined by six other justices, wrote that a misquote has to hit the reader’s mind differently than the right one would to cross the line into libel. It would be up to a jury to decide if Malcolm’s writing and Masson’s words shared “the substance, the gist, the sting.” (When the Times asked for comment, Malcolm was back in Italy.)Inside a federal court in California in May 1993, Malcolm took the stand. Masson’s lawyer, a former quarterback and Air Force navigator named Charles O. Morgan Jr., asked Malcolm about a key monologue at the Berkeley restaurant Chez Panisse. “You reported in the article that the entire statement was made by Mr. Masson at lunch,” Morgan asked.1“Yes,” Malcolm said.“And that is not true.”“That’s right,” Malcolm said. She had compressed conversations over seven months into one monologue, she told the jury, using them like “sketches incorporated into one painting.” Anything else, she testified, would be foolish: “I do not want to write the exact words, I do not want to write a transcript,” she went on. “This thing called speech is sloppy, redundant, repetitious, full of uhs and ahs.” When that line surfaces in the introduction to The Last Interview, Roiphe cites it admiringly, praising Malcolm for improving on the “casualness and mediocrity of expression” by “trimming and shaping.” Morgan, the attorney, argued otherwise.“Do you call that rearranging events?” he asked.“I don’t know what that means.”“Do you call it creating a conversation?”“I wouldn’t put it that way, no.”Before the trial ended, Malcolm testified that the chaos and contradiction of speech had forced her hand: “He’s trying to tell too many things at the same time. You had to work hard to get the story straight because he was all over the place.”Jurors decided for Masson. But they couldn’t agree on damages, so the judge announced they’d start the whole thing over. In one of her most gripping pieces, published in the New York Review of Books months before her death, Malcolm recalls the makeover she gave herself with a speech coach before returning to the stand. Back in the courthouse, she gave “a long speech about the monologue technique that Morgan kept interrupting but was unable to stop,” Malcolm writes. “I went relentlessly on and on. I talked about the difference between the full and compelling account of his rise and fall in the Freud Archives that Masson gives in the article and his wandering incomplete speech in the restaurant. I spoke of the months of interviews out of which, bit by bit, the monologue was formed. I concluded by saying, ‘I have taken this round-about way of answering your question, Mr. Morgan, because I wanted the jury to know how I work, and what we’re talking about here in talking about this monologue.’”After one kind of Malcolm monologue about another, the jury dismissed concerns about three of the five quotes in question. But jurors decided the two others—about the sterility of psychoanalysis and Masson’s bosses—were false, and that the latter qualified as defamation. They also decided, though, that Malcolm hadn’t been reckless enough to cross the legal line of libel. She won and wept.What happened a year later gets more attention in the book of her interviews than her testimony. “I was in my country house, and there was something red on the floor, and I picked it up, this red notebook. My granddaughter had seen something red in the bookcase and pulled it out, and there were the notes”—the ones she took after tripping over the recorder’s cord. “I felt like I was going to faint.” If only she had found them earlier, she tells Wachtel, “the whole thing could’ve been avoided.” She emails the story to the Believer, too: “The jury had decided to believe me anyway. But if the notebook hadn’t got misplaced, there would have been no lawsuit.”The fact is that Malcolm’s accounts, as she liked to write about other people, “don’t add up.” The missing red notebook had three of the quotes in question, but not the two that had bothered the jury. “Do we ourselves add up?” Goldberg asks Malcolm in the Salon interview.“No,” Malcolm answers. “Of course we don’t.”That someone so thoughtful about the moral perils of journalism could be messy enough to collage scenes together—then blithe enough to testify it was all for the best and deluded enough to say a missing notebook explained it all—is a paradox that we Malcolm fans have to live with. It’s also, of course, a version of what her work was warning about. Whether her shortcomings bring her journalism to life or do something more like undermine it is the kind of thing you would want to ask her, right before running out of the room.Roiphe’s introduction to The Last Interview avoids Malcolm’s mysteries and messes. It’s less about the morals or machinery of her journalism than what it was like to be her bud. “When a friend texted me that Janet Malcolm had died, I experienced more than the usual amount of disbelief,” it begins. “This is one of the conversations I wish I’d had with Janet herself at Choshi,” Roiphe writes later, “the now-closed sushi place she favored around the corner from her house. I know she would have had thoughts on it.” If you’re in a generous mood, you can read the intro as a kind of pun on Malcolm’s interest in the “I” who narrates literary nonfiction: “I wonder,” “I confessed,” “I suggested,” “I fixate,” I understand,” “I hear,” Roiphe writes, and then “I love” three times in a row.It all comes to a crescendo with a sort of Freudian slip. “I have always used her writing to teach confidence,” she writes, meaning to refer to Malcolm’s authoritativeness but channeling the line that follows The Journalist and the Murderer’s famous opening: “He is a kind of confidence man, preying on people’s vanity, ignorance, or loneliness, gaining their trust and betraying them without remorse.”The first of the interviews, with Salon, opens with the kind of reportorial antagonism I found myself missing as the book went on: If journalists are murderers, Goldberg asks, why was Malcolm speaking to one? Malcolm twists away by complimenting the question and pointing out she used to avoid reporters entirely. “When the book came out and people wanted to ask me questions, I said, ‘Well, read the book.”“I did,” Goldberg answers, standing her ground. “That’s why I’m asking.” Instead of saying why she agreed to the interview, Malcolm explains why she shouldn’t have: “I’m just not very good at it. I often have no answers to the questions; I think of the answers later.”It’s a moment when the tension and confrontation that the book mostly suppresses manages to leak out, but not the only one. In the Believer, when Beal dares to ask if getting sued changed her approach with subjects, Malcolm shows her teeth: “Until this moment you were the first interviewer who did not bring Jeffrey Masson into the discussion. I guess that isn’t possible after all.” You again get the sense that the nastiness of journalism was only fun for Malcolm to consider when it was someone else’s.Beal backs off: “Sorry to be so tiresome,” she writes back. “Just like the rest.” The exchange ends soon after, but, following a line break, an italicized note says Malcolm read the transcript and sent this in an email: “I read the interview in the way one looks at photographs of oneself, and, except for one place, I thought I came out looking okay. But the exception may be the most interesting part of the interview.” It’s Masson. “Until that moment the atmosphere of the interview is friendly and collegial, almost conspiratorial. Now it turns icy.” Malcolm goes on: “What is most interesting about this moment in our interview is the illustration it offers of a subject’s feeling of betrayal when he or she realizes that the journalist is writing his or her own story.”If a goal of Malcolm’s journalism was to measure the distance between the stories that reporters shape and the ones subjects tell about themselves, this book makes the mistake of letting her maintain almost complete control of the tape—or, in the case of Roiphe’s Paris Review interview, the notebook. “I want to talk about that moment in our meeting at my apartment last week, when I left the room to find a book and suggested that while I was away you might want to take notes about the living room for the descriptive opening of this interview,” Malcolm writes in an email that’s quoted in the interview’s introduction. “You obediently took out a notebook, and gave me a rather stricken look, as if I had asked you to do something faintly embarrassing.” Malcolm fills it in for her in the interview: “My living room has an oakwood floor, Persian carpets, floor-to-ceiling bookcases, a large ficus and large fern, a fireplace with a group of photographs and drawings over it, a glass-top coffee table with a bowl of dried pomegranates on it, and sofas and chairs covered in off-white linen.”That’s not to say deference is so terrible. Even though the “By the Book” feature in the Times only asks Malcolm about reading, she looks around her bookshelves and spots “petulant desperation,” “wild terrain,” “the eye of eternity,” and on top of her night table “a box of Kleenex, a two-year-old Garnet Hill catalog and a cough drop on it.”But you find yourself yearning for confrontation and catharsis. Did she, in her later writing, stick with the collage method she defended on the stand, or was it sublimated into the delicate collages she started making—and exhibiting at art galleries—from handwriting, typewritten papers and book pages?Salon asks this: “Do you see any relationship between your collages and your writing?”“I think so,” Malcolm answers. “I like to think about my work as kind of collage-like.” There isn’t a followup, because their interview, right there, is over.Roiphe ends her essay with herself, so I get to end mine with me.When I was hired as a newspaper reporter in 2006, taking over the New York Observer’s weekly column on the city’s most expensive real estate, I was introduced to business journalism at the peak of what turned out to be a bubble. I wrote about buyers and sellers who played a role in inflating and then bursting it: “Money just doesn’t mean anything,” one of the city’s high-end brokers told me not long before Bear Stearns collapsed and the era came to an end. I switched beats in 2009 to report on the landscape of Wall Street’s culture, covering some of the same people, only how they made their money instead of where they spent it.I fell in love with Malcolm’s journalism because it seemed to me at the time to be a map to the crises I would have to navigate. I imagined that the tensions her books describe only multiply if interview subjects are rich and powerful. Higher stakes, I figured, only complicate access and relationships. Reading these interviews makes me think I was wrong. They are reminders that even the subjects who best understand what’s happening can’t fully explain themselves to outsiders, and that no journalist knows how to get them to reveal it all.What is there to do about it? “Perhaps the way to minimize one’s feeling that one has not been as straightforward with the subject as one should have been,” she tells the Believer, “is to be a little more straightforward.” Sometimes I talk to my subjects about the trouble Malcolm picked up on, and the trouble she got into. And sometimes I can feel the acknowledgement make us both relax, at least for a little bit.But Malcolm wouldn’t want anybody to get too comfortable. When the Believer asks if subjects occupy a different place in her mind at the end of writing than they had during interviews, she writes back that she isn’t sure she understands the question. Beal tries again: “How do you make the switch from supplicant or equal interviewer to authority writer?”“Yes, it is a problem,” Malcolm answers, “and no, it can’t be resolved.”Quotes from the trials come from coverage in the New York Times, Village Voice and Washington Post. ↩ If you like this article, please subscribe or leave a tax-deductible tip below to support n+1.    The Burglaries Were Never the Story Related Articles  Issue 2 HappinessKeith Gessen replies: The genital flag?Issue 2LettersThe Editors   Issue 5 Decivilizing ProcessIt has lately become clear that nothing burdens a life like an email account.Issue 5Against EmailThe Editors   Issue 39 Take CareI can tell you only what I found helpful.Issue 39Baby YeahAnthony Veasna So   Issue 6 MainstreamCanons in daily life just demarcate the books you can count on other people feeling comfortable about in conversation.Issue 6The Spirit of RevivalThe Editors More by this Author August 20, 2021“Listen to the last half-hour of ‘Dark Star’ in a darkened room and see if you feel remotely secure.”Online OnlyIn the Dead ArchivesMax Abelson  n+1n+1 is a print and digital magazine of literature, culture, and politics published three times a year. We also post new online-only work several times each week and publish books expanding on the interests of the magazine.MagazineCurrent IssueRenew SubscriptionSubscribeGift SubscriptionsYour AccountBack IssuesBookstoresLibrariesAdvertiseRSSAbout n+1AboutFoundationEventsContactBack to TopCopyright © 2023 n+1 Foundation, Inc.Terms & Conditions | Privacy Policy                                        "
https://news.ycombinator.com/rss,CircleCI says hackers stole encryption keys and customers’ source code,https://techcrunch.com/2023/01/14/circleci-hackers-stole-customer-source-code/,Comments,"










 


CircleCI says hackers stole encryption keys and customers' secrets • TechCrunch















































































 
 



 













CircleCI says hackers stole encryption keys and customers’ secrets




			Zack Whittaker		

@zackwhittaker
 / 
						23 hours		







CircleCi, a software company whose products are popular with developers and software engineers, confirmed that some customers’ data was stolen in a data breach last month.
The company said in a detailed blog post on Friday that it identified the intruder’s initial point of access as an employee’s laptop that was compromised with malware, allowing the theft of session tokens used to keep the employee logged in to certain applications, even though their access was protected with two-factor authentication.
The company took the blame for the compromise, calling it a “systems failure,” adding that its antivirus software failed to detect the token-stealing malware on the employee’s laptop.
Session tokens allow a user to stay logged in without having to keep re-entering their password or re-authorizing using two-factor authentication each time. But a stolen session token allows an intruder to gain the same access as the account holder without needing their password or two-factor code. As such, it can be difficult to differentiate between a session token of the account owner, or a hacker who stole the token.
CircleCi said the theft of the session token allowed the cybercriminals to impersonate the employee and gain access to some of the company’s production systems, which store customer data.
“Because the targeted employee had privileges to generate production access tokens as part of the employee’s regular duties, the unauthorized third party was able to access and exfiltrate data from a subset of databases and stores, including customer environment variables, tokens, and keys,” said Rob Zuber, the company’s chief technology officer. Zuber said the intruders had access from December 16 through January 4.
Zuber said that while customer data was encrypted, the cybercriminals also obtained the encryption keys able to decrypt customer data. “We encourage customers who have yet to take action to do so in order to prevent unauthorized access to third-party systems and stores,” Zuber added.
Several customers have already informed CircleCi of unauthorized access to their systems, Zuber said.
The post-mortem comes days after the company warned customers to rotate “any and all secrets” stored in its platform, fearing that hackers had stolen its customers’ code and other sensitive secrets used for access to other applications and services.
Zuber said that CircleCi employees who retain access to production systems “have added additional step-up authentication steps and controls,” which should prevent a repeat-incident, likely by way of using hardware security keys.
The initial point of access — the token-stealing on an employee’s laptop — bears some resemblance to how the password manager giant LastPass was hacked, which also involved an intruder targeting an employee’s device, though it’s not known if the two incidents are linked. LastPass confirmed in December that its customers’ encrypted password vaults were stolen in an earlier breach. LastPass said the intruders had initially compromised an employee’s device and account access, allowing them to break into LastPass’ internal developer environment.
Updated headline to better reflect the customer data that was taken.















 
 


"
https://news.ycombinator.com/rss,America’s trustbusters plan to curtail the use of non-compete clauses. Good,https://www.economist.com/leaders/2023/01/12/americas-trustbusters-plan-to-curtail-the-use-of-non-compete-clauses-good,Comments,"LeadersAmerica’s trustbusters plan to curtail the use of non-compete clauses. GoodThe clue is in the name Jan 12th 2023ShareThree-quarters of Americans who work, do so for a firm. They have contracts setting out their pay, holiday, benefits and sometimes the appropriate way to dress (although not in journalism). A lot of contracts also say whether employees may work for a competitor if they leave the company. It is hard to know what share of American workers are restricted by these non-compete clauses, but the available evidence suggests it may be as high as one in five. More worrying, these clauses are as likely to apply to workers operating deep-fat fryers in fast-food kitchens as they are to workers operating in the conference rooms of white-shoe law firms. The Federal Trade Commission (FTC) has these clauses in its sights, on the grounds that they are anticompetitive and suppress wages. Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Listen to this storySave time by listening to our audio articles as you multitaskOKFans of non-compete clauses argue that scrapping them by decree will invite the state into the realm of private contracts, a symptom of regulatory excess. They have a point, but the FTC’s case is stronger.It is easy to see why firms like non-compete clauses, which are designed to suppress competition. It suits companies to be able to prevent a star employee from joining a rival, or starting out on their own and wooing clients. But there are also some arguments that non-competes could serve the public interest. Companies sometimes say that the clauses incentivise them to think about talent in a longer-term way. Why bother to spend time and money on training employees if they then join rival firms that go on to reap the benefit of the investment? Some companies also have legitimate worries about trade secrets walking off to a competitor when an employee leaves.However, a non-compete clause is a heavy-handed way to achieve those ends. Intellectual-property law and non-disclosure agreements exist to preserve true trade secrets, and lots of firms find ways to keep valuable employees without shackling them to their jobs using non-compete clauses. The public interest also conflicts with the firm’s interest: innovation and productivity spread when better ideas about how to do things become widely adopted. Hiring people with specific knowledge and experience can speed this process up, which is one reason why the engineers fired by tech giants like Meta and Twitter are sought after by firms in older industries eager to learn.If the theory points in one direction, the evidence from how non-competes are used practically screams. In 2014 Jimmy John’s, a chain of sandwich shops, was found to have inserted a two-year non-compete clause in its employees’ contracts which barred them from seeking employment with any rival business that made money by “selling submarine, hero-type, deli-style, pita and/or wrapped or rolled sandwiches” within a three-mile radius of where they currently worked. After this egregious example came to light, the company ended the practice. But franchises often stop employees from going to work at other outlets of the same franchise, reducing their bargaining power.It strains credulity to argue that these workers are the guardians of trade secrets. Instead, the evidence is that firms use non-compete clauses to drive down wages by lowering the value of workers in the job market. About half of people with non-compete clauses in their contracts work in states where they cannot legally be enforced. They may get away with it because employees do not know their rights, especially those in the low-paid part of the labour market. The evidence that non-compete clauses really make companies more innovative and higher-skilled is scarcely more convincing. Washington state, home to Amazon and Microsoft, takes a middle path by restricting non-compete clauses to the contracts of high-earners. California, the global hq of disruptive innovation, goes a step further and bans non-competes altogether. The FTC should do the same, on the grounds that they are anticompetitive. ■This article appeared in the Leaders section of the print edition under the headline ""The clue is in the name""Leaders January 14th 2023The destructive new logic that threatens globalisationThe West should supply tanks to UkraineHow Brazil should deal with the bolsonarista insurrectionAmerica’s trustbusters plan to curtail the use of non-compete clauses. GoodFixing Britain’s health service means fixing its family doctorsFrom the January 14th 2023 editionDiscover stories from this section and more in the list of contents Explore the editionShareReuse this contentMore from LeadersThe glory of grandparentsWhy the soaring number of grandmas and grandpas is a good thingHow Brazil should deal with the bolsonarista insurrectionPunish those who broke the law, but govern inclusivelyFixing Britain’s health service means fixing its family doctorsDon’t change the partnership model. Do change the targets"
https://news.ycombinator.com/rss,Wobbly clock,https://somethingorotherwhatever.com/wobble-clock/,Comments,"


Wobbly clock!













"
https://news.ycombinator.com/rss,Godot for AA/AAA game development – What's missing?,https://godotengine.org/article/whats-missing-in-godot-for-aaa/,Comments,"














					Godot for AA/AAA game development - What's missing?
				

					By: Juan Linietsky
					 16 January 2023



News



Godot 4.0 is coming out soon. It includes major improvements all across the board in features, performance, and usability. Still, one of the biggest questions the community has is: How does it compare with mainstream commercial offerings?
Godot 4.0 improvements
Rendering
Godot 4.0 has an entirely new rendering architecture, which is divided into modern and compatibility backends.
The modern one does rendering via RenderingDevice (which is implemented in drivers such as Vulkan, Direct3D 12, and more in the future). Additionally, the modern backend can implement rendering methods, such as forward clustered, mobile, and more in the future (such as deferred clustered, cinematic, etc.).
The compatibility backend is based on OpenGL ES 3.0 / OpenGL 3.3 / WebGL 2.0 and is intended to run on very old PC hardware as well as most older (still working) mobile phones.
Rendering is significantly more efficient in Godot 4.0, using data oriented algorithms to process the culling of objects and both secondary command buffers and automatic batching to efficiently submit the draw primitives.
The features offered are also a lot more reminiscent of AAA games, such as far more material options and advanced visual effects (including circle DOF, volumetric fog, AMD FSR, etc.). Additionally, Godot 4.0 supports advanced global illumination techniques such as lightmapping (including SH lightmapping), Voxel GI (which is fully real-time) and SDFGI (which is a single click, open world GI solution). Screen space GI can be used to enhance the realism even more.
Physics
After an unsatisfactory attempt at using Bullet, Godot 4.0 returns to its own physics engine which, despite not being a high end physics engine like PhysX, aims to offer a lot more flexibility and “just works” capabilities to users.
Several features were added to Godot Physics since 3.x, such as soft bodies and cylinder shape support, as well as several optimizations to make use of multiple threads.
The custom physics engine still has a considerable amount of issues remaining but we are working hard to ensure it is in a decent state for shipping when 4.0 reaches stability. It will continue seeing improvements afterwards, during the following 4.x release cycles.
That said, Godot 4.0 introduces the ability to bind custom physics engines at runtime (without recompiling Godot) via GDExtension, so it’s perfectly possible for the community to integrate other engines such as PhysX, Jolt, or Box2D if need to be.
Scripting
Godot 4.0 has a new version of GDScript, which is far more powerful and overcomes most shortcomings found in 3.x. Majorly, the addition of lambdas, first class functions/signals and a much reduced reliance on string identifiers (which are prone to errors). It also has more useful built-in data types such as integer vectors.
Core engine
The core engine has been significantly optimized, especially on the memory and data-oriented areas. Core and Variant have been massively cleaned up and made more extensible. Besides being faster and more modern, the core codebase is now significantly easier to maintain and extend.
GDExtension
It is now possible to extend Godot and add features to it practically in any language and without recompiling the engine, thanks to the new GDExtension system. Aside from Godot C++ (which makes it easy to extend Godot as easy as with modules but allowing pluggable, dynamic add-ons), there are other bindings in the work such as Python, Rust, Go, etc.
A lot more
Several other areas got improvements, like the editor (which has been vastly reworked), UI system, multiplayer, navigation, audio, animation, etc. This is a major release with major improvements all across the board.
So, what’s missing?
Do not be mistaken: A lot is still missing from Godot in order to be used comfortably for large projects and teams. That said, what remains is now much less work than it was for Godot 3.x.
First of all, most of the new features still have significant bugs and performance problems that will not be solved in time for the upcoming 4.0 release (there is just too much new code that needs to be tested throughly).
These problems will be fixed across the 4.x point releases (which we are now intending to do more often, allowing several releases per year). It may be an extra year or even two until everything feels as solid and fast as everyone expects. See this article about our plans for 4.0 and beyond.
But other than that, there are still some fundamental aspects missing in Godot. The following is an incomplete list of the most important ones:
Streaming
The traditional way to make games longer since the beginning of times is to divide them in stages. As soon as one stage is completed, it is unloaded while the new one is loaded.
Many games still use this approach nowadays (after all, if it’s not broken, don’t fix it) but, increasingly, game design has been moving from “individual stages” to “open” or “continuous” worlds where the boundaries between levels disappear. Creating games this way is, as a result, more challenging.
This is handled nowadays by a type of technology called “streaming”. It means that assets are pulled from disk on demand (loaded only at the time they are needed), rather than as a part of a larger stage. The most common types of streaming are:

Texture streaming: All textures are loaded in a tiny size by default. As textures get closer to the camera, higher resolution versions (or mip-maps) are streamed from disk. Textures which haven’t been used for some frames are freed instead. At any given time, the textures loaded (and their detail) closely reflect the place the player is in.
Mesh streaming: Models are loaded as low detail (few vertices). As they gradually approach the camera, higher resolution versions are streamed from disk. Models that were not used (displayed) since a while are often just freed and will be loaded again when needed.
Animation streaming: Modern games have long cinematics, which require a lot of animation data. Loading those animations require a lot of memory and loading them takes a lot of time. To avoid this, animations are streamed by generally keeping the first second or two in memory and then new sections are loaded on demand as the animation plays. Godot 4.0 supports strong animation compression and animation pages, so most of the work is already done.
Audio streaming: Similar to animation streaming, it requires storing the first second or two of audio and then streaming the rest directly from disk.

Of the above, most are relatively straightforward to implement. The most complex is mesh streaming, which generally needs to be implemented together with a GPU culling strategy to ensure that very large amounts of models can be drawn at no CPU cost. This is more or less what techniques like Nanite do in Unreal, although Godot does not need to implement something that complex to be of use in most cases.
Streaming is the most important feature missing for managing large scenes or open worlds. Without it, Godot users are subject to long loading times (as every texture, model and animation has to load before anything is shown). There is also a risk of running out of memory if too many assets are loaded in parallel instead of streaming them.
Low level rendering access
Despite the new renderer in Godot 4.0, there is no architecture that can be considered a one size fits all solution. Often developers need to implement rendering techniques, post processing effects, etc. that don’t come bundled with the engine.
The Godot philosophy has always been to cater to solving the most common use cases, and leave the door open for users to solve the less common on their own.
As such, this means that low level access to all the rendering server structures needs to be exposed via GDExtension. This will allow creating custom renderers or plugging custom code during the rendering steps, which is very useful for custom rendering techniques or post processes.
Scene job system
Most of the work done for the Godot 4.0 involved large feature and performance improvements to all the servers (rendering, physics, navigation, etc.). Servers are also now multithreaded and optimized. Even asset loading can now be done multithreaded (using multiple threads to load multiple assets).
Still, the scene system (which uses those servers), despite several usability improvements, has not seen significant optimization.
Scenes nodes in Godot are mostly intended to carry complex high level behaviors (such as animation trees, kinematic characters, IK, skeletons, etc.) for limited amounts of objects (in the hundreds at most). Currently, no threading happens at all and only a single CPU core is used. This makes it very inefficient.
This makes it an ideal target for optimizing with multithreading. There is an initial proposal on threaded processing for scene nodes, which should give complex scenes a very significant performance boost.
Swarms
Scenes, as mentioned before, are designed for complex high level behaviors in the hundreds of instances. Still, sometimes, some games require larger amounts of instances but less complex behaviors instead.
This is needed for some types of game mechanics such as:

Projectiles (bullet hell for example).
Units in some types of strategy games with thousands of entitites roaming across a map.
Cars/people in city simulators, where thousands appear all across the city.
Sandbox style simulations.
Complex custom particles that run on CPU.
Flocks, swarms, mobs, debris, etc.

More experienced programmers can use the servers directly or even plug C++ code to do the heavy lifting. ECS is often also proposed as a solution for this. Even GPU Compute (which is fully supported in Godot) can be easily used to solve this pattern.
But for the sake of keeping Godot accessible and easy to use, the idea is to create a swarm system that takes care of the rendering/physics/etc. in large amounts of those objects and the user only has to fill in the code logic.
Large team VCS support
Godot’s text file formats are very friendly to version control. They only write what is needed (no redundant information), keep the ordering of sections and are simple enough to understand changes by just looking at the diff. Few other technologies work as well in this area.
Despite that, this is far from enough to enable large team collaboration. To enable this, Godot VCS support has to improve in several areas:

Better integration with the filesystem dock.
Better real-time refresh of assets if they were modified externally (and checked out).
Support for permissions and file locking: Git does not support this out of the box, but Git LFS and Perforce do. This feature is essential for large teams to avoid conflicts and keep files protected from unintended modifications (e.g. a team member modifying code or a scene they don’t own by mistake).

Unless the support for this is solid, using Godot in large teams will remain difficult.
Commercial asset store
While for very large studios this is not an area of interest, medium-sized studios still rely on significant amounts of assets and pre-made functionality. The Asset Library currently existing in Godot only links to open source resources (e.g. hosted on GitHub or GitLab) and is unable to be used for commercial assets.
For the Godot project, a commercial asset store would be a great way to add an extra source of income, but it was not legally possible given our legal status until recently. With the move to the Godot Foundation, this is a new possibility that opens up.
Is solving these problems enough for Godot to become a top AA / AAA game engine?
The answer is “it depends”. Godot, at its core, is and will always be (by design) a very general purpose game engine. This mean that the tools provided, while certainly capable, are still game neutral. The goal for Godot is to provide a great set of building blocks that can be used and combined to create more specialized game functions and tools.
In contrast, other types of game engines already come with a lot of high level and ready to use components and behaviors.
I don’t meant to say that Godot should not support any of that in the future. If it does, though, it will most certainly be as official extensions.
So, what kind of features are we talking about? Well..
Game specific templates and behaviors
As an example, Unreal comes with a player controller, environment controller, and a lot of tools to manage the game pacing and flow. Most likely aimed at TPS/FPS games, which is the most popular game type made with the engine.
Some of these can be found as templates in Godot’s Asset Library but are nowhere close to that functionality. Eventually, official ones should be created that are more powerful and complete.
Visual scripting
While Godot had visual scripting in the past, we found that the form we had implemented didn’t really prove adequate for the needs of the community, so it was discontinued.
What we realized is that visual scripting really shines when combined together with the premade behaviors mentioned in the previous section. Without a significant amount of high level behaviors available, visual scripting is cumbersome to use as it requires a lot of work to achieve simple things by itself.
All this means that, if we produce a visual scripting solution again, it needs to go hand in hand with high level behaviors and, as such, it should be part of a set of extensions to the engine.
Specialized artist UIs
When doing tasks such as shader editing, VFX (particles) or animation, there is a large difference between Godot and engines such as Unreal.
The difference is not so much in features supported. In fact, the feature set is fairly similar! The main actual difference is in how they are presented to the user.
Godot is a very modular game engine: this means that you achieve results by combining what is there. As an example, editing a particle system in Godot means a lot of subsystems must be understood and used in combination:

GPUParticles node.
GPUParticlesMaterial resource (or even an optional dedicated shader).
Mesh resource for each pass of the particle.
Mesh material resource for each surface of the mesh (or even an optional dedicated shader).

As another example, the AnimationTree in Godot requires that AnimationNodes are laid out in a tree fashion. They can export parameters, sections can be reused (because they are resources), etc.
Or even more. Godot’s animation system is often praised because anything can be animated. Any property, other nodes, etc.
This makes Godot an extremely powerful engine that gives developers a lot of flexibility, but…
It also assumes that the user is knowledgable enough about Godot and all its inner workings in order to take advantage of it. To clarify, none of these systems are too technically complex and this is part of what makes Godot appealing and accessible, but it still requires a certain level of technical and engine knowledge.
In contrast, engines like Unreal have entirely dedicated and isolated interfaces for each of these tasks (materials, cinematic timeline, VFX, animation, etc.).
Sure, they are monolithic and hence less flexible, but for a large team with high amounts of specialization, an artist does not need to understand as much in-depth how the engine works in order to produce content with it.
This shows the fundamental difference of target user between engines. If Godot wants to appeal to larger studios, it needs to provide simpler and more monolithic interfaces for artists to be able to do their job without requiring significant more time investment in learning the technology.
This could, again, be supplied via official add-ons and, like the sections above, would require a significant amount of research to understand how to build it, since without actual feedback from artists we would only be guessing what is needed. But the question here is, is it worth it?
So, are we not even close?
While the goal of this article is to make clear how significant is the work remaining to make Godot an offering closer to the ones in the commercial segment, it is important to not forget one key detail:
Godot is Free and Open Source Software. And as such, it can be modified by anyone to fit any purpose.
Currently, many large studios have the ability to create their own in-house technology. Still, as hardware becomes more and more complex to develop for, they are giving up in favor of spending money on pre-existing commercial technology offerings.
Godot, on the other hand, serves as an excellent platform to build upon, as it solves the vast majority of problems already. As a result, more and more studios are using Godot as a base to derive their own technology from.
This is a win/win situation, as it allows them to keep their freedom to innovate and, at the same time, avoid paying expensive technology licensing costs.
Time will tell how Godot transitions from its current state to something more widely used by larger studios, but it will definitely need significantly more work from our side.
Future
I hope that this write up made more evident why Godot is such a key technology for the future of the game industry. We will continue working hard to ensure that more and more individuals and companies find Godot useful! But we need your help to happen, so please consider donating to the project.



"
https://news.ycombinator.com/rss,Reverse engineering a neural network's clever solution to binary addition,https://cprimozic.net/blog/reverse-engineering-a-small-neural-network/,Comments,"Reverse Engineering a Neural Network's Clever Solution to Binary Addition - Casey Primozic's Homepagecprimozic.net | @ameobea10cprimozic.net@ameobea10•Portfolio•Contact•Blog•Professional ExperienceReverse Engineering a Neural Network's Clever Solution to Binary AdditionSubscribe to Blog via RSS 

Training the Network


Unique Activation Functions


Dissecting the Model


The Network's Clever Solution

Summary



Epilogue

There's a ton of attention lately on massive neural networks with billions of parameters, and rightly so.  By combining huge parameter counts with powerful architectures like transformers and diffusion, neural networks are capable of accomplishing astounding feats.
However, even small networks can be surprisingly effective - especially when they're specifically designed for a specialized use-case.  As part of some previous work I did, I was training small (<1000 parameter) networks to generate sequence-to-sequence mappings and perform other simple logic tasks.  I wanted the models to be as small and simple as possible with the goal of building little interactive visualizations of their internal states.
After finding good success on very simple problems, I tried training neural networks to perform binary addition.  The networks would receive the bits for two 8-bit unsigned integers as input (converted the bits to floats as -1 for binary 0 and +1 for binary 1) and would be expected to produce properly-added output, including handling wrapping of overflows.
Training example in binary:

  01001011 + 11010110 -> 00100001

As input/output vectors for NN training:

  input:  [-1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1]
  output: [-1, -1, 1, -1, -1, -1, -1, 1]
What I hoped/imagined the network would learn internally is something akin to a binary adder circuit:










I expected that it would identify the relationships between different bits in the input and output, route them around as needed, and use the neurons as logic gates - which I'd seen happen in the past for other problems I tested.
Training the Network
To start out, I created a network with a pretty generous architecture that had 5 layers and several thousand parameters.  However, I wasn't sure even that was enough.  The logic circuit diagram above for the binary adder only handles a single bit; adding 8 bits to 8 bits would require a much larger number of gates, and the network would have to model all of them.
Additionally, I wasn't sure how the network would handle long chains of carries.  When adding 11111111 + 00000001, for example, it wraps and produces an output of 00000000.  In order for that to happen, the carry from the least-significant bit needs to propagate all the way through the adder to the most-significant bit.  I thought that there was a good chance the network would need at least 8 layers in order to facilitate this kind of behavior.
Even though I wasn't sure if it was going to be able to learn anything at all, I started off training the model.
I created training data by generating random 8-bit unsigned integers and adding them together with wrapping.  In addition to the loss computed during training of the network, I also added code to validate the network's accuracy on all 32,385 possible input combinations periodically during training to get a feel for how well it was doing overall.
After some tuning of hyperparameters like learning rate and batch size, I was surprised to see that the model was learning extremely well!  I was able to get it to the point where it was converging to perfect or nearly perfect solutions almost every training run.










I wanted to know what the network was doing internally to generate its solutions. The networks I was training were pretty severely overparameterized for the task at hand; it was very difficult to get a grasp of what they were doing through the tens of thousands of weights and biases. So, I started trimming the network down - removing layers and reducing the number of neurons in each layer.
To my continued surprise, it kept working! At some point perfect solutions became less common as networks become dependent on the luck of their starting parameters, but I was able to get it to learn perfect solutions with as few as 3 layers with neuron counts of 12, 10, and 8 respectively:
Layer (type)           Input Shape    Output shape  Param #
===========================================================
input1 (InputLayer)    [[null,16]]    [null,16]     0
___________________________________________________________
dense_Dense1 (Dense)   [[null,16]]    [null,12]     204
___________________________________________________________
dense_Dense2 (Dense)   [[null,12]]    [null,10]     130
___________________________________________________________
dense_Dense3 (Dense)   [[null,10]]    [null,8]      88
===========================================================
That's just 422 total parameters! I didn't expect that the network would be able to learn a complicated function like binary addition with that few.
It seemed too good to be true, to be honest, and I wanted to make sure I wasn't making some mistake with the way I was training the network or validating its outputs.  A review of my example generation code and training pipeline didn't reveal anything that looked off, so the next step was to actually take a look at the parameters after a successful training run.
Unique Activation Functions
One important thing to note at this point is the activation functions used for the different layers in the model.  Part of my previous work in this area consisted of designing and implementing a new activation function for use in neural networks with the goal of doing binary logic as efficiently as possible.  Among other things, it is capable of modeling any 2-input boolean function in a single neuron - meaning that it solves the XOR problem.
You can read about it in more detail in my other post, but here's what it looks like:

It looks a bit like a single period of a flattened sine wave, and it has a couple controllable parameters to configure how flat it is and how it handles out-of-range inputs.
For the models I was training for binary addition, all of them used this activation function (which I named Ameo) in the first layer and used tanh for all the other layers.
Dissecting the Model
Although the number of parameters was now pretty manageable, I couldn't discern what was going on just by looking at them. However, I did notice that there were lots of parameters that were very close to ""round"" values like 0, 1, 0.5, -0.25, etc.
Since lots of the logic gates I'd modeled previously were produced with parameters such as those, I figured that might be a good thing to focus on to find the signal in the noise.
I added some rounding and clamping that was applied to all network parameters closer than some threshold to those round values. I applied it periodically throughout training, giving the optimizer some time to adjust to the changes in between. After repeating several times and waiting for the network to converge to a perfect solution again, some clear patterns started to emerge:
layer 0 weights:
[[0         , 0         , 0.1942478 , 0.3666477, -0.0273195, 1         , 0.4076445 , 0.25     , 0.125    , -0.0775111, 0         , 0.0610434],
 [0         , 0         , 0.3904364 , 0.7304437, -0.0552268, -0.0209046, 0.8210054 , 0.5      , 0.25     , -0.1582894, -0.0270081, 0.125    ],
 [0         , 0         , 0.7264696 , 1.4563066, -0.1063093, -0.2293   , 1.6488117 , 1        , 0.4655252, -0.3091895, -0.051915 , 0.25     ],
 [0.0195805 , -0.1917275, 0.0501585 , 0.0484147, -0.25     , 0.1403822 , -0.0459261, 1.0557909, -1       , -0.5      , -0.125    , 0.5      ],
 [-0.1013674, -0.125    , 0         , 0        , -0.4704586, 0         , 0         , 0        , 0        , -1        , -0.25     , -1       ],
 [-0.25     , -0.25     , 0         , 0        , -1        , 0         , 0         , 0        , 0        , 0.2798074 , -0.5      , 0        ],
 [-0.5      , -0.5226266, 0         , 0        , 0         , 0         , 0         , 0        , 0        , 0.5       , -1        , 0        ],
 [1         , -0.9827325, 0         , 0        , 0         , 0         , 0         , 0        , 0        , -1        , 0         , 0        ],
 [0         , 0         , 0.1848682 , 0.3591821, -0.026541 , -1.0401837, 0.4050815 , 0.25     , 0.125    , -0.0777296, 0         , 0.0616584],
 [0         , 0         , 0.3899804 , 0.7313382, -0.0548765, -0.021433 , 0.8209481 , 0.5      , 0.25     , -0.156925 , -0.0267142, 0.125    ],
 [0         , 0         , 0.7257989 , 1.4584024, -0.1054092, -0.2270812, 1.6465081 , 1        , 0.4654536, -0.3099159, -0.0511372, 0.25     ],
 [-0.125    , 0.069297  , -0.0477796, 0.0764982, -0.2324274, -0.1522287, -0.0539475, -1       , 1        , -0.5      , -0.125    , 0.5      ],
 [-0.1006763, -0.125    , 0         , 0        , -0.4704363, 0         , 0         , 0        , 0        , -1        , -0.25     , 1        ],
 [-0.25     , -0.25     , 0         , 0        , -1        , 0         , 0         , 0        , 0        , 0.2754751 , -0.5      , 0        ],
 [-0.5      , -0.520548 , 0         , 0        , 0         , 0         , 0         , 0        , 0        , 0.5       , 1         , 0        ],
 [-1        , -1        , 0         , 0        , 0         , 0         , 0         , 0        , 0        , -1        , 0         , 0        ]]

layer 0 biases:
[0          , 0         , -0.1824367,-0.3596431, 0.0269886 , 1.0454538 , -0.4033574, -0.25    , -0.125   , 0.0803178 , 0         , -0.0613749]
Above are the final weights generated for the first layer of the network after the clamping and rounding. Each column represents the parameters for a single neuron, meaning that the first 8 weights from top to bottom are applied to bits from the first input number and the next 8 are applied to bits from the second one.
All of these neurons have ended up in a very similar state. There is a pattern of doubling the weights as they move down the line and matching up weights between corresponding bits of both inputs. The bias was selected to match the lowest weight in magnitude. Different neurons had different bases for the multipliers and different offsets for starting digit.
The Network's Clever Solution
After puzzling over that for a while, I eventually started to understand how its solution worked.
Digital to analog converters (DACs) are electronic circuits that take digital signals split into multiple input bits and convert them into a single analog output signal.
DACs are used in applications like audio playback where a sound files are represented by numbers stored in memory. DACs take those binary values and convert them to an analog signal which is used to power the speakers, determining their position and vibrating the air to produce sound. For example, the Nintendo Game Boy had a 4-bit DAC for each of its two output audio channels.
Here's an example circuit diagram for a DAC:










If you look at the resistances of the resistors attached to each of the bits of the binary input, you can see that they double from one input to another from the least significant bit to the most significant.  This is extremely similar to what the network learned to do with the weights of the input layer.  The main difference is that the weights are duplicated between each of the two 8-bit inputs.
This allows the network to both sum the inputs as well as convert the sum to analog all within a single layer/neuron and do it all before any activation functions even come into play.
This was only part of the puzzle, though.  Once the digital inputs were converted to analog and summed together, they were immediately passed through the neuron's activation function.  To help track down what happened next, I plotted the post-activation outputs of a few of the neurons in the first layer as the inputs increased:

The neurons seemed to be generating sine wave-like outputs that changed smoothly as the sum of the binary inputs increased.  Different neurons had different periods; the ones pictured above have periods of 8, 4, and 32 respectively.  Other neurons had different periods or were offset by certain distances.
There's something very remarkable about this pattern: they map directly to the periods at which different binary digits switch between 0 and 1 when counting in binary.  The least significant digit switches between 0 and 1 with a period of 1, the second with a period of 2, and so on to 4, 8, 16, 32, etc.  This means that for at least some of the output bits, the network had learned to compute everything it needed in a single neuron.
Looking at the weights of neurons in the two later layers confirms this to be the case.  The later layers are mostly concerned with routing around the outputs from the first layer and combining them.  One additional benefit that those layers provide is ""saturating"" the signals and making them more square wave-like - pushing them closer to the target values of -1 and 1 for all values.  This is the exact same property which is used in digital signal processing for audio synthesis where tanh is used to add distortion to sound for things like guitar pedals.
While playing around with this setup, I tried re-training the network with the activation function for the first layer replaced with sin(x) and it ends up working pretty much the same way.  Interestingly, the weights learned in that case are fractions of π rather than 1.
For other output digits, the network learned to do some over clever things to generate the output signals it needed.  For example, it combined outputs from the first layer in such a way that it was able to produce a shifted version of the signal not present in any of the first-layer neurons by adding signals from other neurons with different periods together.  It worked out pretty well, more than accurate enough for the purpose of the network.
The sine-based version of the function learned by the network (blue) ends up being roughly equivalent to the function sin(1/2x + pi) (orange):










I have no idea if this is just another random mathematical coincidence or part of some infinite series or something, but it's very neat regardless.
Summary
So, in all, the network was accomplishing binary addition by:

Converting the binary inputs into ""analog"" using a version of a digital to audio converter implemented using the weights of the input layer
Mapping that internal analog signal into periodic sine wave-like signals using the Ameo activation function (even though that activation function isn't periodic)
Saturating the sine wave-like signal to make it more like a square wave so outputs are as close as possible to the expected values of -1 and 1 for all outputs

As I mentioned, before, I had imagined the network learning some fancy combination of logic gates to perform the whole addition process digitally, similarly to how a binary adder operates.  This trick is yet another example of neural networks finding unexpected ways to solve problems.
Epilogue
One thought that occurred to me after this investigation was the premise that the immense bleeding-edge models of today with billions of parameters might be able to be built using orders of magnitude fewer network resources by using more efficient or custom-designed architectures.
It's an exciting prospect to be sure, but my excitement is somewhat dulled because I was immediately reminded of The Bitter Lesson.  If you've not read it, you should read it now (it's very short); it really impacted the way I look at computing and programming.
Even if this particular solution was just a fluke of my network architecture or the system being modeled, it made me even more impressed by the power and versatility of gradient descent and similar optimization algorithms.  The fact that these very particular patterns can be brought into existence so consistently from pure randomness is really amazing to me.
I plan to continue my work with small neural networks and eventually create those visualizations I was talking about.  If you're interested, you can subscribe to my blog via RSS at the top of the page, follow me on Twitter @ameobea10, or Mastodon @ameo@mastodon.ameo.dev.

"
https://news.ycombinator.com/rss,GPT-3 Is the Best Journal I’ve Ever Used,https://every.to/superorganizers/gpt-3-is-the-best-journal-you-ve-ever-used,Comments,"


GPT-3 Is the Best Journal I've Ever Used - Superorganizers - Every






































Subscribe





≡


About
Founders‘ Letter
Publications
Collections

Contact Us
Become a Sponsor
Login











Superorganizers




          GPT-3 Is the Best Journal I’ve Ever Used
        
My slow and steady progression to living out the plot of the movie 'Her'

by Dan Shipper
January 13, 2023
♥ 192





Listen







This is a joke, but it's not entirely wrong either.





Sponsor Every

Do you run a software company looking to reach an audience of early-adopters? Consider sponsoring our smart long-form essays on tech, AI, and productivity:

﻿Sponsor Every﻿


Want to hide ads? Become a subscriber

For the past few weeks, I’ve been using GPT-3 to help me with personal development. I wanted to see if it could help me understand issues in my life better, pull out patterns in my thinking, help me bring more gratitude into my life, and clarify my values.I’ve been journaling for 10 years, and I can attest that using AI is journaling on steroids. To understand what it’s like, think of a continuum plotting levels of support you might get from different interactions:Talking to GPT-3 has a lot of the same benefits of journaling: it creates a written record, it never gets tired of listening to you talk, and it’s available day or night. If you know how to use it correctly and you want to use it for this purpose, GPT-3 is pretty close, in a lot of ways, to being at the level of an empathic friend:If you know how to use it right, you can even push it toward some of the support you’d get from a coach or therapist. It’s not a replacement for those things, but given its rate of improvement, I could see it being a highly effective adjunct to them over the next few years. People who have been using language models for much longer than I have seem to agree:
Nick@nickcammarata

Replying to @nickcammarata

@krismartens I'm afraid of seeming hyperbolic, but also don't want to lie or hide information. GPT-3 is really just an incredible therapist, and is able to uncover complex patterns in my thinking and distill clean narratives that helps me a lot. It's also a lot warmer than most therapists

July 17th 2020, 4:55am EST

6 Retweets36 Likes

(Nick is a researcher at OpenAI. He’s also into meditation and is generally a great follow on Twitter.)It sounds wild and weird, but I think language models can have a productive, supportive role in any personal development practice. Here’s why I think it works.Why chatbots are great for journalingJournaling is already an effective personal development practice. It can help you get your thoughts out of your head, rendering them less scary. It shows you patterns in your thinking, which increases your self-awareness and makes it easier for you to change.It creates a record of your journey through life, which can tell you who you are at crucial moments. It can help you create a new narrative or storyline for life events so that you can make meaning out of them.It can also guide your focus toward emotional states like gratitude, or directions you want your life to go in, rather than letting you get swept up in whatever is currently going on in your life. But journaling has a few problems. For one, it’s sometimes hard to sit down and do it. It can be difficult to stare at a blank page and know what to write. For another, sometimes it feels a little silly—is summarizing my day really worth something?Once you get over those hurdles, as a practice it tends to get stale. You don’t read through your old entries that often, so the act of writing down your thoughts and experiences doesn’t compound in the way that it should. The prompts you use often get old: one like, “What are you grateful for today?” might work for the first few weeks, but after a while you need something fresh in order for the question to feel genuine.You want your journal to feel like an intimate friend that you can confide in—someone who’s seen you in different situations and can reflect back to you what’s important in crucial moments. You want your journal to be personal to you, and the act of journaling to feel fresh and full of hope and possibility every time you do.Unfortunately, paper isn’t great at those things. But GPT-3 is. Journaling in GPT-3 feels more like a conversation, so you don’t have to stare at a blank page or feel silly because you don’t know what to say. The way it reacts to you depends on what you say to it, so it’s much less likely to get stale or old. (Sometimes it does repeat itself, which is annoying but I think long-term solvable.) It can summarize things you’ve said to it in new language that helps you look at yourself in a different light and reframe situations more effectively. In this way, GPT-3 is a mashup of journaling and more involved forms of support like talking to a friend. It becomes a guide through your mind—one that shows unconditional positive regard and acceptance for whatever you’re feeling. It asks thoughtful questions, and doesn’t judge. It’s around 24/7, it never gets tired or sick, and it’s not very expensive.Let me tell you about how I use it, what its limitations are, and where I think it might be going.How I started with GPT-3 journalingI didn’t think of using GPT-3 in this way myself. I saw Nick Cammarata’s tweets about it over the years first. My initial reaction was a lot of skepticism mixed with some curiosity. After we launched Lex and I got more interested in AI, I remembered those tweets and decided to play around for myself. I started in the OpenAI playground—a text box where you input a prompt that tells GPT-3 how you want it to behave, and then interact with it:I had a bunch of ideas to start. I tried one from a Facebook PM, Mina Fahmi, whom I met at the AI hackathon I wrote about a few weeks ago. He suggested telling GPT-3 to take on a persona, and told me that he’d had great results asking it to be Socrates.GPT-3 as famous compassionate figureI started experimenting with prompts like this:The green messages are responses from GPT-3. I tried Socrates, the Buddha, Jesus, and a few others, and found I liked Socrates the best (apologies to my Christian and Buddhist readers). The GPT-3 version of him is effective at driving toward the root of an issue and helping you figure out small steps to take to resolve it.There’s a long tradition in various religions of visualizing and interacting with a divine, compassionate figure as a way of getting support—and this was a surprisingly successful alternative route to a similar experience.After a while, though, I became a little bored of Socrates. I’m a verified therapy nerd, so the obvious next step was to try asking GPT-3 to do interactions based on various therapy modalities.GPT-3 as therapy modality expertI tried asking GPT-3 to become a bot that’s well-versed in Internal Family Systems—a style of therapy that emphasizes the idea that the self is composed of many different parts or sub-personalities, and that a lot of growth comes from learning to understand and integrate those parts. It turns out, GPT-3 isn’t bad at that:﻿I also tried asking it to be a psychoanalyst and a cognitive behavioral therapist, both of which were interesting and useful. I even asked it to do Jungian dream interpretation:I don’t know what to make of the efficacy of dream interpretation in general, nor do I know what an actual Jungian might say about this interpretation. But I have found that having dreams reflected back to me in this way can help me understand some of what I’ve been feeling day to day but haven’t been able to put into words. GPT-3 as gratitude journalAnother thing I tried is asking GPT-3 to help me increase my sense of gratitude and joy—like a better gratitude journal:You’ll notice it starts by acting like a normal gratitude journal, asking me to list three things I’m grateful for. But once I respond, it probes about details of what you’re grateful for to get you past your stock answers and into the emotional experience of gratitude. GPT-3 as values coachOne of my favorite therapy modalities is ACT—acceptance and commitment therapy—because I love its focus on values. ACT emphasizes helping people understand what’s most important to them and uses that knowledge to help them navigate difficult emotions and experiences in their lives.Values work is challenging because sometimes it’s hard to connect your day-to-day experiences to your values. So I wanted to see if GPT-3 could help. This is one of the experiments I tried:This works well, and one of the cool things about it is how the prompt works. I took a sample therapy dialog from an ACT-focused values book that I love, Values in Therapy, and asked GPT-3 to generalize from that dialog to learn how to talk to me about values.It worked—successfully guiding our conversation toward talking about what was most important to me. It’s not perfect, but it suggests interesting possibilities for things to try going forward.Problems and limitationsWhile I liked these early experiments, they had a few significant problems.First, the OpenAI playground isn’t designed to facilitate chats, so it’s hard to use. Second, it doesn’t record inputs between sessions, so I ended up having to re-explain myself every time I started a new session. Third, it sometimes gets repetitive and asks the same questions.These are solvable, though. I know because I built a solution: a web app with a chatbot interface that remembers what I say in every session so I never have to repeat myself.  The bot lets me select a persona—like Socrates or an Internal Family Systems therapist—which corresponds to the prompts above. Then I can have a conversation with it. It will help me work through something I’m dealing with, or set goals, or bring my attention to something I’m grateful for. It can even output and save a summary of the session to help me notice patterns in my thinking over time. It’s still early and there are a lot of problems to fix, but I find myself gravitating toward it every day. I feel like I’m building up a record of myself and my patterns over time, and the more I write in it, the more it compounds.I’ll be releasing the bot soon for paying Every members, so if you want access, make sure to subscribe. What’s nextHere’s what I’ve learned so far through all of these experiments with GPT-3 as a journaling tool.There is something innately appealing about  building a relationship with an empathetic friend that you can talk to any time. It’s comforting to know that it’s available, and it’s exciting to think about all of the different prompts you can experiment with to help it support you in the way you need.There is also something weird about all of this. Spilling your guts to a robot somehow cheapens the experience because it doesn’t cost much for a robot to tell you it understands you. This mix of feelings is reflected in this Twitter thread by Rob Morris, the founder of a peer-to-peer support app called Koko:
Rob Morris@RobertRMorris

We provided mental health support to about 4,000 people — using GPT-3. Here’s what happened 👇

January 6th 2023, 2:50pm EST

1k Retweets6k Likes

When people were using GPT-3 to help them provide support to peers, their responses were rated significantly more highly than responses that were generated by humans alone:
Rob Morris@RobertRMorris

Replying to @RobertRMorris

Messages composed by AI (and supervised by humans) were rated significantly higher than those written by humans on their own (p < .001). Response times went down 50%, to well under a minute.

January 6th 2023, 2:50pm EST

66 Retweets785 Likes

But they had to stop using the GPT-3 integration because people felt like getting a response from GPT-3 wasn’t genuine and ruined the experience. Those feelings are understandable, but whether or not they ruin the experience depends on how the interaction is framed to you, and how familiar you are with these tools.I don’t think these objections will last over time for most people. It’s more likely a temporary result of contact with new technology. When you see a movie that you loved, does it cheapen the experience to know that you were touched by a set of pixels moving in the correct sequence over the course of a few hours? Obviously not, but if I had to bet, when movies were first introduced many people probably felt it was a cheaper version of a live performance experience.As these kinds of bots get more common, and we learn to interact with them and depend on them for different parts of our lives, we’ll be less likely to feel that our interactions with them are cheap or stilted.(None of this, by the way, means that in-person interactions aren’t valuable anymore—just that there’s probably more room for bot interactions in your life than you might realize.)If you're someone that's journaled for a long time, you'll find a lot of value in trying GPT-3 out as an alternative to your day-to-day practice. And if you've never journaled before this might be a good way to get started.I’ll be experimenting with this a lot more over the coming weeks and months, and I’ll be sharing everything I learn with you here. I’m excited for what’s next.




What did you think of this post?

Amazing
Good
Meh
Bad





Send Privately

      Your feedback has been saved anonymously. If you want it to be attributed to you, login or sign up.
    



Like this?Become a subscriber.
Subscribe →
Or, learn more.




Read this next:








Superorganizers


Managing Your Manager
How Helping Your Manager Succeed Will Help You Succeed

♥ 173

          Mar 9, 2022
          by Brie Wolfson











Superorganizers


The Four Kinds of Side Hustles
The CEO of Kettle and Fire breaks down how he thinks about side business opportunities

♥ 411

          Sep 16, 2020
          by Justin Mares











Superorganizers


The Fall of Roam
I don’t use Roam anymore. Why?

♥ 208

          Feb 12, 2022
          by Dan Shipper











Napkin Math


In Defense of the Unoptimized Life
Give yourself the space to be inspired

♥ 189

          Jan 12, 2023
          by Evan Armstrong











Every


Introducing: Thesis
Meet some of the internet’s best writers in person

♥ 1
🔒 
          Jan 12, 2023
      






Comments







Post









Post



    You need to login before you can comment.
    Don't have an account? Sign up!














@nattaliehartwig
about 2 hours ago


Loved this, are any of your projects available to try?



♡ 0

      ·
      Reply










✕
Thanks for reading Every!
Sign up for our daily email featuring the most interesting thinking (and thinkers) in tech.
Subscribe
Already a subscriber? Login




Contact Us ·
            Become a Sponsor ·
            Search ·
            Terms

©2023 Every Media, Inc





"
https://news.ycombinator.com/rss,The Cab Ride I'll Never Forget,https://kentnerburn.com/the-cab-ride-ill-never-forget/,Comments,"





















The Cab Ride I'll Never Forget | Kent Nerburn













































































 









		Skip to content













					Kent Nerburn
				


				wandering, wondering, writing
			
 





About

Menu Toggle





Interviews


Photo Gallery


Books
Speaking | Book Clubs
Musings
Shop
Contact
Home
 







 










					Kent Nerburn
				


				wandering, wondering, writing
			
 







Main Menu

 









About

Menu Toggle

InterviewsBook Review: Native EchoesBooksBooks-oldContactDan and Grover talk about Indian MascotsDancing with the Gods: Reflections on Life and ArtKent Nerburn

Menu Toggle

Join our mailing listMusingsPhoto GalleryPrivacyShopSouth Dakota TravelogueSpeaking

Menu Toggle

Presentation OptionsSubscribeThe Cab Ride I’ll Never Forget 

















 










The Cab Ride I'll Never Forget 




There was a time in my life twenty years ago when I was driving a cab for a living. It was a cowboy’s life, a gambler’s life, a life for someone who wanted no boss, constant movement and the thrill of a dice roll every time a new passenger got into the cab.What I didn’t count on when I took the job was that it was also a ministry. Because I drove the night shift, my cab became a rolling confessional. Passengers would climb in, sit behind me in total anonymity and tell me of their lives.We were like strangers on a train, the passengers and I, hurtling through the night, revealing intimacies we would never have dreamed of sharing during the brighter light of day. I encountered people whose lives amazed me, ennobled me, made me laugh and made me weep. And none of those lives touched me more than that of a woman I picked up late on a warm August night.I was responding to a call from a small brick fourplex in a quiet part of town. I assumed I was being sent to pick up some partiers, or someone who had just had a fight with a lover, or someone going off to an early shift at some factory for the industrial part of town.When I arrived at the address, the building was dark except for a single light in a ground-floor window. Under these circumstances, many drivers would just honk once or twice, wait a short minute, then drive away. Too many bad possibilities awaited a driver who went up to a darkened building at 2:30 in the morning.But I had seen too many people trapped in a life of poverty who depended on the cab as their only means of transportation. Unless a situation had a real whiff of danger, I always went to the door to find the passenger. It might, I reasoned, be someone who needs my assistance. Would I not want a driver to do the same if my mother or father had called for a cab?So I walked to the door and knocked.“Just a minute,” answered a frail and elderly voice. I could hear the sound of something being dragged across the floor. After a long pause, the door opened. A small woman somewhere in her 80s stood before me. She was wearing a print dress and a pillbox hat with a veil pinned on it, like you might see in a costume shop or a Goodwill store or in a 1940s movie. By her side was a small nylon suitcase. The sound had been her dragging it across the floor.The apartment looked as if no one had lived in it for years. All the furniture was covered with sheets. There were no clocks on the walls, no knickknacks or utensils on the counters. In the corner was a cardboard box filled with photos and glassware.“Would you carry my bag out to the car?” she said. “I’d like a few moments alone. Then, if you could come back and help me? I’m not very strong.”I took the suitcase to the cab, then returned to assist the woman. She took my arm, and we walked slowly toward the curb. She kept thanking me for my kindness.“It’s nothing,” I told her. “I just try to treat my passengers the way I would want my mother treated.”“Oh, you’re such a good boy,” she said. Her praise and appreciation were almost embarrassing.When we got in the cab, she gave me an address, then asked, “Could you drive through downtown?”“It’s not the shortest way,” I answered.“Oh, I don’t mind,” she said. “I’m in no hurry. I’m on my way to a hospice.”I looked in the rearview mirror. Her eyes were glistening. “I don’t have any family left,” she continued. “The doctor says I should go there. He says I don’t have very long.”I quietly reached over and shut off the meter. “What route would you like me to go?” I asked.For the next two hours we drove through the city. She showed me the building where she had once worked as an elevator operator. We drove through the neighborhood where she and her husband had lived when they had first been married. She had me pull up in front of a furniture warehouse that had once been a ballroom where she had gone dancing as a girl. Sometimes she would have me slow in front of a particular building or corner and would sit staring into the darkness, saying nothing.As the first hint of sun was creasing the horizon, she suddenly said, “I’m tired. Let’s go now.”We drove in silence to the address she had given me. It was a low building, like a small convalescent home, with a driveway that passed under a portico. Two orderlies came out to the cab as soon as we pulled up. Without waiting for me, they opened the door and began assisting the woman. They were solicitous and intent, watching her every move. They must have been expecting her; perhaps she had phoned them right before we left.I opened the trunk and took the small suitcase up to the door. The woman was already seated in a wheelchair.“How much do I owe you?” she asked, reaching into her purse.“Nothing,” I said.“You have to make a living,” she answered.“There are other passengers,” I responded.Almost without thinking, I bent and gave her a hug. She held on to me tightly. “You gave an old woman a little moment of joy,” she said. “Thank you.”There was nothing more to say. I squeezed her hand once, then walked out into the dim morning light. Behind me, I could hear the door shut. It was the sound of the closing of a life.I did not pick up any more passengers that shift. I drove aimlessly, lost in thought. For the remainder of that day, I could hardly talk. What if that woman had gotten an angry driver, or one who was impatient to end his shift? What if I had refused to take the run, or had honked once, then driven away? What if I had been in a foul mood and had refused to engage the woman in conversation? How many other moments like that had I missed or failed to grasp?We are so conditioned to think that our lives revolve around great moments. But great moments often catch us unawares. When that woman hugged me and said that I had brought her a moment of joy, it was possible to believe that I had been placed on earth for the sole purpose of providing her with that last ride.I do not think that I have ever done anything in my life that was any more important. 































 







Copyright © 2023 Kent Nerburn | Powered by kincaid-burrows
 










































"
https://news.ycombinator.com/rss,Show HN: Sketch – AI code-writing assistant that understands data content,https://github.com/approximatelabs/sketch,Comments,"








approximatelabs

/

sketch

Public




 

Notifications



 

Fork
    4




 


          Star
 163
  









        AI code-writing assistant that understands data content
      





163
          stars
 



4
          forks
 



 


          Star

  





 

Notifications












Code







Issues
0






Pull requests
0






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







approximatelabs/sketch









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





6
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




bluecoconut

update readme wording




        …
      




        9d567ec
      

Jan 16, 2023





update readme wording


9d567ec



Git stats







133

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github/workflows



remove python 3.7, add tests, remove uneeded code



Dec 15, 2022









sketch



some bug fix and copy addition



Jan 16, 2023









tests



remove python 3.7, add tests, remove uneeded code



Dec 15, 2022









.gitignore



update with edit and work on text2sql



Oct 13, 2022









README.md



update readme wording



Jan 16, 2023









dev-requirements.txt



starting to rebuild sketch



Dec 15, 2022









pyproject.toml



rename to pandas extension, add missing requirement, use base64 encoding



Jan 11, 2023









setup.py



starting to rebuild sketch



Dec 15, 2022




    View code
 


















sketch
Demo
How to use
.sketch.ask
.sketch.howto
.sketch.apply
Sketch currently uses prompts.approx.dev to help run with minimal setup
How it works





README.md




sketch
Sketch is an AI code-writing assistant for pandas users that understands the context of your data, greatly improving the relevance of suggestions. Sketch is usable in seconds and doesn't require adding a plugin to your IDE.
pip install sketch
Demo
Here we follow a ""standard"" (hypothetical) data-analysis workflow, showing a Natural Language interace that successfully navigates many tasks in the data stack landscape.

Data Catalogging:

General tagging (eg. PII identification)
Metadata generation (names and descriptions)


Data Engineering:

Data cleaning and masking (compliance)
Derived feature creation and extraction


Data Analysis:

Data questions
Data visualization








sketch-demo.mp4





Try it out in colab: 
How to use
It's as simple as importing sketch, and then using the .sketch extension on any pandas dataframe.
import sketch
Now, any pandas dataframe you have will have an extension registered to it. Access this new extension with your dataframes name .sketch
.sketch.ask
Ask is a basic question-answer system on sketch, this will return an answer in text that is based off of the summary statistics and description of the data.
Use ask to get an understanding of the data, get better column names, ask hypotheticals (how would I go about doing X with this data), and more.
df.sketch.ask(""Which columns are integer type?"")
.sketch.howto
Howto is the basic ""code-writing"" prompt in sketch. This will return a code-block you should be able to copy paste and use as a starting point (or possibly ending!) for any question you have to ask of the data. Ask this how to clean the data, normalize, create new features, plot, and even build models!
df.sketch.howto(""Plot the sales versus time"")
.sketch.apply
apply is a more advanced prompt that is more useful for data generation. Use it to parse fields, generate new features, and more. This is built directly on lambdaprompt. In order to use this, you will need to set up a free account with OpenAI, and set an environment variable with your API key. OPENAI_API_KEY=YOUR_API_KEY
df['review_keywords'] = df.sketch.apply(""Keywords for the review [{{ review_text }}] of product [{{ product_name }}] (comma separated):"")
df['capitol'] = pd.DataFrame({'State': ['Colorado', 'Kansas', 'California', 'New York']}).sketch.apply(""What is the capitol of [{{ State }}]?"")
Sketch currently uses prompts.approx.dev to help run with minimal setup
In the future, we plan to update the prompts at this endpoint with our own custom foundation model, built to answer questions more accurately than GPT-3 can with its minimal data context.
You can also directly call OpenAI directly (and not use our endpoint) by using your own API key. To do this, set 2 environment variables.
(1) SKETCH_USE_REMOTE_LAMBDAPROMPT=False
(2) OPENAI_API_KEY=YOUR_API_KEY
How it works
Sketch uses efficient approximation algorithms (data sketches) to quickly summarize your data, and feed that information into language models. Right now it does this by summarizing the columns and writing these summary statistics as additional context to be used by the code-writing prompt. In the future we hope to feed these sketches directly into custom made ""data + language"" foundation models to get more accurate results.









About

      AI code-writing assistant that understands data content
    
Topics



  python


  data-science


  data


  ai


  tabular-data


  pandas


  df


  sketches


  dataframe


  copilot


  codex


  ds


  datasketches


  gpt3


  lambdaprompt


  datasketch



Resources





      Readme
 


Stars





163
    stars

Watchers





2
    watching

Forks





4
    forks







    Releases





6
tags







    Packages 0


        No packages published 







        Used by 22
 




























            + 14
          







    Contributors 2








bluecoconut
Justin Waugh

 






jmbiven
Mike Biven

 





Languages










Python
100.0%











"
https://news.ycombinator.com/rss,EasyPost (YC S13) Is Hiring,https://www.easypost.com/careers,Comments,"Careers at EasyPost - EasyPostSolutionsexpand_lessProductsDiscounted ShippingCreate a LabelShipping APISmartRate APITracking APIAddress Verification APIShipping InsuranceCarbon Offset APIPartner White Label APIUse CasesstoreSmall Businessshopping_cartEcommercestorefrontMarketplacelayersPlatformlocal_shippingFulfillmentPartnersFind a PartnerBecome a PartnerDevelopersexpand_lessGetting StartedAPI DocsClient LibrariesAPI StatusEngineering BlogCarriersCompanyexpand_lessBlogCase StudiesNewsletterAbout UsCareersHelp CenterContact SalesPricingSolutionschevron_rightDeveloperschevron_rightCarriersCompanychevron_rightPricingSolutionsSolve complex shipping logistics problems with a single integration.View All Solutionschevron_rightProductsDiscounted ShippingCreate a LabelShipping APISmartRate APITracking APIAddress Verification APIShipping InsuranceCarbon Offset APIPartner White Label APIPartnersFind a PartnerBecome a PartnerUse CasesstoreSmall Businessshopping_cartEcommercestorefrontMarketplacelayersPlatformlocal_shippingFulfillmentDevelopersAccess our developer resources and learn how to easily integrate with the EasyPost API.Getting StartedAPI DocsClient LibrariesAPI StatusEngineering BlogCompanyExplore our company resources to learn more about EasyPost and the shipping industry.BlogCase StudiesNewsletterAbout UsCareersHelp CenterContact SalesSign up freeLog inCareers at EasyPostOur team of problem solvers brings a modern approach to shipping logistics. We collaborate across departments, ask challenging questions, explore new solutions, and take accountability for our wins and mistakes.See all job openingsThe future of youAs industry experts, we're working not only to help our customers make sense of the industry, but to define where it's headed. We are looking for candidates who are approachable, dynamic, inventive, intelligent, and reliable to join our team in unpacking the future of shipping.Join usThe future of shippingHow can modern, flexible technology improve the customer experience of shipping? What if every business was able to offer same-day shipping? How much waste would be removed from the environment if all our shipments were consolidated into one delivery per week? At EasyPost, we're figuring out the answer to these questions and more.Join usLife at EasyPostlooks_oneAdaptiveEmbrace new challenges to grow your skill set.looks_twoSimpleCreate efficient solutions that are easy to execute.looks_3InclusiveShare new ideas and work collaboratively across teams.Team and technologyWe're a fun group of passionate entrepreneurs who built our own revolutionary software designed to make shipping simple. EasyPost started as an Engineering first company and we are proud to have a pragmatic approach to software development. Our team has a wealth of diverse experience and different backgrounds ranging from startups to large technology companies.Be part of a leading technology company:CI/CD inspired workflows - we deploy dozens of times a daySmall services over monoliths - we've deployed hundreds of servicesStrong engineering tooling and developer supportTransparency and participation around architecture and technology decisionsCulture of blamelessness and improving today from yesterday's shortcomingsCheck out our engineering blogSee openingsBenefits and perksmedical_servicesMedical, dental, vision plansaccess_timeFlexible time-offauto_graphStock option opportunitiessavings401(k) matchsupervisor_accountCross-functional learningtodayMonthly virtual eventsIn the past 3 years, I've learned a staggering amount from our colleagues and have had the best experience of my career thus far.When I work in this type of environment, with such talented and knowledgeable teammates, I really thrive and am extremely motivated to help make EasyPost more successful.Kyle GravesEngineering @ EasyPostStart your adventure at EasyPost© Simpler Postage 2023SolutionsPricingCarriersDiscounted shippingCode-free label creationShipping APISmartRate APITracking APIAddress verificationShipping insuranceCarbon Offset APIPartner White Label APIPartnersDevelopersGuidesAPI DocsClient librariesEngineering blogStatusContact usTalk to supportContact salesCompanyAbout usBlogCareersCase studiesNewsletterPrivacy & termsSupport© Simpler Postage 2023Switch to Desktop Versioneasypost-web-202301132331-5e82544d2f-master"
https://news.ycombinator.com/rss,Speeding up the JavaScript ecosystem part 2 – Module Resolution,https://marvinh.dev/blog/speeding-up-javascript-ecosystem-part-2/,Comments,"


Speeding up the JavaScript ecosystem - module resolution

written by@marvinhagemeist15 January 2023


📖 tl;dr: Whether you’re building, testing and/or linting JavaScript, module resolution is always at the heart of everything. Despite its central place in our tools, not much time has been spent on making that aspect fast. With the changes discussed in this blog post tools can be sped up by as much as 30%.

In part 1 of this series we found a few ways to speed various libraries used in JavaScript tools. Whilst those low level patches moved the total build time number by a good chunk, I was wondering if there is something more fundamental in our tooling that can be improved. Something that has a greater impact on the total time of common JavaScript tasks like bundling, testing and linting.
So over the next couple of days I collected about a dozen CPU profiles from various tasks and tools that are commonly used in our industry. After a bit of inspection, I came across a repeating pattern that was present in every profile I looked at and affected the total runtime of these tasks by as much as 30%. It’s such a critical and influential part of our infrastructure that it deserves to have its own blog post.
That critical piece is called module resolution. And in all the traces I looked at it took more time in total than parsing source code.
The cost of capturing stack traces
It all started when I noticed that the most time consuming aspect in those traces was spent in captureLargerStackTrace an internal node function responsible for attaching stack traces to Error objects. That seemed a bit out of the ordinary, given that both tasks succeeded without showing any signs of errors being thrown.


							After clicking through a bunch of occurrences in the profiling data a clearer picture emerged as to what was happening. Nearly all of the error creations came from calling node’s native fs.statSync() function and that in turn was called inside a function called isFile. The documentation mentions that fs.statSync() is basically the equivalent to POSIX’s fstat command and commonly used to check if a path exists on disk, is a file or a directory. With that in mind we should only get an error here in the exceptional use case when the file doesn’t exist, we lack permissions to read it or something similar. It was time to take a peek at the source of isFile.
						
function isFile(file) {	try {		const stat = fs.statSync(file);		return stat.isFile() || stat.isFIFO();	} catch (err) {		if (err.code === ""ENOENT"" || err.code === ""ENOTDIR"") {			return false;		}		throw err;	}}
From a quick glance it’s an innocent looking function, but was showing up in traces nonetheless. Noticeably, we ignore certain error cases and return false instead of forwarding the error. Both the ENOENT and ENOTDIR error codes ultimately mean that the path doesn’t exist on disk. Maybe that’s the overhead we’re seeing? I mean we’re immediately ignoring those errors here. To test that theory I logged out all the errors that the try/catch-block caught. Low and behold every single error that was thrown was either a ENOENT code or an ENOTDIR code.

							A peek into node’s documentation of fs.statSync reveals that it supports passing a throwIfNoEntry option that prevents errors from being thrown when no file system entry exists. Instead it will return undefined in that case.
						
function isFile(file) {	const stat = fs.statSync(file, { throwIfNoEntry: false });	return stat !== undefined && (stat.isFile() || stat.isFIFO());}
Applying that option allows us to get rid of the if-statment in the catch block which in turn makes the try/catch redundant and allows us to simplify the function even further.
This single change reduced the time to lint the project by 7%. What’s even more awesome is that tests got a similar speedup from the same change too.
The file system is expensive
With the overhead of stack traces of that function being eliminated, I felt like there was still more to it. You know, throwing a couple of errors shouldn’t really show up at all in traces captured over the span of a couple of minutes. So I injected a simple counter into that function to get an idea how frequently it was called. It became apparent that it was called about 15k times, about 10x more than there were files in the project. That smells like an opportunity for improvement.
To module or not to module, that is the question
By default there are three kind of specifiers for a tool to know about:

Relative module imports: ./foo, ../bar/boof
Absolute module imports: /foo, /foo/bar/bob
Package imports foo, @foo/bar

The most interesting of the three from a performance perspective is the last one. Bare import specifiers, the ones that don’t start with a dot . or with a slash /, are a special kind of import that typically refer to npm packages. This algorithm is described in depth in node’s documentation. The gist of it is that it tries to parse the package name and then it will traverse upwards to check if a special node_modules directory is present that contains the module until it reaches the root of the file system. Let’s illustrate that with an example.
Let’s say that we have a file located at /Users/marvinh/my-project/src/features/DetailPage/components/Layout/index.js that tries to import a module foo. The algorithm will then check for the following locations.

/Users/marvinh/my-project/src/features/DetailPage/components/Layout/node_modules/foo/
/Users/marvinh/my-project/src/features/DetailPage/components/node_modules/foo/
/Users/marvinh/my-project/src/features/DetailPage/node_modules/foo/
/Users/marvinh/my-project/src/features/node_modules/foo/
/Users/marvinh/my-project/src/node_modules/foo/
/Users/marvinh/my-project/node_modules/foo/
/Users/marvinh/node_modules/foo/
/Users/node_modules/foo/

That’s a lot of file system calls. In a nutshell every directory will be checked if it contains a module directory. The amount of checks directly correlates to the number of directories the importing file is in. And the problem is that this happens for every file where foo is imported. Meaning if foo is imported in a file residing somewhere else, we’ll crawl the whole directory tree upwards again until we find a node_modules directory that contains the module. And that’s an aspect where caching the resolved module greatly helps.
But it gets even better! Lots of projects make use of path mapping aliases to save a little bit of typing, so that you can use the same import specifiers everywhere and avoid lots of dots ../../../. This is typically done via TypeScript’s paths compiler option or a resolve alias in a bundler. The problem with that is that these typically are indistinguishable from package imports. If I add a path mapping to the features directory at /Users/marvinh/my-project/src/features/ so that I can use an import declaration like import {...} from “features/DetailPage”, then every tool should know about this.
But what if it doesn’t? Since there is no centralized module resolution package that every JavaScript tool uses, they are multiple competing ones with various levels of features supported. In my case the project makes heavy use of path mappings and it included a linting plugin that wasn’t aware of the path mappings defined in TypeScript’s tsconfig.json. Naturally, it assumed that features/DetailPage was referring to a node module, which led it to do the whole recursive upwards traversal dance in hopes of finding the module. But it never did, so it threw an error.
Caching all the things
Next I enhanced the logging to see how many unique file paths the function was called with and if it always returned the same result. Only about 2.5k calls to isFile had a unique file path and there was a strong 1:1 mapping between the passed file argument and the returned value. It’s still more than the amount of files in the project, but it’s much lower than the total 15k times it was called. What if we added a cache around that to avoid reaching out to the file system?
const cache = new Map();function resolve(file) {	const cached = cache.get(file);	if (cached !== undefined) return cached;	// ...existing resolution logic here	const resolved = isFile(file);	cache.set(file, resolved);	return file;}
The addition of a cache sped up the total linting time by another 15%. Not bad! The risky bit about caching though is that they might become stale. There is a point in time where they usually have to be invalidated. Just to be on the safe side I ended up picking a more conservative approach that checks if the cached file still exists. This is not an uncommon thing to happen if you think of tooling often being run in watch mode where it’s expected to cache as much as possible and only invalidate the files that changed.
const cache = new Map();function resolve(file) {	const cached = cache.get(file);	// A bit conservative: Check if the cached file still exists on disk to avoid	// stale caches in watch mode where a file could be moved or be renamed.	if (cached !== undefined && isFile(file)) {		return cached;	}	// ...existing resolution logic here	for (const ext of extensions) {		const filePath = file + ext;		if (isFile(filePath)) {			cache.set(file, filePath);			return filePath;		}	}	throw new Error(`Could not resolve ${file}`);}
I was honestly expecting it to nullify the benefits of adding a cache in the first place since we’re reaching to the file system even in the cached scenario. But looking at the numbers this only worsened the total linting time only by 0.05%. That’s a very minor hit in comparison, but shouldn’t the additional file system call matter more?
The file extension guessing game
The thing with modules in JavaScript is that the language didn’t have a module system from the get go. When node.js came onto the scene it popularized the CommonJS module system. That system has several “cute” features like the ability to omit the extension of the file you’re loading. When you write a statement like require(""./foo"") it will automatically add the .js extension and try to read the file at ./foo.js. If that isn’t present it will check for json file ./foo.json and if that isn’t available either, it will check for an index file at ./foo/index.js.
Effectively we’re dealing with ambiguity here and the tooling has to make sense of what ./foo should resolve to. With that there is a high chance of doing wasted file system calls as there is no way of knowing where to resolve the file to, in advance. Tools literally have to try each combination until they find a match. This is worsened if we look at the total amount of possible extensions that exist today. Tools typically have an array of potential extensions to check for. If you include TypeScript the full list for a typical frontend project at the time of this writing is:
const extensions = [	"".js"",	"".jsx"",	"".cjs"",	"".mjs"",	"".ts"",	"".tsx"",	"".mts"",	"".cts"",];
That’s 8 potential extensions to check for. And that’s not all. You essentially have to double that list to account for index files which could resolve to all those extensions too! This means that our tools have no other option, other than looping through the list of extensions until we find one that exists on disk. When we want to resolve ./foo and the actual file is foo.ts, we’d need to check:

foo.js -> doesn’t exist
foo.jsx -> doesn’t exist
foo.cjs -> doesn’t exist
foo.mjs -> doesn’t exist
foo.ts -> bingo!

That’s four unnecessary file system calls. Sure you could change the order of the extensions and put the most common ones in your project at the start of the array. That would increase the chances of the correct extension to be found earlier, but it doesn’t eliminate the problem entirely.
As part of the ES2015 spec a new module system was proposed. All the details weren’t fleshed out in time, but the syntax was. Import statements quickly took over as they have very benefits over CommonJS for tooling. Due to its staticness it opened up the space for lots more tooling enhanced features like most famously tree-shaking where unused modules and or even functions in modules can be easily detected and dropped from the production build. Naturally, everyone jumped on the new import syntax.
There was one problem though: Only the syntax was finalized and not how the actual module loading or resolution should work. To fill that gap, tools re-used the existing semantics from CommonJS. This was good for adoption as porting most code bases only required syntactical changes and these could be automated via codemods. This was a fantastic aspect from an adoption point of view! But that also meant that we inherited the guessing game of which file extension the import specifier should resolve to.
The actual spec for module loading and resolution was finalized years later and it corrected this mistake by making extensions mandatory.
// Invalid ESM, missing extension in import specifierimport { doSomething } from ""./foo"";// Valid ESMimport { doSomething } from ""./foo.js"";
By removing this source of ambiguity and always adding an extension, we’re avoiding an entire class of problems. Tools get way faster too. But it will take time until the ecosystem moves forward on that or if at all, since tools have adapted to deal with the ambiguity.
Where to go from here?
Throughout this whole investigation I was a bit surprised to find that much room for improvement in regards to optimizing module resolution, given that it’s such a central in our tools. The few changes described in this article reduced the linting times by 30%!
The few optimizations we did here are not unique to JavaScript either. Those are the same optimizations that can be found in toolings for other programming languages. When it comes to module resolution the four main takeaways are:

Avoid calling out to the file system as much as possible
Cache as much as you can to avoid calling out to the file system
When you're using fs.stat or fs.statSync always set the throwIfNoEntry: false
Limit upwards traversal as much as possible

The slowness in our tooling wasn’t caused by JavaScript the language, but by things just not being optimized at all. The fragmentation of the JavaScript ecosystem doesn't help either as there isn’t a single standard package for module resolution. Instead, there are multiple and they all share a different subset of features. That’s no surprise though as the list of features to support has grown over the years and there is no single library out there that supports them all at the time of this writing. Having a single library that everyone uses would make solving this problem once and for all for everyone a lot easier.


"
https://news.ycombinator.com/rss,Servo to Advance in 2023,https://servo.org/blog/2023/01/16/servo-2023/,Comments,"














          How to start
        

          Contributing
        

          Blog
        

          Governance
        




GitHub





Twitter





Mastodon








Servo to Advance in 2023
(2023-01-16) A brief update on the Servo project's renewed activity in 2023.

We would like to share some exciting news about the Servo project. This year, thanks to new external funding, a team of developers will be actively working on Servo. The first task is to reactivate the project and the community around it, so we can attract new collaborators and sponsors for the project.
The focus for 2023 is to improve the situation of the layout system in Servo, with the initial goal of getting basic CSS2 layout working. Given the renewed activity in the project, we will keep you posted with more updates throughout the year. Stay tuned!
About Servo 


Created by Mozilla Labs in 2012, the Servo project is a Research & Development effort meant to create an independent, modular, embeddable web engine that allows developers to deliver content and applications using web standards.  Servo is an experimental browser engine written in Rust, taking advantage of the memory safety properties and concurrency features of the language.  Stewardship of Servo moved from Mozilla Labs to the Linux Foundation in 2020, where its mission remains unchanged.


Back



"
https://news.ycombinator.com/rss,"Show HN: Terra Firma, a playable erosion simulation",https://store.steampowered.com/app/1482770/Terra_Firma/,Comments,"





Terra Firma on Steam























































									Login								

		Store	

Home
Discovery Queue
Wishlist
Points Shop
News
Stats


			Community		

Home
Discussions
Workshop
Market
Broadcasts


		Support	


									Change language								

										View desktop website									





								© Valve Corporation. All rights reserved. All trademarks are property of their respective owners in the US and other countries.								
Privacy Policy
									 |  Legal
									 |  Steam Subscriber Agreement
									 |  Refunds








































		STORE	

Home
Discovery Queue
Wishlist
Points Shop
News
Stats


			COMMUNITY		

Home
Discussions
Workshop
Market
Broadcasts


				ABOUT			

		SUPPORT	






							Install Steam						

login
											 | 
						language


简体中文 (Simplified Chinese)
繁體中文 (Traditional Chinese)
日本語 (Japanese)
한국어 (Korean)
ไทย (Thai)
Български (Bulgarian)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Español - España (Spanish - Spain)
Español - Latinoamérica (Spanish - Latin America)
Ελληνικά (Greek)
Français (French)
Italiano (Italian)
Magyar (Hungarian)
Nederlands (Dutch)
Norsk (Norwegian)
Polski (Polish)
Português (Portuguese - Portugal)
Português - Brasil (Portuguese - Brazil)
Română (Romanian)
Русский (Russian)
Suomi (Finnish)
Svenska (Swedish)
Türkçe (Turkish)
Tiếng Việt (Vietnamese)
Українська (Ukrainian)
Report a translation problem


















								Cart								(0)
							









Your Store
Your Store






										Home									

                                            Community Recommendations                                        

										Recently Viewed									

                                            Steam Curators                                        





New & Noteworthy
New & Noteworthy






											Steam Replay 2022										

											Top Sellers										

											Most Played										

										New & Trending                                    

										Special Offers									

                                        Recently Updated                                    

                                        Popular Upcoming                                    





Categories
Categories






Special Sections

														Free to Play													

Demos


														Early Access													

Controller-Friendly


Remote Play


                                                Software											

												Soundtracks											

VR Titles


VR Hardware


Steam Deck


Great on Deck


													macOS												

													SteamOS + Linux												

For PC Cafés




													Genres
												

														Action													


															Action														


Arcade & Rhythm
Fighting & Martial Arts
First-Person Shooter
Hack & Slash
Platformer & Runner
Third-Person Shooter
shmup


														Adventure													


															Adventure														


Adventure RPG
Casual
Hidden Object
Metroidvania
Puzzle
Story-Rich
Visual Novel

 




														Role-Playing													


															Role-Playing														


Action RPG
Adventure RPG
JRPG
Party-Based
Rogue-Like
Strategy RPG
Turn-Based


														Simulation													


															Simulation														


Building & Automation
Dating
Farming & Crafting
Hobby & Job
Life & Immersive
Sandbox & Physics
Space & Flight

 




														Strategy													


															Strategy														


Card & Board
City & Settlement
Grand & 4X
Military
Real-Time Strategy
Tower Defense
Turn-Based Strategy


														Sports & Racing													


															Sports & Racing														


All Sports
Fishing & Hunting
Individual Sports
Racing
Racing Sim
Sports Sim
Team Sports

 

														Themes													

														Themes													

Adult Only
Anime
Horror
Mystery & Detective
Open World
Sci-Fi & Cyberpunk
Space
Survival
 

														Player Support													

														Player Support													

Co-Operative
LAN
Local & Party
MMO
Multiplayer
Online Competitive
Singleplayer

 


Points Shop


News


Labs







































All Games
																					> Simulation Games
																				> Terra Firma







Community Hub



Terra Firma
















Terra Firma

Developer

Working as Intended 
Publisher

Working as Intended 
Released

											Jun 27, 2021										



								Play god and create your own world in this complex simulation. Plate tectonics, wind and water erosion and plant life all develop and interact to produce a world that is viewable from a single tree to kilometres across.							



Recent Reviews:

Very Positive

													(15)
												

												- 100% of the 15 user reviews in the last 30 days are positive.											



All Reviews:

Very Positive

													(214)
												

												- 92% of the 214 user reviews for this game are positive.											









Release Date:
Jun 27, 2021


Developer:

Working as Intended 


Publisher:

Working as Intended 




Tags

Popular user-defined tags for this product:


												Simulation												
												Strategy												
												God Game												
												Life Sim												
												Sandbox												
												3D												
												Nature												
												Destruction												
												Early Access												
												Relaxing												
												Physics												
												Singleplayer												+


Reviews


All Reviews:

Very Positive

													(92% of 214) All Time




Recent Reviews:

Very Positive

													(100% of 15) Recent




























































































































Sign in to add this item to your wishlist, follow it, or mark it as ignored














Links & info




Is this game relevant to you?


									Sign in to see reasons why you may or may not like this based on your games, friends, and curators you follow.
								

Sign In

																			or										Open in Steam

Features


Single-player 

Profile Features Limited 
										







								Languages:



		English	






Interface
Full Audio
Subtitles



				English			

✔ 

✔ 










Title: Terra Firma
Genre: Simulation, Strategy, Early Access

Developer:
Working as Intended


Publisher:
Working as Intended

Release Date: Jun 27, 2021
Early Access Release Date: Jun 27, 2021




mehwoot on Twitter



			View update history		

			Read related news		

			View discussions		

            Find Community Groups        





Share
Embed
 








Early Access Game
Get instant access and start playing; get involved with this game as it develops.
Note: This Early Access game is not complete and may or may not change further. If you are not excited to play this game in its current state, then you
									should wait to see if the game progresses further in development. Learn more


What the developers have to say:

Why Early Access?
										“This game is in early access in order to give people a chance to play and give feedback on our initial, working version whilst we continue to develop it.”

										Approximately how long will this game be in Early Access?
										“3 years”

										How is the full version planned to differ from the Early Access version?
										“The full version of the game will feature a greatly expanded simulation that builds on what is currently available.  Extra simulation layers related to the weather, such as temperature, air pressure, wind, ocean currents will provide a much richer set of interactions in the simulated world.

We expect to greatly expand the plants and animals that inhabit the world to provide a more interesting outcomes of the user's choice of geography.

The full version will have vastly better graphics than the current version, which uses only a minimum of graphical assets in order to release a playable version of the game.”

										What is the current state of the Early Access version?
										“The current early access version is fully playable, featuringWater and plate tectonic simulationsAbility to change the terrain at willDifferent types of plant life developing and responding to the geography of the worldAbility to view the world from the scale of a single tree to dozens of square kilometres at once”

										Will the game be priced differently during and after Early Access?
										“We plan to gradually raise the price as we ship new content and features.”

										How are you planning on involving the Community in your development process?
										“We'll be closely monitoring feedback from the community both from reviews and community content to decide what features should be added and how the game should be improved.”

																	 
Read more







				Play Terra Firma			



							Free						


Play Game








View Community Hub

 






See all discussions

Report bugs and leave feedback for this game on the discussion boards





About This Game
							Terra Firma gives you the power to play god in a simulated world where the forces of nature interact in complex ways.  Currently in early access, right now you canWatch water flow through the landscape, eroding and depositing the land to form complex arrangements of tributary river systems, river deltas, lakes and oceans in an emergent fashion.Observe the ecosystem respond to the availability of water.  Plants automatically gross across the lanscape with different plants thriving in different environments depending on the distribution of water, nutrients and the geometry of the land.Create your landscape using simulated plate tectonics, watching mountains form, grow and erode as millions of years pass in the blink of an eye.  Customize the landscape to your will, raising, lowering and flattening it to see how the enviornment changes as a resultZoom all the way from looking at an individual tree or plant to view the entire map across hundreds of square kilometresIn future releases, the game willAllow saving and loading worldsHave a fully simulated enviornment with not just water but also climate factors such as temperature, weather, ice, snow, wind and ocean currents forming and changing depending on the geography as well as affecting each otherFeature a wide variety and animal and plant species that flourish across the map according to the geography as well as each other.  Animals will populate accroding to the presence of either plants or other animals which they consume 


System Requirements




Minimum:OS: Windows 7+Memory: 2048 MB RAMGraphics: Dedicated Graphics Card 



Recommended:OS: Windows 7+Memory: 8096 MB RAMGraphics: Dedicated Graphics Card 









See all

More like this







View all
What Curators Say

					1 Curator has reviewed this product. Click here to see them.				








Customer reviews













Overall Reviews:
Very Positive
(214 reviews)









Recent Reviews:
Very Positive
(15 reviews)










Review Type



All (214)

Positive (199)

Negative (15)




Purchase Type



All (214)

Steam Purchasers (0) 

Other (214) 




Language



All Languages (214)

Your Languages (181) 
Customize




Date Range



							To view reviews within a date range, please click and drag a selection on a graph above or click on a specific bar.							
Show graph


Lifetime

Only Specific Range (Select on graph above) 

Exclude Specific Range (Select on graph above) 




Playtime



Brought to you by Steam Labs


							Filter reviews by the user's playtime when the review was written:						

No Minimum

Over 1 hour

No minimum to No maximum








Display As: 

Summary
Most Helpful
Recent
Funny



Off-topic Review Activity



							When enabled, off-topic review activity will be filtered out.  This defaults to your Review Score Setting. Read more about it in the blog post.						
Enabled




Show graph  
Hide graph  





Filters

Filters				




Excluding Off-topic Review Activity
Playtime: 






			Loading reviews...		


			Loading reviews...		


			Loading reviews...		


			Loading reviews...		


			Loading reviews...		














There are no more reviews that match the filters set above
Adjust the filters above to see other reviews













Loading reviews...









Review Filters




















You can use this widget-maker to generate a bit of HTML that can be embedded in your website to easily allow customers to purchase this game on Steam.
Enter up to 375 characters to add a description to your widget:




Create widget




Copy and paste the HTML below into your website to make the above widget appear




Link to the game's store pagehttps://store.steampowered.com/app/1482770/Terra_Firma/





Popular user-defined tags for this product:(?)




Sign In
Sign in to add your own tags to this product.


Sign In











 







© 2023 Valve Corporation.  All rights reserved.  All trademarks are property of their respective owners in the US and other countries.
VAT included in all prices where applicable.  

            Privacy Policy
              |  
            Legal
              |  
            Steam Subscriber Agreement
              |  
            Refunds
              |  
            Cookies



View mobile website







About Valve
          |  Jobs
          |  Steamworks
          |  Steam Distribution
          |  Support
        		  |  Gift Cards
		  |   Steam
		  |   @steam



 
 

"
https://news.ycombinator.com/rss,"ASCII table and history – Or, why does Ctrl+i insert a Tab in my terminal?",https://bestasciitable.com,Comments,"




ASCII table and history (or, why does Ctrl+i insert a Tab in my terminal?)







ASCII table and history
Or, why does Ctrl+i insert a Tab in my terminal?



DecHexBinaryChar
00x0000 00000NUL
10x0100 00001SOH
20x0200 00010STX
30x0300 00011ETX
40x0400 00100EOT
50x0500 00101ENQ
60x0600 00110ACK
70x0700 00111BEL
80x0800 01000BS
90x0900 01001HT
100x0a00 01010LF
110x0b00 01011VT
120x0c00 01100FF
130x0d00 01101CR
140x0e00 01110SO
150x0f00 01111SI
160x1000 10000DLE
170x1100 10001DC1
180x1200 10010DC2
190x1300 10011DC3
200x1400 10100DC4
210x1500 10101NAK
220x1600 10110SYN
230x1700 10111ETB
240x1800 11000CAN
250x1900 11001EM
260x1a00 11010SUB
270x1b00 11011ESC
280x1c00 11100FS
290x1d00 11101GS
300x1e00 11110RS
310x1f00 11111US


DecHexBinaryChar
320x2001 00000SPACE
330x2101 00001!
340x2201 00010""
350x2301 00011#
360x2401 00100$
370x2501 00101%
380x2601 00110&
390x2701 00111'
400x2801 01000(
410x2901 01001)
420x2a01 01010*
430x2b01 01011+
440x2c01 01100,
450x2d01 01101-
460x2e01 01110.
470x2f01 01111/
480x3001 100000
490x3101 100011
500x3201 100102
510x3301 100113
520x3401 101004
530x3501 101015
540x3601 101106
550x3701 101117
560x3801 110008
570x3901 110019
580x3a01 11010:
590x3b01 11011;
600x3c01 11100<
610x3d01 11101=
620x3e01 11110>
630x3f01 11111?


DecHexBinaryChar
640x4010 00000@
650x4110 00001A
660x4210 00010B
670x4310 00011C
680x4410 00100D
690x4510 00101E
700x4610 00110F
710x4710 00111G
720x4810 01000H
730x4910 01001I
740x4a10 01010J
750x4b10 01011K
760x4c10 01100L
770x4d10 01101M
780x4e10 01110N
790x4f10 01111O
800x5010 10000P
810x5110 10001Q
820x5210 10010R
830x5310 10011S
840x5410 10100T
850x5510 10101U
860x5610 10110V
870x5710 10111W
880x5810 11000X
890x5910 11001Y
900x5a10 11010Z
910x5b10 11011[
920x5c10 11100\
930x5d10 11101]
940x5e10 11110^
950x5f10 11111_


DecHexBinaryChar
960x6011 00000`
970x6111 00001a
980x6211 00010b
990x6311 00011c
1000x6411 00100d
1010x6511 00101e
1020x6611 00110f
1030x6711 00111g
1040x6811 01000h
1050x6911 01001i
1060x6a11 01010j
1070x6b11 01011k
1080x6c11 01100l
1090x6d11 01101m
1100x6e11 01110n
1110x6f11 01111o
1120x7011 10000p
1130x7111 10001q
1140x7211 10010r
1150x7311 10011s
1160x7411 10100t
1170x7511 10101u
1180x7611 10110v
1190x7711 10111w
1200x7811 11000x
1210x7911 11001y
1220x7a11 11010z
1230x7b11 11011{
1240x7c11 11100|
1250x7d11 11101}
1260x7e11 11110~
1270x7f11 11111DEL



The binary representation has the most significant bit first
		(“big endian”).

		ASCII is 7-bit; because many have called encodings such as 
		CP437,
		ISO-8859-1,
		CP-1252,
		and others “extended ASCII” some are under the misapprehension that
		ASCII is 8-bit (1 byte).

Understanding ASCII (and terminals)


To understand why Control+i inserts a Tab in your terminal you need to understand
			ASCII, and to understand ASCII you need know a bit about its history and the world it
			was developed in. Please bear with me.
Teleprinters
Teleprinters evolved from the telegraph. Connect a printer and keyboard to a
			telegraph and you’ve got a teleprinter. Early versions were called “printing
			telegraphs”.
Most teleprinters communicated using the ITA2 protocol. For the most part this would
			just encode the alphabet, but there are a few control codes: WRU (“Who R U”) would cause
			the receiving teleprinter to send back its identification, BEL would ring a bell, and it
			had the familiar CR (Carriage Return) and LF (Line Feed).
This is all early 20th century stuff. There are no electronic computers; it’s all
			mechanical working with punched tape. ITA2 (and codes like it) were mechanically
			efficient; common letters such as “e” and “t” required only a single hole to be
			punched.
These 5-bit codes could only encode 32 characters, which is not even enough for just
			English. The solution was to add the FIGS and LTRS codes, which would switch between
			“figures” and “letters” mode. “FIGS R W” would produce “42”. This worked, but typo’ing a
			FIGS or LTRS (or losing one in line noise) would result in gibberish. Not ideal.
Terminals
In the 1950s teleprinters started to get connected to computers, rather than other
			teleprinters. ITA2 was designed for mechanical machines and was awkward to use. ASCII
			was designed specifically for computer use and published in 1962. Teleprinters used with
			computers were called terminals (as in “end of a connection”, like “train
			terminal”). Teleprinters were also called
			“TeleTYpewriter”, or TTY for short, and you can still
			find names like /dev/tty or /bin/stty on modern systems.
People really programmed computers using teleprinters. Here’s a
			video of a teleprinter in action,
			and here’s a somewhat cheesy (but interesting and cute) video which explains how they
			were used to program a PDP 11/10.
A terminal would connect to a computer with a serial port
			(RS-232),
			which simply transfers bytes back and forth. A terminal is more akin to a monitor with a
			keyboard, rather than a computer on its own. A modern monitor connected with HDMI is
			told “draw this pixel in this colour”, in the 1960s the computer merely said “here are a
			bunch of characters”.
If you’re wondering what a “shell” is: a shell is a program to interact with your
			computer. It provides a commandline, runs programs, and displays the result. The
			terminal just displays characters. It’s the difference between a TV and a DVD
			player.
Teleprinters needed some way to communicate events such as “stop sending me data” or
			“end of transmission”. This is what control characters are for. The exact meaning of
			control characters has varied greatly over the years (which is why extensive
			termcap databases
			are required). ASCII is more than just a character set; it’s a way to communicate
			between a terminal and a computer.
An additional method to communicate are
			escape sequences.
			This is a list of characters starting with the ESC control character (0x1b). For example
			F1 is <Esc>OP and the left arrow is <Esc>[OD.
			Computers can give instructions to terminals, too: <Esc>[2C is move
			the cursor 2 positions forward and <Esc>[4m underlines all subsequent
			text. This is also how the Alt key works: Alt+a is <Esc>a.
Modern systems and ASCII properties
All of this matters because modern terminals operate on the same principles as those
			of the 1960s. If you’re opening three xterm or iTerm2 windows then you’re emulating
			three terminals connecting to a “mainframe”.
If you look at the ASCII table above then there are some interesting properties: in
			the 1st column you can see how the left two bits are always set to zero, and
			that the other 5 bits count to 31 (32 characters in total; it starts at 0). The
			2nd column repeats this pattern but with the 6th bit set to 1
			(remember, read binary numbers from right-to-left, so that’s 6th counting
			from the right). The 3rd column repeats this pattern again with the
			7th bit set, and the final column has both bits set.
The interesting part here is that the letters A-Z and some punctuation map directly
			to the control characters in the 1st column. All that’s needed is removing
			one bit, and that’s exactly what the Control key did: clear the 7th bit.
			Lowercase and uppercase letters align in the 3rd and 4th columns,
			and this is what the Shift key did: clear the 6th bit.
Pressing Control+i (lowercase) would mean sending “)”, which is not very useful. So
			most terminals interpret this as Control+I (uppercase), which sends HT. DEL is last is
			so all bits are set to 1. This is how you “deleted” a character in punch tapes: punch
			all the holes!
This is kind of neat and well designed, but for us it means:

There is no way to see if the user pressed only Control or Shift, because from a
					terminal’s perspective all they do is modify a bit for the typed character.
There is no way to distinguish between the Tab key and Control+i. It’s not just
					‘the same’ as Tab, Control+i is Tab.
There is no way to distinguish between Control+a and Control+Shift+a.
Sending Control with a character from the 2nd column is useless.
					Control clears the 7th bit, but this is already 0, so Control+# will
					just send “#”.

The world has not completely stood still and there have been improvements since the
			1960s, but terminals are still fundamentally ASCII-based text interfaces, and programs
			running inside a terminal – like a shell or Vim – still have very limited facilities for
			modern key events. Non-terminal programs don’t have these problems as they’re not
			restricted to a 1960s text interface.
Note: for brevity’s sake many
			aspects have been omitted in the above: ITA2 was derived from Murray code, the 1967
			ASCII spec changed many aspects (1962 ASCII only had uppercase), there were other
			encodings (e.g. EBCDIC), graphical terminals such as the Tektronix 4014 (which xterm can
			emulate), ioctls, etc.
			References and further reading:
				An annotated history of some character codes,
				7-bit character sets,
				Control characters in ASCII and Unicode,
				The TTY demystified





Image 1, a printing telegraph produced in 1907. The
					alphabetically sorted piano keys are a great example of how the first generation
					of new innovations tends to resemble whatever already exists, and that it takes
					a few more innovations to really get the most out of it. This style of piano
					keyboards was introduced in the 1840s, and while the keyboard as we know it
					today was introduced in the 1870s, it took a while for it to replace all
					piano-style keyboards; this is probably among the last models that was
					made).



Image 2, the Teletype model 33 ASR, introduced in 1963. This is
					one first ASCII teleprinters. Note the machinery on the left; you could feed
					this with a punched tape to automatically type a program for you, similar to how
					you would now load a program from a disk.
					The Teletype model 33 was massively popular, and the brand name Teletype became
					synonymous with terminal.
				



Image 3, Ken Thompson working on the PDP-11 using a Teletype
					(model 33?). What always struck be about this image is the atrocious ergonomics
					of … everything. The keyboard, the chair, everything about the posture: it’s all
					terrible. Inventing Unix almost seems easy compared to dealing with
					that!



Image 4, DEC VT100, a kind of terminal that a terminal emulator
				such as xterm emulates. It has a visual display and supports the essential escape
				sequences still in use today. These were known as “visual terminals”, referring to
				the visual screen with characters, as opposed to printing them out.




Created by Martin Tournoij,
		because I’ve had to explain “Control+i is Tab” once too many
		times and figured an in-depth explanation would be helpful.
Source on GitHub;
		PRs and issues welcome.

			Image credits:
			
				Image 1 by Science Museum; CC BY-NC-SA |
			
				Image 2 by AlisonW; CC BY-SA |
			
				Image 3 by Peter Hamer; CC BY-SA |
			
				Image 4 by Jason Scott; CC BY







"
https://news.ycombinator.com/rss,Granian – a Rust HTTP server for Python applications,https://github.com/emmett-framework/granian,Comments,"








emmett-framework

/

granian

Public







 

Notifications



 

Fork
    9




 


          Star
 352
  









        A Rust HTTP server for Python applications
      
License





     BSD-3-Clause license
    






352
          stars
 



9
          forks
 



 


          Star

  





 

Notifications












Code







Issues
7






Pull requests
0






Discussions







Actions







Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Discussions
 


                  Actions
 


                  Security
 


                  Insights
 







emmett-framework/granian









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











master





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





13
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




gi0baro

Update CI release workflow




        …
      




        b843a90
      

Jan 13, 2023





Update CI release workflow


b843a90



Git stats







132

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github



Update CI release workflow



Jan 13, 2023









benchmarks



Update benchmarks



Jan 13, 2023









docs/spec



Fix typos (#14)



Nov 17, 2022









granian



Follow WSGI spec on response iterable (#29)



Jan 13, 2023









lib/pyo3-asyncio



Bump pyo3-asyncio to 0.17



Oct 25, 2022









src



Code cleanup



Jan 13, 2023









tests



Fix wsgi.input out of spec (close #24)



Jan 12, 2023









.gitignore



first implementation



Apr 15, 2022









Cargo.lock



Add PyPy support



Jan 3, 2023









Cargo.toml



Add PyPy support



Jan 3, 2023









LICENSE



first implementation



Apr 15, 2022









README.md



Update benchmarks results



Dec 24, 2022









build.rs



Add PyPy support



Jan 3, 2023









pyproject.toml



Add PyPy support



Jan 3, 2023









setup.py



review package meta



Apr 18, 2022




    View code
 















Granian
Rationale
Features
Quickstart
Project status
License





README.md




Granian
A Rust HTTP server for Python applications.
Rationale
The main reasons behind Granian design are:

Have a single, correct HTTP implementation, supporting versions 1, 2 (and eventually 3)
Provide a single package for several platforms
Avoid the usual Gunicorn + uvicorn + http-tools dependency composition on unix systems
Provide stable performance when compared to existing alternatives

Features

Supports ASGI/3, RSGI and WSGI interface applications
Implements HTTP/1 and HTTP/2 protocols
Supports HTTPS
Supports Websockets over HTTP/1 and HTTP/2

Quickstart
You can install Granian using pip:
$ pip install granian

Create an ASGI application in your main.py:
async def app(scope, receive, send):
    assert scope['type'] == 'http'

    await send({
        'type': 'http.response.start',
        'status': 200,
        'headers': [
            [b'content-type', b'text/plain'],
        ],
    })
    await send({
        'type': 'http.response.body',
        'body': b'Hello, world!',
    })
and serve it:
$ granian --interface asgi main:app

You can also create an app using the RSGI specification:
async def app(scope, proto):
    assert scope.proto == 'http'

    proto.response_str(
        status=200,
        headers=[
            ('content-type', 'text/plain')
        ],
        body=""Hello, world!""
    )
and serve it using:
$ granian --interface rsgi main:app

Project status
Granian is currently under active development.
Granian is compatible with Python 3.7 and above versions on unix platforms and 3.8 and above on Windows.
License
Granian is released under the BSD License.









About

      A Rust HTTP server for Python applications
    
Topics



  python


  rust


  http


  http-server


  asyncio


  asgi



Resources





      Readme
 
License





     BSD-3-Clause license
    



Stars





352
    stars

Watchers





6
    watching

Forks





9
    forks







    Releases
      13







Granian 0.2.1

          Latest
 
Jan 13, 2023

 

        + 12 releases





Sponsor this project



 

 

 Sponsor
 
Learn more about GitHub Sponsors







    Packages 0


        No packages published 







        Used by 6
 




























    Contributors 4





 



 



 



 







Languages












Rust
83.1%







Python
16.5%







Other
0.4%











"
https://news.ycombinator.com/rss,Wikipedia editors serving long sentences in Saudi Arabia since 2020,https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-01-16/Special_report,Comments,"



Wikipedia:Wikipedia Signpost/2023-01-16/Special report - Wikipedia


































Wikipedia:Wikipedia Signpost/2023-01-16/Special report

From Wikipedia, the free encyclopedia
< Wikipedia:Wikipedia Signpost‎ | 2023-01-16


Jump to navigation
Jump to search
Coverage of 2022 bans reveals editors serving long sentences in Saudi Arabia since 2020: Long-time contributors imprisoned for 32 and 8 years after ""swaying public opinion"" and ""violating public morals"".

← Back to ContentsView Latest Issue16 January 2023
Special report
Coverage of 2022 bans reveals editors serving long sentences in Saudi Arabia since 2020


Contribute  —  
Share this


 PDF download
 E-mail
 Facebook
 Twitter
 LinkedIn
 Reddit



By Andreas Kolbe and JPxG




Former Arabic Wikipedia administrators Osama Khalid (left) and Ziyad Alsufyani (right), both now in prison in Saudi Arabia.



Related articlesNation-state involvement

Missed and Dissed
28 November 2022
Editor given three-year sentence, big RfA makes news, Guy Standing takes it sitting down
26 June 2022
A net loss: Wikipedia attacked, closing off Russia? welcoming back Turkey?
30 September 2019
WMF staff turntable continues to spin; Endowment gets more cash; RfA continues to be a pit of steely knives
31 January 2019
Court-ordered article redaction, paid editing, and rock stars
1 December 2018





More articles





Wales in China; #Edit2015
16 December 2015
Russia temporarily blocks Wikipedia
26 August 2015
Turkish Wikipedia censorship; ""Can Wikipedia survive?""; PR editing
24 June 2015
China blocks secure version of Wikipedia
5 June 2013
French intelligence agents threaten Wikimedia volunteer
8 April 2013
Russian Wikipedia shuts down to fight censorship threat; E3 team and new tools; Wikitravel proposal bogged down
9 July 2012
Censorship, social media in schools, and more
30 March 2009







Wikipedians jailed for 32 and 8 years respectively
On January 5, 2023, we learnt that two Wikipedians, Osama Khalid (User:OsamaK) and Ziyad Alsufyani (User:Ziad), have been sitting in jail for more than two years, sentenced to serving 32 and eight years respectively in al-Ha'ir Prison, a Saudi Arabian maximum security facility. The offenses with which they were charged, according to the press release that broke the news, were ""swaying public opinion"" and ""violating public morals"".
The press release in question was published jointly by Democracy for the Arab World Now (DAWN, a human rights organisation co-founded by slain Saudi journalist Jamal Khashoggi) and Lebanese NGO Social Media Exchange (SMEX). It said that Osama and Ziyad had been arrested on the same day in 2020, and sentenced to 5 and 8 years respectively. In September 2022, Osama's sentence was increased to 32 years after an appeal was made; this reflects a recent trend in Saudi Arabia of imposing ever more draconian prison sentences for online criticism of the Saudi government, as reported by human rights organisation ALQST and The Washington Post. DAWN reports that in 2022, Saudi Arabia's Specialized Criminal Court sentenced women to 34 and 45 years of imprisonment for ""tweeting in support of reform"".
The DAWN/SMEX press release combined its report on Osama's and Ziyad's prison sentences with the news that the WMF had recently banned sixteen Wikipedians in the Middle East/North Africa region, including seven Arabic Wikipedia administrators, for alleged conflict-of-interest editing and advancing ""the aims of external parties"" (see Signpost coverage earlier this month).




DAWN (Democracy for the Arab World Now) was co-founded by murdered Saudi journalist Jamal Khashoggi


Internal Wikimedia Investigation Results in Termination of Entire Saudi-Based Team of Administrators
(January 5, 2023 – New York and Beirut): The Saudi Arabian government infiltrated Wikipedia by recruiting the organization's highest ranked administrators in the country to serve as government agents to control information about the country and prosecuting those who contributed critical information about political detainees, said SMEX and Democracy for the Arab World Now (DAWN) today.

Following an internal investigation in 2022, Wikimedia terminated all of its Wikipedia administrators in Saudi Arabia in December. DAWN and SMEX documented Wikipedia's infiltration by the Saudi government based on interviews with sources close to Wikipedia and the imprisoned administrators.
The authors of the press release added:

It's wildly irresponsible for international organizations and businesses to assume their affiliates can ever operate independently of, or safely from, Saudi government control.
The DAWN/SMEX press release was quickly picked up by AFP, resulting in a spate of media reports led by The Guardian and Middle East Eye, followed the next day by Ars Technica and many others.
While these press articles followed the pattern set by DAWN and SMEX, covering the sixteen WMF bans and the imprisonment of the two editors together, it is unclear what connection there is between these two sets of events, or indeed if there is any connection at all. Ars Technica hypothesizes that the prior arrest of Osama and Ziyad may have been related to Saudi infiltration efforts that led to the bans. The Wikimedia Foundation's Trust & Safety office has stated that the December 2022 bans were unrelated to the 2020 arrests.

Who are the jailed Wikimedians?



Osama organized this Wikipedia medical training and editing event at King Saud bin Abdulaziz University for Health Sciences in 2015


Both were longstanding Wikimedia contributors. Osama's first contributions to the English and Arabic Wikipedias date back to 2007. All in all, he made over 870,000 contributions to Wikidata, over 19,000 to the English Wikipedia, around 16,500 to the Arabic Wikipedia, over 16,000 to Commons, over 5,000 to the Arabic Wiktionary, and nearly 800 to Meta-Wiki. 
Ziyad started editing Arabic Wikipedia in 2009, making over 20,000 edits to Wikidata, around 7,500 to Commons, about 6,500 to Arabic Wikipedia, and exactly 100 to English Wikipedia. As medical students, both were particularly involved in editing and translating medical topics in Wikipedia. The Wiki Project Med Foundation, a Wikimedia affiliate specialising in improving Wikimedia projects' coverage of medical topics, issued the following statement to The Signpost: 

Wiki Project Med appreciates the medical editing which Osama Khalid and Ziyad Alsufyani contributed to Wikipedia. They are both Wikimedia editors in good standing who have organized medical editing, training of physicians to edit Wikipedia's medical topics, and good community discussions about improving Wikipedia's coverage of medical topics for Arabic language. The arrest is shocking to us and beyond our understanding. We know nothing about this except that these two are friendly Wikipedia editors who have been highly engaged in our Wikimedia community activities.



Ziyad uploaded this picture of himself to Commons in 2015, with the description ""Arabic Wikipedian"".


Both attended Wikimedia conferences. Osama joined multiple Wikimania events in person, and participated in the medical meetups there (see images on Wikimedia Commons); he also organized the Translation task force, importing Wikipedia medical articles from English to Arabic (and from Arabic to English).

Wikimedia responses to press coverage
Responding to the media coverage, Wikimedia Foundation spokespeople highlighted ""material inaccuracies"" in the press release. According to Ars Technica, for example:

A Wikimedia spokesperson told Ars that there are ""material inaccuracies in the statement released by SMEX/DAWN"" and in a Guardian report. ""There was no finding in our investigation that the Saudi government 'infiltrated' or penetrated Wikipedia's highest ranks,"" Wikimedia's spokesperson told Ars. ""And there are in fact no 'ranks' among Wikipedia admins. There was also no reference to Saudis acting under the influence of the Saudi government in our investigation. While we do not know where these volunteers actually reside, the bans of any volunteers who may have been Saudi were part of a much broader action globally banning 16 editors across the MENA region.""
The Wikimedia Foundation also published a longer statement on the Wikimedia-l mailing list on 6 January, titled ""Recent press around December Office Action"": 

Hello everyone,
Over the last couple of days, there have been several media reports about
the Foundation’s most recent office action, taken on December 6.
More are certain to follow. These media reports are based on a release from
SMEX and Democracy for the Arab World Now (DAWN) that contains many
material inaccuracies. Some of the errors will be obvious to our community
– for perhaps the most obvious, the report states that the 16 users are all
based in Saudi Arabia. This is unlikely to be the case. While we do not
know where these volunteers actually reside, the bans of any volunteers who
may have been Saudi were part of a much broader action globally banning 16
editors across the MENA region. Indeed, many of them are not active in the
Arabic language projects. These organizations did not share the statement
with the Foundation, and ""sources of knowledge"" as cited in their release
can get things wrong. In addition, we do not have staff in the country
named and never have, contrary to a message put out by the same groups on
social media.
As we noted in December in our statement, we are unable to discuss
Foundation office actions in detail. The Foundation always lists accounts banned as a result of its investigations.
It is our goal to be as transparent as we can be within essential
protection policies, which is why we do not ban in secret, but instead
disclose accounts impacted and (when large numbers are involved) have
disclosed the rationale.
The roots of our December action stretch back over several years. We were
initially contacted by outside experts who made us aware about concerns
they had about Farsi Wikipedia. We can’t comment on that report right now,
but it will be published by that organization soon. This report not only
contributed to our August 23, 2021 modification of our non-disclosure agreement to make it harder for rights-holders to be coerced, but led to further
evaluation of issues across MENA. The December bans were the culmination of
those evaluations.
Wikimedia is, as mentioned above, an open knowledge platform, and it
thrives on open participation. Investigations and global bans are not
things that any of us take lightly, but the Foundation is committed to
supporting the knowledge-sharing models that have created so many valuable
information resources in hundreds of languages across the world. Our first
line of defense of our Terms of Use are our volunteers themselves. Where issues present a credible threat of
harm to our users and to the security of Wikimedia platforms, we will do
the best we can to protect both.
We trust and hope that our communities understand that misinformation about
this action has the potential to cause harm to the individuals involved. We
believe in the incredible value produced by our volunteers across the
globe, but even so we recognize that being found in contravention of a
website’s Terms of Use — even in a manner that organization finds serious
enough to warrant a ban — is not the equivalent of being convicted of any
crime. Accordingly, we ask you to please be conscious of the real people
involved, in the spirit of our long established respect for living people on our sites. We realize that it is tempting to speculate, but we do ask you all to
recall that people’s employment options, their relationships, and even
their physical safety may be compromised by speculation.
If anyone feels unsafe on Wikimedia projects, please use the local
community processes or contact us. The Foundation and community will work
together or in parallel to enhance the safety of all volunteers. To contact
the Trust & Safety team please email ca(a)wikimedia.org .
Best regards,
WMF Office/Trust and Safety
Analysis



Sarah Leah Whitson, the Executive Director of DAWN, is a former director of the Middle East and North Africa division of Human Rights Watch.


Notably, this statement does not contain any reference to the two imprisoned Wikipedians. On the other hand, it does express consideration for the people behind the accounts banned last month, whose role in Wikipedia has suddenly become international news, in a way the Wikimedia Foundation clearly had not intended during their initial listing of the bans.
Democracy for the Arab World Now (DAWN) Executive Director Sarah Leah Whitson, a Human Rights Watch veteran, responded to the WMF statements in an update to the Ars Technica article, added a few hours after publication: 

Whitson told Ars that Wikimedia is ""playing technical word games"" in its statement and that ""it's really important for Wikimedia to be transparent about what they have described as a conflict of interest among its editors."" She said that Wikimedia should ""provide more transparency about the 16 users that they banned"" and ""the safety precautions they're going to take to avoid further endangering Wikipedia editors in totalitarian states, because there's no denying that two of them are now languishing in Saudi prisons"" and the problem goes ""well beyond Saudi Arabia."" Whitson urges Wikimedia to reconsider its global model of relying on Wikipedia editors based in totalitarian states, not just because it can endanger the editors, but also because Wikipedia ""loses its credibility"" when information edited in these states cannot be trusted.
These are important points. The WMF is now widely reported to have ""denied claims the Saudi government infiltrated its team in the Middle East"" – as a BBC article puts it – but this does create some inconsistencies. A month ago, on December 6, the WMF's Trust & Safety office issued a confident assertion that ""we were able to confirm that a number of users with close connections with external parties were editing the platform in a coordinated fashion to advance the aim of those parties"". The post stated that ""these connections are a source of serious concern for the safety of our users that go beyond the capacity of the local language project communities targeted to address"" and emphasised that the Foundation had issued these bans ""to keep our users and the projects safe"". But it has provided no information on who these parties threatening users' safety are, if they are indeed unrelated to the Saudi government.
The WMF statement does mention that the roots of the December 2022 bans lie in concerns expressed to the WMF about the Farsi Wikipedia some years ago. There is a public record of concerns about state interference in the Farsi Wikipedia being voiced by Open Democracy, for example, in a September 2019 article titled ""Persian Wikipedia: an independent source or a tool of the Iranian state?"", and by Justice for Iran in an October 2019 Radio Farda article titled ""Critics Say Some Persian Wikipedia Content Manipulated By Iran's Government"".




Radio Farda, the Iranian branch of the U.S. government-funded Radio Free Europe/Radio Liberty, reported on alleged manipulation of Farsi Wikipedia content by Iran's government in 2019. The WMF says concerns expressed about the Farsi Wikipedia a few years ago eventually led to its 2022 investigation that resulted in 16 global bans in December 2022, including bans of seven Arabic Wikipedia administrators


The DAWN/SMEX press release and the many press reports based on it did contain errors. The press release referred to ""16 Saudi administrators""; as reported earlier this month in The Signpost, only seven of the ten banned Arabic Wikipedia users were administrators, and six of the 16 banned users were contributors to the Farsi Wikipedia rather than the Arabic Wikipedia. Moreover, Osama and Ziyad, the two imprisoned Wikipedians, were not administrators at the time of their arrest – both had had their admin rights on Arabic Wikipedia withdrawn years before. The reason? They weren't using them, both having scaled down their Wikipedia activity considerably in recent years, presumably to focus on their medical studies. Ten years ago, however, Osama had uploaded pictures of a number of Saudi human rights activists to Commons; Ziyad uploaded Wikipedia's image of Loujain Alhathloul in 2016.
The headline of the article in The Guardian read: ""Saudi Arabia jails two Wikipedia staff in 'bid to control content'"". This will have left many readers once again with the false impression that Wikimedia Foundation staff administer Wikipedia's day-to-day content and community processes. (There is a reason headlines are not considered reliable sources in Wikipedia – the body of The Guardian's article referred correctly to ""volunteer administrators"".)
The WMF's claim that admins have ""no ranks"", however, is less persuasive. Two of the banned users, for example, had bureaucrat and checkuser rights in addition to administrator privileges (elevated rights that reqire users to sign non-disclosure agreements). Moreover, the entire Arabic Wikipedia – a project with 1.2 million articles – only had a grand total of 26 administrators prior to the global bans (it is now down to 20). To a person in the street, surely that makes any of the 26 people administering the project ""high-ranking"". 
Even more significant is the fact that the banned Arabic Wikipedia administrators include three of the four people who founded the Saudi Wikimedia User Group, the Wikimedia Foundation's official affiliate in Saudi Arabia – among them the affiliate's principal contact person. In total, seven of the ten banned Arabic users are listed as members of the Saudi user group. As for the other three, two, including one of the checkusers, say on their user pages that they are members of the Arabian Gulf Wikimedia User Group, which does not seem to be an officially recognised affiliate yet, and one (the other checkuser) says they're from Kuwait.
The Wikimedia Foundation made another statement on 8 January, saying, in part:

Our investigation and these bans are not connected to the arrest of these two users. The ban decision impacted 16 users, not all of whom were administrators, from Arabic and Farsi Wikipedia. As stated below, we have no reason to believe that these individuals are all residents of Saudi Arabia; on the contrary, this seems extremely unlikely. Further, we imagine you are all aware that editors are volunteers, not paid by the Foundation, and that the Foundation does not have offices or staff in Saudi Arabia.
While, as stated, the December office action is unrelated to the arrests of
two Wikimedians in Saudi Arabia, the safety of Wikimedia volunteers always
remains our utmost concern. We understand the desire to take action or
speak out. Know that we need to act in the interests of any volunteer whose
safety is under threat. As indicated in yesterday's message, additional
publicity around such cases can cause harm, as can speculation and
misinformation. We are confident that everyone values the safety of their
fellow volunteers and can understand the constraints this might create. 
Arabic Wikipedia community statement



The Arabic Wikipedia community has condemned the WMF action, arguing the bans are at odds with the model of decentralized governance that the Foundation always talks about.


The Arabic Wikipedia community has released a statement on the global bans, adopted with 38 in support, 2 opposed, and 0 neutral. What follows is an English translation of the community statement originally issued in Arabic:

Wikipedia: Statement regarding the events of December 6, 2022
This is a statement issued by the Arabic Wikipedia community to comment on the events of December 6, 2022, and the accompanying global ban that included ten user accounts on the Arabic Wikipedia, including seven administrators.
In the Arabic Wikipedia, we focus on a decentralized governance model in which all community members play roles in the decision-making process, oversight over the drafting of the encyclopedia's policies as well as guidelines, and their enforcement. This can be achieved through direct participation in the election of administrators, and in resolving conflicts and disagreements that occur in the encyclopedia. We do expect the Wikimedia Foundation, which has always supported this governance model, to follow it when dealing, not only with the Arabic community but with all other communities to ensure full transparency and mutual accountability.
We do condemn, in the strongest terms, the work model based on confidential complaints and non-public investigations, which creates a toxic work environment that is incompatible with the nature of volunteering and undermines the main Wikipedia principles of transparency and the assumption of good faith. At the same time, we call on the Foundation to adopt a transparent model in which it has no guardianship over communities, and where it accepts, without restrictions, mutual accountability from communities. The relationship should be based on the grounds that all parties, involved in a transparent governance process, are equal in all the stages of the process.
We also understand the existence of complications associated with attempts to manipulate the content of the Arabic Wikipedia, to polish or distort the image of certain parties; we condemn all these attempts without any reservations and stress the need for Wikipedia to be a platform that adopts a neutral point of view. At the same time, we call on the Foundation to involve local communities in the content protection process by sharing information with them in a way that does not harm the privacy of the users involved in the process and does not put them at risk.
If a user violates the policies, even if they hold administrator rights, they will be dealt with firmly in accordance with the local policies approved by our community. We do not tolerate the abuse of administrative powers nor the manipulation of encyclopedic content to serve third parties whatsoever, including directed editing, and we have policies governing these matters. They apply to all users equally without distinction. Therefore, we are surprised, in light of all this, that the institution imposes its supervision on our self-governing society without prior notice and issues irrevocable decisions without explanation.
We also point out the severe harm that the ban has done to our local community. We lost seven active administrators in one fell swoop! This represents 30% of the administrators in our community, including two bot operators. This has set our community back years and does not, surely, contribute to encyclopedia growth. Mainly, we have suffered the consequences of this ban at the technical level in the encyclopedia, and we appeal to the technical team in the Foundation and the open-source communities to provide the necessary technical assistance to maintain the continuity of the project as much as possible.
The Arabic community has chosen a committee of four people to follow up with the Wikimedia Foundation on the basis of mutual accountability on the issue of the above bans. We are waiting, and we hope, for the Wikimedia Foundation to cooperate with this committee, facilitate its work and share with it the information in its possession without harming the privacy of any user on the Arabic Wikipedia or its sister projects.
Wikimedia Foundation reply posted on the Arabic Wikipedia



Vinicius Siqueira, Osama Khalid, Netha Hussain, Emily Temple-Wood, Anthony Cole, Jake Orlowitz, Daniel Mietchen, Lane Rasberry, James Heilman and Peter Coti (clockwise starting front left) at a WikiProject Med meetup at the 2013 Hong Kong Wikimania conference


On 10 January, the Wikimedia Foundation replied to the Arabic Wikipedia community statement on the associated talk page. It is the first Foundation statement to actually use the imprisoned Wikipedians' names. The reply was posted in Arabic; a machine-aided (Google/Bing) translation follows below:

Update from the Wikimedia Foundation
Hello all
We know the past few weeks have been difficult for the community. We also realize that this situation remains confusing and worrying in light of the media reports that have emerged. As an organization, we regret the distress and concern this situation has caused the community. While we know we can't answer all of your questions, we want to make sure you understand our processes and the rationale behind them. We also want to ensure that our actions are in the best interests of the community to the best of our ability and with the tools available to us. As mentioned, the measures were not linked in any way to the recent media reports that are currently circulating, nor in any way to the arrests. The Foundation has learned of the arrest of Osama and Ziyad, and is actively following up on their situations.
As we know that not everyone will have read all of the data, we would like to reiterate that the process of reaching the decision to take action in December 2022 was not easy or rushed. The investigation into violations of the Terms of Use took a long time starting with the Persian Wikipedia and moving on as new information emerged, and the final decision was guided by multiple levels of review by several employees across different functions. After consideration, it was unanimously agreed that the action is necessary to keep the community and platforms safe. Proper implementation of this measure was equally important in keeping the community and platforms safe, and thus adhering to established policies and procedures.
We realize that media reports and recent actions in December 2022 make many of you skeptical and perhaps even apprehensive about participating in the projects. We want you to know that the projects are owned by everyone, and most of all, that you are the creators and curators of the content. Foundation interventions in content or management issues on the sites are rare and limited to exceptionally problematic circumstances. No one should fear that the Foundation will take action on unintentional mistakes made while participating as editors in good faith.
As many of you already know, the Foundation fully supports community autonomy and the principle of subsidiarity as part of our commitment to respecting and promoting community autonomy. Not only do we feel this is the right approach to our shared values, but it is the only approach that can make these amazing projects work. To ensure we maintain this commitment, we do not deal with general community or community member disputes that might otherwise be addressed through existing community actions, nor do we act as a means of appealing community policies and decisions. If such situations arise, we look forward to working to help the community members who need help, but most of the time, this assistance will consist of guiding the community members to find the right community avenue that will solve their problem.
On some occasions, the Foundation considers cases of abuse. This only occurs when it has been brought to our attention that the local community lacks the necessary processes to effectively address the situation, or when the organization has a legal obligation as a platform provider to act in the interests of the safety of users and the platform. When we intervene, we are limited in the course of action we can take. Our procedures are guided by the Office's work policies, which allow us to issue global bans, event bans, issue warnings, interaction bans, and advanced permission removal. While this responsibility rests with us, we do not take our interventions lightly; these investigations take a lot of time and effort and require multiple staff members across different departments to ensure that we provide a comprehensive understanding of the matter before we take any action. For the size of our communities, we have issued very few centralized global bans. Collective global bans like the one we issued in December 2022 are only put in place in the most exceptional circumstances, when the evidence strongly supports a serious threat to the organization's Terms of Use that all contributors must agree to abide by when editing the projects.
Our December 6 Office action was the result of the Foundation's multiple, long-term investigations undertaken as part of our duties as a platform provider. It was not related to the media reports currently circulating. While there are still limits to what we can disclose in order to protect the safety and privacy of our users, we truly understand and sympathize with the fact that this continues to be an upsetting situation and would like you to know that we would not have taken this action if it were not necessary.
We also want to acknowledge that the media reports have created significant doubt in people's minds about the safety of participating in Wikimedia projects, because of their direct linkage to cases of volunteers being arrested. It is unfortunate that many organizations relied on incomplete facts and indirect sources in their coverage, which directly contradicts our principles. Regardless of the current situation, the Foundation is well aware that such risks exist globally, and we want our community members to be aware too - and work with us to take precautions to stay safe. Six months ago, the United Nations published an article describing the rise of disinformation as a ""global disease"".
In late May 2020, the Board included protecting projects and communities from ""misinformation and bad actors"" in its Statement on Community Culture. On August 23, 2021, we amended our Non-Disclosure Agreement to make it more difficult to coerce rights holders, by restricting access in certain high-risk regions where individuals may be particularly vulnerable to threats to themselves and their families. We continue to work to secure the safety of those combating this ""global disease"" – disinformation – not just through Office actions but in terms of proactively encouraging safe practices, as in our recent blog post on protecting online anonymity. This assessment by external experts has identified a number of areas to support our approach, the Board has issued a policy symbolizing our commitment to this improvement, and our Human Rights Team continues to work to provide resources of information and support to users on the ground. We are also working on making additional digital security resources available to community members who feel unsafe online, which we will finalize soon.
We respect and realize that this action represents a major setback for the community and that is why we are open to providing the community with the support needed and what help we can provide. If there is anything we can do to help the community during this time, please do not hesitate to let us know via ca@wikimedia.org. As mentioned earlier, we are ready to provide you with the required support to the best of our ability.
Best Regards,
Wikimedia Foundation Office WMFOffice (talk) 09:09, 10 January 2023 (UTC)
Much to ponder
The WMF mentioned a change to the Non-Disclosure Agreement in the statements above. This concerns a document VRT volunteers, CheckUsers, Oversighters and Stewards are required to sign. The change, made on 23 August 2021, added the following words to the relevant page on Meta-Wiki:

The Foundation shall not grant Foundation volunteer NDA recognition to applicant(s) for volunteer roles if the applicants live in jurisdictions that block(ed) access to Wikimedia projects AND there is reason to believe that their domicile is known to others than the individual applicant(s) and the Foundation. Exemptions may be granted in individual cases following a request for review by the Legal department. Granting such NDAs would put the applicant(s) as well as other volunteers relying on the Foundation’s platform at undue risk. All NDA-based access rights granted to users fulfilling both criteria in the proposed adjustment shall be revoked at the point of policy adjustment.
This still seems weak, given the risk of decade-long prison sentences served in high-security facilities. Even if an editor's place of residence is only known to them and the Foundation today, there is no guarantee at all that others won't discover it at some point in the future. A checkuser whose identity becomes known to a present (or future) authoritarian government would not just be at risk personally, but could also be compelled – legally or otherwise – to collect user data and pass these on to state organs, putting other users at risk of prosecution.
There is much to ponder here about project governance, government influence on Wikimedia projects, and the vulnerability of editors and administrators to coercion and imprisonment. But the most pressing question is perhaps what we, as a movement, can do to help Osama and Ziyad. 
The Wikimedia Foundation, DAWN and SMEX clearly got off on the wrong foot – it would be good to see them engage in constructive dialogue now, and pool their resources, at least inasmuch as our fellow Wikimedians are concerned. According to DAWN Executive Director Sarah Leah Whitson, who discussed the case with The Signpost, campaigning for their release at this point, over two years into their sentences, is very unlikely to do them harm, and may do some good.

External links
""Foundation Trust & Safety action in the MENA Region"", Wikimedia-l mailing list thread, 6 December 2022 onwards
""Saudi Arabia: Government Agents Infiltrate Wikipedia, Sentence Independent Wikipedia Administrators to Prison"", DAWN press release, 5 January 2023, also published on the SMEX website
""Saudi Arabia jails two Wikipedia staff in 'bid to control content'"", The Guardian, 5 January 2023
""Saudi Arabia 'infiltrated' Wikipedia to control content, activists say"", Middle East Eye, 5 January 2023
""Wikipedia admin jailed for 32 years after alleged Saudi spy infiltration"", Ars Technica, 6 January 2023
""Recent press around December Office Action"", Wikimedia-l mailing list thread, 6 January 2023 onwards
""Wikipedia operator denies Saudi infiltration claim"", BBC, 7 January 2023, edited 10 January 2023
""Saudi Government Narrative Control Efforts Now Include The Jailing Of Wikipedia Administrators"", TechDirt, 10 January 2023






← Previous ""Special report""
In this issue16 January 2023From the team
Special report
News and notes
In the media
Technology report
In focus
Serendipity
Gallery
Humour
Opinion
Featured content
Traffic report
From the archives


+ Add a commentDiscuss this story
These comments are automatically transcluded from this article's talk page. To follow comments, add the page to your watchlist. If your comment has not appeared here, you can try purging the cache.

From what I remember reading on the Arabic Wikipedia discussion about the bans, there were a significant number of other editors there making blatantly pro-SA government statements and were angry at the editor accounts being banned in relation to that. I have concerns that the Arabic (and possibly Persian) language Wikipedia communities are entirely subsumed by blatantly biased pro-government accounts. Because the reason for the bans was never a mystery to anyone, not seriously. Even if the WMF has been trying to be vague about it all. Even this very Signpost article is quite clear and direct on the fact that we all know that the banned accounts were people working directly for the SA government in order to push their own personal views of events and to downplay the ongoing human rights atrocities that Saudi Arabia's administration is committing. With our unfortunate two editors discussed above being only a single example among many. SilverserenC 05:43, 16 January 2023 (UTC)Reply[reply]

That is an absolute monarchy for you.  scope_creepTalk 13:38, 16 January 2023 (UTC)Reply[reply]
I've said this before, but I believe that if there's any way for the WMF to use its considerable funds and influence to promote the spread of free knowledge in autocratic nations, then that should be one of its highest priorities. Free knowledge is why we're here. We as the Wikipedia communities, regardless of language, should be some of Khalid and Alsufyani's strongest advocates. Thebiguglyalien (talk) 17:38, 16 January 2023 (UTC)
Reply[reply]






The Signpost needs your help putting together the next issue.


Home
About
Archives
Newsroom
Subscribe
Suggestions





Retrieved from ""https://en.wikipedia.org/w/index.php?title=Wikipedia:Wikipedia_Signpost/2023-01-16/Special_report&oldid=1134032349""
Categories: Wikipedia Signpost archives 2023-01Wikipedia Signpost RSS feed



Navigation menu



Personal tools


Not logged inTalkContributionsCreate accountLog in





Namespaces


Project pageTalk





English









Views


ReadEditView history





More

























Navigation


Main pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate




Contribute


HelpLearn to editCommunity portalRecent changesUpload file




Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage information




Print/export


Download as PDFPrintable version




Languages



Add links






 This page was last edited on 16 January 2023, at 17:59 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License 3.0;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Mobile view
Developers
Statistics
Cookie statement








"
https://news.ycombinator.com/rss,Rendering like it's 1996 – Bitmap fonts and DOS,https://marioslab.io/posts/rendering-like-its-1996/dos-nostalgia/,Comments,"













Mario's Lab





Mario's Lab
Mastodon
Twitter
Github
RSS



Rendering like it's 1996 - Bitmap fonts and DOS
December 07, 2022




This screen has burned itself into my retina.



To follow along this blog post with running code, make sure you've installed the prerequisites. Then:

git clone https://github.com/badlogic/r96
cd r96
git checkout 04-dos-nostalgia
./tools/download-tools.sh
code .

Last time we learned about loading images and blitting. That was over 3 weeks ago, making me miss my target of posting one series entry a week. But there's a reason for it! I was rather busy in those two weeks.
After using Hopper to generate control flow graphs to discuss performance optimization, I got a little sick of the workflow and built my own assembly CFG viewer. Just paste some x86 or ARM assembly generated by MSVC, Clang, or GCC into the left panel, and view the control flow graph of each function on the right. I also made it a re-usable NPM package. Going forward, I can embed those fancy CFGs directly.
Then I drifted off into yet another rabbit hole. Spurred by a mean comment on Reddit about how the r96 code doesn't even run in DOS, I made the code of the series run in DOS.
First, I built a DOS backend for MiniFB. Then, I forked an old GDB version which is capable of remotely debugging 32-bit protected mode DOS programs as produced by DJGPP, the GCC fork I use to build C/C++ DOS programs. I also forked DOSBox-x to fix it up so my forked GDB can actually connect to DOS programs via the serial port/TCP emulation.
Finally, I took the barely functional GDB stub that comes with DJGPP, rewrote it and added a ton of functionality to it, so I can now debug DOS programs running in DOSBox-x from the comforts of Visual Studio Code.
All of that work culminated in a VS Code extension, which lets you go from 0 to debugging a simple DOS mode 13h demo app in VS code in about 80 seconds:

With all of that out of my system, I built some shell scripts that will help you install (almost) all the tools to compile, run, and debug the r96 project for desktop, web, and DOS. And I added some VS Code magic so you can comfortably start debugging sessions on each platform.
And to round it all off, I cleaned up the Git repo, so each blog post maps to exactly one commit. And I rewrote the first 3 blog posts in the series. So yeah.
I can now happily continue writing the series. Promise. Unless I'll add Android and iOS support in the future. I currently don't feel that specific masochism piling up inside of me.
Today, we're looking at DOS support, and then load and draw some bitmap fonts.
Demo: Hello DOS
Alright, go get the latest and greatest from the r96 repository. Follow the README.md to install the tools, including the new DOS tools. The README.md will also get you up to speed on how to build and debug everything in VS Code or on the command line. Or, if you want a detailed run-down of the project and its build and IDE support, read the first entry of the series.
To celebrate DOS support, I've added a new demo called 12_hello_dos.c:

#include <MiniFB.h>
#include <stdio.h>
#include ""r96/r96.h""
#include ""stdlib.h""
#include <math.h>

#define GDB_IMPLEMENTATION
#include ""dos/gdbstub.h""

#define num_grunts 100

typedef struct grunt {
	int x, y, vx, vy;
} grunt;

int main(void) {
	gdb_start();
	r96_image image;
	if (!r96_image_init_from_file(ℑ, ""assets/grunt.png"")) {
		printf(""Couldn't load file 'assets/grunt.png'\n"");
		return -1;
	}

	r96_image output;
	r96_image_init(&output, 320, 240);
	struct mfb_window *window = mfb_open(""12_hello_dos"", output.width, output.height);

	grunt grunts[num_grunts];
	for (int i = 0; i < num_grunts; i++) {
		grunt *grunt = &grunts[i];
		grunt->x = rand() % 320;
		grunt->y = rand() % 200;
		grunt->vx = 1;
		grunt->vy = 1;
	}
	do {
		r96_clear_with_color(&output, 0xff222222);
		for (int i = 0; i < num_grunts; i++) {
			grunt *grunt = &grunts[i];
			if (grunt->x < 0) {
				grunt->x = 0;
				grunt->vx = -grunt->vx;
			}
			if (grunt->x > 320 - 64) {
				grunt->x = 320 - 64;
				grunt->vx = -grunt->vx;
			}
			if (grunt->y < 0) {
				grunt->y = 0;
				grunt->vy = -grunt->vy;
			}
			if (grunt->y > 240 - 64) {
				grunt->y = 240 - 64;
				grunt->vy = -grunt->vy;
			}
			grunt->x += grunt->vx;
			grunt->y += grunt->vy;
			r96_blit_keyed(&output, ℑ, grunt->x, grunt->y, 0x00000000);
		}
		if (mfb_update_ex(window, output.pixels, output.width, output.height) != STATE_OK) break;
		gdb_checkpoint();
	} while (mfb_wait_sync(window));

	r96_image_dispose(ℑ);
	r96_image_dispose(&output);
	return 0;
}

This is our first animated demo!
The demo draws 100 moving grunts, that bounce off of the screen boundaries. Each grunt is stored in a simple grunt struct, which in turn stores the grunt's position (x, y) and velocity on the x- and y-axis (vx, vy) in pixels per frame. During initialization, we give each grunt a random position within the screen boundaries and set its velocity on each axis to 1 (lines 29-35).
What's a frame you may ask? A frame can be many things, but in our case, a frame is simply one iteration of the main loop of your program (lines 36-62). In each frame, we check whether each grunt is still inside the screen boundaries. If a grunt is outside the screen boundaries on the x- or y-axis (or both), we move them back inside the bounds and negate their velocity on the axis they left the screen on.
E.g. a grunt moving to the right (vx = 1), leaving the screen on the x-axis (x > 320 - 64), will be moved back inside the screen boundaries (x = 320 - 64), and its velocity on the x-axis will become -1. Starting in the next frame, the grunt will then move to the left, until it exits the screen boundaries on the left side of the screen. The same happens on the y-axis.
Once all the checks are complete, we add the grunt's velocity to its position. Each frame, the grunt's position thus changes by vx pixels on the x-axis, and vy pixels on the y-axis. Hence why vx and vy are given as pixels per frame.

Note: This is a very basic form of explicit Euler integration. It's much less scary than it sounds! Go learn your fundamentals.

Now, there's one big problem with this type of moving objects: it depends on the speed of execution.
We call mfb_wait_sync(), which waits for a vertical refresh, effectively limiting the number of frames per second to the screen refresh rate, so 60Hz, 90Hz, 120Hz, or whatever other wonky screen refresh rate the display has.
On a 60Hz screen a grunt will thus move 60 pixels per second, on a 120Hz it will move 120 pixels.
For a game, that's not great: different players will experience the game at different speeds, depending on their hardware. We'll look into this issue in a future series entry.

Note: Many old DOS games actually did have this problem: they would not take into account how much time has passed since the last frame, but instead update game object positions at a fixed rate each frame. There's a reason Wikipedia has an entry on the notorious PC turbo button.

Here's the little demo on the web:





And here it is running in DOSBox-x, telling DOSBox-x to go full speed.

DOSBox-x on my system syncs to 60Hz in windowed mode, while Chrome runs the web demo at the full 120Hz of my display. In the video above, there is some smearing and artifacts. That's mostly due to the MP4 encoding and doesn't look like that when actually running the demo in DOSBox-x on your system.
Is the DOSBox-x performance indicative of performance on old systems? No. DOSBox-x is going full speed, which is way faster than what my old 486 could do. However, you can modify the emulation speed via the DOSBox-x menu CPU > Emulated CPU speed. In the following video, I've set the emulated CPU speed to be equivalent to a 486DX2 with 66Mhz:

While that's more accurate, it's still not quite the same as real hardware. To get a more accurate sense of how the program would perform on a real 486, we can use 86Box. 86Box is as cycle accurate emulator for various old x86 systems.

Looks like DOSBox-x isn't far off with its emulation. So why is it so slow?

Note: Setting up virtual machine images for 86Box is a bit terrible. I've created 2 images you can download, a 486 image and a Pentium image, pre-installed with MS-DOS 6.22, a mouse driver, and a CD-ROM driver. You can run them via 86box -c 486/86box.conf and 86box -c pentium/86box.conf. The images also include QBasic 1.1. And NIBBLES.BAS and GORILLA.BAS. Just saying.

Why is it so slow on a 486?
The MiniFB DOS backend sets up a video mode with either 24-bit or 32-bit color depth through VESA. MiniFB assumes 32-bit color depth, so we have to abide by that and go VESA.
This works pretty well from Pentium class machines onwards, if the (emulated) video card supports VESA. Here's the demo on Pentium class hardware in 86Box:

A 486 may support 24-bit and 32-bit color depth video modes, depending on the graphics card. Mine did. However, that doesn't mean the system is fast enough to actually deal with that amount of data. A run of the mill 486 would have memory throughput somewhere in the range of 10-25MB/s. You read that right.
In our demo above, we render to a 320x240 output r96_image. The call to r96_clear_with_color() has to touch 0.3MB worth of pixels. Rendering a single grunt means reading 64x64x4 bytes from the grunt image and writing them to a 64x64x4 bytes big region in the output r96_image. For 100 grunts, that's reading 1.6MB and writing 1.6MB. Finally, the output r96_image is transferred to the VESA linear buffer, a memory mapped region from which the graphics card will read what it should output to the display. That's another 320x240x4 bytes, or 0.3MB. Each frame we thus touch 0.3 + 1.6 + 1.6 + 0.3 = 3.8MB of memory. And while this simple analysis doesn't account for memory caches, it does align with what we experience when running the demo on a (emulated) 486. We do indeed only get something like 3-5 frames per second, which is 11.4-19MB of data pushed by the demo per second.
That's one of the reasons pretty much all older DOS games targeting 386 or 486 would use mode 13h or derivatives like Mode X. Both of these video modes use 8 bits to encode a pixel's color. But instead of directly encoding the color's red, green and blue component, the 8-bit value is an index into a palette with a total of 256 colors. That cuts down on memory and bandwidth needs considerably.
If we went mode 13h in our demo, we'd go from 3.8MB to 0.95MB of data per frame. That translates to 12-20 frames per second, which is still not great, but often playable enough. That's about the frame rate I got when playing MicroProse's Formula One Grand Prix on my 486.
So what's the solution? Draw less each frame! DOOM and Quake relied on various techniques like binary space partitioning to avoid drawing things that are invisible or occluded. Drawing less means touching less memory. Consider that 100 grunts are about 5.3 screens worth of pixels. That's a lot of overdraw.
Yes, we could probably squeeze a lot of cycles out of the blitting functions if we handcrafted some 32-bit x86 assembly. But DJGPP actually does a pretty good job at producing fast machine code. And I don't want to drop down into assembly land.

Note: modern hardware won't save you from these issues either sometimes. When NVIDIA sent me a prototype Tegra board in the early 2010s, I soon found out that you could only render about 2 full-screen alpha blended rectangles through OpenGL ES before the frame-rate takes a heavy hit.

Excursion: DOS debugging support
When we debug the demo on the desktop, the debugger will spawn the demo process and use system APIs to stop, resume, inspect, and otherwise manipulate the process.
For DOS applications running in DOSBox-x or on a real machine, we do not have the luxury of a debugger. Instead, we use a piece of code called GDB stub that we integrate in our program. Here's how that works in 12_hello_dos.c.
Of note are 3 pieces of code in the demo above, which do nothing on any platform other than DOS. In lines 7-8 we have:
#define GDB_IMPLEMENTATION
#include ""dos/gdbstub.h""

This pulls in my GDB stub implementation for DJGPP/DOS, which is a single header file library.
The stub's task is it to communicate with the debugger over the serial port, and tell it when the program has stopped due to a breakpoint, or segfault, or other reason. The stub then waits for commands from the debugger to execute, like setting breakpoints, inspecting memory and CPU registers, stepping, continuing, etc.
This GDB stub type of debugging is a cooperative debugging approach. The stub needs to be integrated with the program itself. This explains the other two GDB related lines of code in the demo.
The gdb_start() function is called at the beginning of main(). It waits for the debugger to connect on the serial port. When the debugger tells the stub to continue execution of the program, the stub stops communicating with the debugger for the time being, and gives back control to the program.
The stub then waits for a system level signal to be raised, like a breakpoint or segfault, for which the stub has registered handlers. If such a signal happens, the stub takes over control from the program again, tells the debugger about the program being stopped, and waits for debugger commands to execute.
The final GDB related line is gdb_checkpoint() in line 61. It is placed at the end of our main loop. This is required so the stub can check if the debugger asked to interrupt the program, in which case the stub will take control of the program again and talk to the debugger.
The GDB stub expects all communication to happen through serial port COM1. Some emulators and virtual machines, like DOSBox-x or VirtualBox, can expose the emulated serial port as a TCP port to programs on the host OS. That's what's happening when we debug a demo in DOSBox-x. DOSBox-x exposes the serial port on TCP port 5123, to which GDB connects via TCP. DOSBox-x will then translate TCP packages to writes to the serial port, which the GDB stub reads from COM1. If the GDB stub writes to COM1, then DOSBox-x will forward the data through TCP to GDB.
In theory, the GDB stub should also work on real-hardware. Sadly, I do not have my 486 anymore, nor a serial cable or a serial port on my MacBook.
If you want to debug any of the demos in DOS, you'll have to add the 3 pieces of GDB stub related code to the demo's sources as outlined above. Only the 12_hello_dos.c demo is currently set-up for DOS debugging. Since our code is cross-platform, there won't be a need to debug in DOS a lot though.

Note: when debugging the demos compiled for DOS, we'll be using DOSBox-x instead of 86Box. Two reasons: getting data into and out of 86Box is very annoying. And there is no serial port over TCP emulation in 86Box, so the debugger couldn't even connect. It should be possible to hook the debugger up with a program running in MS-DOS or FreeDOS in VirtualBox though.

Bitmap fonts
Rendering text these days is really, really hard. When we go zooming around documents or web pages via mouse wheel or touch zoom, we expect text to scale seamlessly and stay crisp. If we want to get fancy, we add kerning and hinting to the mix.
It gets even harder when non-latin scripts like arabic script or CJK script need to get put on a screen. Now you have to deal with (more) ligatures, mixed left-to-right and right-to-left layouting, and various other complexities.
And to top it all off, what you get out of a font file is usually a vector representation of not a character, but a glyph, which can be a character, or a part of a character, and oh my, this is all very complicated.
Thankfully, there are various libraries that can help us draw text. For translating a text string to a set of glyphs, or shaping as it's usually called, you can use HarfBuzz. If you want to rasterize those glyphs, which are usually given in vector form, you can use FreeType. If you  want to use your GPU to do most of that, you can use Slug. Your operating system usually also comes with APIs to draw text.
We aren't going to do any of that though. We'll be going somewhat old school and draw inspiration from VGA text mode fonts, but with a 2022 spirit (aka being wasteful).
Before we can look at font pixels, we need to talk about how text is stored in the tubes of our computerers.
Character encodings
Text is composed of characters. When we store text digitally, those characters need to be stored as a sequence of (binary) numbers. When we read characters from a file to draw them to the screen, or translate key strokes to characters, we need to map numbers back to characters. Similarly, when the C compiler encounters a string literal like const char *text = ""Hello world"", it will convert the characters in the string to a sequence of numbers that gets embedded in the final executable.
Mapping those sequences of numbers to characters and vice versa is what character encodings are for.
One of the oldest character encodings is ASCII. Each character is encoded in 1 byte. Well, actually, ASCII only uses the first 7-bits, so it encodes a total of 128 characters. Well, that's not quite true either. Only 95 of these characters are printable. The other 33 ""characters"" are what's called control codes. Notable ones are \t or 9, which indicates a tab, and \n or 10, the line feed. See, it's already complicated!
Here are all the printable characters and non-printable control codes contained in ASCII with their (hexa-)decimal codes.

> ascii -d
Dec Hex    Dec Hex    Dec Hex  Dec Hex  Dec Hex  Dec Hex   Dec Hex   Dec Hex
  0 00 NUL  16 10 DLE  32 20    48 30 0  64 40 @  80 50 P   96 60 \`  112 70 p
  1 01 SOH  17 11 DC1  33 21 !  49 31 1  65 41 A  81 51 Q   97 61 a  113 71 q
  2 02 STX  18 12 DC2  34 22 ""  50 32 2  66 42 B  82 52 R   98 62 b  114 72 r
  3 03 ETX  19 13 DC3  35 23 #  51 33 3  67 43 C  83 53 S   99 63 c  115 73 s
  4 04 EOT  20 14 DC4  36 24 $  52 34 4  68 44 D  84 54 T  100 64 d  116 74 t
  5 05 ENQ  21 15 NAK  37 25 %  53 35 5  69 45 E  85 55 U  101 65 e  117 75 u
  6 06 ACK  22 16 SYN  38 26 &  54 36 6  70 46 F  86 56 V  102 66 f  118 76 v
  7 07 BEL  23 17 ETB  39 27 '  55 37 7  71 47 G  87 57 W  103 67 g  119 77 w
  8 08 BS   24 18 CAN  40 28 (  56 38 8  72 48 H  88 58 X  104 68 h  120 78 x
  9 09 HT   25 19 EM   41 29 )  57 39 9  73 49 I  89 59 Y  105 69 i  121 79 y
 10 0A LF   26 1A SUB  42 2A *  58 3A :  74 4A J  90 5A Z  106 6A j  122 7A z
 11 0B VT   27 1B ESC  43 2B +  59 3B ;  75 4B K  91 5B [  107 6B k  123 7B {
 12 0C FF   28 1C FS   44 2C ,  60 3C <  76 4C L  92 5C \  108 6C l  124 7C |
 13 0D CR   29 1D GS   45 2D -  61 3D =  77 4D M  93 5D ]  109 6D m  125 7D }
 14 0E SO   30 1E RS   46 2E .  62 3E >  78 4E N  94 5E ^  110 6E n  126 7E ~
 15 0F SI   31 1F US   47 2F /  63 3F ?  79 4F O  95 5F _  111 6F o  127 7F DEL

The codes 0-31 are control codes, including the \t (9) and \n (10) codes we discussed above. Printable characters start at code 32 (  or space) and go to code 126. The final code 127 is another control code.
ASCII is short for ""American Standard Code for Information Interchange"". Unsurprisingly, the ASCII encoding really only contains characters used in US English, and by coincidence, some other western scripts.
Now, I'm not 'merican. And based on my server logs, chances are good you aren't 'merican either. What about other fancy characters, like 'ö' or 'ê'? Or characters from the arabic or CJK scripts? Well, that's a lot more complicated and historically involves something called code pages, which was and still is an utter mess.
The alternative to code pages is Unicode. Unicode defines codes (or code points in Unicode parlance) for almost 150,000 characters used in scripts from all around the world, including historic ones. It also includes emojis, for better or worse. Your parents' brains have probably also switched to emoji only instant messaging communication. And they said computers would make us kids dumb. Thanks, Unicode.
Unicode has multiple encodings, like UTF-8, UTF-16, and so on. Thankfully, the world has now mostly standardized on UTF-8, for good reasons. UTF-8 is a multi-byte encoding. Depending on the character, we may need 1 to 4 bytes to store it.
For our demos, we'll store text either in C source code as literals ala const char *text = ""Hello world"", or in text files in the assets/ folder of the r96 project. Both the C sources and text files will be encoded using UTF-8. Anything else would be pain. This means we have to deal with UTF-8 when rendering text.
But as I said earlier, we do not want to go full Unicode text rendering, as that'd require us to integrate all the fancy libraries mentioned above. We want a simpler solution. Enter Unicode's first 256 code points. These code points are split up into 2 blocks.
The first block from code point 0-127 is called the Basic Latin Unicode block. The code points are the exact same codes as used in ASCII, including both non-printable control codes (0-31 and 127) and printable characters (32-126). When encoding text with UTF-8, the resulting sequence of bytes is backwards compatible with ASCII: the first 128 Unicode code points get encoded as a single byte in UTF-8.
The second block from code point 128-255 is called the Latin 1 Supplement block. It contains another set of non-printable control codes (128-159) called C1 controls, which we can safely ignore for the purpose of rendering text. The remaining code points in the block (160-255) include additional characters used in some western scripts. These Unicode code points are encoded with 2 bytes in UTF-8.
Surprise! Those first 256 Unicode code points map directly onto an old code page, namely, the  ISO-8859-1 character set. It is sometimes incorrectly referred to as extended ASCII. Here are the characters contained in the set.


The ISO-8859-1 character set Source: Wikipedia

E.g. ö is encoded as 0xF6 or 246 in decimal. The gray blocks are the control codes.
Alright, we've decided to use the first 2 Unicode blocks spanning code points 0-255. All our C source code containing string literals will be stored UTF-8 encoded. And any text files we put into assets/ to be read by our demos will also be UTF-8 encoded. There are two minor complications.
The first complication is how C compilers handle string literals. When the compiler encounters something like const char *text = ""Hello world"", it will use a character encoding to turn the literal ""Hello world"" into a sequence of bytes embedded in the executable. Which encoding is chosen, depends on the compiler. By default, Clang and GCC convert the string literal to UTF-8 and embed the corresponding byte sequence. Clang even assumes that the source file encoding is UTF-8 and refuses to compile anything else. MSVC is ... different. Luckily, we do not care for MSVC in this series. If you do care for some reason, just make sure to pass /utf8 as a compiler flag to ensure MSVC embeds string literals as UTF-8 as well.
The second complication is actually reading the code points of a UTF-8 encoded text string, whether it comes from a C string literal or a UTF-8 encoded file read from disk. We have to deal with the multi-byte nature of the UTF-8 encoding, as code points above 127 are encoded as two bytes. Luckily, I've taken care of that with the function r96_next_utf8_character():

uint32_t r96_next_utf8_code_point(const char *data, uint32_t *index, uint32_t end) {
	static const uint32_t utf8_offsets[6] = {
			0x00000000UL, 0x00003080UL, 0x000E2080UL,
			0x03C82080UL, 0xFA082080UL, 0x82082080UL};

	uint32_t character = 0;
	const unsigned char *bytes = (const unsigned char *) data;
	int num_bytes = 0;
	do {
		character <<= 6;
		character += bytes[(*index)++];
		num_bytes++;
	} while (*index != end && ((bytes[*index]) & 0xC0) == 0x80);
	character -= utf8_offsets[num_bytes - 1];

	return character;
}

This function takes a sequence of bytes (data) encoding a UTF-8 string, an index into the byte sequence, and the last valid index (end). Both indices are byte offsets, not character offsets!
The function then reads the next UTF-8 character, which may be 1 to 4 bytes long, and returns its code point. Additionally, it increments the index accordingly, so we know at what byte offset the next character starts.

Note: I stole the original of this function many years ago from ... somewhere. I can not remember anymore. I've since modified it to my needs. To the original author: I'm deeply sorry I forgot who you are.

We can use this function to iterate all UTF-8 characters in a byte sequence and get their code points:

const char *utf8_text = ""¡ÄÖ$\n\t"";
uint32_t index = 0;
uint32_t end = strlen(utf8_text);
while (index != end) {
	uint32_t code_point = r96_next_utf8_code_point(utf8_text, &index, end);
	printf(""code point: %i/%x\n"", code_point, code_point);
}

Which prints the code point of each character in decimal and hexadecimal.
code point: 161/a1
code point: 196/c4
code point: 214/d6
code point: 36/24
code point: 10/a
code point: 9/9

As expected. Compare the output to the ISO-8859-1 chart above for validation.
This function can deal with any valid UTF-8 byte sequence and returns code points as a 32-bit unsigned integer. For our purposes, we are only interested in code points 0-255 and will ignore any other code points.
The glyph atlas
Alright, we have all our encoding bases covered. The next question is: how do we turn a code point like 64 (0x41) into the corresponding glyph image for the character A from a font, so we can blit it onto the screen?
To make things easy for us, we'll define some limits:

We'll only render the printable Unicode code points between 0-255 as described above.
We'll only use fixed-width or monospaced fonts. Each glyph in such a font has the same width. We can entirely ignore things like kerning this way.
The font size is fixed.

With these limits in place, the basic idea of a glyph atlas goes like this:

Pick a monospaced font, like the original IBM VGA 8x16 font.
Use a glyph rendering library like FreeType to load the font and render out a glyph image for each printable Unicode code point between 0-255.
Pack those glyph images into a single image called the glyph atlas in some order which makes mapping from a code point to the glyph image coordinates inside the glyph atlas trivial.

Here's an example of what such a glyph atlas could look like.




I've super-imposed a red grid de-marking each glyph's boundaries. An atlas we can use would not have that grid on it. The pixels of the glyph are fully opaque white (0xffffffff), while the background pixels are transparent (0x00000000);
The atlas above contains glyph images from the IBM VGA 8x16 font for the Unicode code points 0-255. Each glyph is 8x16 pixels in size. Each row consists of 16 glyphs. There are 16 rows in total, so 256 glyphs in total, one for each code point.
The glyphs in the first row map to code points 0-15, the glyphs in the second row map to code points 16-31, and so on. The first, second, ninth, and tenth row are empty, as these are the glyphs for non-printable control characters. The other rows contain the glyphs for all printable characters.
If you compare this glyph atlas with the ISO-8859-1 table above, you'll see that they are equivalent, except that the last glyph in the bottom right corner is missing from the atlas. The IBM VGA 8x16 font simply does not have a glyph for that code point.
So how do we generate this atlas? We don't. At least we won't write code for that as part of this series. I've already written a web tool based on FreeType that does exactly what we need. It's called Mario's (B)it(m)ap (F)ont (G)enerator (I'm a a dad, I'm allowed to name it like that) and you can run it in your browser here.
The tool lets you load a monospaced TrueType font, set the pixel height of the glyphs you want, and spits out a 16x14 grid of glyph images for the code points 32-255. It omits the code points 0-31 and thus the first two rows of the atlas as those are non-printable control codes anyways. The above atlas thus becomes this:




We're still wasting two rows in the middle for the second set of control codes. But keeping them around makes converting code points to glyph image coordinates easier.
We can store the generated glyph atlas as a .png file in the assets/ folder. I did just that using the file name assets/ibmvga.png. The generator also tells us that each glyph has a size of 8x16 pixels. We'll need to remember that for when we actually draw text later. Since the glyph atlas is a plain old image, we can load it via r96_image_init_from_file().
We're almost ready to render a text string. We need two more things:
* Being able to map a Unicode code point to a region in the glyph atlas image, where a region is defined by its top-left corner x- and y- pixel coordinates in the glyph atlas, and its width and height in pixels.
* Being able to not just blit an entire r96_image to another, but also blit regions of an r96_image to another r96_image.
Let's start with the mapping problem.

Note: We could put both the atlas and the glyph size information into some custom file format. I decided that's not worth it, so we'll go with a .png and some hard coded glyph sizes in the code.

Mapping code points to glyph atlas pixel coordinates
How can we map a code point to the pixel coordinates of the top left corner of a glyph image in the atlas?
Before we resolve pixel coordinates for a code point, it's actually easier to use a different coordinate system. Let's give each glyph in the atlas an x- and y-coordinate.


For our example glyph atlas in the last section above, each cell represents a glyph image of size 8x16 pixels. In the diagram, the cell shows both the glyph and its code point.
The top-left glyph image has coordinate (0, 0) and the bottom-right glyph image has coordinate (15, 13). We can define a simple equation that goes from glyph coordinates to code point, just like we did for pixel coordinates to pixel address:
code_point = glyph_x + glyph_y * glyphs_per_row + 32

Why the + 32? Because the first glyph has code point 32 (space). Without it, we'd get 0 for glyph_x = 0 and glyph_y = 0.
We can reverse this glyph coordinates to code point mapping as follows:
glyph_x = (code_point - 32) % glyphs_per_row;
glyph_y = (code_point - 32 - glyph_x) / glyphs_per_row;

The % glyphs_per_row basically strips the glyph_y * glyphs_per_row component from the original equation above, leaving us with the glyph x-coordinate.
To calculate glyph_y, we can then subtract the just calculated glyph_x, which gives us the code point of the first glyph in the row, and divide by glyphs_per_row to arrive at the glyph_y coordinate.
All that's left to get the pixel coordinate of the top left corner of a glyph is to multiply the glyph coordinates by the glyph pixel width and height of the font, 8 and 16 in the example above.
glyph_pixel_x = glyph_x * glyph_width;
glyph_pixel_y = glyph_x * glyph_height;

Blitting regions
Alright, we can generate glyph atlases for the first 255 Unicode code points, and we can calculate the pixel coordinates of a glyph image in the atlas corresponding to a code point. We also know the size of each glyph in pixels, as we specified that when generating the glyph atlas.
But we have one more problem: our current blitting functions can only blit an entire r96_image. What we need is blitting functions that blit just a region from a r96_image. Luckily, that's trivial, given our existing blitting functions! Here's a blitting function that blits a region from one r96_image to another.

void r96_blit_region(r96_image *dst, r96_image *src, int32_t dst_x, int32_t dst_y, int32_t src_x, int32_t src_y, int32_t src_width, int32_t src_height) {
	assert(src_x + src_width - 1 < src->width);
	assert(src_y + src_height - 1 < src->height);

	int32_t dst_x1 = dst_x;
	int32_t dst_y1 = dst_y;
	int32_t dst_x2 = dst_x + src_width - 1;
	int32_t dst_y2 = dst_y + src_height - 1;
	int32_t src_x1 = src_x;
	int32_t src_y1 = src_y;

	if (dst_x1 >= dst->width) return;
	if (dst_x2 < 0) return;
	if (dst_y1 >= dst->height) return;
	if (dst_y2 < 0) return;

	if (dst_x1 < 0) {
		src_x1 -= dst_x1;
		dst_x1 = 0;
	}
	if (dst_y1 < 0) {
		src_y1 -= dst_y1;
		dst_y1 = 0;
	}
	if (dst_x2 >= dst->width) dst_x2 = dst->width - 1;
	if (dst_y2 >= dst->height) dst_y2 = dst->height - 1;

	int32_t clipped_width = dst_x2 - dst_x1 + 1;
	int32_t dst_next_row = dst->width - clipped_width;
	int32_t src_next_row = src->width - clipped_width;
	uint32_t *dst_pixel = dst->pixels + dst_y1 * dst->width + dst_x1;
	uint32_t *src_pixel = src->pixels + src_y1 * src->width + src_x1;
	for (int32_t y = dst_y1; y <= dst_y2; y++) {
		for (int32_t i = 0; i < clipped_width; i++) {
			*dst_pixel++ = *src_pixel++;
		}
		dst_pixel += dst_next_row;
		src_pixel += src_next_row;
	}
}

This is basically our old r96_blit() function with additional arguments. We sepcify the destination (dst) and source (src) image as before. We also specify the coordinates (dst_x, dst_y) at which the source image should be blitted in the destination image. Those used to be called x and y. Finally, we specify the region from the source image we want to blit, given as its top-left corner (src_x, src_y) and width and height (src_width, src_height).
The implementation itself then only has three minor modifications compared to r96_blit().
The function starts with two asserts that ensure that the source region is valid. Next, dst_x2 and dst_y2 are calculated using the source region width and height instead of the source image width and height. Finally, src_x1 and src_y1 aren't initialized to 0, but to src_x and src_y.
That's it! The rest, including the clipping, is exactly the same as r96_blit(). We can already use this function to blit glyph images from the glyph atlas. And for some use cases, that'd be good enough.
However, if we only want to blit the white pixels of a glyph and ignore it's background pixels, we need color keying.
Easy, just copy r96_blit_keyed() and apply the same modifications.

void r96_blit_region_keyed(r96_image *dst, r96_image *src, int32_t dst_x, int32_t dst_y, int32_t src_x, int32_t src_y, int32_t src_width, int32_t src_height, uint32_t color_key) {
	assert(src_x + src_width - 1 < src->width);
	assert(src_y + src_height - 1 < src->height);

	int32_t dst_x1 = dst_x;
	int32_t dst_y1 = dst_y;
	int32_t dst_x2 = dst_x + src_width - 1;
	int32_t dst_y2 = dst_y + src_height - 1;
	int32_t src_x1 = src_x;
	int32_t src_y1 = src_y;

	if (dst_x1 >= dst->width) return;
	if (dst_x2 < 0) return;
	if (dst_y1 >= dst->height) return;
	if (dst_y2 < 0) return;

	if (dst_x1 < 0) {
		src_x1 -= dst_x1;
		dst_x1 = 0;
	}
	if (dst_y1 < 0) {
		src_y1 -= dst_y1;
		dst_y1 = 0;
	}
	if (dst_x2 >= dst->width) dst_x2 = dst->width - 1;
	if (dst_y2 >= dst->height) dst_y2 = dst->height - 1;

	int32_t clipped_width = dst_x2 - dst_x1 + 1;
	int32_t dst_next_row = dst->width - clipped_width;
	int32_t src_next_row = src->width - clipped_width;
	uint32_t *dst_pixel = dst->pixels + dst_y1 * dst->width + dst_x1;
	uint32_t *src_pixel = src->pixels + src_y1 * src->width + src_x1;
	for (dst_y = dst_y1; dst_y <= dst_y2; dst_y++) {
		for (int32_t i = 0; i < clipped_width; i++) {
			uint32_t src_color = *src_pixel;
			uint32_t dst_color = *dst_pixel;
			*dst_pixel = src_color != color_key ? src_color : dst_color;
			src_pixel++;
			dst_pixel++;
		}
		dst_pixel += dst_next_row;
		src_pixel += src_next_row;
	}
}

But we can do even better. No text rendering engine is complete without support for colored text! As is stands, we can only draw white text, as that's the color the glyph atlas generator spits out. On-top of color keying, we can also apply what's usually known as tinting.
We'll implement tinting in the simplest possible way: multiply the red, green, and blue color component of the source pixel with the red, green, and blue color component of the specified tinting color. That result of the multiplication is then normalized back to the 0-255 range for each component by dividing by 255. This effectively mixes the two colors.
tinted_red = ((source_red * tint_red) >> 8) & 0xff;
tinted_green = ((source_green * tint_green) >> 8) & 0xff;
tinted_blue = ((source_blue * tint_blue) >> 8) & 0xff;


Note: for the case of tinting glyphs images as generated by the generator, we could just write the tint color to the destination if the source pixel color doesn't match the color key. However, this approach above also works for tinting arbitrary source pixel colors. We'll see why that's useful in a later demo.

Here's the final region blitting routine, which takes both a color key and a tinting color:

void r96_blit_region_keyed_tinted(r96_image *dst, r96_image *src, int32_t dst_x, int32_t dst_y, int32_t src_x, int32_t src_y, int32_t src_width, int32_t src_height, uint32_t color_key, uint32_t tint) {
	assert(src_x + src_width - 1 < src->width);
	assert(src_y + src_height - 1 < src->height);

	int32_t dst_x1 = dst_x;
	int32_t dst_y1 = dst_y;
	int32_t dst_x2 = dst_x + src_width - 1;
	int32_t dst_y2 = dst_y + src_height - 1;
	int32_t src_x1 = src_x;
	int32_t src_y1 = src_y;

	if (dst_x1 >= dst->width) return;
	if (dst_x2 < 0) return;
	if (dst_y1 >= dst->height) return;
	if (dst_y2 < 0) return;

	if (dst_x1 < 0) {
		src_x1 -= dst_x1;
		dst_x1 = 0;
	}
	if (dst_y1 < 0) {
		src_y1 -= dst_y1;
		dst_y1 = 0;
	}
	if (dst_x2 >= dst->width) dst_x2 = dst->width - 1;
	if (dst_y2 >= dst->height) dst_y2 = dst->height - 1;

	uint32_t tint_r = R96_R(tint);
	uint32_t tint_g = R96_G(tint);
	uint32_t tint_b = R96_B(tint);

	int32_t clipped_width = dst_x2 - dst_x1 + 1;
	int32_t dst_next_row = dst->width - clipped_width;
	int32_t src_next_row = src->width - clipped_width;
	uint32_t *dst_pixel = dst->pixels + dst_y1 * dst->width + dst_x1;
	uint32_t *src_pixel = src->pixels + src_y1 * src->width + src_x1;
	for (dst_y = dst_y1; dst_y <= dst_y2; dst_y++) {
		for (int32_t i = 0; i < clipped_width; i++) {
			uint32_t src_color = *src_pixel;
			uint32_t dst_color = *dst_pixel;
			*dst_pixel = src_color != color_key ? R96_ARGB(
														  R96_A(src_color),
														  ((R96_R(src_color) * tint_r) >> 8) & 0xff,
														  ((R96_G(src_color) * tint_g) >> 8) & 0xff,
														  ((R96_B(src_color) * tint_b) >> 8) & 0xff)
												: dst_color;
			src_pixel++;
			dst_pixel++;
		}
		dst_pixel += dst_next_row;
		src_pixel += src_next_row;
	}
}

Since we've already extensively benchmarked and optimized the original blitter functions, and since these new functions only change some setup code, we have no need to do another optimization pass. Whew.
Alright, let's put everything we learned into a little demo.
Demo: Blitting regions
In this demo, we are going to blit the glyphs for the string ""Hello world!"" sourced from the glyph atlas in assets/ibmvga.png, which I generated via Mario's BMFG. We'll apply what we learned and created above, from iterating UTF-8 encoded characters, calculating pixel coordinates for glyphs from code points, to blitting regions in various ways.
Here's 13_blit_region.c:

#include <MiniFB.h>
#include <stdlib.h>
#include <string.h>
#include ""r96/r96.h""

int main(void) {
	const int window_width = 320, window_height = 240;
	struct mfb_window *window = mfb_open(""13_blit_region"", window_width, window_height);
	r96_image output;
	r96_image_init(&output, window_width, window_height);

	r96_image glyph_atlas;
	int32_t glyph_width = 8;
	int32_t glyph_height = 16;
	int32_t glyphs_per_row = 16;
	r96_image_init_from_file(&glyph_atlas, ""assets/ibmvga.png"");

	do {
		r96_clear_with_color(&output, R96_ARGB(0xff, 0x22, 0x22, 0x22));

		const char *text = ""Hello world!"";
		uint32_t text_length = strlen(text);
		uint32_t char_index = 0;
		uint32_t x_offset = 100;
		while (char_index < text_length) {
			uint32_t code_point = r96_next_utf8_code_point(text, &char_index, text_length);
			int32_t glyph_x = (code_point - 32) % glyphs_per_row;
			int32_t glyph_y = (code_point - 32 - glyph_x) / glyphs_per_row;
			int32_t glyph_pixel_x = glyph_x * glyph_width;
			int32_t glyph_pixel_y = glyph_y * glyph_height;

			r96_blit_region(&output, &glyph_atlas, x_offset, 50, glyph_pixel_x, glyph_pixel_y, glyph_width, glyph_height);
			r96_blit_region_keyed(&output, &glyph_atlas, x_offset, 100, glyph_pixel_x, glyph_pixel_y, glyph_width, glyph_height, 0x0);
			r96_blit_region_keyed_tinted(&output, &glyph_atlas, x_offset, 150, glyph_pixel_x, glyph_pixel_y, glyph_width, glyph_height, 0x0, 0xffff00ff);
			x_offset += glyph_width;
		}

		if (mfb_update_ex(window, output.pixels, window_width, window_height) != STATE_OK) break;
	} while (mfb_wait_sync(window));
	return 0;
}

As usual, we start out by creating a window and an output r96_image to which we draw, which gets later drawn to the window.
Next, we define the properties of our glyph atlas and the glyphs contained there-in, and load the glyph atlas image.
In the main loop, we clear the output image, then iterate through the characters in the text string via r96_next_utf8_code_point(). We then calculate the glyph pixel coordinates for the code point in the glyph atlas and use that information to blit the glyph to the screen three times, using the normal blit, keyed blit, and keyed and tinted blit functions.
Take special note of x_offset. It specifies at what x-coordinate the next glyph will be blitted in the output image. As our font is monospaced, we can easily advance the drawing position on the x-axis by glyph_width. All glyphs have the same width. Variable width fonts are quite a bit more complex to get right in that regard.
And here is the web version.





Let's pack all of this up into re-useable code.
r96_font
Looking at the last demo, we can almost see a struct for fonts plop out:

r96_image glyph_atlas;
int32_t glyph_width = 8;
int32_t glyph_height = 16;
int32_t glyphs_per_row = 16;

This is the minimum information we need to store for a font to draw text with it, which translates to the following struct:

typedef struct r96_font {
	r96_image glyph_atlas;
	int32_t glyph_width, glyph_height;
	int32_t glyphs_per_row;
	int32_t tab_size;
} r96_font;

We load the glyph_atlas from an image file. glyph_width and glyph_height are parameters we'll need to specify when initializing the r96_image font. glyphs_per_row we can actually automatically deduce from the glyph atlas width and the glyph width, reducing the amount of parameters we need to specify when initializing a font. tab_size will make sense in a minute! Here's r96_font_init():

bool r96_font_init(r96_font *font, const char *path, int32_t glyph_width, int32_t glyph_height) {
	if (!r96_image_init_from_file(&font->glyph_atlas, path)) return false;
	font->glyph_width = glyph_width;
	font->glyph_height = glyph_height;
	font->glyphs_per_row = font->glyph_atlas.width / glyph_width;
	font->tab_size = 3;
	return true;
}

Unremarkable. And the corresponding r96_font_dispose():

void r96_font_dispose(r96_font *font) {
	r96_image_dispose(&font->glyph_atlas);
}

The rendering logic from the last example can be directly translated to a re-usable function. But we'll add two more features. We'll interpret \n and \t and adjust the rendering position for the next glyph accordingly.

void r96_text(r96_image *image, r96_font *font, const char *text, int32_t x, int32_t y, uint32_t tint) {
	int32_t cursor_x = x;
	int32_t cursor_y = y;
	uint32_t text_length = strlen(text);
	uint32_t index = 0;
	while (index < text_length) {
		uint32_t c = r96_next_utf8_code_point(text, &index, text_length);
		if (c == '\t') {
			cursor_x += font->tab_size * font->glyph_width;
			continue;
		}
		if (c == '\n') {
			cursor_x = x;
			cursor_y += font->glyph_height;			
			continue;
		}
		if (c < 32 || c > 255) {
			cursor_x += font->glyph_width;
			continue;
		}

		int32_t glyph_index = c - 32;
		int32_t glyph_x = (glyph_index % font->glyphs_per_row);
		int32_t glyph_y = (glyph_index - glyph_x) / font->glyphs_per_row;
		glyph_x *= font->glyph_width;
		glyph_y *= font->glyph_height;

		r96_blit_region_keyed_tinted(image, &font->glyph_atlas, cursor_x, cursor_y, glyph_x, glyph_y, font->glyph_width, font->glyph_height, 0x0, tint);

		cursor_x += font->glyph_width;
	}
}

The function takes the image we want to render the text to, the font to render with, the text as a null-terminated UTF-8 string, and the x and y position to start rendering the first glyph at in the image. It's final parameter is the tint color.
Inside the function, we keep track of the position to render the next glyph at in cursor_x and cursor_y. We also keep track of the text length in bytes and the byte index from which we'll read the next Unicode code point from the text.
The loop then iterates over all code points in the text via r96_next_utf8_code_point(). In case we encounter \t, we advance the cursor position by font->tab_size * font->glyph_width and continue on to the next glyph. In case of \n, we reset cursor_x to the original x, essentially moving the cursor to the beginning of the text line. We then increase cursor_y by the glyph height to move it to the next line below. Yay, multi-line rendering!
Before we actually render the glyph for the current code point, we also check that the code point is within 32-255, so we don't try to draw a glyph that's not inside the glyph atlas.
The remainder of the function maps the code point to the glyph in the glyph atlas and uses r96_blit_region_keyed_tinted() to draw the glyph to the current cursor position. Finally, we advance the cursor by the glyph width.
Not counting the region blitting functions, the entire text rendering code code is about 70 LOC now. Let's add a few more lines of code.
In the previous demo, we positioned the glyphs at hard coded coordinates. If we wanted to center the text on the screen, or apply other alignments, we need to know the width and height of the text, also known as its bounds.
Let's write a little function that calculates exactly that.

void r96_font_get_text_bounds(r96_font *font, const char *text, int32_t *width, int32_t *height) {
	*width = 0;
	*height = font->glyph_height;
	int32_t current_line_width = 0;
	uint32_t text_length = strlen(text);
	uint32_t index = 0;
	while (index < text_length) {
		uint32_t c = r96_next_utf8_code_point(text, &index, text_length);
		if (c == '\t') {
			current_line_width += font->tab_size * font->glyph_width;
			continue;
		}
		if (c == '\n') {
			*width = current_line_width > *width ? current_line_width : *width;
			*height += font->glyph_height;
			current_line_width = 0;
			continue;
		}
		current_line_width += font->glyph_width;
	}
	*width = current_line_width > *width ? current_line_width : *width;
}

The function takes the font that the text will be rendered with, as well as pointers width and height to which we write the calculated bounds.
The function then mirrors parts of the rendering logic in r96_text(), calculating the maximum line width, as well as how many lines there actually are.
Alright, let's use all this in a little demo.
Demo: using r96_font and friends
Here's 14_fonts.c, our cute font demo:

#include <MiniFB.h>
#include <stdlib.h>
#include ""r96/r96.h""

int main(void) {
	const int window_width = 320, window_height = 240;
	struct mfb_window *window = mfb_open(""14_fonts"", window_width, window_height);
	r96_image output;
	r96_image_init(&output, window_width, window_height);
	r96_font font;
	r96_font_init(&font, ""assets/ibmvga.png"", 8, 16);

	do {
		r96_clear_with_color(&output, R96_ARGB(0xff, 0x22, 0x22, 0x22));

		const char *text = ""The quick brown fox jumps\nover the lazy dog\n""
						   ""¡¢£¤¥¦§¨©ª«¬\n""
						   ""ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏ"";

		int32_t text_x, text_y, text_width, text_height;
		r96_font_get_text_bounds(&font, text, &text_width, &text_height);
		text_x = window_width / 2 - text_width / 2;
		text_y = window_height / 2 - text_height / 2;

		r96_rect(&output, text_x, text_y, text_width, text_height, R96_ARGB(0xff, 0xff, 0x0, 0xff));
		r96_text(&output, &font, text, text_x + 1, text_y + 1, 0x00000000);
		r96_text(&output, &font, text, text_x, text_y, 0xffffffff);

		if (mfb_update_ex(window, output.pixels, window_width, window_height) != STATE_OK) break;
	} while (mfb_wait_sync(window));
	return 0;
}

We start off by loading the font in line 11, specifying the glyph atlas image path, the glyph width, and the glyph height. r96_font_init loads the glyph atlas image and sets up all the fields of the font as we saw earlier.
In the main loop, we clear the output image, then define the text we want to render. The text consists of 3 lines, using characters from the code point range we support.
The next block of code calculates the bounds of the text via r96_font_get_text_bounds(), which we use to calculate the text's top-left corner position in such a way, that the text is centered in the middle of the screen.
In the final block, we render a background rectangle using the text bounds, followed by rendering the text offset by 1 pixel on both axes with a black tint. Finally, we render the text at the calculated position with a white tint. Rendering the text twice this way gives us a simple shadow effect. Here's the demo running on the web.





Great success.
Demo: fun with fonts
While the original IBM VGA font is nice, it's also a bit of an outdated, and dare I say boring look.
I've added two more glyph atlases to the assets/ folder. The first one is derived from the awesome Tamzen font (assets/tamzen.png).




It has a lighter, more modern appearance and is well suited to display stats, like performance counters.
The other font was ripped from some old demo from the 90ies by Ian Hanschen. He's put up a GitHub repo with a gargantuan amount of ripped fonts. Most of them do not have attribution. This is the one I picked (assets/demofont.png).




Each font is basically just a glyph atlas. However, the atlas layout doesn't match the one generated by BMFG.
For the font I picked, we see that it only contains glyphs for the first few code points. Instead of 16 glyphs, it contains 20 glyphs per row. Thankfully, r96_init_font() can deal with this by calculating the number of glyphs per row based on the glyph atlas width and glyph width. The only thing we need to watch out for is to not use any code points that go above Z in our text strings.
This demo doesn't come with an explanation. Consider it to be a puzzle for your brain noggins! Can you figure out how it works? 15_font_fun.c:

#include <MiniFB.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>
#include ""r96/r96.h""

int main(void) {
	const int window_width = 320, window_height = 240;
	struct mfb_window *window = mfb_open(""15_font_fun"", window_width, window_height);
	r96_image output;
	r96_image_init(&output, window_width, window_height);
	r96_font font;
	r96_font_init(&font, ""assets/demofont.png"", 16, 16);
	float counter = 0;
	struct mfb_timer *timer = mfb_timer_create();
	do {
		r96_clear_with_color(&output, R96_ARGB(0xff, 0x22, 0x22, 0x22));

		const char *text = ""--(2022 DEMO CREW)--"";
		int32_t text_x = 0;
		uint32_t text_length = strlen(text);
		uint32_t char_index = 0;
		while (char_index < text_length) {
			char character[] = {0, 0};
			character[0] = (char) r96_next_utf8_code_point(text, &char_index, text_length);
			int32_t text_y = output.height / 2 - font.glyph_width / 2 + (int32_t) (sinf(counter + char_index / 10.0f) * output.height / 4);
			r96_text(&output, &font, character, text_x, text_y, 0xffffffff);
			text_x += font.glyph_width;
		}

		counter += M_1_PI * mfb_timer_delta(timer) * 12;
		mfb_timer_reset(timer);

		if (mfb_update_ex(window, output.pixels, window_width, window_height) != STATE_OK) break;
	} while (mfb_wait_sync(window));
	return 0;
}

And here it is in action.





Next time on ""Mario writes a lot of words""
Our little code base is shaping up to be kinda useful. Next time, we're going to look into drawing lines. Possibly with sub-pixel precision. Unless I can't figure that out.
Discuss this post on Twitter or Mastodon.




"
https://news.ycombinator.com/rss,The IAB loves tracking users. But it hates users tracking them,https://shkspr.mobi/blog/2023/01/the-iab-loves-tracking-users-but-it-hates-users-tracking-them/,Comments,"The IAB loves tracking users. But it hates users tracking them.By 
@edent

 on 
2023-01-16
advertising email privacy12 comments550 words
The Interactive Advertising Bureau (IAB) is a standards development group for the advertising industry. Their members love tracking users. They want to know where you are, who you're with, what you're buying, and what you think. All so they can convince you to spend slightly more on toothpaste.  Or change your political opinions. Either way, they are your adversaries.The IAB's tech lab is working on a system called UID2. It's a more advanced way to track you no matter what you do and no matter what steps you take to avoid it.
UID2 is a framework that enables deterministic identity for advertising opportunities on the open internet for many participants across the advertising ecosystem. The UID2 framework enables logged-in experiences from publisher websites, mobile apps, and Connected TV (CTV) apps to monetize through programmatic workflows.Basically, they tie your email address to everything you do. Signed in to watch a TV show? Better sell that info to the advertisers so when you sign in to a different site they can send you targetted messages. Yuck.One of the ways privacy conscious users normally avoid this is by subtly altering their email addresses for each service they use.  For example, GMail ignores any dots in your username. So if you are Han.Solo@gmail.com you can also use H.ansolo@gmail.com or ha.ns.ol.o@gmail.com.  A user might sign up to a service and use a specifically ""dotted"" email address.  If they later start receiving spam to that address, they know the service has leaked or sold their info.You can go one step further and use plus addressing.  For example han.solo+amazon@gmail.com and han.solo+github@gmail.com. They both will appear in your normal inbox, but are unique for every service you use. Again, this is great for making sure that someone hasn't sold your email address to spammers.The IAB hates this.As part of the UID2 API they specifically describe how an advertiser must ""normalise"" their users' email addresses.This means h.a.n.solo+iab@gmail.com becomes plain old hansolo@gmail.comI think this is pretty shitty behaviour. If someone has deliberately set their email address in this form it is because the user does not want their identities to be commingled.Last year, I asked them to respect users' privacy and reverse this change.  They finally responded:
Thank you for your input, we thought long about this update and ultimately as it stands today it is not a change we would like to add.So, there you have it. If you want to take even the smallest step to preserve your privacy - tough.
If you want to track which IAB members are using your data - tough.
If you want to track users even if they don't want to be tracked - the IAB is happy to help.If you want to opt out of this - and you trust the IAB to handle your data safely - you can submit your email address and phone number to https://transparentadvertising.org/.Personally, I recommend installing the uBlock advert blocker on all devices which support it.Share the love:MastodonTwitterFacebookLinkedInRedditHackerNewsLobstersEmailPocketWhatsAppTelegramMore posts from around the site:
12 thoughts on “The IAB loves tracking users. But it hates users tracking them.”

2023-01-16 12:38

 Ian Betteridge says:@Edent I’ve noticed several brands now blocking services like iCloud’s relay, which lets you sign up with a random email address that’s not related to yours. Firefox relays ducks around that by letting you use your own domain, which makes it much harder for them to block sign-ups, but that’s obviously only applicable to a few users.Reply

2023-01-16 12:55

 Gabor says:I've been loving fastmail's masked email functionality, which gives you a random email alias like ""salty.hotdog8233@fastmail.com"", plus it has 1password integration, so signing up to places is fairly straightforward if you use 1p.Reply

2023-01-16 13:00

 That Privacy Guy says:I just read this and the solution I use is my own instance of AnonAddy - and I create a new and unique email address for every site/service I use. If you don't want to run it yourself there is a SaaS version - plus it is FOSS.


Reply

2023-01-16 13:40

 HackerNewsTop10 says:The IAB loves tracking users. But it hates users tracking them
Link: shkspr.mobi/blog/2023/01/t…
Comments: news.ycombinator.com/item?id=344000…Reply

2023-01-16 13:42

 Kazaii says:@Edent wow, that's rather unsettling. Thanks for shedding light on this.Reply

2023-01-16 13:49

 That Privacy Guy says:I have been using my own installation of AnonAddy for a couple of years now. I used to just have a catchall in my mail server which would forward anything which was sent to a non existing email address to a delegated account



Reply

2023-01-16 13:55

 Fazal Majid says:The plus convention is not specific to GMail (Sendmail, MS Exchange, Postfix and other email software have it), but they only require stripping it for @gmail.com domains. I have my own dedicated domain for vendors so I won't be impacted, and Apple's email masking feature will do the same, along with competing offerings from DuckDuckGo et al.Hashing PII like an email is also PII and this proposal is a blatant violation of GDPR, of course.Reply

2023-01-16 13:55

 Nikki says:Personally my opinion of anyone involved in advertising is so poor that I'd probably not be allowed to express it here. I can easily imagine a world without advertising as the web allows you to find anything you want without having someone trying to force it down your throat. Also the idea that many parts of the web could not exist without advertising support is facile. It's a bit like saying that free and open parks cannot exist without employing pick pockets to gather funds to pay for maintenance. If there are any parts of the web that really can not exist without advertising, they must be so bankrupt of alternatives ideas that their services could not be trusted to be useful.Reply

2023-01-16 15:09

 Anonymous says:A link says uBlock but points to uBlock Origin. uBlock is different from uBlock Origin: https://github.com/gorhill/uBlock/wiki/uBlock-Origin-is-completely-unrelated-to-the-web-site-ublock.orgReply

2023-01-16 15:21

 Oli says:I’m a big fan of Fastmail’s masked addresses for this reason.Word dot word four digit number at my own domain, goes in the password manager, never thought about again!Reply

2023-01-16 15:44

 Privacy Matters says:Hi @IABTechLab  What is the legal basis relied on  to alter the email identities of individuals who will be targeted by those using UID2?Oh, & I note domain reg details for transparentadvertising.org are redacted for privacy reasons. Who owns the domain pls?

Reply

2023-01-16 15:56

 trinity says:I own my name dot [tld] so I can do slingshit@me.com. Looks like I'm still gonna be doing alright. Cloudflare's mail forwarding works well for this, before that I used ImprovMX. Both just point the proper DNS records from your site to someone's mail server for quick relay+disposal. I imagine having all mail filter through a magic box is technically A Bit Troublesome but it's still better than Google Mail!ReplyLeave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment *Name * Email * Website  Notify me of follow-up comments by email. Notify me of new posts by email. 
Δ
To respond on your own website, enter the URL of your response which should contain a link to this post's permalink URL. Your response will then appear (possibly after moderation) on this page. Want to update or remove your response? Update or delete your post and re-enter your post's URL again. (Learn More)



"
https://news.ycombinator.com/rss,Using paleogenomics to elucidate 10k years of immune system evolution,https://www.pasteur.fr/en/press-area/press-documents/using-paleogenomics-elucidate-10000-years-immune-system-evolution,Comments,"



You are hereHomePress areaPress documentsUsing paleogenomics to elucidate 10,000 years of immune system evolution








Using paleogenomics to elucidate 10,000 years of immune system evolution 



© Adobe Stock








  
      Press release  




2023.01.13




Print
|

Share
   





Scientists from the Institut Pasteur, Université Paris Cité, the CNRS and the Collège de France have used paleogenomics to trace 10,000 years of human immune system evolution. They analyzed the genomes of more than 2,800 individuals who lived in Europe over the past ten millennia. They were able to date the increase in frequency of most of the mutations that are advantageous in defending against pathogens to after the Bronze Age, 4,500 years ago. The scientists also observed that mutations conferring a higher risk of developing inflammatory disorders have become more frequent over the past 10,000 years. These enlightening results on the effects of natural selection on immunity genes were published in the journal Cell Genomics on January 13, 2023. 
In the 1950s, the geneticist J.B.S. Haldane attributed the maintenance or persistence of the mutation responsible for anomalies in red blood cells commonly observed in Africa to the protection these anomalies provided against malaria, an endemic infection that claims millions of lives. This theory suggested that pathogens are among the strongest selective pressures faced by humans. Several population genetics studies subsequently confirmed the theory. But major questions remained, especially regarding the specific epochs during which the selective pressures exerted by pathogens on human populations were strongest and their impact on the present-day risk of developing inflammatory or autoimmune disorders.
To address these questions, scientists from the Institut Pasteur, Université Paris Cité, the CNRS and the Collège de France, in collaboration with the Imagine Institute and The Rockefeller University (United States), adopted an approach based on paleogenomics. This discipline, which studies the DNA from fossil remains, has led to major discoveries about the history and evolution of humans and human diseases, as illustrated by the decision to award the 2022 Nobel Prize in Physiology or Medicine to the paleogeneticist Svante Pääbo. In the study led by the Institut Pasteur, published on January 13 in the journal Cell Genomics, the scientists analyzed the variability of the genomes of more than 2,800 individuals who lived in Europe over the past ten millennia – a period covering the Neolithic, the Bronze Age, the Iron Age, the Middle Ages and the present.
By reconstituting the evolution over time of hundreds of thousands of genetic mutations, the scientists initially identified mutations that rapidly increased in frequency in Europe, indicating that they were advantageous. These mutations that evolved under ""positive"" natural selection are mainly located in 89 genes enriched in functions relating to the innate immune response, including especially the OAS genes – which are responsible for antiviral activity – and the gene responsible for the ABO blood group system. Surprisingly, most of these positive selection events, which demonstrate a genetic adaptation to the pathogenic environment, began recently, from the start of the Bronze Age, around 4,500 years ago. The scientists explain this ""acceleration"" in adaptation by the growth in the human population during this period and/or by strong selective pressures exerted by pathogens in the Bronze Age, probably linked to the spread of severe infectious diseases such as plague.
At the same time, the scientists also looked at the opposite situation, in other words, mutations whose frequency fell significantly over the past ten millennia. These mutations are probably subject to ""negative"" selection because they increase the risk of disease. They noted that once again, these selection events mainly began in the Bronze Age. Many of these disadvantageous mutations were also located in genes associated with the innate immune response, such as TYK2, LPB, TLR3 and IL23R, and have been confirmed in experimental research to have a deleterious effect in terms of infectious disease risk. The results emphasize the value of adopting an evolutionary approach in research on genetic susceptibility to infectious diseases.
Finally, the scientists explored the theory that the selection exerted by pathogens in the past gave an advantage to alleles conferring resistance to infectious diseases, but that in turn these alleles have increased the present-day risk of autoimmune or inflammatory disorders. They investigated the few thousand mutations known to increase susceptibility firstly to tuberculosis, hepatitis, HIV or COVID-19, and secondly to rheumatoid arthritis, systemic lupus erythematosus or inflammatory bowel disease. By looking at the evolution of these mutations over time, they observed that those associated with an increased risk of inflammatory disorders – including Crohn's disease – became more frequent over the past 10,000 years, while the frequency of those associated with a risk of developing infectious diseases decreased. ""These results suggest that the risk of inflammatory disorders has increased in Europeans since the Neolithic period because of a positive selection of mutations improving resistance to infectious diseases,"" explains Lluis Quintana-Murci, director of the study and Head of the Human Evolutionary Genetics Unit (Institut Pasteur/CNRS Evolutionary Genomics, Modeling and Health Unit/Université Paris Cité).
The results of the study, which harnessed the huge potential of paleogenomics, show that natural selection has targeted human immunity genes over the past ten millennia in Europe, especially since the start of the Bronze Age, and contributed to present-day disparities in terms of the risk of infectious and inflammatory diseases.
As well as the institutions mentioned above, this research was supported by the French Foundation for Medical Research (FRM), the Allianz-Institut de France Foundation and the Fondation de France. 





Explanatory diagram. © Gaspard Kerner, Institut Pasteur
Source
Genetic adaptation to pathogens and increased risk of inflammatory disorders in post-Neolithic Europe, Cell Genomics, January 13, 2023
Gaspard Kerner,1* Anna-Lena Neehus,2,3 Quentin Philippot,2,3 Jonathan Bohlen,2,3 Darawan Rinchai,4 Nacim Kerrouche,4 Anne Puel,2,3 Shen-Ying Zhang,2,3,4 Stéphanie Boisson-Dupuis,2,3,4 Laurent Abel,2,3,4 Jean-Laurent Casanova,2,3,4,5,6 Etienne Patin,1,8 Guillaume Laval,1,8 and Lluis Quintana-Murci,1,7,8,9,*
1Institut Pasteur, Université Paris Cité, CNRS UMR2000, Human Evolutionary Genetics Unit, F-75015 Paris, France
2Laboratory of Human Genetics of Infectious Diseases, INSERM UMR 1163, Necker Hospital for Sick Children, 75015 Paris, France.
3University Paris Cité, Imagine Institute, 75015 Paris, France.
4St. Giles Laboratory of Human Genetics of Infectious Diseases, The Rockefeller University, New York, NY 10065, United States.5Howard Hughes Medical Institute, New York, NY 10065, United States
6Department of Pediatrics, Necker Hospital for Sick Children, 75015 Paris, France
7Collège de France, Chair of Human Genomics and Evolution, F-75005 Paris, France
8Senior author
9Lead contact
*Corresponding author
 









"
https://news.ycombinator.com/rss,Intro to GCC bootstrap in RISC-V,https://ekaitz.elenq.tech/bootstrapGcc0.html,Comments,"


Ekaitz's tech blog


















Home
Series



Feed
About

 





Ekaitz's tech blog:I make stuff at ElenQ Technology and I talk about it



Intro to GCC bootstrap in RISC-V



      Mon 14 February 2022
    

      By           Ekaitz Zárraga



You probably already know about how I spent more than a year having fun with
RISC-V and software bootstrapping from source.
As some may know from my FOSDEM talk, NLNet / NGI-Assure put the
funds to make me spend more time on this for this year and I decided
to work on GCC’s bootstrapping process for RISC-V.
Why GCC
GCC is probably the most used compiler collection, period.  With GCC we can
compile the world and have a proper distribution directly from source, but who
compiles the compiler?1
Well, someone has to.
The bootstrap
Bootstrapping a compiler with a long history like GCC for a new architecture
like RISC-V involves some complications, starting on the fact that the first
version of GCC that supports RISC-V needs a C++98 capable compiler in order to
build. C++98 is a really complex standard, so there’s no way we can bootstrap a
C++98 compiler at the moment for RISC-V. The easiest way we can think of at
this point is to use an older version of GCC for that, one of those that are
able to build C++98 programs but they only require a C compiler to build. Older
versions of GCC, of course, don’t have RISC-V support so… We need a
backport2.
So that’s what I’m doing right now. I’m taking an old version of GCC that only
depends on C89 and is able to compile C++98 code and I’m porting it to RISC-V
so we can build newer GCCs with it.
Only needing C to compile it’s a huge improvement because there are Tiny C
Compilers out there that can compile C to RISC-V, and those are written using
simple C that we can bootstrap with simpler tools of a more civilized world.
In summary:

C++98 is too complex, but C89 is fine.
GCC is the problem and also the solution.

What about GNU Mes?
When we3 started with this effort we wanted to prepare GNU Mes, a small C
compiler that is able to compile a Tiny C Compiler, to work with RISC-V so we
could start to work in this bootstrap process from the bottom.
Some random events, like someone else working on that part, made us rethink our
strategy so we decided to start from the top and try to combine both efforts at
the end. We share the same goal: full source bootstrap for RISC-V.
Tiny C Compilers?
There are many small C compilers out there that are written in simple C and are
able to compile an old GCC that is written in C. Our favorite is TinyCC (Tiny C Compiler).
GNU Mes is able to build a patched version of TinyCC, which already supports
RISC-V (RV64 only), and we can use that TinyCC to compile the GCC version I’m backporting.
We’d probably need to patch some things in both projects to make everything
work smoothly but that’s also included in the project plan.
Binutils
Binutils is also a problem mostly because GCC, as we will talk about in the
future, does not compile to binary directly. GCC generates assembly code and
coordinates calls to as and ld (the GNU Assembler and Linker) to generate
the final binaries. Thankfully, TinyCC can act as an assembler and a linker,
and there’s also the chance to compile a modern binutils version because it is
written in C.
In any case, the binary file generation and support must be taken in account,
because GCC is not the only actor in this film and RISC-V has some weird things
on the assembly and the binaries that have to be supported correctly.
Conclusion
This is a very interesting project, where I need to dig in BIG stuff, which
is cool, but also has a huge level of uncertainty, which scares the hell out of
me. I hope everything goes well…
In any case, I’ll share all I learn here in the blog and I keep you all posted
with the news we have.
That’s all for this time. If you have any question or comment or want to share
your thoughts and feelings with me5 you can find my
contact information here.


PS: Big up to NlNet / NGI-Assure for the money.










wHo wATcHes tHE wAtchMEN? ↩


Insert “Back to the Future” music here. ↩


“We” means I shared my thoughts and plans with other people who have a
  much better understanding of this than myself. ↩


But there are some others that are really interesting (see
cproc, for example) ↩


Or even hire me for some freelance IT stuff 🤓 ↩








          Supported by:
        











 



"
https://news.ycombinator.com/rss,"New Sony Walkman music players feature good looks, Android 12",https://arstechnica.com/gadgets/2023/01/new-sony-walkman-music-players-feature-stunning-good-looks-android-12/,Comments,"






      Where do you put the cassette tape?    —

New Sony Walkman music players feature stunning good looks, Android 12
Sony holds onto the beautiful dream of standalone portable audio players. 


Ron Amadeo
    -  Jan 13, 2023 7:53 pm UTC

 




reader comments
238
 with 0 posters participating


Share this story

Share on Facebook
Share on Twitter
Share on Reddit












                    The Sony Walkman NW-A300. It's a shame Sony never became a force in smartphones, because, wow, their product designs are still so good.                  


                                          Sony                                      









                    Yep, that's regular Android.                   


                                          Sony                                      









                    The bottom. From left to right we've got a headphone jack, lanyard hole, USB-C port, and a microSD slot.                   


                                          Sony                                      









                    Music buttons! So many music buttons!                  


                                          Sony                                      









                    The back has this nice scallop texture.                   


                                          Sony                                      









                    The frame is aluminum.                   


                                          Sony                                      









                    It comes in colors. That gray one really hits me in the nostalgic Sony sweet spot.                   


                                          Sony                                      









                    There are little folio cases! They are so cute.                  


                                          Sony                                      









                    The folio case lets it stand up!                  


                                          Sony                                      





Sony has a pair of new Android Walkmans out, the NW-A300 and NW-ZX700. Yes, that's right, Walkmans, Sony's legendary music player brand from the 1980s. Apple may have given up on the idea of a smartphone-adjacent music player when it killed the iPod Touch line recently, but Sony still makes Android-powered Walkmans and has for a while. The first was in 2012 with the Android 2.3 Gingerbread-powered NWZ-Z1000, which looked like Sony just stripped the modem out of an Xperia phone and shoved it onto the market as a music player. Since then, Sony has made designs with more purpose-built hardware, and today there are a whole series of Android-powered Walkman music players out there. Sadly these new ones seem to only be for sale in Japan, the UK, and Europe, for now.
We'll start with the most consumer-friendly of the two, the NW-A300. This basic design debuted in 2019 with the NW-A105, but that shipped with Android 9. This is an upgraded version of that device with a less-ancient version of Android, a new SoC, and a scalloped back design. In Sony's home of Japan, the 32GB version is 46,000 yen (about $360), while in Europe, it's 399 euro (about $430).
The NW-A300 is a tiny little device that measures 56.6×98.5×12 mm, so pretty close to a deck of playing cards. And really, just look at these pictures. Sony might not be the consumer electronics juggernaut it used to be, but it still has an incredible product design department. I have no use for a standalone music player, but both of these Walkmans are so pretty that I just want to hold one. 
Advertisement 


The front is dominated by a 3.6-inch, 60 Hz, 1280×720 touchscreen LCD. There's 32GB of storage, and the device supports Wi-Fi 802.11AC and Bluetooth 5. That's about all Sony wants to talk about for official specs. It touts ""longer battery life"" but won't say how big the battery is, promising only ""36 hours* of 44.1 KHz FLAC playback, up to 32 hours* of 96 KHz FLAC High-Resolution Audio playback."" Presumably, that's all with the screen off.
For more specs, we can visit The Walkman Blog, a wonderful site that is very serious about these little music players. In October, the site found documentation for the A300 listing a 1500 mAh battery. The system-on-a-chip in the older NW-A100 model was the NXP i.MX8M-Mini, a wildly slow 28 nm SoC that has just four Arm Cortex-A53 CPUs and 4GB of RAM. You can say, ""This is just a music player,"" but that's not really true since it still runs full Android with an app store and everything. Geekbench scores show this has a new quad-core Qualcomm chip of some kind with 4GB of RAM, but we can't be sure of the model number. A newer chip with smaller transistors would probably account for a lot of that ""better battery life"" promise.
This is a music player, so of course, there's a headphone jack on the bottom of the unit. You'll also find a spot for a lanyard, a speedy USB-C 3.2 Gen1 port for quick music transfers, and a MicroSD slot for storing all your music. Buttons along the side of the device also give you every music control you could want, like a hold switch, previous, play/pause, next, volume controls, and power.

Page: 1 2 Next →













reader comments
238
 with 0 posters participating


Share this story

Share on Facebook
Share on Twitter
Share on Reddit







Ron Amadeo
      Ron is the Reviews Editor at Ars Technica, where he specializes in Android OS and Google products. He is always on the hunt for a new gadget and loves to rip things apart to see how they work.    

Email ron@arstechnica.com
//
Twitter @RonAmadeo








Advertisement 





















Channel Ars Technica




← Previous story Next story →




Related Stories









Today on Ars







"
https://news.ycombinator.com/rss,Interactive Music Theory Cheat Sheet,https://muted.io/cheat-sheet/,Comments,"







                Music Theory Cheat Sheet: Keys, Scales, Chords, Notes & Intervals
              


💖 share it →






























  ✨🎵✨ An interactive music theory cheat sheet to get all you need at a
  glance:
  keys, scales, modes, notes, chords and
    intervals. 
  Just select a major or minor key and you'll get the notes of
  the scale, scale formula, the relative major or minor, modal scales for that
  key, scale degrees/intervals, the key signature, the diatonic chords, the
  diatonic 7th chords, the chord functions and the relationship with other keys
  on the Circle of Fifths (aka circle of
  fourth, when going counter-clockwise). 
  → For more reference on chords and scales, you may like this
  scale formula chart,
  chord formula chart and this
  list of chords.














      C
    

      C♯


      D♭


      D
    

      D♯


      E♭


      E
    

      F
    

      F♯


      G♭


      G
    

      G♯


      A♭


      A
    

      A♯


      B♭


      B
    



      Major
    

      Minor
    



key:


C
C♯
D♭
D
D♯
E♭
E
F
F♯
G♭
G
G♯
A♭
A
A♯
B♭
B




      major
    

      minor
    









C♯D♭


D


D♯E♭


E


F


F♯G♭


G


G♯A♭


A


A♯B♭


B





C♯D♭


D


D♯E♭


E


F


F♯G♭


G


G♯A♭


A


A♯B♭


B








C
Major




C
Major
      Scale
    







formula:
1 2 3 4 5 6 7
1 2 ♭3 4 5 ♭6 ♭7
steps:
whole whole half
        whole whole whole half

whole half whole whole half
        whole whole




        relative minor →
        A Minor












C
          Harmonic Minor Scale
        







formula:
1 2 ♭3 4 5 ♭6 7




C
          Melodic Minor Scale
        







formula:
1 2 ♭3 4 5 6 7





        relative major →
        A Major










Modes of the Major Scale






C
          Major Pentatonic Scale
        







formula:
1 2 3   5 6  




C
          Major Blues Scale
        







formula:
1 2 ♭3 3 5   6  






C
          Minor Pentatonic Scale
        







formula:
1   ♭3 4 5   ♭7




C
          Minor Blues Scale
        







formula:
1   ♭3 4 ♭5 5   ♭7





C
Major
      Scale Degrees & Intervals
    


        1- Tonic:
        
          C
        
        → Unison
      

        2- Supertonic:
        
          D
        
        → Major 2nd
      

        3- Mediant:
        
          E
        
        →
        Major 3rd
      

        4- Subdominant:
        
          F
        
        → Perfect 4th
      

        5- Dominant:
        
          G
        
        → Perfect 5th
      

        6- Submediant:
        
          A
        
        →
        Major 6th
      

        7-
        Leading Tone:
        
          B
        
        →
        Major 7th
      

        8/1- Octave/Tonic:
        
          C
        
        → Perfect 8th
      





C
Major
        Key Signature & Notation
      



Theoretical Scale

        The
        

 scale
        
        is a theoretical scale that contains double accidentals. For this
        reason, that scale is not used often. The enharmonic equivalent scale is
        used instead most of the time.
      



C
Major
      Diatonic Chords
    




I
ii
iii
IV
V
vi
vii°





C

C E G






Dm

D F A






Em

E G B






F








G








Am








B°












C
      Major Diatonic 7th Chords
    




IM7
iim7
iiim7
IVM7
V7
vim7
viiø7





C

C E G B






Dm7

D F A C






Em

E G B D






F

F A C E






G








Am7








Bø7












      In functional harmony for a major key:
      

          the tonic chords are chords
          I, iii & vi
        

          the subdominant chords are chords
          IV & ii
        

          the dominant chords are chords
          V & vii°
        






i
ii°
III
iv
v
VI
VII





C

C Eb G






D°

D F Ab






Eb

Eb G Bb






Fm








Gm








A








B












C
      Natural Minor Diatonic 7th Chords
    




im7
iiø7
IIIM7
ivm7
vm7
VIM7
vii7





C

C Eb G Bb






Dm7

D F Ab C






Em

E G B D






F

F A C E






G








AM7








B7












      In functional harmony for a minor key:
      

          the tonic chords are chords
          i & III
        

          the subdominant chords are chords
          iv, VI & ii°
        

          the dominant chords are chords
          V, v, VII & vii°
        


        The harmonic minor scale has a
        raised 7th scale degree compared to the natural minor
        scale, which makes that 7th scale degree into a
        leading tone and makes the V chord a
        major chord. In functional harmony, that V chord from
        the harmonic minor scale is used most often because the leading tone
        gives a stronger sense of wanting to be resolved to the tonic.
      



C
Major
      on the Circle of Fifths
    













      Common
      C
      Chords
    

click to hear the different chords









Order of sharps
F C G D A E B


Order of flats
B E A D G C F


Chromatic Scale

C-D♭-D-E♭-E-F-G♭-G-A♭-A-B♭-B
alternative enharmonic spelling:
C-C♯-D-D♯-E-F-F♯-G-G♯-A-A♯-B



Solfège syllables

Do Re Mi Fa So(l) La Ti Do



Accidentals

      Double Sharp: 𝄪
      Sharp: ♯
      Natural: ♮
      Flat: ♭
      Double Flat: ♭♭



Chord Symbols

      Major: M, maj, △
      Minor: m, min, -
      Dominant 7th: 7, dom7
      Diminished: dim, °
      Half-diminished: m7b5, ø
      Augmented: aug, +





I hope this music theory cheat sheet is useful! 😎
  
  You can get in touch with me here if you think I
  should add something else to this page.



Piano Samples

    The piano samples used for this music theory cheat sheet are from “
    Salamander Grand Piano V3
    ” by Alexander Holm, licensed under
    CC BY 3.0.
  












That's me, Seb, I'm creating muted.io. 😄

      Consider supporting this site by
      making a small donation via ko-fi here. Your support means so much and goes directly towards allowing me to
      spend more time creating fun and useful things for muted.io. 🙏
    
- Seb, ✌️ + ❤️




      Stay in the Loop
    



😎 Subscribe to be updated when I add something cool to the
            site:













"
https://news.ycombinator.com/rss,"Selfie – A tiny RISC-V C compiler, emulator and hypervisor",http://selfie.cs.uni-salzburg.at/,Comments,"






selfie | An educational software system of a tiny self-compiling C compiler, a tiny self-executing RISC-V emulator, and a tiny self-hosting RISC-V hypervisor.

























selfie
An educational software system of a tiny self-compiling C compiler, a tiny self-executing RISC-V emulator, and a tiny self-hosting RISC-V hypervisor.
View the Project on GitHub cksystemsteaching/selfie

Download ZIP File
Download TAR Ball
View On GitHub



Selfie is a project of the Computational Systems Group at the Department of Computer Sciences of the University of Salzburg in Austria.
The Selfie Project provides an educational platform for teaching undergraduate and graduate students the design and implementation of programming languages and runtime systems. The focus is on the construction of compilers, libraries, operating systems, and even virtual machine monitors. The common theme is to identify and resolve self-reference in systems code which is seen as the key challenge when teaching systems engineering, hence the name.
README for an overview of the system and all available resources.


This project is maintained by cksystemsteaching
Hosted on GitHub Pages — Theme by orderedlist





"
https://news.ycombinator.com/rss,Wasavi – Vi editor for any webpage,http://appsweets.net/wasavi/,Comments,"








wasavi - appsweets akahuku labs.











wasavi (VI editor for any web page)

Tweet 


#

wasavi is an extension for Chrome, Opera and Firefox. wasavi transforms TEXTAREA element of any page into a VI editor, so you can edit the text in VI.  wasavi supports almost all VI commands and some ex commands.
wasavi is under development. Any bug report or feature request is welcome.
And we also welcome a donation to continue development:


日本語版のREADME




A Quick Walkthrough

Here is a native TEXTAREA.  Focus the TEXTAREA, and press Ctrl+Enter to launch wasavi


Salient Features

wasavi supports some ex commands. This is the output of :set all

Vim's incremental search

wasavi online app. Open this link on a browser that has wasavi extension. wasavi will launch automatically. Then you can read and write files at your Dropbox/Google Drive/OneDrive account or local files.






How to install
Currently, wasavi is available for following browsers only. Select your browser and click the link. Standard extension installation procedure of your browser will follow. These extensions are hosted at the addons store of their respective browser.

Google Chrome extension
Opera addon
Firefox addon

Source code and latest development releases are hosted at Github:

Latest and unstable version of wasavi for Chrome
Latest and unstable version of wasavi for Blink Opera
Latest and unstable version of wasavi for Firefox

A note for Chrome users
Chrome has reserved some fundamental shortcuts, such as Ctrl+T, Ctrl+W and Ctrl+N. Although these keys cannot be used in wasavi, you can use Alt+T, Alt+W and Alt+N.




Frequently Asked Questions
How to launch wasavi
Focus TEXTAREA and press Ctrl+Enter.
How to quit wasavi
To quit wasavi press ZZ or :q or :wq or any other VI quit command.
Which options are accepted by the :set command?
See this table.
Note: there are also options which are accepted but don't have any effect yet.
How to modify initial settings:
Open preference wasavi extension (or enter :options on wasavi),
and edit ""exrc"" textbox.
How to control beep
Add set noerrorbells to your exrc to disable beep sound.  If you prefer a visual bell, add set visualbell instead.
Also, a chime at wasavi startup can be disabled with set nolaunchbell.
The volume of any beeps can be controlled with set bellvolume=N.  Range of value N is 1 to 100.
How to access local files
See document.
How to use wasavi with Vimperator/Keysnail/VimFx on Firefox
Vimperator
Put wasavi_mediator.js in your Vimperator plugin directory, for example,  ~/.vimperator/plugin or %HOME%\vimperator\plugin.
This plugin will control the pass-through mode of Vimperator according to the state of wasavi.
Keysnail
Put wasavi_mediator.ks.js in your Keysnail plugin directory.
This plugin will control suspend mode of Keysnail according to the state of wasavi.
VimFx
Latest VimFx recognizes wasavi as editable element.  While wasavi is running, VimFx suspends temporarily.
To use VimFx's key binding while wasavi is running, click outside area of wasavi or enter :set esctoblur and press <esc> in normal mode.  Then keyboard focus would be removed from wasavi, and you can use VimFx's key binding.
How to use wasavi as an independent text editor
Install the wasavi extension and open the link to wasavi online app. wasavi will start automatically. You can use ex commands :read, :write, :edit or :file to access your Dropbox/Google Drive/OneDrive files or local files. You will have to authorize wasavi via OAuth to access these storages.
About automatic setting override
The :set commands which you input while wasavi is running are stored to extension's persistent storage, and those are regenerated when you launch wasavi next time.
This setting override mechanism works each independent URLs (max 30). If you think this is unnecessary, put :set nooverride in your exrc. Then overriding will be skipped.
How to cooperate with Migemo
wasavi for Chrome can Migemo search.  Install Migemo Server, then input a special meta character \M in search query of / or ? command.  If \M included in search query, these search commands are executed via migemo.
I have noticed a bug
Please create an issue on wasavi issue tracker
Tips and Tricks

to maximize the wasavi: :set fullscreen or :set fs
to restore the wasavi: :set nofullscreen or :set nofs
to change a color theme: :set theme=blight or :set theme=charcoal or :set theme=matrix or :set theme=solarized or :set theme=solarized_dark
abbreviate syntax is


:abbreviate displays all the abbreviations currently registered.
:abbreviate [clear] clears all the abbreviations.
:abbreviate lhs displays the abbreviation corresponding to lhs.
:abbreviate lhs rhs registers a abbreviation which expands lhs to rhs.
:abbreviate [noremap] lhs rhs also registers, but it is not effected remap mechanism.

map syntax is


:map displays all the mappings currently registered.
:map [clear] clears all the mappings.
:map lhs rhs registers a rule which translates lhs to rhs. Its translation is recursive. About syntax of key stroke descriptor like <esc> in the lhs and rhs, see this page.
:map [noremap] lhs rhs also registers, but it is non-recursive.
:map targets the normal mode mappings. On the other hand,
:map! targets the insert mode. This is equivalent to vim's :imap.
For more detailed information, see Syntax of map command.

j k ^ $ moves cursor by physical row, on the other hand,
gj gk g^ g$ moves by wrapped row. To swap the behavior: :set jkdenotative
f F t T extension for Japanese: these commands recognizes reading (ro-ma ji
expression) of hiragana, katakana, and kanji. For example, fk will place
a cursor on 'か', 'カ', '漢' and so on.
f F t T extension for Latin script: these commands recognizes the base alphabet
of diacritical marked letter. For example, fa will place a cursor on
'å', 'ä', 'à', 'â', 'ā' and so on. Also see mapping table.
use a online storage as file system:


:filesystem status shows all file systems currently available.
:filesystem default shows default file system. You can set default file system
via :filesystem default dropbox or :filesystem default gdrive or :filesystem default onedrive.
:filesystem reset discards the access token for online storage.
You can place the file system name at the head of a file name explicitly:
for instance, :read dropbox:/hello.txt.

When you read from the register of A to Z, some registers returns special content:


B register: user agent string
D register: current date time string (formatted by using datetime option as template of strftime(3))
T register: title string
U register: URL string
W register: version string of wasavi

To return a setting to default state:


:set <option-name>& or :set <option-name>&default

To return all settings to default state:


:set all& or :set all&default

To return a setting to the state just after evaluation of exrc:


:set <option-name>&exrc

To return all settings to the state just after evaluation of exrc:


:set all&exrc

To submit a form automatically after writing text and closing wasavi:


:wqs
:submit (this can be shortened to :sub )


Commands implemented

[count] operation [count] motion
[count] operation [count] range-symbol
[count] surround-operation [count] motion surround-string
[count] surround-operation [count] range-symbol surround-string
[count] de-surround-operation [count] surround-identifier
[count] re-surround-operation [count] surround-identifier surround-string
[count] operation-alias
[count] surround-operation-alias surround-string
[count] motion
[count] scroll-command
[count] edit-command
[count] : ex-command

Operations
c y d > < gq gu gU
Operation Aliases
cc yy dd >> << C Y D gqq guu gUU yss ySS
A counter can be inserted in front of the last 1 character.
Surround Operations

to surround: ys yS
to remove a surround: ds
to change a surround: cs

Motions
- + ^ <home> $ <end> % | , ;
_ / ? ' ` ( ) { } [[ ]] <enter> 0
j k h l ^N ^P ^H
<down> <up> <left> <right> <space>
w W b B e E gg gj gk g^ g$ G H M L f F t T n N
Range symbols (Vim text objects)

a"" a' a` a[ a] a{ a} aB a< a> a( a) ab aw aW ap as at
i"" i' i` i[ i] i{ i} iB i< i> i( i) ib iw iW ip is it

Scroll commands
^U ^D ^Y ^E ^B ^F <pageup> <pagedown> z<enter> z. zz z-
Edit commands
x X <delete> p P J . u ^R ~ ^L ^G ga gv m @ q r R a A i I o O & s S v V ZZ gi ^A ^X
ex commands
abbreviate cd chdir copy delete edit file filesystem global join k map mark marks move options print put pwd quit read redo s & ~ set sort submit registers to unabbreviate undo unmap version v write wq wqs xit yank > < @ *
The addressing in ex command is fully supported:

whole buffer: %s/re/rep/
current line: .p
the last line of buffer: $p
absolute line number: 1,2p
relative line number: +1,+2p
regal expression: /re/p ?re?p
mark referencing: 'a,'bp

In addition to this wasavi also accepts offset, for example: /re/+1p.
Two addresses are usually connected by a ,, wasavi also supports ;.
Input mode commands

^@ input the most recently input text, and exit input mode. this key stroke is actually Ctrl+Space.
^D unshift. but if the last input character is 0 or ^, delete all indentation
^H delete a character
^R paste register's content
^T shift
^U delete all the characters entered in the current input session
^V literal input
^W delete a word

Line input mode commands

^A move cursor to top of line
^B back
^E move cursor to end of line
^F forward
^H delete a character
^N next history
^P previous history
^R paste register's content
^U delete whole line
^V literal input
^W delete a word
tab complete ex command name, set option name, file name argument of read/edit/write/file

Bound mode commands
Bound mode is similar to vim's visual mode.

c delete the bound, and switch to insert mode
d delete the bound
y yank the bound
< unshift the bound
> shift the bound
C delete the line-wise bound, and switch to insert mode
S surround the bound
R same as C
D delete the line-wise bound
X same as D
Y yank the line-wise bound
g prefix commands
a, i prefix range symbols
~ swap lower case and upper case in the bound
: switch to line input mode
J join the bound
p delete the bound, and paste a register's content
P same as p
r fill the bound up with inputted letter
s same as c
u lower-ize the bound
U upper-ize the bound
v character wise bound mode
V line wise bound mode
x same as d
^A add the counter to all numeric strings within the bound
^X subtract the counter to all numeric strings within the bound

Surrounding identifiers

quotations: one of !#$%&*+,\-.:;=?@^_|~""'`
brackets: one of abBrt[]{}()

Surrounding string

quotations: one of !#$%&*+,\-.:;=?@^_|~""'`
brackets: one of abBr[]{}()
tags: one of ^T ,<Tt

Vim features in wasavi

multiple level undo/redo
incremental search
range symbols (aka, Vim text objects)
following registers


"" unnamed register
: last executed ex command
* reading from and writing to the system clipboard
/ last searched string
= evaluate math expression. supported operators are: + - * / %. supported numeric expressions are: integer, float (including exponential form), binary (with leading 0b), octal (with leading 0), hex (with leading 0x)

auto-reformat in input mode, and reformat operator (gq command) on the state of textwidth > 0
bound mode (aka, Vim visual mode)
options: iskeyword, incsearch, smartcase, undolevels, quoteescape, relativenumber, textwidth, expandtab, cursorline, cursorcolumn, nrformats
writing to the register of A to Z
gu / gU + motion: lowerize or upperize a region
partial functionality of Surround.vim
partial functionality of :sort (regex pattern, r and i options)
^A to increase a numeric string. ^X to decrease a numeric string.







"
https://news.ycombinator.com/rss,The peculiar event sourced deadlock,https://jappie.me/the-peculiar-event-sourced-deadlock.html,Comments,"The peculiar event sourced deadlock / JappieJappie


<

About 📂
About me

>




<

Hire 🐧
Jappie for hire

>




<

Raster 🚀
Easy rosters for restaurants startup

>




<

Email ✉
Contact me

>


  The peculiar event sourced deadlock  published: 15日 01月 2023年 One thing that always surprises me is how casually serious problems are phrased by business people in their blissful ignorance. “Hey why am I seeing the down for maintenance screen?” “Oh try it now, the pack uploading has finished”, Said the QA engineer to the product manager. Once I saw this on slack, I grew really suspicious and started asking questions. After all, isn’t it a bit odd we’re seeing a down for maintenance screen in one part of the system, simply because another part is being used?Initially we thought this was caused by high CPU usage. The graphs showed high CPU load while processing packs, so maybe the rest of the system was being deprioritized somehow. Before assuming that was the cause however, I decided to reproduce the issue first. Here I noticed I could for example load the risk index easily (a read operation), but connecting a risk to a pack (a write operation), would hang forever. This made me suspect that the issue wasn’t CPU usage at all, so I asked Postgres to list it’s locks. Which showed several locks in progress. This lead me to the event source system. The event source system is at the core of all our business logic. In essence, it provides a ledger of all important business write activities that can happen. This is useful for auditing purposes for example.Welcome to an after action report of a complicated system level bug. It took me a week to find a satisfying solution. To start I need to sketch context. I’ll only use raw SQL because this entire story is related to the database and how we use it for event sourcing. So consider the tables of an event source system:CREATE TABLE event (
    id serial PRIMARY KEY NOT NULL,
    payload jsonb NOT NULL,
    type character varying NOT NULL,
    created timestamp with time zone NOT NULL
);

CREATE TABLE event_last_applied (
    id serial PRIMARY KEY NOT NULL,
    event_id bigint NOT NULL REFERENCES event (id)
);
In here the type and payload fields contains the information to (re)apply that event. The type will indicate what business logic or queries to execute, and the payload holds information for that logic. As we’ll see later, these queries will involve modifying other normal tables within a transaction. This application of events, or re-application through business logic or queries is called projecting. A type can for example be create-user and the payload would contain the data required for creating said user, for example {email:'hi@jappie.me'}. The id provides a unique global ordering, and the created field contains a timestamp of when the event was created, which is used for database administration purposes. Finally, the event_last_applied table is used to indicate whichever event was last applied, so the system can figure out if additional events need to be re-projected from the event table.Inserting an event works by projecting an event to normal Postgres tables in a transaction. Once this operation is not rejected by foreign keys, type errors or program exceptions, the event gets recorded in the ledger, also known as the event table. For example:begin;

/* left out projection code, insert user into tables here,
or do other projection stuff, as dictated by the event type*/

INSERT INTO event (payload, type, created)
    VALUES ('{""email"":""hi@jappie.me""}', 'create-user', now());
INSERT INTO event_last_applied (id, event_id)
SELECT 1, max(id) FROM event
ON CONFLICT (id)
    DO UPDATE SET
        event_id = lastval();
commit;
If the projection fails the entire event gets rejected, which means all changes within the transaction get rolled back by Postgres. This applies relational guarantees, to a non-relational system trough a transaction. We also weave this transaction trough business logic code, so that in case of an exception, we rollback. Quite an elegant solution, which I didnot invent.On system boot we figure out if we need to reproject or not, the query is rather simple:SELECT type, payload FROM event
WHERE
    id > (
        SELECT event_id FROM event_last_applied
        WHERE id = 1)
ORDER BY
    id ASC;
which returns something like this, telling the system what to do:    type     |          payload          
-------------+---------------------------
 create-user | {""email"": ""hi@jappie.me""}
With that, we can reproject, also known as replaying history. Replaying history involves truncating all tables that are event sourced. And then truncating the event_last_applied table, which in this case just removes the one row. Then the system will notice it needs to replay events on boot for example. This is a rather dangerous operation, because if any event fails, you may have potentially lost data. A lot of things can go wrong with a large history, foreign keys, exceptions, serialization mismatches, events out of order etc. Transactions can help here as well, and make this re-projection safe.DeadlockThere is one more important piece of context: An event maybe composed with other events into larger transactions. For example, if we create a user, we may also assign him to a company within the same transaction. In SQL that looks like this:BEGIN;

/* left out projection code, insert user into tables here */

INSERT INTO event (payload, type, created)
    VALUES (
        /* whatever event source data*/
        '{""email"":""hi@jappie.me""}', 'create-user', now());
INSERT INTO event_last_applied (id, event_id)
SELECT 1, max(id) FROM event
ON CONFLICT (id)
    DO UPDATE SET
        event_id = lastval();

/* left out projection code, connect user to company */

INSERT INTO event (payload, type, created)
    VALUES (
        /* whatever event source data*/
        '{""company-id"":2, ""user-id"": 1}', 'connect-company', now());
INSERT INTO event_last_applied (id, event_id)
SELECT 1, max(id) FROM event
ON CONFLICT (id)
    DO UPDATE SET
        event_id = lastval();
COMMIT;
Transactions form proper monoids, and they can grow arbitrarily large. This is good because even for large chuncks of business logic we always gaurantee our event log remains in a valid state. We’d expect our re-projections to always work, because only correct ones get recorded. Where does this go wrong then?The issue is concurrency, consider connection A and B:A opens a transaction and inserts a user, but has to do other projections and event insertions as wellB opens a transaction and wants to insert an event, B has to wait until A completes. This is because A made an update to the event_last_applied on row number 1, as part of the insert event logic. This row is locked until A completes, so B has to wait.A completes and releases the lock on row 1.B can now complete as well.This is not a deadlock as long as A completes. B can wait a long time because our transactions can grow arbitrarily large. For example when we’re inserting millions of rows of data, taking up half an hour. Which is far beyond the HTTP session length of 30 seconds, or whatever length a user finds acceptable. This was indeed the production bug encountered at supercede. One user was doing pack ingestion, which involves reading millions of excell file rows, and the rest of the system became unusable because of that.Now what?At first I started with the most obvious solution. I re-grouped how event sourcing took place. I put the event sourcing code at the end of the transaction in pack ingestion, so that the event source table remained available for other transactions up till that point. Because event sourcing is only a small part of normal transactions, this created a small locking window. Thus this worked! However it only worked for this transaction with pack ingestation, I didn’t know if there were any other transactions like this in our code base. Furthermore, I had to bypass parts of the event sourcing interface to make this work. For example, I had to project events by hand, and insert events by hand, rather then using the internal library. I decided this was a bad precedence to set. I was afraid other engineers would copy this approach when it wasn’t necessary. So I went looking for other solutions.Another idea is that instead of doing the large transaction, we could split it up into smaller ones. Allowing other events to clear while this bigger one was in progress. I didn’t like this either. For one this code was old, tried and tested, making a rather large modification like splitting the transaction could introduce many unintended bugs. For example when cleanup doesn’t happen correctly on failure. I thought this was likely because this transaction was large, and covered many tables. Also our normal tools such as types and integration tests wouldn’t help a lot with guaranteeing cleanup. So this would become difficult to maintain fast. Which is problematic for a piece of code which is the “money maker”, and needs to change often. Furthermore I had a much more simple but thorough solution in mind.I decided to redesign the event source tables. Naturally my colleagues exclaimed shouts of joy when I decided to modify an even older system. The event source system described above is almost as old as supercede. But I believed it was easier to modify, and more importantly, easier to test for correctness. Furthermore this would also solve the problem for other, possibly unknown, or future, large transactions. This change would keep our code easy to maintain and solve a bug. The new schema looks almost identical to the old one:CREATE TABLE event (
    id serial PRIMARY KEY NOT NULL,
    payload jsonb NOT NULL,
    type character varying NOT NULL,
    created timestamp with time zone NOT NULL
);

CREATE TABLE event_applied (
    id serial PRIMARY KEY NOT NULL,
    event_id bigint NOT NULL REFERENCES event (id),
    created timestamp with time zone NOT NULL
);
The big difference is that we renamed event_last_applied to event_applied and added a created field. With this change, inserting events is also quite similar to the initial system:BEGIN;
INSERT INTO event (payload, type, created)
    VALUES ('{""email"":""hi@jappie.me""}', 'create-user', now());
INSERT INTO event_applied (event_id, created)
SELECT last_value, now() FROM event_id_seq;
COMMIT;
The big difference is that instead of modifying always row number 1 to be the latest ID, we insert a new row into event_applied with the latest id. This avoids locking of row number 1. For re-projection we truncate the event_applied table, allowing the code to rerun all those events. The big difference is in figuring out which events haven’t been applied yet:SELECT type, payload FROM event AS e
WHERE
    NOT EXISTS (
        SELECT 1 FROM event_applied
        WHERE event_id = e.id)
ORDER BY
    id ASC;
We compare the event table to the event_applied table, and return any events that don’t exist in that. We’re still ordering by id to ensure the correct order. Is this correct? Let’s consider concurrency once more with connection A and B:A opens a transaction and inserts a user, but has to do other event source queries as well.B opens a transaction does it’s projection work and wants to insert an event, B creates a new row in the even_applied table and completes. There is no need to wait since there is no single row lock. So B finishes.A finishes it’s other event sourcing completes.This doesn’t deadlock. However it’s not completely correct in that A get’s id 1. and B get’s id 2, but A‘s transaction finishes after B by inserting another event with id 3. So on reprojection one of A‘s events get’s applied before B. But in the initial projection, all of A‘s event happened after B. So the first event of A is out of order. This may cause issues. This problem was also present in the original implementation, since an id is acquired before the lock waiting happens. I think a solution would be to group the events by transaction id, and then order by last created event. In this case all events created before B in A‘s transaction would be pushed behind it by an event happening after B finishes. If we do that, the event table gets an extra field:CREATE TABLE event (
    id serial PRIMARY KEY NOT NULL,
    payload jsonb NOT NULL,
    type character varying NOT NULL,
    created timestamp with time zone NOT NULL,
    transaction_id bigint NOT NULL
);
Our insert function retrieves the transaction id with txid_current:BEGIN;
INSERT INTO event (payload, type, created, transaction_id)
    VALUES ('{""email"":""hi@jappie.me""}'
           , 'create-user'
           , now()
           , txid_current());
INSERT INTO event_applied (event_id, created)
SELECT last_value, now() FROM event_id_seq;
COMMIT;
And our unnaplied events query now groups:SELECT
    array_agg(type) AS types,
    array_agg(payload) AS payloads
FROM event AS e
WHERE NOT EXISTS (
      SELECT 1 FROM event_applied WHERE event_id = e.id
    )
GROUP BY transaction_id
ORDER BY max(id) ASC;
If we run that unnaplied events query on an event table like this:id |        payload        |      type       | created    | transaction_id 
---+-----------------------+-----------------+------------+----------------
 6 | {email: hi@jappie.me} | delete-user     | 2023-01-15 | 77958
 7 | {email: hi@jappie.me} | create-user     | 2023-01-15 | 77959
 8 | {company-id: 2}       | delete-company  | 2023-01-15 | 77958
We’d get a result like:             types             |              payloads
-------------------------------+-----------------------------------------
 {create-user}                 | {{email: 'hi@jappie.me'}}
 {delete-user,delete-company}  | {{email: 'hi@jappie.me'},{company-id: 2}}
Which is what we want. Even though the create user event happened while the delete user event was happening, the delete user event was part of a larger transaction. So the create user even should come first when re-projecting. This allows arbitrary sized transactions to project alongside each-other and provides better ordering guarantees then the original implementation.Closing thoughtsPhew, that was a lot. I didn’t think this would become such a large post. Designing an event source system on Postgres transactions is rather hard. All I wanted to do is clear my thoughts on the matter, but that grouping issue is another bug I just found by writing about this 😅.I think the biggest lesson I’ve (re)learned from the deadlock bug itself is to make sure you reproduce an issue first before diving into solutions. Even nasty business threatening system level bugs like these can sometimes be solved with some minor modifications to the system. If we had skipped this small step of reproducing the issue, we may have focused on the CPU observation and moved pack ingestation to a separate machine, which would’ve taken weeks to implement and not solve anything.Furthermore, it’s humbling to see that even after having used relational databases for more then a decade, I still can learn new things about them. For example Postgres’ auto increment sidesteps the transaction, which was quite shocking to me. A rather important detail to keep in mind when reasoning about these systems.I made a github repository for playing around with the queries more easily. I hope you enjoyed this article, please leave a comment if you have any questions or suggestions below.Resources
The code in this blogpost
Postgres Lock monitoring
Blogs on event sourcing
Presentation on event sourcing


        Posted by Jappie J. T. Klooster
        in 

 reflection


  published: 
  15日 01月 2023年 


#postgres
#deadlock
#programming
#sql
#database

Recent stuff




                The peculiar event sourced deadlock
            





                Summerhouse Paradis Aruba
            





                Why do I still write this blog?
            





                Zurich hack 2022 Denotational Design
            





                Restoring mysql innodb on windows.
            





                Failing in Haskell
            





                Installing a NixOS desktop tracked with git
            





                A brief intro to MTL
            

Tagsaustraliabuild-toolscssdatabasedevopsfrphaskellindonesiajakartajoblinuxnixnixosopinionpainpragmatic-haskellprogrammingreflexservantstacktesttimetoolstraveltutorialvirtualizationwebsitework
Linkedin
Twitch
Youtube
Github
Penguin
Raster
Facebook
Twitter
Reddit
Discord
 Those who know do not speak. Those who speak do not know.  Powered by Pelican. Source code, licensed under GPLv3. "
https://news.ycombinator.com/rss,Ask HN: When to make the jump to freelance/consultant?,https://news.ycombinator.com/item?id=34400435,Comments,"

Ask HN: When to make the jump to freelance/consultant? | Hacker News

Hacker News
new | past | comments | ask | show | jobs | submit 
login




 Ask HN: When to make the jump to freelance/consultant?
67 points by mxmpawn 4 hours ago  | hide | past | favorite | 54 comments 

I'm working full time as a data engineer/scientist but I also have one ongoing customer (a previous employer).Another previous employer is launching a startup and has recently pinged me because they need to build a series of data pipelines and ML models for their product.I've to talk to them about specifics but I don't see myselft having enough available time to make it work.I've been thinking about starting my own data/ml services company for a while but I don't really know when to make the jump. I think (is a guess for the time being) that the income from this new job, in addition to the income of my current client, could be enough for my living expenses of this year, so I'm thinking if this is a good time to make the jump or not.The problem that I see is that this new lead is from a previous partner of my current client, so both jobs are related to my previous employer, I don't have a pipeline of possible prospects for my service, so I'm not sure if I'll be able to generate a pool of prospects while doing the work for this customers.My guess is that I should wait until there is a sign that I could probably get a stream of clients to keep the wheel going, and try to find the time to take this new job, maybe negotiating terms to make it possible.What do you think?Edit: I've savings already as I plan to buy a new home (I'm in Argentina, we buy it cash, no mortgage). I need three more months of salary to accomplish the home budget. 
 
  
 
wpietri 2 minutes ago  
             | next [–] 

My general answer for people asking this question is: when you think you can sustain yourself. And it sounds like you're almost there.It sounds like the question for you is whether you can find new clients at a rate fast enough to replace old ones. This is a question you'll never be able to answer fully as an employee, because employers generally don't like you putting out an ""open for business"" sign. So in your shoes I'd put together a marketing/sales plan, take the new customer, and devote some time over the next N months to seeing if you can build up enough interest and leads that you will have plenty of work down the road.It sounds like the worst case here is that you get to the end of your contracts and have to go get another job again. Which is not a terrible outcome as long as you don't burn your bridges. And it's worth noting that after a period of freelancing you may decide you'd rather have a job anyhow, so you can look at the next months/years as an experiment not just on prospects, but on whether or not you will be happy with it.
 
reply



  
 
dadro 2 hours ago  
             | prev | next [–] 

Every scenario is going to be different, I took the plunge as a software consultant and can provide some datapoints.I was laid off from my job as a software engineer about 5 years ago. I had previously worked in software consulting and knew the operating model and had a handful of potential clients. I had some savings and my wife works full time so I decided to give it a go for 6 months to see how it would go. I partnered with a colleague who was also laid off and had same appetite for risk as I did.  * The first year was a grind, especially doing the non-technical tasks like networking/business development/invoicing/etc. 
  * We ended up being profitable on year 1, I made less than my previous gig as a lead engineer. We grew from 2 of us to having 1 FTE and 3-4 dev contractors in year 1. 
  * Keep in touch with engineers you liked working with, hiring is WAY harder than I expected. If you decide to grow, you will need a team for larger contracts. 
  * Business development is largely a numbers game, you have to get out of your comfort zone and talk to a lot of people/companies and get on their radar. 
  * One of our first non-technical hires was business development. We did this when we hit $1mm/yr rev. 
  * Being in a niche can be helpful if you are able to explain your value prop AND to differentiate yourself.
  * Don't view other consulting firms as competition. We've formed some great relationships with other companies that align with our engineering process and refer work when we have too much and get work when they have too much.

Over the last 5 years we grew from 2 ""founders"" (along with some former colleagues as contractors) to about ~40 employees (80% FTE's/20% contractors) all remote, US based. In hindsight, I think my favorite size was when we were ~8 people. It was big enough to take on 1-2 large-ish contracts, but less stress in keeping pipeline full. The risk tolerance for having a 6 figure payroll every 2 weeks is not for everyone!(edit formatting)
 
reply



  
 
eddsh1994 1 hour ago  
             | parent | next [–] 

How many years experience did you have? :)
 
reply



  
 
dadro 9 minutes ago  
             | root | parent | next [–] 

I had about 15 years professional experience as a developer.
 
reply



  
 
PragmaticPulp 1 hour ago  
             | prev | next [–] 

> Edit: I've savings already as I plan to buy a new home (I'm in Argentina, we buy it cash, no mortgage). I need three more months of salary to accomplish the home budget.This is a significant last-minute edit. There's a lot of good advice throughout this thread, but I would suggest waiting until you've completed the home purchase and settled in before considering the jumping to freelance. It's only a few more months and you'll have a much better understanding of any issues and unexpected expenses of the new home.Beyond that, you need to make sure that you can do the process of networking, selling your services, cultivating a pipeline of clients, and collecting from clients who don't pay on time. Doing the work is only half of the battle when you're a freelancer. If you don't excel at the business side, you might not enjoy it after you run out of immediate contacts with work for you to do.
 
reply



  
 
ljm 44 minutes ago  
             | parent | next [–] 

This is true. I can't speak to the process of buying a home in Argentina but buying a house and moving is a significant change, and leaving your job to go self-employed is another significant change.Finish the home project before you commit to the self-employment project. Your full-time job is a safety net you won't have when you first go contract.No harm in putting feelers out meanwhile though, networking and making yourself known in relevant circles. Worst case is that you end up with more full-time job prospects and not freelance ones.
 
reply



  
 
ernestipark 10 minutes ago  
             | prev | next [–] 

How much work do you think the new work would take? Depending on the nature of the engagement, it doesn't have to be a 'jump' at all. If possible, an option would be to make it a more reduced hours engagement that you can moonlight. Then you can get the consulting experience, maintain the income of your full-time job, and see how it goes. Once you have one engagement under your belt, then it also becomes much easier to get more through WOM and very basic marketing/promotion through LinkedIn on what your skills are and what you can offer.As others have stated in the comments also, a lot of consultants never have to do marketing or sales, they just get it word of mouth through their networks as they continue to do good work. I think doing that first job well, but in a way that's not as scary as jumping away from your full-time job is a good choice if you have it.
 
reply



  
 
nocubicles 3 hours ago  
             | prev | next [–] 

I just started this today actually. Today is the first day where I am no longer employeed by any company and will need to find my own customers, invoice them etc. My field is ERP development and consultation.I have 10+ years experience in the field and I was working for a consultation company. But on the same time I would always get messages on Linkedin from recruiters and at some point I thought I should give it a go and try to work on the projects during the nights and weekends.Did that for like 6 months and felt I could do it full time and then I just kinda did it and will be doing it in the future.
 
reply



  
 
cableshaft 2 hours ago  
             | parent | next [–] 

Sounds like you got freelance work from recruiters contacting you for full-time job opportunities. How did you ask them about that, out of curiosity?
 
reply



  
 
NiagaraThistle 2 hours ago  
             | root | parent | next [–] 

I've done this - with Web Development, but still got projects this way. I just responded to the recruiter or directly to the company if I had their info, and stated my background, my rate, and asked if they were interested in working with my in a contractor capacity. It's gone very well, and I have several ongoing contracts from this process.Basically just state that you are interested in the work, but not the current agreement and see if the company is open to hiring you as a contractor. If your experience is good and viewable/provable, they probably will be. And Bonus points if you are a good communicator and timely with such and delivering deadlines - many contractors are not and companies/recruiters get burned by them and those who ARE dependable are worth their weight in gold to companies.
 
reply



  
 
nocubicles 2 hours ago  
             | root | parent | prev | next [–] 

I always told them that i'm not looking full time but only part time but that only did work once I think. Most success came from building a small persona online, participating in the discussions, networking on Discords/Twitter and then the opportunities came.
 
reply



  
 
MuffinFlavored 47 minutes ago  
             | parent | prev | next [–] 

how different are WMS and ERP?
 
reply



  
 
janetacarr 1 hour ago  
             | prev | next [–] 

Personally, as a freelancer/consultant, I never think there is a right time for anything in my life. Of course, you might feel differently (and it probably is different). I'm offering advice from my perspective only, and I don't have a home to make, or children to worry about. I've been freelance/consulting/whatever for about 2 years, so make of this what you will, but I'm still early stages having just been where you're at.First I want to say, the risk might not be as big as you might think it is. You've a pretty in-demand skill set right now, more so than most SWEs. If things don't pan out as you expect, you can always get another full-time position despite the 'looming recession'.That said, here is some unsolicited advice. Rebuttal as necessary :)Regarding leads/prospects, if you do decide to make the jump, chances are you'll be tapping your professional network for leads, or pitching strangers if you happen to get on a call with them via cold email. This can work for a while, maybe get your first recurring clients, but I don't recommend working with a matchmaker like Toptal, Fiverr, Upwork, or an agency.The platforms tend to have poor pay and bad clients. Agencies will limit your ability to build a direct relationship with the clients. I know people have built successful businesses using both of them, but for me having a direct relationship with high-value clients seems to have paid off doubly as I can set my own terms (fixed rate / value based) rather than the typical hourly model. If you can/want to build such a relationship, you should *get on the phone with a decision maker* (a huge unlock for closing work). Regular dev/engineer interview channels will yield regular dev work, pay, and circumstances (maybe that's a feature for you).After a few sales calls for clients, You may realize getting leads to come to you is best, or at minimum people should have a reason to answer your emails like having some kind of branding or marketing, so start writing, coding, tweeting, or whatever regularly to get attention. Keep at it. It's hard work.On top of everything, you will fuck up, and that's okay, so give yourself some breathing room financially and mentally.
 
reply



  
 
xeromal 1 hour ago  
             | prev | next [–] 

One element required is to have a network big enough to sustain itself off recommendations. I think that's absolutely necessary at least for 80% of your business revenue. If you're just hopping from one client to the next, you're not going to be in a strong decision to make good choices for your company when it comes to negotiations.
 
reply



  
 
sirsinsalot 1 hour ago  
             | parent | next [–] 

I've been a consultant through my own company for 15 years and never needed to do this.I simply apply for contract roles.My day rate is 150+% of the average for my role and area and my contract terms are ridiculously weighted in my favor.You can learn to negotiate going from one client to the next.And you mainly have to be able to negotiate better than the client. That's pretty easy usually.
 
reply



  
 
moneywoes 1 hour ago  
             | root | parent | next [–] 

Where do you find these roles
 
reply



  
 
Nextgrid 1 hour ago  
             | root | parent | next [–] 

Seconded.The vast majority of contract roles I see here in the UK are brokered by recruiters who will enforce their own standard contract terms & rates on a take-it-or-leave-it basis.This kind of approach seems like it has no chance of working as the recruiter would rather place someone at an ""okay"" rate rather than having to risk submitting a higher-rate candidate and having the client balk and go away altogether.
 
reply



  
 
sirsinsalot 13 minutes ago  
             | root | parent | next [–] 

Yes recruiters, and outside IR35.It isn't true that they set rates and dictate terms. Push back, negotiate with them.If you have recruiters who know you deliver, which drives more business for them and makes them look good, they'll listen to you and break their backs to place you even at rates higher than other people. They become your marketing team.They're salespeople. Make their job easy and make yourself a tool that generates more long term commission for them. They'll bank on you and you'll be first in line.All they care about is an easy win and their commission and sales targets. If you're their easy win, you're golden. The higher the rate they can place you at the more it works for them because the commission is percentile.I interview really well. I'm a salesman too. They know if they can get me in the door I'll get the gig and they can get their cut of 50% above market rate.I get to strike terms from contracts, set my notice period ... whatever I want really.Build the relationships.
 
reply



  
 
sirsinsalot 1 hour ago  
             | prev | next [–] 

15 year+ consultant through my own company with recurring clients and more work than I can handle (and no interest in delegating or growing)Never needed to network. Don't have to market myself. Never had or needed a full time position and never been short of work.Get to know good recruiters who have streams of contract work with goal based outcomes.Learn how the contracts are managed by middle men and understand why consultants and contractors are needed and what risk profiles they serve.I'm used when times are tough, deadlines are tight or goals absolutely have to be hit.My day rate is higher than my peers.I'd say do it, find your niche and be ome the goto person for a specific kind of work.
 
reply



  
 
aantix 1 hour ago  
             | parent | next [–] 

Why work with recruiters at all?They’ll charge 30-50% over your rate. They provide zero value besides the initial contact.Their overhead impacts your negotiation leverage for a higher rate.
 
reply



  
 
sirsinsalot 18 minutes ago  
             | root | parent | next [–] 

Recruiters here typically take 10-15% and I don't care as long as I'm getting what I want in revenue.Most recruiters are absolute snakes. They can be very useful and you don't have to let them negotiate for you.At the end of the day, as long as they get their cut they don't much care.Saves me endless hours at $X/hr searching myself. It's more cost effective for me to bill a client for those hours than market myself.If you know the industry and find good recruiters they'll break their back to place you.
 
reply



  
 
benjaminwootton 1 hour ago  
             | root | parent | prev | next [–] 

Recruiters typically charge approximately 10% in the markets I am familiar with.They also act as a free sales force and have supplier agreements with end clients.  These are very hard to secure directly for one man bands.I would argue that it's not really consulting or running your own business if you are working through a recruiter.  It's more akin to short term employment.  That may be fine for your aims however.
 
reply



  
 
sirsinsalot 10 minutes ago  
             | root | parent | next [–] 

Recruiters are also an insurance policy for clients. They mitigate some risk.
 
reply



  
 
sjducb 3 hours ago  
             | prev | next [–] 

It sounds like the perfect time to jump to freelance. You've got 2 clients already.However... What happens if both clients back out and you suddenly get no work for a year? Do you have 1 year of savings runway? Will your family and living situation survive no income for a year? You need at least a year of runway because it's perfectly normal to go for 2 months with no income and if you've only got 3 months of savings then you'll start panicking.Also getting a mortgage as a freelancer is more complicated. Have you bought all of the houses you want for the next 2 years? (Freelancer mortgages require 2 years of company accounts, so you won't be able to get a mortgage for the next 2 years)
 
reply



  
 
angarg12 41 minutes ago  
             | prev | next [–] 

I'm in a similar boat (worse actually, since I'm on a visa) so I can't give specific advice, but @patio11 [1] greatly encouraged me to consider freelance/consulting as a career path.I am going through the grind of prepping for the tech interview all over again, and it just occurred to me that I might be approaching this from the wrong angle. If instead of spending all these hours every day solving leetcode problems or trying to ""grok"" the system design interview, I spent them networking and building a portfolio, it might result in a much better ROI.But alas, my visa status doesn't allow me to do anything either way. Maybe one day.[1] https://www.kalzumeus.com/2012/09/17/ramit-sethi-and-patrick...
 
reply



  
 
indymike 1 hour ago  
             | prev | next [–] 

> The problem that I see is that this new lead is from a previous partner of my current client, so both jobs are related to my previous employer, I don't have a pipeline of possible prospects for my service, so I'm not sure if I'll be able to generate a pool of prospects while doing the work for this customers.Work on building your pipeline, and having customers from launch day on.
 
reply



  
 
comprev 3 hours ago  
             | prev | next [–] 

When you have enough savings to take the plunge and survive until you either pick up work or return to a perm position.Many people love the idea of being their own boss but the reality can be quite different and they return to perm roles.
 
reply



  
 
brownrw8888 2 hours ago  
             | prev | next [–] 

Toptal is what made it easy for me.  The most exhausting thing about running your own consultancy is networking and maintaining a funnel of new clients.  Unfortunately this kept me away from freelancing for years :(I was in a similar situation like you (FTE + consultant) where I was looking for prospects and not able to find enough.  It's a full-time job in itself...  I vastly expanded my opportunities by tapping into a bigger talent network with Toptal.Please reach out and I'd be happy to share morehttps://www.toptal.com/qal80m/worlds-top-talent
 
reply



  
 
bigmanwalter 2 hours ago  
             | parent | next [–] 

What kind of hourly rates can you expect to find on Top Tal?
 
reply



  
 
pastacacioepepe 1 hour ago  
             | root | parent | next [–] 

Definitely a lot higher than on Upwork, on average.
 
reply



  
 
Nextgrid 1 hour ago  
             | root | parent | next [–] 

That's not a high bar though.
 
reply



  
 
pxue 1 hour ago  
             | parent | prev | next [–] 

Who sets the rates on Toptal?I was going to use lemon.io but they cap out at $100usd an hour.Not worth my time.
 
reply



  
 
bckygldstn 23 minutes ago  
             | root | parent | next [–] 

You (the consultant) sets the rate you get paid. Toptal adds a secret margin on top of that which is what the client pays.I haven't done Toptal in a while, but my rate was always higher than that. I got pushback from from Toptal's internal recruiters whenever I asked to increase my rate and a few of their clients did turn me down due to the rate. But I never backed down and it never took more than a few weeks between starting applying to jobs on Toptal to signing a contract.I wouldn't want to depend on Toptal for my income. But it's great as a way to fill in between other engagements if you're firm with your rate and the kind of work you want.
 
reply



  
 
moneywoes 1 hour ago  
             | parent | prev | next [–] 

Aren’t these sites just a race to the bottom for devs especially with global reach
 
reply



  
 
thegeomaster 2 hours ago  
             | parent | prev | next [–] 

Looks like they are not accepting new devs right now.
 
reply



  
 
akmittal 1 hour ago  
             | parent | prev | next [–] 

In last 2 years developer pool has increased a lot on Toptal. There are not enough jobs there now.
 
reply



  
 
physcab 3 hours ago  
             | prev | next [–] 

If you have enough connections to get you through your first 6 months or so then you can make the jump. Often when consulting your rate is double your normal take home so it balances out the period of searching for more work. But you highlighted the key anxiety in going freelance: you will always have to be selling your services. Its just like any startup except you are the product. Having to get new gigs will always be part of the job whether its now, 6 months from now, or 3 years from now
 
reply



  
 
anon223345 24 minutes ago  
             | prev | next [–] 

Freelance is a very different thing than actual consulting by the wayWhat I mean is working for a consulting firm is substantially different than what you do as a freelancer.
 
reply



  
 
collyw 21 minutes ago  
             | parent | next [–] 

That depends, it's like saying a software engineer is different from a programmer. Can be true, or it can just be a different term for the same type of thing.
 
reply



  
 
tptacek 1 hour ago  
             | prev | next [–] 

I left full time work for consulting after I did a project and discovered my bill rate was so high I could replicate my salary income while being less than 40% utilized. I wasn't doing something especially specialized; that's just what the market clearing rate for that contracting work was. I would not have guessed that rate before discovering it.So my somewhat tangential advice is: figure out your rate, and start with a rate that doesn't require you to constantly hustle for clients to make your nut. It's easier to figure out the right (high) rate when you've still got a salary to fall back on, and much harder when you're grinding it out as a full time contractor.
 
reply



  
 
simne 2 hours ago  
             | prev | next [–] 

You should consider two most important things (unfortunately, each other opposite):1. Yes, better to start own business when favorable environment, for example, when see macroeconomic grow (now recession, and high probability of crisis), or when You have some money accumulation.2. But business is by definition, PRACTICE of make stakes and earn profits on success.- You will not practice if not try. And best practice at crisis.Exist good solution of this contradiction. If You could afford it, look out at nearest to You business community (offline), and find experienced mentor, to mentor You and You'll pay him from Your current payment.Any way, early start business is much better then late. Mostly, because when You younger, You have more health, and could do things faster.
 
reply



  
 
kdazzle 1 hour ago  
             | parent | next [–] 

> better to start own business when favorable environment, for example, when see macroeconomic grow (now recession, and high probability of crisis),I could actually see the opposite being true. Lots of companies might not want to hire FTEs right now due to a maybe recession, but there's still a lot of work to be done, so they just use contractors instead.
 
reply



  
 
choult 2 hours ago  
             | prev | next [–] 

I started on my own at the end of September; I'd resigned my job at Datto after an acquisition proved itself disastrous (never work for Kaseya, or any Insight Partners portfolio company), and the position I had lined up fell through for one reason or another.I figured - I'm 40, I've always wanted to do my own thing, I've got runway... Why not? So I formed a company[0], started planning out a B2B product and networking like crazy; I'm fortunate to have a wide diaspora of former colleagues.As I want to bootstrap my own SaaS app, I needed to get revenue coming in, so I began to offer my services on a consultancy basis. I got my first small customer at the end of November - a charity needing hosting and maintenance - and then landed my first major consulting gig at the end of December 2022.Who knows what I'll be doing in a month's time, but I know that I can build a track record of helping others, and it's so varied! Every business or organization I talk to gives me more ideas for my app, so it's also a great way to collect more knowledge about the wants and needs of my potential customers.If you have the opportunity and runway to give it a shot, I'd recommend trying it - put a plan together for how you'll achieve it, how long you can last without a gig and what you'll offer others. And then execute on it.You'll never know unless you try.[0] https://xarma.co
 
reply



  
 
fhd2 3 hours ago  
             | prev | next [–] 

I personally took the plunge, with enough savings to make it several months until the first invoice is paid. Some clients pay on time, some don't, some go bankrupt or just drag it out infinitely, from what I've seen.Without that kind of savings (and being comfortable with watching them meld away for a while, taking the risk that you might not recover them), I would suggest to try to find a safe way to experiment with this. For example reducing your hours, or taking on a new main job with less hours, and taking on part time freelance work. Maybe there are things you can do for your potential clients with less hours than what you/they think is needed.
 
reply



  
 
nicolas_t 2 hours ago  
             | parent | next [–] 

With regards to clients not paying on time or going bankrupt, one data point, after being freenlance for 18 years, I have lost about 3% to unpaid work, conflicts with clients etc... Besides this, I've had quite a few clients who have been late (up to 3 months)When I first started, I remember researching a lot and reading that to expect 5% loss as a rule of thumb and have enough runway to last a year without income, I think that's a good rule to keep in mind.
 
reply



  
 
chrisa 2 hours ago  
             | prev | next [–] 

I did the same thing several years ago (left my job once I had 2 clients). I was also worried, but it turned out just fine. It's true that it will take a bit of work to get clients, but it becomes easier and easier as you go. Your current clients may know others, etc.I have two big pieces of advice:1. What other comments here say about money and runway is definitely true. It can be common to go a bit without clients - and even once you find a new one it might take 30 days for the project to start, then maybe you bill after the first 30 days, then it takes them 30 days to pay - that's 3 months from the time you found them to when you get the first money coming in! So make sure you account for that.2. Have something you can point to which says ""this is what I do, and why I'm good at it"". In my case it was an ebook, but it could be a white paper, or sample projects, etc - but you need something that ""tells a story"" to the clients that you can do the job.And good luck! It's scary to take the jump, but also very rewarding :)
 
reply



  
 
andjelam990 1 hour ago  
             | prev | next [–] 

I would advise you to find a partner who is also a data engineer/scientist and team up, it would be much easier also to share the workload, ideally even someone with network will bring in some new clients.
 
reply



  
 
misiti3780 3 hours ago  
             | prev | next [–] 

I could be another client as I need help with this, ping me at joseph dot misiti at mathandpencil.com
 
reply



  
 
davidw 2 hours ago  
             | prev | next [–] 

I've done contracting and it's ok, but it's best if you have something more to sell than just your time. ""Do you earn money while you're asleep?""
 
reply



  
 
ushercakes 2 hours ago  
             | prev | next [–] 

I would really advise heavily against just separating freelance/full time as black and white, one at a time.Really, it should be a slow transition - you work full time, you take up 1-2 freelance clients on the side. As you start to get more clients on the side, and you see a clear path to replacing your income, then you can make that jump.If you just make the jump as a clear break, it totally can work, it does all the time. But it's just a bit riskier, and it adds a lot more weight and pressure to get deals, which can lead to a situation where you charge less than you are actually worth.Btw, in my profile, the site I run is for freelancers to basically share their hourly rates. It's essentially levels.fyi for for consultants. May or may not be helpful to you at this stage of your journeyTLDR: 
- Keep your job, start taking clients on the side 
- Once you have enough clients and you see a clear path to replacing your FT income, quit and take the leap
 
reply



  
 
atemerev 2 hours ago  
             | prev | next [–] 

As a consultant, urgently looking for a full time job: not now. Most probably not now.
 
reply



  
 
moneywoes 1 hour ago  
             | parent | next [–] 

Wouldn’t contracts be more in demand during a recession?
 
reply



  
 
hocuspocus 1 hour ago  
             | root | parent | next [–] 

In places where layoffs are complicated, contractors are obviously the first to go.
 
reply



  
 
justsomehnguy 2 hours ago  
             | prev [–] 

Yesterday.
 
reply







Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
Search:  


"
https://news.ycombinator.com/rss,Heat pumps of the 1800s are becoming the technology of the future,https://knowablemagazine.org/article/technology/2023/heat-pumps-becoming-technology-future,Comments,"








CREDIT: DANA SMITH


Heat pumps offer a green, effective way to heat homes using electricity, not fossil fuels. New designs are making them more efficient and suitable for more conditions.





Technology

How heat pumps of the 1800s are becoming the technology of the future

          Innovative thinking has done away with problems that long dogged the electric devices — and both scientists and environmentalists are excited about the possibilities
          

By Chris Baraniuk
01.11.2023

FacebookTwitterLinkedinRedditFlipboardEmailPrintRepublish




Support sound science and smart storiesHelp us make scientific knowledge accessible to all
Donate today





It was an engineering problem that had bugged Zhibin Yu for years — but now he had the perfect chance to fix it. Stuck at home during the first UK lockdown of the Covid-19 pandemic, the thermal engineer suddenly had all the time he needed to refine the efficiency of heat pumps: electrical devices that, as their name implies, move heat from the outdoors into people’s homes.The pumps are much more efficient than gas heaters, but standard models that absorb heat from the air are prone to icing up, which greatly reduces their effectiveness.

YOU MAY ALSO LIKE









Food & Environment


How cities can fight climate change












Food & Environment


Now is the time to prepare for the economic shocks of battling climate change












Technology


How smart windows save energy




Yu, who works at the University of Glasgow, UK, pondered the problem for weeks. He read paper after paper. And then he had an idea. Most heat pumps waste some of the heat that they generate — and if he could capture that waste heat and divert it, he realized, that could solve the defrosting issue and boost the pumps’ overall performance. “I suddenly found a solution to recover the heat,” he recalls. “That was really an amazing moment.”Yu’s idea is one of several recent innovations that aim to make 200-year-old heat pump technology even more efficient than it already is, potentially opening the door for much greater adoption of heat pumps worldwide. To date, only about 10 percent of space heating requirements around the world are met by heat pumps, according to the International Energy Agency (IEA). But due to the current energy crisis and growing pressure to reduce fossil fuel consumption in order to combat climate change, these devices are arguably more crucial than ever.Since his 2020 lockdown brainstorming, Yu and his colleagues have built a working prototype of a heat pump that stores leftover heat in a small water tank. In a paper published in the summer of 2022, they describe how their design helps the heat pump to use less energy. Plus, by separately rerouting some of this residual warmth to part of the heat pump exposed to cold air, the device can defrost itself when required, without having to pause heat supply to the house.The idea relies on the very principle by which heat pumps operate: If you can seize heat, you can use it. What makes heat pumps special is the fact that instead of just generating heat, they also capture heat from the environment and move it into your house — eventually transferring that heat to radiators or forced-air heating systems, for instance. This is possible thanks to the refrigerant that flows around inside a heat pump. When the refrigerant encounters heat — even a tiny amount in the air on a cold day — it absorbs that modicum of warmth.A compressor then forces the refrigerant to a higher pressure, which raises its temperature to the point where it can heat your house. It works because an increase of pressure pushes the refrigerant molecules closer together, increasing their motion. The refrigerant later expands again, cooling as it does so, and the cycle repeats. The entire cycle can run in reverse, too, allowing heat pumps to provide cooling when it’s hot in summer.





Air-source heat pumps, the most common design, capture heat from the outdoor air.


The magic of a heat pump is that it can move multiple kilowatt-hours of heat for each kWh of electricity it uses. Heat pump efficiencies are generally measured in terms of their coefficient of performance (COP). A COP of 3, for example, means 1 kWh of juice yields 3 kWh of warmth — that’s effectively 300 percent efficiency. The COP you get from your device can vary depending on the weather and other factors.It’s a powerful concept, but also an old one. The British mathematician, physicist and engineer Lord Kelvin proposed using heat pump systems for space heating way back in 1852. The first heat pump was designed and built a few years later and used industrially to heat brine in order to extract salt from the fluid. In the 1950s, members of the British Parliament discussed heat pumps when coal stocks were running low. And in the years following the 1973-74 oil crisis, heat pumps were touted as an alternative to fossil fuels for heating. “Hope rests with the future heat pump,” one commentator wrote in the 1977 Annual Review of Energy.Now the world faces yet another reckoning over energy supplies. When Russia, one of the world’s biggest sources of natural gas, invaded Ukraine in February 2022, the price of gas soared — which in turn shoved heat pumps into the spotlight because with few exceptions they run on electricity, not gas. The same month, environmentalist Bill McKibben wrote a widely shared blog post titled “Heat pumps for peace and freedom” in which, referring to the Russian president, he argued that the US could “peacefully punch Putin in the kidneys” by rolling out heat pumps on a massive scale while lowering Americans’ dependence on fossil fuels. Heat pumps can draw power from domestic solar panels, for instance, or a power grid supplied predominantly by renewables.Running the devices on green electricity can help to fight climate change, too, notes Karen Palmer, an economist and senior fellow at Resources for the Future, an independent research organization in Washington, DC, who coauthored an analysis of policies to enhance energy efficiency in the 2018 Annual Review of Resource Economics. “Moving towards greater use of electricity for energy needs in buildings is going to have to happen, absent a technology breakthrough in something else,” she says.




This video illustrates the principle behind heat pumps.
CREDIT: THIS OLD HOUSE

The IEA estimates that, globally, heat pumps have the potential to reduce carbon dioxide emissions by at least 500 million metric tons in 2030, equivalent to the annual CO2 emissions produced by all the cars in Europe today.Despite their long history and potential virtues, heat pumps have struggled to become commonplace in some countries. One reason is cost: The devices are substantially more expensive than gas heating units and, because natural gas has remained relatively cheap for decades, homeowners have had little incentive to switch.There has also long been a perception that heat pumps won’t work as well in cold climates, especially in poorly insulated houses that require a lot of heat. In the UK, for example, where houses tend to be rather drafty, some homeowners have long considered gas boilers a safer bet because they can supply hotter water (around 140 to 160 degrees Fahrenheit), to radiators, which makes it easier to heat up a room. By contrast, heat pumps tend to be most efficient when heating water to around 100 degrees Fahrenheit.The cold-climate problem is arguably less of an issue than some think, however, given that there are multiple modern air source devices on the market that work well even when outside temperatures drop as low as minus 10 degrees Fahrenheit. Norway, for example, is considered one of the world leaders in heat pump deployment. Palmer has a heat pump in her US home, along with a furnace as backup. “If it gets really cold, we can rely on the furnace,” she says.Innovations in heat pump design are leading to units that are even more efficient, better suited to houses with low levels of insulation and — potentially — cheaper, too. For example, Yu says his and his colleagues’ novel air source heat pump design could improve the COP by between 3 percent and 10 percent, while costing less than existing heat pump designs with comparable functionality. They are now looking to commercialize the technology.Yu’s work is innovative, says Rick Greenough, an energy systems engineer now retired from De Montfort University in the UK. “I must admit this is a method I hadn’t actually thought of,” he says.





This newer heat pump design, by thermal engineer Zhibin Yu of the University of Glasgow, stores residual heat that would otherwise be wasted and uses it to help heat the house or defrost part of the pump itself. This makes the system more efficient.


And there are plenty more ideas afoot. Greenough, for instance, has experimented with storing heat in the ground during warmer months, where it can be exploited by a heat pump when the weather turns cool. His design uses a circulating fluid to transfer excess heat from solar hot-water panels into shallow boreholes in the soil. That raises the temperature of the soil by around 22 degrees Fahrenheit, to a maximum of roughly 66 degrees Fahrenheit, he says. Then, in the winter, a heat pump can draw out some of this stored heat to run more efficiently when the air gets colder. This technology is already on the market, offered by some installers in the UK, notes Greenough.But most current heat pumps still only generate relatively low output temperatures, so owners of drafty homes may need to take on the added cost of insulation when installing a heat pump. Fortunately, a solution may be emerging: high-temperature heat pumps.“We said, ‘Hey, why not make a heat pump that can actually one-on-one replace a gas boiler without having to really, really thoroughly insulate your house?’” says Wouter Wolfswinkel, program manager for business development at Swedish energy firm Vattenfall, which manufactures heat pumps. Vattenfall and its Dutch subsidiary Feenstra have teamed up to develop a high-temperature heat pump, expected to debut in 2023.





Like air conditioners running in reverse, heat pumps such as the one being installed here use refrigerants to capture heat from the outdoors and move it indoors to heat the house.
CREDIT: PHYXTER HOME SERVICES


In their design, they use CO2 as a refrigerant. But because the heat-pump system’s hot, high-pressure operating conditions prevent the gas from condensing or otherwise cooling down very easily, they had to find a way of reducing the refrigerant’s temperature in order for it to be able to absorb enough heat from the air once again when it returns to the start of the heat pump loop. To this end, they added a “buffer” to the system: a water tank where a layer of cooler water rests beneath hotter water above. The heat pump uses the lower layer of cooler water from the tank to adjust the temperature of the refrigerant as required. But it can also send the hotter water at the top of the tank out to radiators, at temperatures up to 185 degrees Fahrenheit.The device is slightly less efficient than a conventional, lower temperature heat pump, Wolfswinkel acknowledges, offering a COP of around 265 percent versus 300 percent, depending on conditions. But that’s still better than a gas boiler (no more than 95 percent efficient), and as long as electricity prices aren’t significantly higher than gas prices, the high temperature heat pump could still be cheaper to run. Moreover, the higher temperature means that homeowners needn’t upgrade their insulation or upsize radiators right away, Wolfswinkel notes. This could help people make the transition to electrified heating more quickly.A key test was whether Dutch homeowners would go for it. As part of a pilot trial, Vattenfall and Feenstra installed the heat pump in 20 households of different sizes in the town of Heemskerk, not far from Amsterdam. After a few years of testing, in June 2022 they gave homeowners the option of taking back their old gas boiler, which they had kept in their homes, or of using the high temperature heat pump on a permanent basis. “All of them switched to the heat pump,” says Wolfswinkel.In some situations, home-by-home installations of heat pumps might be less efficient than building one large system to serve a whole neighborhood. For about a decade, Star Renewable Energy, based in Glasgow, has been building district systems that draw warmth from a nearby river or sea inlet, including a district heating system connected to a Norwegian fjord. A Scandinavian fjord might not be the first thing that comes to mind if you say the word “heat” — but the water deep in the fjord actually holds a fairly steady temperature of 46 degrees Fahrenheit, which heat pumps can exploit.





Ground-source and water-source heat pumps differ from air-source pumps by capturing heat from the ground or from bodies of water.


Via a very long pipe, the district heating system draws in this water and uses it to heat the refrigerant, in this case ammonia. A subsequent, serious increase of pressure for the refrigerant — to 50 atmospheres — raises its temperature to 250 degrees Fahrenheit. The hot refrigerant then passes its heat to water in the district heating loop, raising the temperature of that water to 195 degrees Fahrenheit. The sprawling system provides 85 percent of the hot water needed to heat buildings in the city of Drammen.“That type of thing is very exciting,” says Greenough.Not every home will be suitable for a heat pump. And not every budget can accommodate one, either. Yu himself says that the cost of replacing the gas boiler in his own home remains prohibitive. But it’s something he dreams of doing in the future. With ever-improving efficiencies, and rising sales in multiple countries, heat pumps are only getting harder for their detractors to dismiss. “Eventually,” says Yu, “I think everyone will switch to heat pumps.”



10.1146/knowable-011123-2


Chris Baraniuk is a freelance science journalist and nature lover who lives in Belfast, Northern Ireland. His work has been published by the BBC, the Guardian, New Scientist, Scientific American and Hakai Magazine, among other publications.

Republish This Article



        Technology
      



          Climate Change
        





Share this article
FacebookTwitterLinkedinRedditFlipboardEmailPrintRepublish



Support Knowable Magazine
Help us make scientific knowledge accessible to all
      Donate
 





CloseExplore MoreANNUAL REVIEW OF RESOURCE ECONOMICSAdvances in Evaluating Energy Efficiency Policies and ProgramsTAKE A DEEPER DIVE| Explore Related Scholarly Articles
                ANNUAL REVIEW OF RESOURCE ECONOMICSAdvances in Evaluating Energy Efficiency Policies and ProgramsThere are many possible energy efficiency interventions available, including heat pumps. Some of the financial savings yielded by such interventions are smaller than utilities suggest — but others are cost-effective.ANNUAL REVIEW OF ENVIRONMENT AND RESOURCESFrom Low- to Net-Zero Carbon Cities: The Next Global AgendaStudies suggest that cities can reach net-zero targets with the help of decarbonizing technologies, including heat pumps. Broad success requires systemic transformation of how cities and nearby areas consume energy and sequester carbon.ANNUAL REVIEW OF ENERGYThe Coming Age of ConservationIn 1977, greater understanding of global energy expenditure in various areas, from transportation to home heating, was leading to new thinking on energy consumption — and a drive to improve efficiency. Heat pumps were seen as a key part of that effort.





Stay in the Know
Subscribe to the Knowable Magazine newsletter.




* indicates required

Email Address  *




Country  *


United States of America
Aaland Islands
Afghanistan
Albania
Algeria
American Samoa
Andorra
Angola
Anguilla
Antarctica
Antigua And Barbuda
Argentina
Armenia
Aruba
Australia
Austria
Azerbaijan
Bahamas
Bahrain
Bangladesh
Barbados
Belarus
Belgium
Belize
Benin
Bermuda
Bhutan
Bolivia
Bonaire, Saint Eustatius and Saba
Bosnia and Herzegovina
Botswana
Bouvet Island
Brazil
British Indian Ocean Territory
Brunei Darussalam
Bulgaria
Burkina Faso
Burundi
Cambodia
Cameroon
Canada
Cape Verde
Cayman Islands
Central African Republic
Chad
Chile
China
Christmas Island
Cocos (Keeling) Islands
Colombia
Comoros
Congo
Cook Islands
Costa Rica
Cote D'Ivoire
Croatia
Cuba
Curacao
Cyprus
Czech Republic
Democratic Republic of the Congo
Denmark
Djibouti
Dominica
Dominican Republic
Ecuador
Egypt
El Salvador
Equatorial Guinea
Eritrea
Estonia
Ethiopia
Falkland Islands
Faroe Islands
Fiji
Finland
France
French Guiana
French Polynesia
French Southern Territories
Gabon
Gambia
Georgia
Germany
Ghana
Gibraltar
Greece
Greenland
Grenada
Guadeloupe
Guam
Guatemala
Guernsey
Guinea
Guinea-Bissau
Guyana
Haiti
Heard and Mc Donald Islands
Honduras
Hong Kong
Hungary
Iceland
India
Indonesia
Iran
Iraq
Ireland
Isle of Man
Israel
Italy
Jamaica
Japan
Jersey  (Channel Islands)
Jordan
Kazakhstan
Kenya
Kiribati
Kuwait
Kyrgyzstan
Lao People's Democratic Republic
Latvia
Lebanon
Lesotho
Liberia
Libya
Liechtenstein
Lithuania
Luxembourg
Macau
Macedonia
Madagascar
Malawi
Malaysia
Maldives
Mali
Malta
Marshall Islands
Martinique
Mauritania
Mauritius
Mayotte
Mexico
Micronesia, Federated States of
Moldova, Republic of
Monaco
Mongolia
Montenegro
Montserrat
Morocco
Mozambique
Myanmar
Namibia
Nauru
Nepal
Netherlands
Netherlands Antilles
New Caledonia
New Zealand
Nicaragua
Niger
Nigeria
Niue
Norfolk Island
North Korea
Northern Mariana Islands
Norway
Oman
Pakistan
Palau
Palestine
Panama
Papua New Guinea
Paraguay
Peru
Philippines
Pitcairn
Poland
Portugal
Puerto Rico
Qatar
Republic of Kosovo
Reunion
Romania
Russia
Rwanda
Saint Kitts and Nevis
Saint Lucia
Saint Martin
Saint Vincent and the Grenadines
Samoa (Independent)
San Marino
Sao Tome and Principe
Saudi Arabia
Senegal
Serbia
Seychelles
Sierra Leone
Singapore
Sint Maarten
Slovakia
Slovenia
Solomon Islands
Somalia
South Africa
South Georgia and the South Sandwich Islands
South Korea
South Sudan
Spain
Sri Lanka
St. Helena
St. Pierre and Miquelon
Sudan
Suriname
Svalbard and Jan Mayen Islands
Swaziland
Sweden
Switzerland
Syria
Taiwan
Tajikistan
Tanzania
Thailand
Timor-Leste
Togo
Tokelau
Tonga
Trinidad and Tobago
Tunisia
Turkey
Turkmenistan
Turks & Caicos Islands
Turks and Caicos Islands
Tuvalu
Uganda
Ukraine
United Arab Emirates
United Kingdom
Uruguay
USA Minor Outlying Islands
Uzbekistan
Vanuatu
Vatican City State (Holy See)
Venezuela
Vietnam
Virgin Islands (British)
Virgin Islands (U.S.)
Wallis and Futuna Islands
Western Sahara
Yemen
Zambia
Zimbabwe




Send me *
Newsletter: The latest from Knowable Magazine, along with other curated readings and our favorite science-inspired art delivered weekly.
Events: Invitations to our free online event series, featuring leading scientists, scholars and stakeholders discussing frontiers of knowledge and key societal issues.













Close







DONATE: Keep Knowable free to read and share


More FromRethinking air conditioning amid climate changeThe dazzling history of solar powerThe road to low-carbon concrete
RepublishThank you for your interest in republishing! This HTML is pre-formatted to adhere to our guidelines, which include: Crediting both the author and Knowable Magazine; preserving all hyperlinks; including the canonical link to the original article in the article metadata. Article text (including the headline) may not be edited without prior permission from Knowable Magazine staff. Photographs and illustrations are not included in this license. Please see our full guidelines for more information.
    
How heat pumps of the 1800s are becoming the technology of the future
Innovative thinking has done away with problems that long dogged the electric devices — and both scientists and environmentalists are excited about the possibilities

    By Chris Baraniuk 
    
  
1.11.2023

    It was an engineering problem that had bugged Zhibin Yu for years — but now he had the perfect chance to fix it. Stuck at home during the first UK lockdown of the Covid-19 pandemic, the thermal engineer suddenly had all the time he needed to refine the efficiency of heat pumps: electrical devices that, as their name implies, move heat from the outdoors into people’s homes.The pumps are much more efficient than gas heaters, but standard models that absorb heat from the air are prone to icing up, which greatly reduces their effectiveness.Yu, who works at the University of Glasgow, UK, pondered the problem for weeks. He read paper after paper. And then he had an idea. Most heat pumps waste some of the heat that they generate — and if he could capture that waste heat and divert it, he realized, that could solve the defrosting issue and boost the pumps’ overall performance. “I suddenly found a solution to recover the heat,” he recalls. “That was really an amazing moment.”Yu’s idea is one of several recent innovations that aim to make 200-year-old heat pump technology even more efficient than it already is, potentially opening the door for much greater adoption of heat pumps worldwide. To date, only about 10 percent of space heating requirements around the world are met by heat pumps, according to the International Energy Agency (IEA). But due to the current  energy crisis and growing pressure to reduce fossil fuel consumption in order to combat climate change, these devices are arguably more crucial than ever.Since his 2020 lockdown brainstorming, Yu and his colleagues have built a working prototype of a heat pump that stores leftover heat in a small water tank. In a paper published in the summer of 2022, they describe how their design helps the heat pump to use less energy. Plus, by separately rerouting some of this residual warmth to part of the heat pump exposed to cold air, the device can defrost itself when required, without having to pause heat supply to the house.The idea relies on the very principle by which heat pumps operate: If you can seize heat, you can use it. What makes heat pumps special is the fact that instead of just generating heat, they also capture heat from the environment and move it into your house — eventually transferring that heat to radiators or forced-air heating systems, for instance. This is possible thanks to the refrigerant that flows around inside a heat pump. When the refrigerant encounters heat — even a tiny amount in the air on a cold day — it absorbs that modicum of warmth.A compressor then forces the refrigerant to a higher pressure, which raises its temperature to the point where it can heat your house. It works because an increase of pressure pushes the refrigerant molecules closer together, increasing their motion. The refrigerant later expands again, cooling as it does so, and the cycle repeats. The entire cycle can run in reverse, too, allowing heat pumps to provide cooling when it’s hot in summer.The magic of a heat pump is that it can move multiple kilowatt-hours of heat for each kWh of electricity it uses. Heat pump efficiencies are generally measured in terms of their coefficient of performance (COP). A COP of 3, for example, means 1 kWh of juice yields 3 kWh of warmth — that’s effectively 300 percent efficiency. The COP you get from your device can vary depending on the weather and other factors.It’s a powerful concept, but also an old one. The British mathematician, physicist and engineer Lord Kelvin proposed using heat pump systems for space heating way back in 1852. The first heat pump was designed and  built a few years later and used industrially to heat brine in order to extract salt from the fluid. In the 1950s, members of  the British Parliament discussed heat pumps when coal stocks were running low. And in the years following  the 1973-74 oil crisis, heat pumps were touted as an alternative to fossil fuels for heating. “ Hope rests with the future heat pump,” one commentator wrote in the 1977  Annual Review of Energy.Now the world faces yet another reckoning over energy supplies. When Russia, one of the world’s biggest sources of natural gas, invaded Ukraine in February 2022, the price of gas soared — which in turn shoved heat pumps into the spotlight because with few exceptions they run on electricity, not gas. The same month, environmentalist Bill McKibben wrote a widely shared blog post titled “Heat pumps for peace and freedom” in which, referring to the Russian president, he argued that the US could “peacefully punch Putin in the kidneys” by rolling out heat pumps on a massive scale while lowering Americans’ dependence on fossil fuels. Heat pumps can draw power from domestic  solar panels, for instance, or a power grid supplied predominantly by renewables.Running the devices on green electricity can help to fight climate change, too, notes Karen Palmer, an economist and senior fellow at Resources for the Future, an independent research organization in Washington, DC, who coauthored  an analysis of policies to enhance energy efficiency in the 2018  Annual Review of Resource Economics. “Moving towards greater use of electricity for energy needs in buildings is going to have to happen, absent a technology breakthrough in something else,” she says.The IEA estimates that, globally, heat pumps have the potential to reduce carbon dioxide emissions by at least 500 million metric tons in 2030, equivalent to the annual CO 2 emissions produced by all the cars in Europe today.Despite their long history and potential virtues, heat pumps have struggled to become commonplace in some countries. One reason is cost: The devices are substantially more expensive than gas heating units and, because natural gas has remained relatively cheap for decades, homeowners have had little incentive to switch.There has also long been a perception that heat pumps won’t work as well in cold climates, especially in poorly insulated houses that require a lot of heat. In the UK, for example, where houses  tend to be rather drafty, some homeowners have long considered gas boilers a safer bet because they can supply hotter water ( around 140 to 160 degrees Fahrenheit), to radiators, which makes it easier to heat up a room. By contrast, heat pumps tend to be most efficient when heating water  to around 100 degrees Fahrenheit.The cold-climate problem is arguably less of an issue than some think, however, given that there are multiple modern air source devices on the market that work well even when outside temperatures drop as low as minus 10 degrees Fahrenheit. Norway, for example, is considered one of the world leaders in heat pump deployment. Palmer has a heat pump in her US home, along with a furnace as backup. “If it gets really cold, we can rely on the furnace,” she says.Innovations in heat pump design are leading to units that are even more efficient, better suited to houses with low levels of insulation and — potentially — cheaper, too. For example, Yu says his and his colleagues’ novel air source heat pump design could improve the COP by between 3 percent and 10 percent, while costing less than existing heat pump designs with comparable functionality. They are now looking to commercialize the technology.Yu’s work is innovative, says Rick Greenough, an energy systems engineer now retired from De Montfort University in the UK. “I must admit this is a method I hadn’t actually thought of,” he says.And there are plenty more ideas afoot. Greenough, for instance, has experimented with storing heat in the ground during warmer months, where it can be exploited by a heat pump when the weather turns cool. His design uses a circulating fluid to transfer excess heat from solar hot-water panels into shallow boreholes in the soil. That raises the temperature of the soil by around 22 degrees Fahrenheit, to a maximum of roughly 66 degrees Fahrenheit, he says. Then, in the winter, a heat pump can draw out some of this stored heat to run more efficiently when the air gets colder. This technology is already on the market, offered by some installers in the UK, notes Greenough.But most current heat pumps still only generate relatively low output temperatures, so owners of drafty homes may need to take on the added cost of insulation when installing a heat pump. Fortunately, a solution may be emerging: high-temperature heat pumps.“We said, ‘Hey, why not make a heat pump that can actually one-on-one replace a gas boiler without having to really, really thoroughly insulate your house?’” says Wouter Wolfswinkel, program manager for business development at Swedish energy firm Vattenfall, which manufactures heat pumps. Vattenfall and its Dutch subsidiary Feenstra have teamed up to develop a high-temperature heat pump, expected to debut in 2023.In their design, they use CO2 as a refrigerant. But because the heat-pump system’s hot, high-pressure operating conditions prevent the gas from condensing or otherwise cooling down very easily, they had to find a way of reducing the refrigerant’s temperature in order for it to be able to absorb enough heat from the air once again when it returns to the start of the heat pump loop. To this end, they added a “buffer” to the system: a water tank where a layer of cooler water rests beneath hotter water above. The heat pump uses the lower layer of cooler water from the tank to adjust the temperature of the refrigerant as required. But it can also send the hotter water at the top of the tank out to radiators, at temperatures up to 185 degrees Fahrenheit.The device is slightly less efficient than a conventional, lower temperature heat pump, Wolfswinkel acknowledges, offering a COP of around 265 percent versus 300 percent, depending on conditions. But that’s still better than a gas boiler (no more than 95 percent efficient), and as long as electricity prices aren’t significantly higher than gas prices, the high temperature heat pump could still be cheaper to run. Moreover, the higher temperature means that homeowners needn’t upgrade their insulation or upsize radiators right away, Wolfswinkel notes. This could help people make the transition to electrified heating more quickly.A key test was whether Dutch homeowners would go for it. As part of a pilot trial, Vattenfall and Feenstra installed the heat pump in 20 households of different sizes in the town of Heemskerk, not far from Amsterdam. After a few years of testing, in June 2022 they gave homeowners the option of taking back their old gas boiler, which they had kept in their homes, or of using the high temperature heat pump on a permanent basis. “All of them switched to the heat pump,” says Wolfswinkel.In some situations, home-by-home installations of heat pumps might be less efficient than building one large system to serve a whole neighborhood. For about a decade, Star Renewable Energy, based in Glasgow, has been building district systems that draw warmth from a nearby river or sea inlet, including a district heating system connected to a Norwegian fjord. A Scandinavian fjord might not be the first thing that comes to mind if you say the word “heat” — but the water deep in the fjord actually holds a fairly steady temperature of 46 degrees Fahrenheit, which heat pumps can exploit.Via a very long pipe, the district heating system draws in this water and uses it to heat the refrigerant, in this case ammonia. A subsequent, serious increase of pressure for the refrigerant — to 50 atmospheres — raises its temperature to 250 degrees Fahrenheit. The hot refrigerant then passes its heat to water in the district heating loop, raising the temperature of that water to 195 degrees Fahrenheit. The sprawling system provides 85 percent of the hot water needed to heat buildings in the city of Drammen.“That type of thing is very exciting,” says Greenough.Not every home will be suitable for a heat pump. And not every budget can accommodate one, either. Yu himself says that the cost of replacing the gas boiler in his own home remains prohibitive. But it’s something he dreams of doing in the future. With ever-improving efficiencies, and rising sales in multiple countries, heat pumps are only getting harder for their detractors to dismiss. “Eventually,” says Yu, “I think everyone will switch to heat pumps.”
    
    

10.1146/knowable-011123-2



Chris Baraniuk is a freelance science journalist and nature lover who lives in Belfast, Northern Ireland. His work has been published by the  BBC, the  Guardian,  New Scientist,  Scientific American and  Hakai Magazine, among other publications.

This article originally appeared in Knowable Magazine, an independent journalistic endeavor from Annual Reviews. Sign up for the newsletter.
  Copy htmlClose
"
https://news.ycombinator.com/rss,Rendering like it's 1996 – Baby's first pixel,https://marioslab.io/posts/rendering-like-its-1996/babys-first-pixel/,Comments,"













Mario's Lab





Mario's Lab
Mastodon
Twitter
Github
RSS



Rendering like it's 1996 - Baby's first pixel
December 02, 2022




There's absolutely no chance we'll get to this level of quality.



	In 1996, I was a teen without a gaming console. While my friends enjoyed their Crash Bandicoots, Tekens, and Turoks, I had a beige 486 DX 2 with a turbo button, 16Mb of RAM, a 256Mb hard disk, and a 2x CD-ROM drive running DOS. And then I got a copy of Quake. Did it run great? No. But it did run! And to my young eyes, it was the most beautiful thing I've ever seen on my computer screen. Ok, the most beautiful brown thing.


	3D accelerator graphics cards were in their infancy. Most DOS PC games around that time would render their glorious pixels via the CPU to a dedicated area in RAM, e.g. starting at segment address 0xa000. The (pretty dumb) graphics card would then read and display the contents of that memory area on your bulky CRT. This is known as software rendering or software rasterization.


	I did dabble in some graphics programming back then. I even managed to create a Wolfenstein style first person shooter in QBasic with some assembly before the end of the century.



Actually not a ray casting engine, but a polygonal 3D engine with terrible affine texture mapping.


	But I never really dove into the depths of contemporary graphics technology. And while my subsequent professional career featured plenty of graphics programming, it was mostly the GPU accelerated kind, not the ""worry about each cycle in your inner loops"" software rasterizer kind of type.

(Non-)Goals

	I want to explore the ins and outs of software rasterization, starting from first principles, i.e. getting a pixel on screen. From there, I want to delve into topics like simple demo effects, primitive rasterization, ray casting, voxel terrain, maybe even Quake-style 3D rendering, and whatever else comes to mind.


	Each blog post on a topic will lay out the theory the way I understand it in hopefully simple terms, discuss a naive practical implementation, and finally investigate ways to optimize the implementation until it is reasonably fast.


	The end product(s) should work on Windows, Linux, macOS, and common browsers. Ideally, a little software rasterizer library and demos will fall out at the end, that can serve both as an example implementation of common techniques, or as the basis for other demos or games with DOS game aesthetics.


	You'll be able to follow along both here, and by playing with the code on GitHub. For each blog post, there will be one tagged commit in the main branch you can check out. In addition to the render-y bits, I'll also demonstrate how I set up a cross-platform C project and show you how I structure, build, and debug C code in such a project. I love seeing and learning from other people's workflows. Maybe that's true for you too.


	What I do not want to do is dabble in things like assembly or SIMD optimizations. While that can be fun too, it is unlikely to be necessary on today's hardware, given that I'll target common DOS resolutions like 320x240, or 640x480. I might however inspect and discuss the compiler's assembly output to identify areas that can be improved performance wise in the higher level code.

Tools of the trade

	The weapon of choice will be C99 for aesthetic and practical reasons. I want all the code produced throughout this series to compile anywhere. It should also be easy to re-use the code in other languages through an FFI. C99 is a good choice for both objectives.


	In terms of ompilers, I'll be using Clang with MinGW headers and standard libraries on Windows, Clang through Xcode on macOS, and GCC on Linux. Why Clang on Windows? Because Visual Studio is a multi-gigabyte download, and setting up builds for it is a terrible experience. Clang also generates better code.


	I'll use CMake as the meta build tool, not because I love it, but because my favorite C/C++ IDE CLion has first class support for it. Other development environments understand CMake as well these days, including Visual Studio if that's your kink. For actually executing the builds, I'll use Ninja, which is wicked fast, especially compared to MSBuild and consorts.


	The pixels we'll generate need to be thrown up on the display somehow. On Windows, Linux, and macOS we'll use MiniFB. In a few lines of code, we can open a window, process keyboard and mouse input, and give it a bunch of pixels to draw to the window. It can even upscale our low resolution output if needed. Since MiniFB does not have browser support, I've written a web backend myself and submitted it as a pull request to the upstream repo. In the meantime, we'll use my MiniFB fork, which has web support baked in.


	To get the code running in the browser, we'll use Emscripten to compile the C code to WASM and a small .js file, which loads the .wasm file and exposes our C functions to JavaScript.


	In terms of IDE, you are free to use whatever you want. You'll most likely want something that can ingest CMake builds. For this series, I choose VS Code, not because I love it, but because it's free. The project contains a bunch of VS Code specific settings that make working on the project super simple for all supported platforms.

Getting the source code and tools

	That's a lot of tools! I've tried to make it as simple for you to follow along as possible. Here's what you need to install:


Visual Studio Code
Make sure code can be called on the command line! Open VS Code, press CTRL+SHIFT+P (or CMD+SHIFT+P on macOS), type Shell Command: Install 'code' command in PATH and hit enter.
Windows:

Git for Windows. Make sure its available on the command line via the system PATH.


Linux:

Git, GCC, GDB, Python, CMake, Curl, libx11-dev, libxkbcommon-dev, and libgl1-mesa-dev. On Ubuntu sudo apt install build-essential git gdb python3.11 cmake curl libx11-dev libxkbcommon-dev libgl1-mesa-dev


macOS:

Xcode. Make sure to also install the command line tools




	Once you've installed the above, clone the repository (on Windows, use Git Bash, which comes with Git for Windows):


git clone https://github.com/badlogic/r96
cd r96


	Next, checkout the tag for the blog post you want to follow along with, execute the tools/download-tools.sh script:


git checkout 01-babys-first-pixel
./tools/download-tools.sh


	The download-tools.sh script will download all remaining tools that are needed, like CMake, Ninja, Clang for Windows, Python, a small static file server, Emscripten, and Visual Studio Code extensions needed for C/C++ development. See the README.md for details.


Note: we may add new tools in future blog posts. After checking out a tag for a blog post, make sure to run tools/download-tools.sh again.

The r96 project

	These are the goals for the project scaffold:


Make building and debugging for the desktop and the web trivial.
Allow adding new demo apps that work without code modification on both the desktop and in the browser
Make creating re-usable code easy.


	Let's see how I tried to achieve the above. Open your clone of the r96 Git repository in VS Code and have a look what's inside.


Note: The first time you open the project in VS Code, you'll be asked to select a CMake configure preset.


Note: the first time you open a source file in VS Code, you will be asked if you want to install clangd. Click Yes.

File structure




Let's start in the root folder.

	The .gitignore, LICENSE, and README.md files are self-explanatory.


	The CMakeLists.txt and CMakePresets.json define our build. We'll look into these in a later section.


	The .clang-format file stores the formatting settings used to format the code via, you guessed it, clang-format. The VS Code C/C++ extension uses the settings in that file whenever you format a C/C++ source file. The file can also be used to format the entire code base from the command line.


	The src/ folder contains our code. Re-usable code goes into src/r96/. Demo apps go into the root of the src/ folder. There are two demo apps so far called 00_basic_window.c and 01_drawing_a_pixel.c. Any demo apps we write in subsequent blog posts will also go into src/ and start with a sequential number, so we immediately see in which order they were written.


	The src/web/ folder may be weird, even scary to seasoned C veterans. But we need it to run our demo apps on the web. A small price to pay. It contains one .html file per demo app. The purpose of that file is to:


Load the .js and .wasm files generated by Emscripten for the demo app executable target
Provide the demo app with a HTML5 canvas element to draw to
Kick off the demo apps execution by calling its main() function


	For any demo app we write in the future, we'll add a source file to the src/ folder, and a corresponding .html file to the src/web/ folder.


	The src/web/index.html file is just a plain listing linking to all the .html files of our demo apps. The src/web/r96.css file is a CSS style sheet used to make the elements in the demo app .html files a little prettier.


	The .vscode/ folder contains settings and launch configurations so working on the project is a nice experience in VS Code.


	Finally, the tools/ folder contains scripts to download the necessary tools as well as configuration files for a few of those tools. When executing the tools/download-tools.sh script, some of the tools actually get installed in the tools/ folder so they don't clog up your system. The folder also contains scripts and batch files used by the launch configurations to do their work.


	The details of the .vscode and tools folder are all gory and duct tape-y. You can have a look if you must. For the remained of the series, their content doesn't matter much. Just know that they are setup in a way to make our lives easy.

Building

The first time you open the project in VS Code, you're asked to select a configure preset.







	A configure preset defines for what platform the code should be build and with what compiler and build flags that should happen. The presets are defined in CMakePresets.json.


	For each platform the r96 project supports, there is a corresponding debug and release configure preset. To start, we'll select Desktop debug. You can also select the configure preset in the status bar at the bottom of VS Code.






	To build the project for the selected platform and build type (debug or release), click the Build button in the status bar.






	Alternatively, you can open the VS Code command palette (CTRL+SHIFT+P or CMD+SHIFT+P on macOS), type CMake: Build, and hit enter.


	In both cases, the CMake Tools extension, which was installed as part of tools/download-tools.sh, will configure the CMake build if necessary, then incrementally build the libraries and executables defined in CMakeLists.txt.


	The resulting build output consisting of executables and assets can be found in build/<os>-<build-type>. E.g. for Desktop debug, the build output will be located in build/windows-debug, build/linux-debug, or build/macos-debug depending on what operating system you are on. For Web release the output will be in build/web-output, and so on.


Note: To learn more about how to use VS Code CMake integration, check out the documentation.


	You can of course also build the project on the command line:


# Configure a Windows debug build and execute the build
cmake --preset windows-debug
cmake --build build/windows-debug

# Configure a web release build and execute the build
cmake --preset web-release
cmake --build build/web-release

Debugging

	The launch.json file in the .vscode/ folder defines launch configurations for each platform. Click the launch button in the status bar to select the launch configuration and start a debugging session. 






	After clicking this status bar entry, you'll be asked to select a launch configuration:






	When you first start a debugging session, you'll be asked to select a launch target, aka the executable you want to launch:






	You can also change the launch target in the status bar:






	After selecting the launch target, the code is incrementally rebuild, and the debugging session starts. 


	Instead of going through the status bar, you can also start a new debugging sessions by pressing `F5`. This will launch a session for the currently selected launch configuration, configure preset, and launch target.


Important: the launch configuration MUST match the preset you selected:


Desktop debug target: select the Desktop debug or Desktop release preset.
Web debug target: select the Web debug or Web release preset.


	Debugging a desktop build is the standard experience you are used to. Set breakpoints and watches, interrupt the program at any time, and so on.


	Debugging the C code compiled to WASM directly in VS Code is not possible. When you start a web debugging session, the respective launch configuration starts a local static file server (downloaded via tools/download-tools.sh) and opens the .html file corresponding to the selected launch target in a browser tab.


	When you are done ""debugging"" a web build, close the browser tab, and close the debugging session in VS Code by clicking the ""Stop"" button in the debugger controls.


 If you feel adventurous: it is possible to debug the C and JavaScript code in Chrome.. We'll look into that below.

Dissecting the CMakeLists.txt and CMakePresets.json files
To understand how the build is setup, we need to understand the CMakeLists.txt and CMakePresets.json files.

	We've already had a brief look at the CMakePresets.json file above. It defines a configure preset for each operating system and build type combination. Let's have a look at one of the presets, specifically, the one used for Windows debug builds.


{
	""name"": ""windows-debug"",
	""displayName"": ""Desktop debug"",
	""description"": """",
	""generator"": ""Ninja"",
	""binaryDir"": ""${sourceDir}/build/${presetName}"",
	""cacheVariables"": {
		""CMAKE_BUILD_TYPE"": ""Debug"",
		""CMAKE_MAKE_PROGRAM"": ""${sourceDir}/tools/desktop/ninja/ninja""
	},
	""toolchainFile"": ""${sourceDir}/tools/desktop/toolchain-clang-mingw.cmake"",
	""condition"": {
		""type"": ""equals"",
		""lhs"": ""${hostSystemName}"",
		""rhs"": ""Windows""
	}
},


The important bits are:


generator: we tell CMake to generate a Ninja build.
binaryDir: specifies the output directory for the build. In this case it maps to build/windows-debug through variable substitution.
CMAKE_BUILD_TYPE: tells CMake we want the binaries to include debugging information.
CMAKE_MAKE_PROGRAM: tells CMake were to find the Ninja executable. The tools/download-tools.sh script downloaded the executable to tools/desktop/ninja/
toolchainFile: where to find the compiler and linker. On Windows, we use Clang with MinGW headers and standard libraries., which the tools/download-tools.sh script downloads to tools/desktop/clang. The toolchain file references the compiler and linker in that location and sets up a few other CMake cache variables.
condition: tells CMake to only enable this configure preset if we're running on Windows.


	The other configure presets are pretty similar and only differ in what operating system they should be available on, as well as the toolchain being used. On macOS and Linux, the default toolchain is used (GCC or Xcode's Clang). For the web, the Emscripten toolchain is used through a toolchain file that ships with Emscripten.


	We could work without this presets file, but that would mean we'd have to specify all these parameters manually every time we configure a CMake build. With the presets, this becomes cmake --preset <preset-name>. Much nicer!


	The CMakeLists.txt file defines the actual build itself, i.e. which source files make up which libraries and executables, and what compiler flags to use. Definitions of libraries and executables are called targets in CMake. Let's go through it section by section. We start out with this:


cmake_minimum_required(VERSION 3.21.1)
project(r96)

set(CMAKE_C_STANDARD 99)
set(CMAKE_C_STANDARD_REQUIRED TRUE)
set(CMAKE_EXPORT_COMPILE_COMMANDS TRUE)


	We define the minimum CMake version and project name, and enable (and require) C99 support. The final line makes CMake generate a compile_commands.json file in the build folder. This is also known as a compilation database and used by many IDEs to understand a CMake build. In our case, the file is used by the clangd VS Code extension to provide us with code completion and other niceities.


include(FetchContent)
FetchContent_Declare(minifb GIT_REPOSITORY https://github.com/badlogic/minifb GIT_TAG dos-pr-master)
set(MINIFB_BUILD_EXAMPLES CACHE INTERNAL FALSE)
FetchContent_MakeAvailable(minifb)


	Next we pull in MiniFB via CMake's FetchContent mechanism. CMake veterans may sneer at this and rather use a Git submodule. But I like it that way, thank you very much. This magic incantation will clone my MiniFB fork with web support, disable the MiniFB example targets, and finally make the remaining MiniFB library target available to the targets defined in our own CMakeLists.txt. Nice.


add_compile_options(-Wall -Wextra -Wpedantic -Wno-implicit-fallthrough)


	This section sets the ""pedantic warnings are errors"" compiler flags. We want the code to be reasonably clean and fail if a warning is generated.


add_library(r96 ""src/r96/r96.c"")


	Next we add a library target called r96. It's compiled from the r96/r96.c source file. Any re-usable code we write during the course of this blog post series will go in there. Any of our demo app executable targets can then depend on the r96 library target to pull in its code.


add_executable(r96_00_basic_window ""src/00_basic_window.c"")
add_executable(r96_01_drawing_a_pixel ""src/01_drawing_a_pixel.c"")


	We define two executable targets for the demos of this blog post.


add_custom_target(r96_web_assets
    COMMAND ${CMAKE_COMMAND} -E copy_directory
    ${CMAKE_CURRENT_SOURCE_DIR}/src/web
    $<TARGET_FILE_DIR:r96_00_basic_window>
)


	We define a custom target that copies all the .html files from src/web/ to the output folder. This target is needed for web builds.


get_property(targets DIRECTORY ""${_dir}"" PROPERTY BUILDSYSTEM_TARGETS)
list(REMOVE_ITEM targets minifb r96 r96_assets r96_web_assets)
foreach(target IN LISTS targets)
    target_link_libraries(${target} LINK_PUBLIC minifb r96)    
    if(EMSCRIPTEN)
        add_dependencies(${target} r96_web_assets)
        target_link_options(${target} PRIVATE
                ""-sSTRICT=1""
                ""-sENVIRONMENT=web""
                ""-sLLD_REPORT_UNDEFINED""
                ""-sMODULARIZE=1""
                ""-sALLOW_MEMORY_GROWTH=1""
                ""-sALLOW_TABLE_GROWTH""
                ""-sMALLOC=emmalloc""
                ""-sEXPORT_ALL=1""
                ""-sEXPORTED_FUNCTIONS=[\""_malloc\"",\""_free\"",\""_main\""]""
                ""-sASYNCIFY""
                ""--no-entry""
                ""-sEXPORT_NAME=${target}""
        )
    endif()
endforeach()


	And then stuff gets crazy! The first two lines compiles a list of all demo executable targets. We then iterate through those executable targets and link the r96 library target to each of them. If we build for the web, we also add the custom r96_web_assets target to the executable as a dependency, so the .html files get copied over to the output folder.


	Finally, we add a few Emscripten specific linker options. These are settings I arrived at after working with WASM for the last 2 years. They are all Emscripten specific and do things like allowing heap memory to grow. You can check out all the options in Emscripten's settings.js file. There's a lot.

The purpose of this evil incantation is to reduce the amount of CMake spaghetti needed when adding a new demo. All we need to do is add a single add_executable_target() line, specifiyng the demo name and source files its composed of. The evil incantation will then link the new demo up with all the necessary bits automatically. Nice!
The first demo app: 00_basic_window

	Before we can get our hands dirty with programmatically creating the most beautiful pixels in the world, we need to understand how MiniFB works and how a demo app is structured in terms of code. With no further ado, here's src/00_basic_window.c:


#include <MiniFB.h>
#include <stdlib.h>

int main(void) {
	const int res_x = 320, res_y = 240;
	struct mfb_window *window = mfb_open(""00_basic_window"", res_x, res_y);
	uint32_t *pixels = (uint32_t *) malloc(sizeof(uint32_t) * res_x * res_y);
	do {
		mfb_update_ex(window, pixels, res_x, res_y);
	} while (mfb_wait_sync(window));
	return 0;
}


	This is a minimal MiniFB app that opens a window with a drawing area of 320x240 pixels (line 6). It then allocates a buffer of 320x240 unit32_t elements (line 7). Each uint32_t element encodes the color of a pixel. Next, we keep drawing the contents of the buffer to the window via mfb_update_ex() (line 9) until mfb_wait_sync() returns false (line 10), e.g. because the user pressed the ESC key to quit the app. It can't get any simpler.


	MiniFB has a super minimal API. You can learn more about it here.


Note: The mfb_update_ex() function also returns a status code which can be used to decide if the app should be exited. We're not using this above for brevity's sake.

Running the demo app on the desktop

	To compile and run (or debug) our little demo app on the desktop, select the Desktop debug configure preset in the VS Code status bar, select the r96_00_basic_window target as the launch target, and the Desktop debug target launch configuration. Press F5 and you'll get this:



r96_00_basic_window on the desktop.


	Most impressive. You can make changes to the code and just hit F5 again to incrementally rebuild and restart the demo app. You can also set breakpoints, inspect variables and call stacks, and so on.


	Now how do we run the same demo app in the browser?

Running the demo app on the web

	Select the Web debug configure preset, and the Web debug target launch configuraton and press F5. You'll see this:



r96_00_basic_window running in the browser.


	The launch configuration starts a static file server (tools/web/static-server) which serves the files in build/web-debug/ on port 8123. The static server will also automatically open a browser tab with the URL corresponding to the demo's .html file.

When you're done being amazed by this, close the browser tab, and click the Stop button in the debugger controls in VS Code. If you want to be amazed again, just press F5
How the web version works

	It's kind of magic. Here's how the 00_basic_window.html file for the 00_basic_window.c demo app looks like:


<html>
<!DOCTYPE html>
<html lang='en'>
<head>
    <meta charset='utf-8'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1'>
    <link rel='stylesheet' href='r96.css'>
    <script src='r96_00_basic_window.js'></script>
</head>
<body class='r96_content'>
    <h2>Basic window</h2>
    <canvas id='00_basic_window'></canvas>
</body>
<script>
    async function init() {
        await r96_00_basic_window()
    }

    init();
</script>
</html>


	Ignoring the boring HTML boilerplate, we see that the r96_00_basic_window.js file is loaded via a <script> tag. This file was generated as part of the build by Emscripten. It contains JavaScript code that loads the WebAssembly file that stores our compiled C code and exports functions with which we can interact with the WebAssembly code.

	Next we define a <canvas> with the id 00_basic_window. Finally, a little JavaScript kicks of a call to r96_00_basic_window() (defined in r96_00_basic_window.js) in an asynchronous function. This call will load the r96_00_basic_window.wasm file and run its main() method.


	How does MiniFB know to render to the canvas? In our C code we have this line:


struct mfb_window *window = mfb_open(""00_basic_window"", res_x, res_y);


	Instead of opening a window with ""00_basic_window"" as the title, the MiniFB web backend uses the first argument passed to mfb_open() to search a canvas element with that string as its id. Any calls to mfb_update_ex() will then draw the contents of the provided buffer to this canvas.


	Also of note: We didn't have to modify our C code at all, it just ""works"". If you've ever done any front-end development, that may be very weird to you. The app basically has an infinite loop! If you do that in JavaScript, the browser tab (or the whole browser) will freeze, because the browser engine's event loop will never get a chance to run and process events. How does this magic work?


	The MiniFB web backend I wrote uses an Emscripten feature called Asyncify. In the implementation of mfb_wait_sync(), I call emscripten_sleep(0). This gives back control to the browser engine, so it can process any DOM events and not freeze. Our native C code will then resume again, without our C code ever knowing that it was actually put to sleep. The Asyncify feature rewrites our C code (or rather its WASM representation) to use continuations. That allows pausing and resuming the C code transparently. Super cool!

Can I debug the C code in the browser?

	Yes, we can in Chrome. When we build using the Web debug configure preset, Emscripten will emit DWARF information in the resulting .wasm file. Chrome can use that information to provide native code debugging right in the developer tools. To get that working:


Install Chrome
Install the C/C++ DevTools Support (DWARF) extension in Chrome
Open Chrome Developer Tools, click the gear (⚙) icon in the top right corner of dev tools pane, go to the experiments panel and tick WebAssembly Debugging: Enable DWARF support





Restart Chrome


	 Launch the demo app using the Web debug target launch config in VS code, then open the dev tools in the browser, and click on the Sources tab. You can find all the .c files that make up our little demo app under the file:// node, including the MiniFB sources. Open up 00_basic_window.c and set a breakpoint inside the loop:



C/C++ debugging in Chrome


	And there you have it: C/C++ debugging in Chrome! Since the C code runs the same on both the desktop and in the browser, we'll likely never need this functionality, unless we implement web specific features.


	Speaking of features, let's add a second demo app and draw our first pixel! But first, some very practical ""theory"".

Of colors, pixels, and rasters

	What's a pixel? Rumor has it that pixel is a stylized abbreviation of ""(pic)ture (el)ement"". A precise answer is actually quite involved and may even depend on the decade you are living in.


	Here, we lazily and imprecisely define a pixel as the smallest ""atomic"" area within a raster for which we can define a color. A raster is a rectangular area made up  of pixels. Each pixel in the raster is assumed to have the same size. The width of a raster equals the number of pixels in a row, the height equals the number of pixels in a column.


	The below raster has a width of 23 pixels and a height of 20 pixels. To locate a pixel inside the raster, we use an integer coordinate system, with the x-axis pointing to the right, and the y-axis pointing down. The top left pixel in the raster is at coordinate (0, 0), the top right pixel is at coordinate (22, 0) (or (width - 1, 0)), the bottom right pixel is at coordinate (22, 19) (or (width - 1, height -1)), and so on.



A fishy raster. Source: Wikipedia


	A raster can be a display device's output area, a piece of grid paper, etc. The rasters we'll work with are two-dimensional arrays in memory. Each array element stores the color of the pixel in some encoding.

Color encodings

	We encode the color of a pixel using the RGBA color model, where a color is represented as an additive mix of its red, green, and blue components, and an additional alpha component specifying the pixel's opacity. The opacity comes into play when we blend pixels of one raster with pixels from another raster. That's a topic for another blog post.


	More specifically, we use an ARGB8888 encoding that fits in a 32-bit unsigned integer (or uint32_t in C). Each color component is encoded as an 8-bit integer in the range 0 (no contribution) to 255 (highest contribution). For the alpha component, 0 means ""fully transparent"" and 255 means ""fully opaque"".


	Here's how the components are stored in a 32-bit unsigned integer. The most significant byte stores the alpha component, then come the red, green, and blue bytes.



Storage layout of an ARGB8888 color in a 32-bit unsigned integer. Source: Wikipedia



Here are a few colors in C:



uint32_t red = 0xffff0000;
uint32_t green = 0xff00ff00;
uint32_t blue = 0xff0000ff;
uint32_t pink = 0xffff00ff;
uint32_t fifty_percent_transparent_white = 0x80ffffff;


	More generally, we can compose a color by bit shifting and or'ing its individual components:


uint8_t alpha = 255; // fully opaque
uint8_t red = 20;    // a little red
uint8_t green = 200; // a lot of green
uint8_t blue = 0;    // no blue
uint32_t color = (alpha << 24) | (red << 16) | (green << 8) | blue;


	That looks like a great candidate for a re-usable macro! Why a macro? Because C99 support in Microsoft's C++ compiler is still meh and who knows how it does with inlined functions defined in a header. The macro guarantees that the code is inlined at the use site. Let's put the following in src/r96/r96.h


#include <stdint.h>

#define R96_ARGB(alpha, red, green, blue) (uint32_t)(((uint8_t) (alpha) << 24) | ((uint8_t) (red) << 16) | ((uint8_t) (green) << 8) | (uint8_t) (blue))


	Defining a color then becomes:


uint32_t color = R96_ARGB(255, 20, 200, 0);

Adressing a pixel in a raster

	We now can define colors easily. But how do we work with rasters in code and manipulate the colors of its pixels? We already did! Remember this line from our 00_basic_window demo app?


const int res_x = 320, res_y = 240;
...
uint32_t *pixels = (uint32_t *)malloc(res_x * res_y * sizeof(uint32_t))


	This allocates memory to store a 320x240 raster where each pixel is stored in a uint32_t. Each row of pixels is stored after the other. We can think of it as a one-dimensional array storing a two-dimensional raster.


This raster is passed to mfb_update_ex() to be drawn to the window. The reason the window content remains black is that the pixels all have the color 0x00000000 aka black (at least when building the debug variant or for Emscripten).


	We can set the pixel in the top left corner at coordinate (0, 0) to the color red like this:


pixels[0] = R96_ARGB(255, 255, 0, 0);


	OK, that was obvious. But how about a pixel at an arbitrary coordinate? Let's look at a smaller 4x3 pixel raster:




Our raster is a one dimensional block of memory. The pixel rows are stored one behind the other. The 4 pixels of the first pixel row with y=0 are stored in pixels[0] to pixels[3]. The index of a pixel in the first row is simply its x-coordinate. E.g. the pixel at coordinate (2, 0) is stored in pixels[2].

The pixels of the second row with y=1 are stored in pixels[4] to pixels[7]. The pixels of the third row with y=2 are stored in pixels[8] to pixels[11]. In general, the first pixel of a row at y-coordinate y is located at pixels[y * width]. And to address any pixel inside a row, we just add its x-coordinate! The general formula to go from a pixel's (x, y) coordinate to an index in the one dimensional array representing the raster is thus x + y * width!


Note: this is how C implements two-dimensional arrays under the hood as well. The principle also applies to higher dimensional arrays.


	If we want to set the color of the pixel at (160, 120) to red in our 320x240 pixel raster, we can do it like this:


const int res_x = 320, res_y = 240;
...
pixels[160 + 120 * res_x] = R96_ARGB(255, 255, 0, 0);

Alright, time to draw some pixels!
Demo app: drawing a pixel

	Drawing a single pixel is a bit boring, so how about we flood the screen with a gazillion pixels instead?


#include <MiniFB.h>
#include <stdlib.h>
#include <string.h>
#include ""r96/r96.h""

int main(void) {
	const int res_x = 320, res_y = 240;
	struct mfb_window *window = mfb_open(""01_drawing_a_pixel"", res_x, res_y);
	uint32_t *pixels = (uint32_t *) malloc(sizeof(uint32_t) * res_x * res_y);
	do {
		for (int i = 0; i < 200; i++) {
			int32_t x = rand() % res_x;
			int32_t y = rand() % res_y;
			uint32_t color = R96_ARGB(255, rand() % 255, rand() % 255, rand() % 255);
			pixels[x + y * res_x] = color;
		}

		if (mfb_get_mouse_button_buffer(window)[MOUSE_BTN_1]) {
			memset(pixels, 0, sizeof(uint32_t) * res_x * res_y);
		}

		mfb_update_ex(window, pixels, res_x, res_y);
	} while (mfb_wait_sync(window));
	return 0;
}


	The interesting bit happens in lines 11 to 15. Each frame, we generate 200 pixels at random coordinates with random colors. We ensure that the coordinates are within the raster bounds by % res_x and % res_y. We also clamp the color components to the range 0-255 via modulo.


	We also introduce some light input handling by checking if the left mouse button is pressed. If so, we set all pixels to the color black, giving the ""user"" a way to restart the glorious demo.


	Finally, we pass the pixels to mfb_update_ex(), which will draw them to the window.


	And here it is live in your browser, because why would we spend so much time getting the WASM build to work so nicely. Click/touch to start!







	I sure feel all this build up paid off, don't you?

Mario, WTF

	Yeah, I'm sorry. I sometimes just drift off. But we learned a lot! Next time I likely won't be so wordy. We'll have a looksy at how to draw rectangles. Exciting!


	Read the the next article in the series.


	Discuss this post on Twitter or Mastodon.





"
https://news.ycombinator.com/rss,Tesla Price Drop Angers Current Owners,https://www.bloomberg.com/news/articles/2023-01-13/tesla-price-drop-angers-current-owners-as-much-as-it-hits-profit-margins,Comments,"


Bloomberg - Are you a robot?









Bloomberg
Need help? Contact us


We've detected unusual activity from your computer network
To continue, please click the box below to let us know you're not a robot.




Why did this happen?
Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our Terms of
                Service and Cookie Policy.


Need Help?
For inquiries related to this message please contact
            our support team and provide the reference ID below.
Block reference ID:








"
https://news.ycombinator.com/rss,The art and science of spending money,https://collabfund.com/blog/the-art-and-science-of-spending-money/,Comments,"


























The Art and Science of Spending Money · Collab Fund



















Blog


About


Shared Future


SOS


Currency


Public


Blog












About


Shared Future


SOS


Currency


Public


Blog


Follow @collabfund








            
              The Art and Science of Spending Money
            
          







      Jan 12, 2023
    

SHARE ↓




        by
        Morgan Housel
@morganhousel

























          Copy Link
        












Former General Electric CEO Jack Welch once nearly died of a heart attack. Years later he was asked what went through his mind while he was being rushed to the hospital in what could have been his last moments alive.
“Damn it, I didn’t spend enough money,” was Welch’s response.
The interviewer, Stuart Varney, was puzzled, and asked why in the world that would go through his mind.
“We all are products of our background,” Welch said. “I didn’t have two nickels to rub together [when I was young], so I’m relatively cheap. I always bought cheap wine.”
After the heart attack Welch said he “swore to God I’d never buy a bottle of wine for less than a hundred dollars. That was absolutely one of the takeaways from that experience.”
“Is that it?” Varney asks, stunned.
“That’s about it,” says Welch.
Money is so complicated. There’s a human element that can defy logic – it’s personal, it’s messy, it’s emotional.
Behavioral finance is now well documented. But most of the attention goes to how people invest. Welch’s story shows how much deeper the psychology of money can go. How you spend money can reveal an existential struggle of what you find valuable in life, who you want to spend time with, why you chose your career, and the kind of attention you want from other people.
There is a science to spending money – how to find a bargain, how to make a budget, things like that.
But there’s also an art to spending. A part that can’t be quantified and varies person to person.
In my book I called money “the greatest show on earth” because of its ability to reveal things about people’s character and values. How people invest their money tends to be hidden from view. But how they spend is far more visible, so what it shows about who you are can be even more insightful.
Everyone’s different, which is part of what makes this topic fascinating. There are no black-and-white rules.
But here are a few things I’ve noticed about the art of spending money.
1. Your family background and past experiences heavily influences your spending preferences.
I love this Washington Post headline from June, 1927 – the Roaring ‘20s, the last hurrah before the Great Depression:

This is timeless, and explains so much.
After Covid lockdowns there was the concept of “revenge spending” – a furious blast of conspicuous consumption, letting out everything that had been pent up and held back in 2020.
Revenge spending happens at a broad level, too. The most stunning examples I’ve seen of this are wealthy adults who grew up poor – and were heckled, bullied, and teased for being poor as kids. Their revenge spending mentality can become permanent.
If you dig into it, I think you’ll see that a disproportionate share of those with the biggest homes, the fastest cars, and the shiniest jewelry, grew up “snubbed” in some way. Part of their current spending isn’t about getting value out of flashy material goods; it’s about healing a social wound inflicted when they were younger.
Even when “wound” is the wrong word, the desire to show the world that you’ve made it increases if you grew up snubbed out of what you wanted. To someone who grew up in an old-money affluent family, a Lamborghini might be a symbol of gaudy egotism; to those who grew up with nothing, the car might serve as the ultimate symbol that you’ve made it.
A lot of spending is done to fulfill a deep-seated psychological need.
2. Entrapped by spending: Rather than using money to build a life, your life is built around money.
George Vanderbilt spent six years building the 135,000-square-foot Biltmore house – with 40 master bedrooms and a full-time staff of nearly 400 – but allegedly spent little time there because it was “utterly unaddressed to any possible arrangement of life.” The house nevertheless cost so much to maintain it nearly ruined Vanderbilt. Ninety percent of the land was sold off to pay tax debts, and the house was turned into a tourist attraction.
In 1875 an op-ed said socialites “devote themselves to pleasure regardless of expense.” A Vanderbilt heir responded that actually they “devote themselves to expense regardless of pleasure.”
The Vanderbilt’s are obviously extreme, but that is a common trait among more ordinary people.
The devotion to expense regardless of pleasure.
Part of this is the belief that spending money will make you happier. When it doesn’t – either because it never will or because you haven’t discovered purchases that bring joy – your reaction is that you must not be spending enough, so you double down, again and again.
I’ve often wondered how many personal bankruptcies and financial troubles were caused by spending that brought no joy to begin with. It must be enormous. And it’s a double loss: not only are you in trouble, but you didn’t even have fun getting there.
I have an old friend who buried himself in credit card debt to go skiing in Europe and loved every second of it. I can wrap my head around that decision, even if I wouldn’t recommend it. He’s in control of his finances.
But what about those whose spending is driven by the belief that money is to be spent, regardless of what pleasure it brings? Money has them by the neck; they are held in captivity by its influence.
3. Frugality inertia: a lifetime of good savings habits can’t be transitioned to a spending phase.
I think what many people really want from money is the ability to stop thinking about money. To have enough money that they can stop thinking about it and focus on other stuff.
But that ultimate goal can break down when your relationship with money becomes an ingrained part of your personality. You struggle to break away from focusing on money because the focus itself is a big part of who you are.
If you develop an early system of savings and living well below your means – congratulations, you’ve won. But if you can never break away from that system, and insist on a heavy savings regimen well into your retirement years … what is that? Is it still winning?
A lot of financial planners I’ve talked to say one of their biggest challenges is getting clients to spend money in retirement. Even an appropriate, conservative amount of money. Frugality and savings become such a big part of some people’s identity that they can’t ever switch gears.
I think for some people that’s actually fine. Watching money compound gives them more pleasure than they would get spending it.
But those whose ultimate goal is to stop thinking about money are stuck. Refusing to recognize that you’ve met your goal can be as bad as never meeting the goal to begin with.
4. An emotional attachment to large purchases, particularly a house.
My wife and I pride ourselves on making unemotional financial decisions. But a few years ago we were in the market for our first house. We found one online that we liked, and as we headed out for a tour we promised ourselves we wouldn’t do anything rash – this was just gathering information.
Then we pulled into the driveway and my wife gasped, “I love it!” I did too. We had an infant son – our first – and there was a kids’ tree swing in the front yard. Perfect.
And that was it. Emotion was involved and there was nothing we could do about it.
We have zero regrets – the house really was great. But no one should pretend that you can make life-changing decisions that will massively impact you and your family and treat it like a math problem.
Jason Zweig of The Wall Street Journal once wrote about his mom selling her longtime home:

“I have no emotional attachment to the house; I never liked it physically,” Mom told us. “But everything important that ever happened in our life as a family is here, and I can’t just leave all that behind.”

If I said, “How much are the memories with your kids worth?” you’d say it’s impossible to attach a dollar figure. But if I said, “How much is the home where you formed memories with your kids worth?” or “How much does staying in your local town impact your salary?” you could probably spit out a dollar figure with ease.
Understanding the difference between those two helps explain a lot of spending decisions.
5. The joy of spending can diminish as income rises because there’s less struggle, sacrifice, and sweat represented in purchases.
In his 1903 book The Quest for the Simple Life, William Dawson writes:

The thing that is least perceived about wealth is that all pleasure in money ends at the point where economy becomes unnecessary. The man who can buy anything he covets, without any consultation with his banker, values nothing that he buys.

Consider how you felt when you got your first paycheck from your first job. If you celebrated with as little as a milkshake from Denny’s you probably had a joyous feeling of, “I did this. I bought this. With my own money.” Going from not being able to buy anything to able to buy something is an amazing feeling. The gap between struggle and reward is a big part of what makes people happy.
Contrast that with later in your career, when (hopefully) savings have been built and paychecks have grown. It’s not that spending won’t make you happy – but it won’t be as thrilling and adrenaline-inducing as it was when there was more struggle behind each dollar.
I know a guy with a private chef. He’s served 5-star meals three times a day, an arrangement he’s enjoyed for years. It’s amazing; I’d lie if I said I wasn’t jealous. But I also wonder if the joy diminishes over time. He doesn’t have to struggle to get these meals – there’s no anticipation, no looking forward to a restaurant reservation, no contrasting gap between a “normal” meal and his daily delicacy.
There’s a saying that the best meal you’ll ever taste is a glass of water when you’re thirsty. All forms of spending have that equivalent.
Let me end with a wise quote from, of all people, Richard Nixon:

The unhappiest people of the world are those in the international watering places like the South Coast of France, and Newport, and Palm Springs, and Palm Beach. Going to parties every night. Playing golf every afternoon. Drinking too much. Talking too much. Thinking too little. Retired. No purpose.
So while there are those that would disagree with this and say “Gee, if I could just be a millionaire! That would be the most wonderful thing.” If I could just not have to work every day, if I could just be out fishing or hunting or playing golf or traveling, that would be the most wonderful life in the world – they don’t know life. Because what makes life mean something is purpose. A goal. The battle. The struggle – even if you don’t win it.

6. Asking $3 questions when $30,000 questions are all that matter.
There’s a saying: Save a little bit of money each month, and at the end of the year you’ll be surprised at how little you still have.
Author Ramit Sethi says too many people ask $3 questions (can I afford this latte?) when all that matters to financial success are $30,000 questions (what college should I go to?)
Historian Cyril Parkinson coined a thing called Parkinson’s Law of Triviality. It states: “The amount of attention a problem gets is the inverse of its importance.”
Parkinson described a fictional finance committee with three tasks: approval of a $10 million nuclear reactor, $400 for an employee bike shed, and $20 for employee refreshments in the break room.
The committee approves the $10 million nuclear reactor immediately, because the number is too big to contextualize, alternatives are too daunting to consider, and no one on the committee is an expert in nuclear power.
The bike shed gets considerably more debate. Committee members argue whether a bike rack would suffice and whether a shed should be wood or aluminum, because they have some experience working with those materials at home.
Employee refreshments take up two-thirds of the debate, because everyone has a strong opinion on what’s the best coffee, the best cookies, the best chips, etc.
Many households operate the same.
7. Social aspiration spending: Trickle-down consumption patterns from one socioeconomic group to the next.
Economist Joseph Stiglitz once wrote: “Trickle-down economics may be a chimera but trickle-down behaviorism is very real.”
There is no such thing as an objective level of wealth. Everything is relative to something else. People look around and say, “What’s that person driving, where are they living, what kind of clothes are they wearing?” Aspirations are calibrated accordingly.
I spoke with Wired magazine founding executive editor Kevin Kelly last week. He brought up an interesting point: If you want to know what lower-income groups will aspire to spend their money on in the future, look at what higher-income groups exclusively do today.
European vacations were once the exclusive playground of the rich. Then they trickled down.
Same with college. It was once reserved for the highest income groups. Then it spread.
Same with investing. In 1929 – the peak of the Roaring ‘20s bubble – five percent of Americans owned stocks, virtually all of them the very wealthy. Today, 58% of households own stocks in some form.
Same with two-car households, lawns, walk-in closets, granite countertops, six-burner stoves, jet travel, and even the entire concept of retirement.
Part of the reason these products spread to the masses is that they got cheaper. But the reason they got cheaper is because there was so much demand from the masses – hungered by their aspirations – that pushed companies to innovate new ways of mass production.
People like to mimic others, especially those who appear to be living better lives. Always been like that, always will be.
8. An underappreciation of the long-term cost of purchases, with too much emphasis on the initial price.
It’s common to find someone who bought their home in, say, 1974, for something like $60,000. Today it’s worth perhaps $350,000. The owners no doubt feel they have made the investment of their lives.
But those numbers above equate to an average annual return of 3.75%. Property taxes tend to average roughly 1%, so that brings our real return to 2.75% per year. Maintenance and repairs vary greatly, but spending 1% - 3% of your home’s value per year on upkeep should be expected.
Where does that leave our long-term returns? Ah, quite dim.
Price is easy to calculate. It’s just whatever you paid initially and sold for eventually.
Cost is harder to figure out. They tend to be a slow drip over time, which are easy to ignore but add up quickly.
Same for cars, boats, and hobbies. You can even say the cost of smoking cigarettes is the price of a pack plus the long-term cost of medical care associated with the habit. One is easy to calculate, the other is very difficult.
9. No one is impressed with your possessions as much as you are.
When you see someone driving a nice car, you rarely think, “Wow, the guy driving that car is cool.” Instead, you think, “Wow, if I had that car people would think I’m cool.” Subconsciously or not, this is how people think.
There is a paradox here: people tend to want wealth to signal to others that they should be liked and admired. But in reality those other people often bypass admiring you, not because they don’t think wealth is admirable, but because they use your wealth as a benchmark for their own desire to be liked and admired.
I wrote a letter to my son the day he was born. It says, in part:

You might think you want an expensive car, a fancy watch, and a huge house. But I’m telling you, you don’t. What you want is respect and admiration from other people, and you think having expensive stuff will bring it. It almost never does – especially from the people you want to respect and admire you.

Now, I like nice homes and nice cars as much as anyone. The point here is not to shoo you away from nice things.
It’s just a recognition that no one is as impressed with your stuff as much as you are. Or even that no one is thinking about you as much as you are. They’re busy thinking about themselves!
People generally aspire to be respected and admired by others, and using money to buy fancy things may bring less of it than you imagine. If respect and admiration are your goal, be careful how you seek it. Humility, kindness, and empathy will bring you more respect than horsepower ever will.
10. Not knowing what kind of spending will make you happy because you haven’t tried enough new and strange forms of spending.
Evolution is the most powerful force in the world, capable of transforming single-cell organisms into modern humans.
But evolution has no idea what it’s doing. There’s no guide, no manual, no rulebook. It’s not even necessarily good at selecting traits that work.
Its power is that it “tries” trillions upon trillions of different mutations and is ruthless about killing off the ones that don’t work. What’s left – the winners – stick around.
There’s a theory in evolutionary biology called Fisher’s Fundamental Theorem of Natural Selection. It’s the idea that variance equals strength, because the more diverse a population is the more chances it has to come up with new traits that can be selected for. No one can know what traits will be useful; that’s not how evolution works. But if you create a lot of traits, the useful one – whatever it is – will be in there somewhere.
There’s an important analogy here about spending money.
A lot of people have no idea what kind of spending will make them happy. What should you buy? Where should you travel? How much should you save? There is no single answer to these questions because everyone’s different. People default to what society tells them – whatever is most expensive will bring the most joy.
But that’s not how it works. You have to try spending money on tons of different oddball things before you find what works for you. For some people it’s travel; others can’t stand being away from home. For others it’s nice restaurants; others don’t get the hype and prefer cheap pizza. I know people who think spending money on first-class plane tickets is a borderline scam. Others would not dare sit behind row four. To each their own.
The more different kinds of spending you test out, the closer you’ll likely get to a system that works for you. The trials don’t have to be big: a $10 new food here, a $75 treat there, slightly nice shoes, etc.
Here’s Ramit Sethi again: “Frugality, quite simply, is about choosing the things you love enough to spend extravagantly on—and then cutting costs mercilessly on the things you don’t love.”
There is no guide on what will make you happy – you have to try a million different things and figure out what fits your personality.
11. The social signaling aspect of money, on both things you buy for yourself and charity given to others.
There’s a saying that if you get public recognition for donating money, it’s not charity – it’s philanthropy. And if you demand recognition, it’s not even charity – it’s a business deal. There’s a clear social benefit to you, the giver, in addition to the recipient. I don’t mean that in a negative way: Good donations to worthy causes would plunge if donors didn’t get recognition.
Most forms of spending have two purposes: To bring some sort of utility to the owner, and to signal something to other people.
Homes, cars, clothes, jewelry, obviously fit into that category. But even travel does as well – how many vacation destinations are picked at least in part by what you think will make a good Instagram picture, or just that it sounds cool. (My guess is most Bali vacations fall into that category).
Psychologist Jonathan Haidt says people don’t communicate on social media; they perform for one another. Spending money is like that, too.
It’s not always a bad thing. If you’ve merely thought about what clothes you’ll look best in before you leave in the morning, you’ve engaged in signaling. And it’s not always about looking the best: intentionally dressing casually to a formal meeting sends a powerful message about who holds the power. Before being caught as a sham, Sam Bankman-Fried said he intentionally didn’t wear pants to create a mystique.
The thing to recognize is that spending money “on yourself” is often done with the intent of influencing what other people think.
That should spark three questions: Whose opinion are you trying to influence, why, and are those people even paying attention?
12. The social hierarchy of spending, positioning you against your peers.
An old joke is about two hikers who come across a grizzly bear in the woods. One starts to run, and the other yells, “Are you crazy, you can’t outrun a bear!” The runner replies: “I don’t have to be faster than the bear. I only have to be faster than you.”
All success is simply relative to someone else – usually those around you.
That’s important for spending money, because for so many people the question of whether you’re buying nice things is actually, “are your things nicer than other peoples’ things?” The question of whether your home is big enough is actually, “is your home bigger than your neighbor’s?”
Not only is the urge to one-up your peers, but you may feel the need to continually surpass your own spending. Is this year’s vacation more expensive than last year’s? Is the next car fancier than the old one?
Money to some people is less of an asset and more of a social liability, indebting them to a status-chasing life that can leave them miserable.
It’s a dangerous trap if you don’t recognize the game and how it’s played. Montesquieu wrote 275 years ago, “If you only wished to be happy, this could be easily accomplished; but we wish to be happier than other people, and this is always difficult, for we believe others to be happier than they are.”
13. Spending can be a representation of how hard you’ve worked and how much stress went into earning your paycheck.
Someone who works 100 hours a week and hates their job may have an urge to spend frivolously in an attempt to compensate for the misery of how their paycheck was earned.
Never have I seen money burn a hole in someone’s pocket faster than an investment banker receiving their annual bonus. After 12 months of Excel modeling until 3am, you have an urge to prove to yourself that it was worth it, offsetting what you sacrificed. It’s like someone held underwater for a minute – they do not take a calm breath when they surface; they gasp.
The opposite can hold true. I can only back this up with anecdotal experiences, but those most capable of delayed gratification are often those who enjoy their work. The pay might be good, but the urge to compensate for your hard work with heavy spending isn’t there.
Spending money to make you happy is hard if you’re already happy.
More on this topic:


My book, The Psychology of Money


Lifestyles


Getting Wealthy vs. Staying Wealthy




SHARE Copy Link 















Sign up for more Collab Fund content

Email address












More from the blog…





  by
  
    
  
  — 
  














  by
  
    
  
  — 
  














  by
  
    
  
  — 
  

















Collab Fund

      Collaborative is a leading source of capital for big ideas pushing the world forward.
    



Newsletter
Sign up for updates ↗








Twitter
Follow @collabfund ↗








RSS
Subscribe to the blog ↗











About


Shared Future


SOS


Currency


Public


Blog


Site Credits







Collaborative Fund Management LLC, Collaborative Holdings Management LP and Collab+Currency Management, LLC are distinct investment advisory entities, are not a unitary enterprise and operate independently of one another.  From time to time Collaborative Fund Management LLC may draw on its relationship with Collaborative Holdings Management LP and/or Collab+Currency Management, LLC, but only to the extent consistent with its status as a separate investment adviser.






"
https://news.ycombinator.com/rss,TensorFlow for Python is dying?,https://thenextweb.com/news/why-tensorflow-for-python-is-dying-a-slow-death,Comments,"









                                    Story by
                                


                                        Ari Joury
                                    




Religious wars have been a cornerstone in tech. Whether it’s debating about the pros and cons of different operating systems, cloud providers, or deep learning frameworks — a few beers in, the facts slide aside and people start fighting for their technology like it’s the holy grail.
Just think about the endless talk about IDEs. Some people prefer VisualStudio, others use IntelliJ, again others use plain old editors like Vim. There’s a never-ending debate, half-ironic of course, about what your favorite text editor might say about your personality.
Similar wars seem to be flaring up around PyTorch and TensorFlow. Both camps have troves of supporters. And both camps have good arguments to suggest why their favorite deep learning framework might be the best.
Get your tickets for TNW Valencia in March!The heart of tech is coming to the heart of the Mediterranean
Join now
That being said, the data speaks a fairly simple truth. TensorFlow is, as of now, the most widespread deep learning framework. It gets almost twice as many questions on StackOverflow every month as PyTorch does.
On the other hand, TensorFlow hasn’t been growing since around 2018. PyTorch has been steadily gaining traction until the day this post got published.
For the sake of completeness, I’ve also included Keras in the figure below. It was released at around the same time as TensorFlow. But, as one can see, it’s tanked in recent years. The short explanation for this is that Keras is a bit simplistic and too slow for the demands that most deep learning practitioners have.
PyTorch is still growing, while TensorFlow’s growth has stalled. Graph from StackOverflow trends.
StackOverflow traffic for TensorFlow might not be declining at a rapid speed, but it’s declining nevertheless. And there are reasons to believe that this decline will become more pronounced in the next few years, particularly in the world of Python.
PyTorch feels more pythonic
Developed by Google, TensorFlow might have been one of the first frameworks to show up to the deep learning party in late 2015. However, the first version was rather cumbersome to use — as many first versions of any software tend to be.
That is why Meta started developing PyTorch as a means to offer pretty much the same functionalities as TensorFlow, but making it easier to use.
The people behind TensorFlow soon took note of this, and adopted many of PyTorch’s most popular features in TensorFlow 2.0.
A good rule of thumb is that you can do anything that PyTorch does in TensorFlow. It will just take you twice as much effort to write the code. It’s not so intuitive and feels quite un-pythonic, even today.
PyTorch, on the other hand, feels very natural to use if you enjoy using Python.
PyTorch has more available models
Many companies and academic institutions don’t have the massive computational power needed to build large models. Size is king, however, when it comes to machine learning; the larger the model the more impressive its performance is.
With HuggingFace, engineers can use large, trained and tuned models and incorporate them in their pipelines with just a few lines of code. However, a staggering 85% of these models can only be used with PyTorch. Only about 8% of HuggingFace models are exclusive to TensorFlow. The remainder is available for both frameworks.
This means that if you’re planning to use large models, you’d better stay away from TensorFlow or invest heavily in compute resources to train your own model.
PyTorch is better for students and research
PyTorch has a reputation for being appreciated more by academia. This is not unjustified; three out of four research papers use PyTorch. Even among those researchers who started out using TensorFlow — remember that it arrived earlier to the deep learning party — the majority have migrated to PyTorch now.
These trends are staggering and persist despite the fact that Google has quite a large footprint in AI research and mainly uses TensorFlow.
What’s perhaps more striking about this is that research influences teaching, and therefore defines what students might learn. A professor who has published the majority of their papers using PyTorch will be more inclined to use it in lectures. Not only are they more comfortable teaching and answering questions regarding PyTorch; they might also have stronger beliefs regarding its success.
College students therefore might get much more insights about PyTorch than TensorFlow. And, given that the college students of today are the workers of tomorrow, you can probably guess where this trend is going…
PyTorch’s ecosystem has grown faster
At the end of the day, software frameworks only matter insofar as they’re players in an ecosystem. Both PyTorch and TensorFlow have quite developed ecosystems, including repositories for trained models other than HuggingFace, data management systems, failure prevention mechanisms, and more.
It’s worth stating that, as of now, TensorFlow has a slightly more developed ecosystem than PyTorch. However, keep in mind that PyTorch has shown up later to the party and has had quite some user growth over the past few years. Therefore one can expect that PyTorch’s ecosystem might outgrow TensorFlow’s in due time.
TensorFlow has the better deployment infrastructure
As cumbersome as TensorFlow might be to code, once it’s written is a lot easier to deploy than PyTorch. Tools like TensorFlow Serving and TensorFlow Lite make deployment to cloud, servers, mobile, and IoT devices happen in a jiffy.
PyTorch, on the other hand, has been notoriously slow in releasing deployment tools. That being said, it has been closing the gap with TensorFlow quite rapidly as of late.
It’s hard to predict at this point in time, but it’s quite possible that PyTorch might match or even outgrow TensorFlow’s deployment infrastructure in the years to come.
TensorFlow code will probably stick around for a while because it’s costly to switch frameworks after deployment. However, it’s quite conceivable that newer deep learning applications will increasingly be written and deployed with PyTorch.
TensorFlow is not all about Python
TensorFlow isn’t dead. It’s just not as popular as it once was.
The core reason for this is that many people who use Python for machine learning are switching to PyTorch.
But Python is not the only language out there for machine learning. It’s the O.G. of machine learning, and that’s the only reason why the developers of TensorFlow centered its support around Python.
These days, one can use TensorFlow with JavaScript, Java, and C++. The community is also starting to develop support for other languages like Julia, Rust, Scala, and Haskell, among others.
PyTorch, on the other hand, is very centered around Python — that’s why it feels so pythonic after all. There is a C++ API, but there isn’t half the support for other languages that TensorFlow offers.
It’s quite conceivable that PyTorch will overtake TensorFlow within Python. On the other hand, TensorFlow, with its impressive ecosystem, deployment features, and support for other languages, will remain an important player in deep learning.
Whether you choose TensorFlow or PyTorch for your next project depends mostly on how much you love Python.
This article was written by Ari Joury and was originally published on Medium. You can read it here. 


Get the TNW newsletter
Get the most important tech news in your inbox each week.


                                    Follow @thenextweb
                                


Also tagged with



Python




                                Published January 13, 2023 - 2:47 pm UTC

Back to top





















"
https://news.ycombinator.com/rss,Need for speed: static analysis version,https://semgrep.dev/blog/2022/static-analysis-speed,Comments,"Need for speed: static analysis versionLog inSign up freeRegistryPlaygroundProductsSemgrep AppManage and enforce code standards across your organization. Get started for free.Semgrep Supply ChainFind dependency vulnerabilities in your code.Featured docsGetting started with Semgrep Supply ChainStart finding high-priority security issues in your dependenciesPricingResourcesDocsWant to read all the docs? Start hereTutorialLearn to write Semgrep rules in under 10 minutesBlogGet the latest news about SemgrepContact UsWant to talk to a human? Get in touch with us!Latest blog postsIntroducing Semgrep Supply Chain.Find reachable vulnerable dependencies in your codeGeneral availability support of PHPSemgrep adds PHP support including 40+ new rulesDemystifying taint modeA user-friendly guide to writing rules with Semgrep's taint modeAll blog postsLog inSign up freeBlogdevelopmentNeed for speed: static analysis versiondevelopmentNeed for speed: static analysis versionWhy speed is important in static analysis and how Semgrep achieves ludicrous speedBrandon WuNovember 29, 2022In this articleLudicrous graphsStatic analysis at scaleKnowing is half the battleA unique nicheFindings, fasterSemantics, speedilyConclusionSubscribe to our blogShareShareTL;DR: Semgrep has achieved remarkably fast scan times by prioritizing speed using methods like taint summaries and tree matching in OCaml. In addition, Semgrep’s design as a tool that searches for syntax makes it fast due to designs like purely textual single-file analysis, partial parsing, and optimizations like skipping files that cannot produce matches.Program analysis is an extremely interesting discipline that aims to combine an impossible task (finding undesirable parts of programs) with practical usage (being useful to developers to fix their code). Practical usage takes many forms ranging from convenience of information and quality of findings to the speed at which the analysis is carried out.At r2c, we have one motto which we stick to — “code analysis at ludicrous speed”. After almost 3 years of development, a question remains—what goes into making a code analysis product that can run at “ludicrous speed”, and have we achieved that goal with Semgrep?Ludicrous graphsHow do we qualify “ludicrous speed”? Some results for Semgrep’s speed can be seen here, in graphic form:Here is a graph of Semgrep’s scan time (in seconds) for the Django Python repository, over time. This data serves as a direct reflection of Semgrep’s growth over the past year, as various optimizations and engine upgrades have been carried out:Figure 1: Semgrep scan time (in seconds) for Django Python repository - sourceAnd for the lodash JavaScript repository:Figure 2: Semgrep scan time (in seconds) for lodash JavaScript repository - sourceHere’s the performance of Semgrep on all of its benchmarking repositories over time:Figure 3: Semgrep scan time for different repositoriesOver time, Semgrep has been making a consistent effort towards increased performance. In all benchmarked cases, that scan time using the latest Semgrep version takes place in less than 20 seconds, which is a significantly short enough period to run within a developer’s normal commit workflow.Here’s some data validating Semgrep’s run-time (in Python, on the Django repository) against some other open-source Python analysis tools. All data are averaged and sourced from tests run on an M1 Mac machine.Figure 4: Semgrep scan time as compared to other Python analysis tools on Django repositoryIn this graph, we see that Semgrep performs quite fast, beating out the other tools. It’s worth noting that pylint and flake8 are linting tools, which primarily work in the realm of style enforcement, which notably is not concerned with the behavior of the program, like Semgrep. With features like taint analysis, constant propagation, and dataflow analysis, it’s a fair description that Semgrep performs more computationally intensive analysis than the other options. More than just curiosity, Semgrep’s speed has made it feasible to be run by existing organizations in production to shift left.Static analysis at scaleWhat makes a static analysis (SAST) tool fast? Well, it’s useful to look at what may make a static analysis tool slow. SAST applications have to be able to process large amounts of source code, break it down into a format suitable for analysis, and then run detailed semantic scans on it.This analysis may also be done in a dynamic fashion, where programs are instrumented to detect certain faults during their runtime, but this has the disadvantage of adding extra overhead to the analyzed program, as well as operating closer to the hardware level, as opposed to the language level. In this article, we will focus on static analysis, and stay closer to the language level, which will yield dividends later on with Semgrep.Static analysis is inherently hard because it tries to find answers to questions about program behavior — about programs that may run for a very long time, if not forever. Given that it is static, this analysis must be done without running the program, so any actual evaluation is a non-starter. How can we make such a problem tractable?The way that this generally takes place is in approximation. While we cannot run the program itself to find out what is actually happening, standard techniques allow us to gain (possibly imprecise) knowledge of the state of the program. In particular, dataflow analysis, a classic technique, involves an iterative scan over all possible program points to find out properties that may be true at those points.In order to facilitate this dataflow analysis, static analysis tools need to know something about what paths the program may take during execution. This is achieved by computing a control-flow graph, which is a graph that connects the various parts of the code which may execute after each other. Given a project with many functions, conditionals, and program text in general, however, looping over the entire control-flow graph of a program is not a trivial task. How does this be done in a more optimal way?Knowing is half the battleThe general mantra that Semgrep follows to facilitate its success is that it only picks battles that it can win.Program analysis is a never-ending uphill slope because analyses can always be done more deeply, and more compute time can always be thrown into figuring out more things about the program's behavior. In practice, however, for the majority of applications, program analysis need not be particularly deep or theoretically based to be effective in general.From the beginning of Semgrep, speed has been a major focus. To make sure we stay in line with that, from the beginning, we make sure that we only support features when we know that Semgrep can win, or in other words, that it can be done in a fast way.A unique nicheIn a way, philosophically, Semgrep’s original purpose was in line with this way of thinking. Semgrep occupies a unique niche as a tool that straddles the line between syntactic and semantic, however, it used to be more towards the former. It started as sgrep at Facebook by r2c’s own Yoann Padioleau, an open source tool that was to match program text in a semantically-aware way, but which lacked some of the modern-day features Semgrep possesses, such as constant propagation and taint analysis.This original focus granted Semgrep a unique perspective on program analysis, as sgrep didn’t need to solve many of the problems that other SAST applications aimed to do. Since it was a matching tool, there was no need to be aware of code flow at all, which is the main bottleneck in terms of program analysis. Since it only focused on text, there was no need to have any kind of understanding of programs beyond single files—necessarily, there was no need even to require that analyzed code compiled. This also granted other advantages, such as partial parsing, where programs that are not syntactically correct can be parsed to trees that look “close enough”. This overall had the advantage of making Semgrep an extremely versatile and robust tool, from the get-go.In addition, the unique capabilities of Semgrep as a tool for code review automation, beyond just vulnerability-finding, necessitated that it be able to run quickly. In general, code analysis can occur on a nightly basis, with the goal of reporting any possible errors by the beginning of the next morning, so that engineers can triage those findings. This gives a sizeable 12-hour period for static analysis, which permits a large range of complex inquiries. Semgrep’s role as a customizable tool that catches vulnerabilities on each pull request means that it isn’t working in nearly the same time frame—developers need to be able to interface with it as a regular part of their workflow of merging commits and writing code. This gives an upper limit of minutes, as opposed to hours, for Semgrep. So not only does Semgrep only pick battles it can win—it must.Findings, fasterSpeed is an admirable goal for any static analysis tool. A more interesting question, however, is how this is achieved.In a similar sense, the fact that Semgrep lives so close to the syntax of a program helps again. One of the most helpful improvements made for Semgrep’s speed was in recognizing this. Whereas an arbitrary static analysis may not know specifically where a bug may occur and thus have to check all of a given program, Semgrep rules are typically written with some amount of the desired text to find—for instance, they may contain the name of a sensitive function or some particular primitive in a language.This characteristic made it possible to speed up Semgrep scans by only searching files that are known to contain literal text which matches that in the patterns. For instance, consider the following Semgrep rule which looks for a hardcoded tmp directory within an open:source: Semgrep ruleThe pattern which this rule looks for involves a call to open, which can only occur if the literal string open occurs anywhere in the given file. This can easily be tested in linear time, resulting in Semgrep being able to skip files that are known to be impossible to produce a match in, due to this property, which significantly improves scan times. In this case, it’s interesting how purely syntactic searches can supplement more semantic searches!Another significant speed benefit occurs from the inherent nature of the problem. Semgrep’s core engine is written in OCaml, a functional programming language because functional languages are ideal for the kind of structural decomposition on recursive data (the target program) that Semgrep’s main matching engine needs to do. This engine is used to provide raw matching data to the Python wrapper, which would then do the work of combining and analyzing the matches using the rule’s pattern combinators. This work is merely more structural decomposition on recursive data (the pattern), however, and another performance boost was gained upon porting that section of the logic to OCaml.Semantics, speedilyTree matching has a nearly negligible cost when compared to most deep program analysis techniques, such as pointer analysis or symbolic execution, so this was clearly a winning battle. As Semgrep grew more advanced, more features were added which caused it to err closer to the side of semantics, such as taint analysis and constant propagation.These analyses are not necessarily ones that can be done quickly. Taint analysis, in particular, requires running dataflow analysis over the entire control flow of a program, which can potentially be huge, when considering how large an entire codebase may be, with all of its function calls and tricky control flow logic. To do taint analysis in this way would be to pick a losing battle.Semgrep succeeds in that it only carries out single-file analysis, so the control flow graph never exceeds the size of a file. In addition, taint can be done incrementally. Functions have well-defined points where they begin and end, as well as generally well-defined entrances in terms of the data they accept (the function arguments). Thus, Semgrep collects taint summaries, which essentially (per function) encode the information about what taint may be possible, depending on the taint of the inputs that flow in.So for instance, given a function in Python:1def foo(a, b):
2
3    sink(a)
4
5    return NoneA taint summary for this function foo will note that, if the input a is tainted, then it will reach the tainted sink sink. Regardless of how the function foo is used, this is a fact about the function’s potential use. Then, at the call site to the function, if the input a is tainted, then we know to report a finding.This seems simple, but the end result is that we only ever need to run a dataflow analysis on the code of each function once. Never does a taint summary need to be collected for a given function more than once, meaning that we can simply stitch all these facts together at the end, making for speedy results. The core lesson is that, by collecting summaries and doing taint in an intelligent way, we pick the battle that we can win. It turns out that for our upcoming inter-file extension to Semgrep, by applying this approach in an inter-file manner, we can still reap the speed benefits. This lets us avoid running dataflow analysis on an entire control-flow graph, and instead do small, localized analyses.ConclusionAt the end of the day, solving an undecidable problem at a pace that is useful to security engineers is a hard task. Harder still is solving an undecidable problem at a pace that is useful to the developer, such that it can fit into the normal cycle of writing code and pushing commits. Semgrep’s unique philosophy as a tool has made it capable of bridging this gap, and over time, has proven it to have a speed that is nothing short of ludicrous.AboutSemgrep is a fast, open-source, static analysis tool for finding bugs, detecting vulnerabilities in third-party dependencies, and enforcing code standards.Learn more with Semgrep’s blogStart ScanningBrowse PostsAnnouncementJune 22, 20222 min readAnnouncing Semgrep's general availability support of PHPPablo EstradaAnnouncementMay 11, 20225 min readSemgrep's May 2022 updates: Introducing DeepSemgrep, plus new Playground, and self managed GitHub + GitLab support!Chinmay GaikwadBest practicesOctober 01, 20215 min readProtect Your GitHub Actions with SemgrepGrayson HardawayCode scanning at ludicrous speedFind Bugs and Enforce Code StandardsStart ScanningBook a DemoCode analysis at ludicrous speedStay up to dateTwitterSlackGitHubYouTubeResourcesDocsTutorialBlogAbout usContact usProductsSemgrep AppSemgrep Supply ChainPricingTwitterSlackGitHubYouTube© 2023 r2c. Semgrep is a registered trademark of r2c.Semgrep jobsTermsPrivacy"
https://news.ycombinator.com/rss,A cab ride I'll never forget (1999),https://kentnerburn.com/the-cab-ride-ill-never-forget/,Comments,"





















The Cab Ride I'll Never Forget | Kent Nerburn













































































 









		Skip to content













					Kent Nerburn
				


				wandering, wondering, writing
			
 





About

Menu Toggle





Interviews


Photo Gallery


Books
Speaking | Book Clubs
Musings
Shop
Contact
Home
 







 










					Kent Nerburn
				


				wandering, wondering, writing
			
 







Main Menu

 









About

Menu Toggle

InterviewsBook Review: Native EchoesBooksBooks-oldContactDan and Grover talk about Indian MascotsDancing with the Gods: Reflections on Life and ArtKent Nerburn

Menu Toggle

Join our mailing listMusingsPhoto GalleryPrivacyShopSouth Dakota TravelogueSpeaking

Menu Toggle

Presentation OptionsSubscribeThe Cab Ride I’ll Never Forget 

















 










The Cab Ride I'll Never Forget 




There was a time in my life twenty years ago when I was driving a cab for a living. It was a cowboy’s life, a gambler’s life, a life for someone who wanted no boss, constant movement and the thrill of a dice roll every time a new passenger got into the cab.What I didn’t count on when I took the job was that it was also a ministry. Because I drove the night shift, my cab became a rolling confessional. Passengers would climb in, sit behind me in total anonymity and tell me of their lives.We were like strangers on a train, the passengers and I, hurtling through the night, revealing intimacies we would never have dreamed of sharing during the brighter light of day. I encountered people whose lives amazed me, ennobled me, made me laugh and made me weep. And none of those lives touched me more than that of a woman I picked up late on a warm August night.I was responding to a call from a small brick fourplex in a quiet part of town. I assumed I was being sent to pick up some partiers, or someone who had just had a fight with a lover, or someone going off to an early shift at some factory for the industrial part of town.When I arrived at the address, the building was dark except for a single light in a ground-floor window. Under these circumstances, many drivers would just honk once or twice, wait a short minute, then drive away. Too many bad possibilities awaited a driver who went up to a darkened building at 2:30 in the morning.But I had seen too many people trapped in a life of poverty who depended on the cab as their only means of transportation. Unless a situation had a real whiff of danger, I always went to the door to find the passenger. It might, I reasoned, be someone who needs my assistance. Would I not want a driver to do the same if my mother or father had called for a cab?So I walked to the door and knocked.“Just a minute,” answered a frail and elderly voice. I could hear the sound of something being dragged across the floor. After a long pause, the door opened. A small woman somewhere in her 80s stood before me. She was wearing a print dress and a pillbox hat with a veil pinned on it, like you might see in a costume shop or a Goodwill store or in a 1940s movie. By her side was a small nylon suitcase. The sound had been her dragging it across the floor.The apartment looked as if no one had lived in it for years. All the furniture was covered with sheets. There were no clocks on the walls, no knickknacks or utensils on the counters. In the corner was a cardboard box filled with photos and glassware.“Would you carry my bag out to the car?” she said. “I’d like a few moments alone. Then, if you could come back and help me? I’m not very strong.”I took the suitcase to the cab, then returned to assist the woman. She took my arm, and we walked slowly toward the curb. She kept thanking me for my kindness.“It’s nothing,” I told her. “I just try to treat my passengers the way I would want my mother treated.”“Oh, you’re such a good boy,” she said. Her praise and appreciation were almost embarrassing.When we got in the cab, she gave me an address, then asked, “Could you drive through downtown?”“It’s not the shortest way,” I answered.“Oh, I don’t mind,” she said. “I’m in no hurry. I’m on my way to a hospice.”I looked in the rearview mirror. Her eyes were glistening. “I don’t have any family left,” she continued. “The doctor says I should go there. He says I don’t have very long.”I quietly reached over and shut off the meter. “What route would you like me to go?” I asked.For the next two hours we drove through the city. She showed me the building where she had once worked as an elevator operator. We drove through the neighborhood where she and her husband had lived when they had first been married. She had me pull up in front of a furniture warehouse that had once been a ballroom where she had gone dancing as a girl. Sometimes she would have me slow in front of a particular building or corner and would sit staring into the darkness, saying nothing.As the first hint of sun was creasing the horizon, she suddenly said, “I’m tired. Let’s go now.”We drove in silence to the address she had given me. It was a low building, like a small convalescent home, with a driveway that passed under a portico. Two orderlies came out to the cab as soon as we pulled up. Without waiting for me, they opened the door and began assisting the woman. They were solicitous and intent, watching her every move. They must have been expecting her; perhaps she had phoned them right before we left.I opened the trunk and took the small suitcase up to the door. The woman was already seated in a wheelchair.“How much do I owe you?” she asked, reaching into her purse.“Nothing,” I said.“You have to make a living,” she answered.“There are other passengers,” I responded.Almost without thinking, I bent and gave her a hug. She held on to me tightly. “You gave an old woman a little moment of joy,” she said. “Thank you.”There was nothing more to say. I squeezed her hand once, then walked out into the dim morning light. Behind me, I could hear the door shut. It was the sound of the closing of a life.I did not pick up any more passengers that shift. I drove aimlessly, lost in thought. For the remainder of that day, I could hardly talk. What if that woman had gotten an angry driver, or one who was impatient to end his shift? What if I had refused to take the run, or had honked once, then driven away? What if I had been in a foul mood and had refused to engage the woman in conversation? How many other moments like that had I missed or failed to grasp?We are so conditioned to think that our lives revolve around great moments. But great moments often catch us unawares. When that woman hugged me and said that I had brought her a moment of joy, it was possible to believe that I had been placed on earth for the sole purpose of providing her with that last ride.I do not think that I have ever done anything in my life that was any more important. 































 







Copyright © 2023 Kent Nerburn | Powered by kincaid-burrows
 










































"
https://news.ycombinator.com/rss,Intel Core i9-13900T CPU benchmarks show faster than 12900K 125W performance,https://wccftech.com/intel-core-i9-13900t-cpu-benchmarks-show-faster-than-12900k-125w-performance-at-35w/,Comments,"

HardwareReport
Intel Core i9-13900T CPU Benchmarks Show Faster Than 12900K 125W Performance at 35W

Hassan Mujtaba •
Jan 14, 2023 02:44 PM EST

•
Copy Shortlink
























Intel recently introduced brand new 13th Gen T-series chips which feature the Core i9-13900T that operates at a 35W TDP. The new chip has been benchmarked within Geekbench 5 and showcases impressive performance given its limited power budget.
Intel's 13th Gen Core i9-13900T 35W CPU Beats The 125W Core i9-12900K In Geekbench 5 Benchmark
Starting with the specifications, the Intel Core i9-13900T is a variation of the Core i9-13900 series that comes with a limited TDP design. While the standard chips boast 125W TDP in the unlocked and 65W TDP on the Non-K SKUs, the T-series chip is limited to a 35W TDP.  The Unlocked CPU is rated at up to 253W, the Non-K is rated at up to 219W while the T-series chip is rated at up to 106 Watts which is less than half the power budget of its higher-end siblings.
Related StoryHassan MujtabaIntel Core i9-13900KS, World’s First 6 GHz CPU, Now Available For $699 USThe Intel Core i9-13900T retains the same core configuration with 24 cores that are made up of 8 P-Cores and 16 E-Cores with 32 threads, a base clock of 1.10 GHz, a boost of up to 5.30 GHz & 68 MB of cache (L2+L3). The CPU also comes at a slightly lower price point of $549.00 US. Now the CPU is tested within the Geekbench 5 benchmark using an ASUS TUF Gaming B660M-PLUS WIFI board and coupled with 64 GB of DDR5 memory.

The CPU scored 2178 points in the single-core and 17339 points in the multi-core tests. We used the Intel Core i9-12900K for comparison which scores 1901 points in single-core and 17272 points in multi-core tests. This puts the Intel Core i9-13900T up to 15% faster in single-core and slightly faster in multi-threaded tests which is very impressive considering the Core i9-12900K also has a higher 125W base TDP (3.58x higher) and a peak TDP rating of 241W (2.27x higher).

Intel Core i9-13900KS Single-Thread CPU Benchmark (Geekbench 5)

Single-Core


050010001500200025003000




050010001500200025003000





Core i9-13900KS

2.3k


Core i9-13900K

2.2k


Ryzen 9 7900X

2.2k


Ryzen 9 7950X

2.2k


Ryzen 7 7700X

2.2k


Core i9-13900T

2.2k


Ryzen 5 7600X

2.2k


Ryzen 9 7900

2.1k


Core i9-13900

2.1k


Ryzen 7 7700

2.1k


Core i9-12900KS

2.1k


Core i9-13900HX

2k


Ryzen 5 7600

2k


Core i7-13700K

2k


Core i5-13600K

1.9k


Core i9-12900K

1.9k


Core i7-12700K

1.9k


M2 Max

1.9k


M1 Max

1.8k


Core i5-12600K

1.7k


Ryzen 9 5950X

1.7k


Ryzen 7 5800X

1.7k


Ryzen 9 5900X

1.7k


Ryzen 5 5600X

1.6k







Intel Core i9-13900KS Multi-Thread CPU Benchmark (Geekbench 5)

Multi-Core


050001000015000200002500030000




050001000015000200002500030000





Core i9-13900KS

26.8k


Core i9-13900K

24.3k


Ryzen 9 7950X

24.4k


Core i9-13900HX

20.9k


Core i9-13900

20.1k


Core i7-13700K

19.8k


Ryzen 9 7900X

19.3k


Core i9-12900KS

19k


Ryzen 9 7900

18.6k


Core i9-13900T

17.3k


Core i9-12900K

17.3k


Ryzen 9 5950X

16.5k


Core i5-13600K

16.1k


M2 Max

14.6k


Core i7-12700K

14.1k


Ryzen 7 7700X

14.1k


Ryzen 9 5900X

14k


Ryzen 7 7700

12.7k


M1 Max

12.3k


Core i5-12600K

11.6k


Ryzen 5 7600X

11.4k


Ryzen 5 7600

11.3k


Ryzen 7 5800X

10.3k


Ryzen 5 5600X

8.2k






This goes off to show the immense efficiency that Intel's 10nm ESF process node and the new hybrid architecture packs and we will also get to see some similar results with the mobility lineup, especially the 13th Gen HX parts which are going to ship in enthusiast-grade gaming laptops in the coming months. AMD also introduced its brand new 65W Ryzen 7000 Non-X CPUs which have been showcasing some impressive efficiency feats on their own with the Zen 4 core architecture.
News Source: Benchleaks
				
				Share this story
				 Facebook
 Twitter






Deal of the Day











Further Reading




 AMD Ryzen 9 7950X3D CPU Shown To Beat Intel Core i9-13900K In Games With Up To 24% Lead


 Intel Core i9-13980HX CPU Powered MSI Raider GE78HX Laptop Matches High-End Desktop CPUs In Performance


 Intel Core i9-13980HX Flagship Raptor Lake-HX CPU Spotted In ASUS’s Next-Gen ROG STRIX Laptop


 It’s Over 9000! Intel Core i9-13900KS Becomes The First CPU To Achieve 9 GHz Frequency World Record













Comments




Please enable JavaScript to view the comments.










Trending Stories


NASA Captures Star Eaten By Black Hole 300 Million Light Years Away



				91 Active Readers



Intel Core i9-13900T CPU Benchmarks Show Faster Than 12900K 125W Performance at 35W



				91 Active Readers



SpaceX’s Rockets Split Up In Mid Air For Rare & Stunning Views At 5,000 Km/h+



				41 Active Readers



PlayStation 5 Vertical Orientation Issue Clarified by Technician; Issue Happens on “Unopened” Consoles



				38 Active Readers



Apple M2 Max vs. M1 Max – Manufacturing Process, Specifications, And Upgrades Differences That You Should Know



				24 Active Readers








Popular Discussions


AMD Radeon RX 7900 XTX Failure Rates Reportedly At 11%, RMA’s Piling Up But Users Not Receiving Cards



				3116 Comments



AMD Radeon RX 6000 GPUs Mysteriously Start Dying, German Repair Shop Receives 48 Cards With Cracked Chips



				3020 Comments



Intel Lunar Lake To Feature A Brand New CPU Architecture Built From The Ground-Up, Perf/Watt Focused at Mobile



				2757 Comments



AMD To Give The Love of 3D V-Cache This Valentines With Its Ryzen 7000 X3D CPUs Launch



				2021 Comments



Intel Arc A770 Performs Above AMD & NVIDIA In DirectStorage 1.1 Performance Benchmark



				1797 Comments








	 







"
https://news.ycombinator.com/rss,Conditional CSS,https://ishadeed.com/article/conditional-css/,Comments,"







    Conditional CSS -
    Ahmad Shadeed
  























🎉I published a book about debugging
      CSS.
      Buy now



























Ahmad Shadeed






Home


Articles


Snippets


Journal


About


Hire me



















@shadeed9




Conditional CSS
09 Jan 2023

			

			
			Reading time: ~ 20 mins
		



I like to think of CSS as a conditional design language. Over the years, CSS was known as a way to style web pages. Now, however, CSS has evolved a lot to the point you can see conditional rules. The interesting bit is that those CSS rules aren’t direct (i.e: there is still no if/else in CSS), but the way features in CSS work is conditional.
Design tools like Figma, Sketch, and Adobe XD made a huge improvement for us designers, but they still lack a lot of the flexibility that CSS has.
In this article, I will go over a few CSS features that we use every day, and show you how conditional they are. In addition to that, I will compare a few examples where CSS is much more powerful than design tools.
What is conditional CSS?
In simple words, it’s about design that has certain conditions. When one or more conditions are met, the design is subject to change due to that.
For example, adding a new section to a design must push the other elements underneath it. In the following figure, we have a stack of items on the left. When adding a new one, the other items below it must move down.




Logically, that sounds expected and normal. In design tools, we got this a few years ago. In Figma, we have “Auto Layout” features that do the above. On the web, we have had that from day 1, even without CSS at all.
Conditional CSS
You might be thinking about what the heck conditional CSS is. Is that even a thing? No, there hasn’t been a direct “if” statement in CSS.
The main thing to distinguish is that some CSS properties work in specific conditions or scenarios. For example, when using the CSS :empty selector to check if an element is empty or not, it’s a conditional pseudo selector.
.alert p:empty {
  display: none;
}

If I want to explain the above to my 2 years old daughter, I will do it like this:

If there is nothing here, it will disappear.

Did you notice the if statement here? This is conditional design indirectly. In the following section, I’m going to explore a few CSS features which work similarly to an if/else statement.
The goal? To have a stronger idea and expectation about the CSS you wrote. I mean, you will be able to spot conditional CSS by just looking at the CSS for a component, a section, or a page.
CSS versus Figma
Why Figma? Well, I consider it as the standard for UX design these days, I thought it’s a good idea to do my comparison based on it. I want to share a simple example. There is list of tags that are displayed horizontally.




When you think deeply about it, you will spot some major differences. For example, the CSS version:

Can wrap into a new lines if there is no enough space.
Works with both LTR and RTL directions.
The gap will be used for rows when the items wrap.

Figma doesn’t have any of the above.
In CSS, there are three conditional rules happening:

If flex-wrap is set to wrap, then the items can wrap when there is no available space.
When the items wrap into a new line, the gap will work for the horizontal and vertical spaces.
If the page direction is RTL (right-to-left), the items will switch their order (e.g: design will be the first one from the right).

This is just one example, and I can write a book like that. Let’s explore a few cases where CSS can be conditional.
Conditional CSS examples
Media query
We can’t talk about conditional CSS without mentioning CSS media queries. The CSS spec is named CSS Conditional Rules Module. To be honest, this is the first time that I learn about that title.
When I did my research about who asks or mentions “Conditional CSS”, I found more than one time that media queries are the closest thing to an “if” statement in CSS.
.section {
  display: flex;
  flex-direction: column;
}

@media (min-width: 700px) {
  .section {
    flex-direction: row;
  }
}


If the viewport width is 700px or larger, change the flex-direction of .section to column. That’s explicit if statement, isn’t it?

The same thing can apply to media queries like @media (hover: hover). In the following CSS, the hover style will be applied only if the user is using a mouse or a trackpad.
@media (hover: hover) {
  .card:hover {
    /* Add hover styles.. */
  }
}

Size container query
With container queries, we can check if the parent of a component has a specific size and style the child component accordingly.




.card-wrapper {
  container-type: inline-size;
}

@container (min-width: 400px) {
  .card {
    display: flex;
    align-items: center;
  }
}

I have written about container queries multiple times, and have a place where I share demos about it.
Style container query
At the time of writing this article, this is behind a flag in Chrome Canary and is intended to ship in Chrome stable.
With a style query, we can check if a component is placed within a wrapper that has a specific CSS variable and if yes, we style it accordingly.
In the following figure, we have an article body that is coming from a CMS. We have a default style for the figure and another style that looks featured.




To implement that with style queries, we can style the default one, and then check if the figure has a special CSS variable to allow the custom styling.
figure {
  container-name: figure;
  --featured: true;
}

/* Featured figure style. */
@container figure style(--featured: true) {
  img {
    /* Custom styling */
  }

  figcaption {
    /* Custom styling */
  }
}

And if --featured: true isn’t there, we will default to the base figure design. We can use the not keyword to check when the figure doesn’t have that CSS variable.
/* Default figure style. */
@container figure not style(--featured: true) {
  figcaption {
    /* Custom styling */
  }
}

That’s an if statement, but it’s implicit.
Another example is having a component styled differently based on its parent. Consider the following figure:




The card style can switch to dark if it’s placed within a container that has the --theme: dark CSS variable.
.special-wrapper {
  --theme: dark;
  container-name: stats;
}

@container stats style(--theme: dark) {
  .stat {
    /* Add the dark styles. */
  }
}

If we read the above, it feels like:

If the container stats have the variable --theme: dark, add the following CSS.

CSS @supports
The @supports feature lets us test if a certain CSS feature is supported in a browser or not.
@supports (aspect-ratio: 1) {
  .card-thumb {
    aspect-ratio: 1;
  }
}

We can also test for the support of a selector, like :has.
@supports selector(:has(p)) {
  .card-thumb {
    aspect-ratio: 1;
  }
}

Flexbox wrapping
According to MDN:

The flex-wrap CSS property sets whether flex items are forced onto one line or can wrap onto multiple lines. If wrapping is allowed, it sets the direction in that lines are stacked.

The flex-wrap property allows flex items to wrap into a new line in case there is not enough space available.
Consider the following example. We have a card that contains a title and a link. When the space is small, each child item should wrap into a new line.
.card {
  display: flex;
  flex-wrap: wrap;
  align-items: center;
}

.card__title {
  margin-right: 12px;
}





That sounds like a conditional thing to me. If no available space, wrap into a new line(s).




When each flex item wraps into a line, how do I manage the spacing between the flex items, you asked? Currently, there is a margin-right on the heading, and when they are stacked, that should be replaced by margin-bottom. The problem is we don’t know when the items will wrap because it depends on the content.
The good thing is that the spacing can be conditional with the gap property. When they are in the same line, the spacing is horizontal, and with multiple, the spacing is vertical.
.card {
  display: flex;
  flex-wrap: wrap;
  align-items: center;
  gap: 1rem;
}

This is one of my favorite flexbox features. Here is a visual of how gap switches the spacing.




By the way, I consider flex-wrap as defensive CSS. I almost add it to any flex container to avoid any unexpected issues.
The flex property
Even more, the flex property can work conditionally, too. Considering the following example. I added flex: 1 to the card title to make it fill the available space.
.card__title {
  flex-grow: 1;
}





That works fine, but when the width of the card is too small, the card title will wrap into a new line.




Nothing too bad, but can we do better? For example, I want to tell the title: “Hey, if your width is less than X, then wrap into a new line”. We can do that by setting the flex-basis property.
In the following CSS, I set the maximum width of the title to 190px. If it’s less than that, it will wrap into a new line.
.card__title {
  flex-grow: 1;
  flex-basis: 190px;
}





To learn more about the flex property in CSS, I wrote a detailed article on that.
Take things further, and explain about adding flex-grow, string.. etc along the way.
The :has selector
For me, this is the closest thing to an “if” statement in CSS right now. It works in a way that mimics an if/else statement.
Changing a card style
In this example, we need to have two different styles, depending on if the card has an image or not.




If the card has an image:
.card:has(.card__image) {
  display: flex;
  align-items: center;
}

And if it doesn’t have an image:
.card:not(:has(.card__image)) {
  border-top: 3px solid #7c93e9;
}

That’s an if statement, and I strongly think so. Sorry, I got too excited.
Hiding or showing form items conditionally
In forms, it’s common to have an input field or a group of inputs hidden by default, and it will be shown once the user activates an option from a <select> menu.




With CSS :has, we can check if the other option is selected and if yes, show the input field.
.other-field {
  display: none;
}

form:has(option[value=""other""]:checked) .other-field {
  display: block;
}

Alerts
When there is an alert message on a page, like for example a major warning of something wrong in the system, we might need to make it even more obvious.




In this example, we have an alert within the page, and with CSS :has, we can check if the dashboard has an alert, and if yes, style accordingly.
.main:has(.alert) .header {
  border-top: 2px solid red;
  background-color: #fff4f4;
}

So useful.
Change grid columns based on the number of items
Have you ever needed to display and change the width of a column in a grid based on the number of child items?




CSS :has can do that, conditionally.
.wrapper {
  --item-size: 200px;
  display: grid;
  grid-template-columns: repeat(
    auto-fill,
    minmax(var(--item-size), 1fr)
  );
  gap: 1rem;
}

.wrapper:has(.item:nth-last-child(n + 5)) {
  --item-size: 120px;
}

In the example, it says that if the .wrapper has five items, then the --item-size variable will change to 120px.
To learn more about the CSS :has selector, I wrote an article on it with plenty of examples.
CSS grid minmax() function
The way minmax() works in CSS grid is conditional. When we use auto-fit keyword, we’re telling the browser: “if there is an available space, make the grid items fill the space”.




The adjacent sibling combinator
That combinator matches the second element that comes directly after an element.
In the following example, if an <h3> element is followed by a <p>, the <p> will get custom styles.
h3 + p {
  margin-top: 8px;
}





The <p> top margin has been modified conditionally.
The :focus-within pseudo-class
Another interesting feature in CSS is :focus-within. Say that you want to check whether an input is focused, and if yes, add a border to its parent.
Consider the following example:




We have a search component. When the input is focused, the whole wrapper should have an outline. With :focus-within, we can check if the input is focused, and style accordingly.
.hero-form:focus-within {
  box-shadow: 0 0 0 5px rgb(28 147 218 / 35%);
}





The :not selector
This pseudo-class excludes elements that don’t match a certain selector. For example, it can be useful to check if an item is the last one, and if yes, remove the border.
.item:not(:last-child) {
  border-bottom: 1px solid lightgrey;
}





Conditional border-radius
A while ago, I wrote about how I spotted an interesting conditional approach to add border-radius for a card on the Facebook website.




The idea is that when the card is equal to or larger than the viewport, the radius should be 8px, if not, then it’s 0px.
.card {
  border-radius: max(
    0px,
    min(8px, calc((100vw - 4px - 100%) * 9999))
  );
}

You can read the article here.
Conditional line separator
Another interesting use case where CSS works conditionally is having a line separator that switches its direction and size based on whether the items are wrapped or not.
In the following figure, notice the line separator between the two sections.




I want that line to switch horizontally when the flex items are stacked. By using flex-wrap and clamp comparison, we can achieve that.
.section {
  --: 400px;
  display: flex;
  flex-wrap: wrap;
  gap: 1rem;
}

.section:before {
  content: """";
  border: 2px solid lightgrey;
  width: clamp(0px, (var(--breakpoint) - 100%) * 999, 100%);
}

This has been written on my blog, and the clamp() solution is a suggestion by Temani Afif.




Intrinsic sizing: fit-content
The fit-content keyword is a combination of min-content and max-content. I know, it’s not clear. Let’s take a look at the following flowchart.




If we have an element with width: fit-content, it will work conditionally as per the flowchart above.
h2 {
  width: fit-content;
}

Here is a video of what’s happening on resize:



I wrote about intrinsic sizing on my blog if you’re interested.
Comparison functions
CSS comparison functions are min(), max(), and clamp(). One particular example that feels conditional for me is something that I stumbled upon in a recent article I wrote.




The idea is I have two different containers, one of the article header (title and date), and a container for the main content plus the aside.
I want to align the edge of the header content with the body content.
On mobile, I want the padding from the left to be 1rem, but on larger viewports, it will be dynamic as per the viewport width.




To do that, I can use the max() function to choose one of the two values (1rem or dynamic value) conditionally.
.prose {
  padding-left: max(1rem, (100vw - var(--wrapper-width)) / 2);
}

You can learn more about this technique in my article Inside the mind of a frontend developer: Article layout.
Pseudo-classes
There are a lot of pseudo-classes in CSS, but the ones that came to mind are :focused and :checked.
input:checked + label {
  /* Custom styling */
}

input:focus {
  outline: 2px solid #222;
}

If the input is checked, add those styles to the <label>. If the input is focused..and so on.
But.. CSS isn’t a programming language!
I know, thanks for letting me know. This is argument that I hear a lot. I personally don’t have a strong opinion on that, but CSS is conditional in many ways.
In fact, most of the examples above can’t be implemented in Javascript without using a conditional statement.
Conclusion
I enjoyed writing this article because it reminded me of why I love using CSS. To me, CSS is like a superpower because it allows me to make so many design decisions through its conditional features. Working with design tools can sometimes feel limiting because I feel like I’m constrained within certain walls. I think that the ability to create conditional rules with CSS is what sets it apart and makes it powerful for web design.
That doesn’t mean that I design in the browser. I consider design tools as an open canvas to try and experiment with design ideas, and building polished UI products.
I like to use the browser to tweak designs, instead of designing them completely.
And you, what do you think? I would love to hear your thoughts and ideas.
Thank you for reading.



			Do you like my content? You can support and buy me a coffee. Thank you so much!
		



More articles


Previous Article
 2022 Year In Review





Subscribe to my RSS feed






Subscribe to the newsletter
Get the latest CSS articles published by Ahmad Shadeed, a UX Designer and Front End Developer.


Email Address



First Name 




erorr
success














Ahmad Shadeed
UI, UX Designer & Front-End Developer. You can hire me. I Write about web accessibility on @weba11ymatters and share articles on my blog.







      The online studio of Ahmad Shadeed.
      © 2012–2023 Copyright Ahmad Shadeed. All rights
        reserved.


Find me online


Instagram


Twitter


Behance


CodePen


Github

RSS Feed












"
https://news.ycombinator.com/rss,Will Floating Point 8 Solve AI/ML Overhead?,https://semiengineering.com/will-floating-point-8-solve-ai-ml-overhead/,Comments,"












Will Floating Point 8 Solve AI/ML Overhead?

























































































 
 
 












 










 


Search for:



 Subscribe

中文 English 


















Home
Systems & Design
Low Power - High Performance
Manufacturing, Packaging & Materials
Test, Measurement & Analytics
Auto, Security & Pervasive Computing




Special Reports

Business & Startups
Jobs
Knowledge Center
Technical Papers 

Home';
				AI/ML/DLArchitecturesAutomotiveCommunication/Data MovementDesign & VerificationLithographyManufacturingMaterialsMemoryOptoelectronics / PhotonicsPackagingPower & PerformanceQuantumSecurityTest & AnalyticsTransistorsZ-End Applications


Events & Webinars 

Events
Webinars



Videos & Research

Videos
Industry Research



Newsletters





MENU 

Home
Special Reports
Systems & Design
Low Power-High Performance
Manufacturing, Packaging & Materials
Test, Measurement & Analytics
Auto, Security & Pervasive Computing
Knowledge Center
Videos
Startup Corner
Business & Startups
Jobs
Technical Papers 
Events
Webinars
Industry Research
Special Reports







































Home >
                                                                    Low Power-High Performance >
                                                                Will Floating Point 8 Solve AI/ML Overhead?                                                    

















Low Power-High Performance

Will Floating Point 8 Solve AI/ML Overhead?









Less precision equals lower power, but standards are required to make this work.





								January 12th, 2023 - 

								By: Karen Heyman








While the media buzzes about the Turing Test-busting results of ChatGPT, engineers are focused on the hardware challenges of running large language models and other deep learning networks. High on the ML punch list is how to run models more efficiently using less power, especially in critical applications like self-driving vehicles where latency becomes a matter of life or death.
AI already has led to a rethinking of computer architectures, in which the conventional von Neumann structure is replaced by near-compute and at-memory floorplans. But novel layouts aren’t enough to achieve the power reductions and speed increases required for deep learning networks. The industry also is updating the standards for floating-point (FP) arithmetic.
“There is a great deal of research and study on new data types in AI, as it is an area of rapid innovation,” said David Bell, product marketing director, Tensilica IP at Cadence. “Eight-bit floating-point (FP8) data types are being explored as a means to minimize hardware — both compute resources and memory — while preserving accuracy for network models as their complexities grow.”
As part of that effort, researchers at Arm, Intel, and Nvidia published a white paper proposing “FP8 Formats for Deep Learning.” [1]
“Bit precision has been a very active topic of debate in machine learning for several years,” said Steve Roddy, chief marketing officer at Quadric. “Six or eight years ago when models began to explode in size (parameter count), the sheer volume of shuffling weight data into and out of training compute (either CPU or GPU) became the performance limiting bottleneck in large training runs. Faced with a choice of ever more expensive memory interfaces, such as HBM, or cutting bit precision in training, a number of companies experimented successfully with lower-precision floats. Now that networks have continued to grow exponentially in size, the exploration of FP8 is the next logical step in reducing training bandwidth demands.”
How we got here
Floating-point arithmetic is a kind of scientific notation, which condenses the number of digits needed to represent a number. This trick is pulled off by an arithmetic expression first codified by IEEE working group 754 in 1986, when floating-point operations generally were performed on a co-processor.
IEEE 754 describes how the radix point (more commonly known in English as the “decimal” point) doesn’t have a fixed position, but rather “floats” where needed in the expression. It allows numbers with extremely long streams of digits (whether originally to the left or right of a fixed point) to fit into the limited bit-space of computers. It works in either base 10 or base 2, and it’s essential for computing, given that binary numbers extend to many more digits than decimal numbers (100 = 1100100).
 

Fig. 1: 12.345 as a base-10 floating-point number. Source: Wikipedia
 
While this is both an elegant solution and the bane of computer science students worldwide, its terms are key to understanding how precision is achieved in AI. The statement has three parts:

A sign bit, which determines whether the number is positive (0) or negative (1);
An exponent, which determines the position of the radix point, and
A mantissa, or significand, which represents the most significant digits of the number.


Fig. 2: IEEE 754 floating-point scheme. Source: WikiHow
As shown in figure 2, while the exponent gains 3 bits in a 64-bit representation, the mantissa jumps from 32 bits to 52 bits. Its length is key to precision.
IEEE 754, which defines FP32 bit and FP64, was designed for scientific computing, in which precision was the ultimate consideration. Currently, IEEE working group P3109 is developing a new standard for machine learning, aligned with the current (2019) version of 754. P3109 aims to create a floating-point 8 standard.
Precision tradeoffs
Machine learning often needs less precision than a 32-bit scheme. The white paper proposes two different flavors of FP8: E4M3 (4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa).
“Neural networks are a bit strange in that they are actually remarkably tolerant to relatively low precision,” said Richard Grisenthwaite, executive vice president and chief architect at Arm. “In our paper, we showed you don’t need 32 bits of mantissa for precision. You can use only two or three bits, and four or five bits of exponent will give you sufficient dynamic range. You really don’t need the massive precision that was defined in 754, which was designed for finite element analysis and other highly precise arithmetic tasks.”
Consider a real-world example: A weather forecast needs the extreme ranges of 754, but a self-driving car doesn’t need the fine-grained recognition of image search. The salient point is not whether it’s a boy or girl in the middle of the road. It’s just that the vehicle must immediately stop, with no time to waste on calculating additional details. So it’s fine to use a floating point with a smaller exponent and much smaller mantissa, especially for edge devices, which need to optimize energy usage.
“Energy is a fundamental quantity and no one’s going to make it go away as an issue,” said Martin Snelgrove, CTO of Untether AI. “And it’s also not a narrow one. Worrying about energy means you can’t afford to be sloppy in your software or your arithmetic. If doing a 32-bit floating point makes everything easier, but massively more power consuming, you just can’t do it. Throwing an extra 1,000 layers at something makes it slightly more accurate, but the value for power isn’t there. There’s an overall discipline about energy — the physics says you’re going to pay attention to this, whether you like it or not.”
In fact, to save energy and performance overhead, many deep learning networks had already shifted to an IEEE-approved 16-bit floating point and other formats, including mantissa-less integers. [2]
“Because compute energy and storage is at a premium in devices, nearly all high-performance device/edge deployments of ML always have been in INT8,” Quadric’s Roddy said. “Nearly all NPUs and accelerators are INT-8 optimized. An FP32 multiply-accumulate calculation takes nearly 10X the energy of an INT8 MAC, so the rationale is obvious.”
Why FP8 is necessary
The problem starts with the basic design of a deep learning network. In the early days of AI, there were simple, one-layer models that only operated in a feedforward manner. In 1986, David Rumelart, Geoffrey Hinton, and Ronald Williams published a breakthrough paper on back-propagation [3] that kicked off the modern era of AI. As their abstract describes, “The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units, which are not part of the input or output, come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units.”
In other words, they created a system in which better results could be achieved by adding more and more layers into a model, which would be improved by incorporating “learned” adjustments. Decades later, their ideas so vastly improved machine translation and transcription that college professors remain unsure whether undergraduates’ essays have been written by bots.
But additional layers require additional processing power. “Larger networks with more and more layers were found to be progressively more successful at neural networks tasks, but in certain applications this success came with an ultimately unmanageable increase in memory footprint, power consumption, and compute resources. It became imperative to reduce the size of the data elements (activations, weights, gradients) from 32 bits, and so the industry started using 16-bit formats, such as Bfloat16 and IEEE FP16,” according to the paper jointly written by Arm/Intel/Nvidia.
“The tradeoff fundamentally is with an 8-bit floating-point number compared to a 32-bit one,” said Grisenthwaite. “I can have four times the number of weights and activations in the same amount of memory, and I can get far more computational throughput as well. All of that means I can get much higher performance. I can make the models more involved. I can have more weights and activations at each of the layers. And that’s proved to be more useful than each of the individual points being hyper-accurate.”
Behind these issues are the two basic functions in machine learning, training and inference. Training is the first step in which, for example, the AI learns to classify features in an image by reviewing a dataset. With inference, the AI is given novel images outside of the training set and asked to classify them. If all goes as it should, the AI should distinguish that tails and wings are not human features, and at finer levels, that airplanes do not have feathers and a tube with a tail and wings is not a bird.
“If you’re doing training or inference, the math is identical,” said Ron Lowman, strategic marketing manager for IoT at Synopsys. “The difference is you do training over a known data set thousands of times, maybe even millions of times, to train what the results will be. Once that’s done, then you take an unknown picture and it will tell you what it should be. From a math perspective, a hardware perspective, that’s the big difference. So when you do training, you want to do that in parallel, rather than doing it in a single hardware implementation, because the time it takes to do training is very costly. It could take weeks or months, or even years in some cases, and that just costs too much.”
In industry, training and inference have become separate specialties, each with its own dedicated teams.
“Most companies that are deploying AI have a team of data scientists that create neural network architectures and train the networks using their datasets,” said Bob Beachler, vice president of product at Untether AI. “Most of the autonomous vehicle companies have their own data sets, and they use that as a differentiating factor. They train using their data sets on these novel network architectures that they come up with, which they feel gives them better accuracy. Then that gets taken to a different team, which does the actual implementation in the car. That is the inference portion of it.”
Training requires a wide dynamic range for the continual adjustment of coefficients that is the hallmark of backpropagation. The inference phase is computing on the inputs, rather than learning, so it needs much less dynamic range. “Once you’ve trained the network, you’re not tweaking the coefficients, and the dynamic range required is dramatically reduced,” explained Beachler.
For inference, continuing operations in FP32 or FP16 is just unnecessary overhead, so there’s a quantization step to shift the network down to FP8 or Integer 8 (Int8), which has become something of a de facto standard for inference, driven largely by TensorFlow.
“The idea of quantization is you’re taking all the floating point 32 bits of your model and you’re essentially cramming it into an eight-bit format,” said Gordon Cooper, product manager for Synopsys’ Vision and AI Processor IP. “We’ve done accuracy tests and for almost every neural network-based object detection. We can go from 32-bit floating point to Integer 8 with less than 1% accuracy loss.”
For quality/assurance, there’s often post-quantization retraining to see how converting the floating-point value has affected the network, which could iterate through several passes.
This is why training and inference can be performed using different hardware. “For example, a common pattern we’ve seen is accelerators using NVIDIA GPUs, which then end up running the inference on general purpose CPUs,” said Grisenthwaite.
The other approach is chips purpose-built for inference.
“We’re an inference accelerator. We don’t do training at all,” says Untether AI’s Beachler. “We place the entire neural network on our chip, every layer and every node, feed data at high bandwidth into our chip, resulting in each and every layer of the network computed inside our chip. It’s massively parallelized multiprocessing. Our chip has 511 processors, each of them with single instruction multiple data (SIMD) processing. The processing elements are essentially multiply/accumulate functions, directly attached to memory. We call this the Energy Centric AI computing architecture. This Energy Centric AI Computing architecture results in a very short distance for the coefficients of a matrix vector to travel, and the activations come in through each processing element in a row-based approach. So the activation comes in, we load the coefficients, do the matrix mathematics, do the multiply/accumulate, store the value, move the activation to the next row, and move on. Short distances of data movement equates to low power consumption.”
In broad outline, AI development started with CPUs, often with FP co-processors, then moved to GPUs, and now is splitting into a two-step process of GPUs (although some still use CPUs) for training and CPUs or dedicated chips for inference.
The creators of general-purpose CPU architectures and dedicated inference solutions may disagree on which approach will dominate. But they all agree that the key to a successful handoff between training and inference is a floating-point standard that minimizes the performance overhead and risk of errors during quantization and transferring operations between chips. Several companies, including NVIDIA, Intel, and Untether, have brought out FP8-based chips.
“It’s an interesting paper,” said Cooper. “8-bit floating point, or FP8, is more important on the training side. But the benefits they’re talking about with FP8 on the inference side is that you possibly can skip the quantization. And you get to match the format of what you’ve done between training and inference.”
Nevertheless, as always, there are still many challenges still to consider.
“The cost is one of model conversion — FP32 trained model converted to INT8. And that conversion cost is significant and labor intensive,” said Roddy. “But if FP8 becomes real, and if the popular training tools begin to develop ML models with FP8 as the native format, it could be a huge boon to embedded inference deployments. Eight-bit weights take the same storage space, whether they are INT8 or FP8. The energy cost of moving 8 bits (DDR to NPU, etc.) is the same, regardless of format. And a Float8 multiply-accumulate is not significantly more power consumptive than an INT8 MAC. FP8 would rapidly be adopted across the silicon landscape.  But the key is not whether processor licensors would rapidly adopt FP8. It’s whether the mathematicians building training tools can and will make the switch.”
Conclusion
As the quest for lower power continues, there’s debate about whether there might even be a FP4 standard, in which only 4 bits carry a sign, an exponent, and mantissa. People who follow a strict neuromorphic interpretation have even discussed binary neural networks, in which the input functions like an axon spike, just 0 or 1.
“Our sparsity level is going to go up,” said Untether’s Snelgrove. “There are hundreds of papers a day on new neural net techniques. Any one of them could completely revolutionize the field. If you talk to me in a year, all of these words could mean different things.”
At least at the moment, it’s hard to imagine that lower FPs or integer schemes could contain enough information for practical purposes. Right now, various flavors of FP8 are undergoing the slow grind towards standardization. For example, Graphcore, AMD, and Qualcomm have also brought a detailed FP8 proposal to the IEEE. [4]
“The advent of 8-bit floating point offers tremendous performance and efficiency benefits for AI compute,” said Simon Knowles, CTO and co-founder of Graphcore. “It is also an opportunity for the industry to settle on a single, open standard, rather than ushering in a confusing mix of competing formats.”
Indeed, everyone is optimistic there will be a standard — eventually. “We’re involved in IEEE P3109, as are many, many companies in this industry,” said Arm’s Grisenthwaite. “The committee has looked at all sorts of different formats. There are some really interesting ones out there. Some of them will stand the test of time, and some of them will fall by the wayside. We all want to make sure we’ve got complete compatibility and don’t just say, ‘Well, we’ve got six different competing formats and it’s all a mess, but we’ll call it a standard.”
References 

Micikevicius, P., et al. FP8 Formats for Deep Learning. Last revised Sep 29 2022 arXiv:2209.05433v2. https://doi.org/10.48550/arXiv.2209.05433
Sapunov, G. FP64, FP32, FP16, BFLOAT16, TF32, and other members of the ZOO. Medium. May 16, 2020. https://moocaholic.medium.com/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407
Rumelhart, D., Hinton, G. & Williams, R. Learning representations by back-propagating errors. Nature 323, 533–536 (1986). https://doi.org/10.1038/323533a0
Noune, B. 8-bit Numerical Formats for Deep Neural Networks. Submitted June 6 2022 arXiv:2206.02915 https://doi.org/10.48550/arXiv.2206.02915

Additional Reading:
How to convert a number from decimal to IEEE 754 Floating Point Representation.
Number Representation and Computer Arithmetic
https://web.ece.ucsb.edu/~parhami/pubs_folder/parh02-arith-encycl-infosys.pdf
Computer Representation of Numbers and Computer Arithmetic
https://people.cs.vt.edu/~asandu/Courses/CS3414/comp_arithm.pdf











 Tags: 8-bit floating point AI AI models AI/ML/DL AMD ARM base 10 base 2 BFLOAT16 Cadence Cadence Design Systems ChatGPT deep learning E4M3 E5M2 edge inference floating point FP16 FP32 FP64 FP8 Graphcore IEEE 754 inference INT8 Integer 8 Intel machine learning mantissa ML training neural network Nvidia P3109 Quadric Quadric.io Qualcomm Synopsys training Untether AI





Karen Heyman   (all posts)


							Karen Heyman is a technology editor at Semiconductor Engineering.
						





Leave a Reply Cancel replyComment * Name*(Note: This name will be displayed publicly)
Email*(This will not be displayed publicly) 
 

Δ 




 Knowledge Centers Blogs 
Spiking Neural Network (SNN)

Published on June 23, 2021



Recurrent Neural Network (RNN)

Published on June 25, 2019



Edge Computing

Published on April 4, 2019



Neural Networks

Published on July 25, 2017



Architectures

Published on 



Machine Learning (ML)

Published on May 10, 2017



Convolutional Neural Network (CNN)

Published on 



Artificial Intelligence (AI)

Published on 




Technical Papers
Hardware Trojan Detection Case Study Based on 4 Different ICs Manufactured in Progressively Smaller CMOS Process Technologies January 11, 2023 by Technical Paper LinkQuantum Computing Architecture Enabling  Communication Between Superconducting Quantum Processors (MIT) January 11, 2023 by Technical Paper LinkArbitrary Precision DNN Accelerator Controlled by a RISC-V CPU (Ecole Polytechnique Montreal, IBM, Mila, CMC) January 10, 2023 by Technical Paper LinkTechnique For Printing Electronic Circuits Onto Curved & Corrugated Surfaces Using Metal Nanowires (NC State) January 10, 2023 by Technical Paper LinkFPGA-Based Prototyping Framework For Processing In DRAM (ETH Zurich & TOBB Univ.) January 10, 2023 by Technical Paper Link 

  Trending Articles

RISC-V Pushes Into The Mainstream

Open-source processor cores are beginning to show up in heterogeneous SoCs and packages.


by Marie C. Baca and Ed Sperling



How Secure Are RISC-V Chips?

Open source by itself doesn’t guarantee security. It still comes down to the fundamentals of design.


by Jeff Goldman



Will Floating Point 8 Solve AI/ML Overhead?

Less precision equals lower power, but standards are required to make this work.


by Karen Heyman



RISC-V decoupled Vector Processing Unit (VPU) For HPC



by Technical Paper Link



Startup Funding: December 2022

Wafer manufacturing and GPUs draw investment; 106 companies raise $2.8B.


by Jesse Allen






Knowledge Centers Entities, people and technologies explored
Learn More



Related Articles

Foundational Changes In Chip Architectures

New memory approaches and challenges in scaling CMOS point to radical changes — and potentially huge improvements — in semiconductor designs. 


by Brian Bailey



How Memory Design Optimizes System Performance

Changes are steady in the memory hierarchy, but how and where that memory is accessed is having a big impact.


by John Koon



Startup Funding: October 2022

113 startups raise $3.5B; batteries, AI, and new architectures top the list.


by Jesse Allen



Startup Funding: November 2022

127 startups raise $2.6B; data center connectivity, quantum computing, and batteries draw big funding.


by Jesse Allen



IC Stresses Affect Reliability At Advanced Nodes

Thermal mismatch in heterogeneous designs, different use cases, can impact everything from accelerated aging to warpage and system failures.


by Ann Mutschler



Will Floating Point 8 Solve AI/ML Overhead?

Less precision equals lower power, but standards are required to make this work.


by Karen Heyman



3D-IC Reliability Degrades With Increasing Temperature

Electromigration and other aging factors become more complicated along the z axis.


by Ann Mutschler



On-Chip Power Distribution Modeling Becomes Essential Below 7nm

Why and when it’s needed, and what tools and technologies are required.


by Ann Mutschler











Sponsors






























Advertise with us





Advertise with us





Advertise with us





Newsletter Signup



Popular Tags2.5D
5G
7nm
advanced packaging
AI
ANSYS
Apple
Applied Materials
ARM
Atrenta
automotive
business
Cadence
EDA
eSilicon
EUV
finFETs
GlobalFoundries
Google
IBM
imec
Intel
IoT
IP
Lam Research
machine learning
memory
Mentor
Mentor Graphics
MIT
Moore's Law
Nvidia
NXP
Qualcomm
Rambus
Samsung
security
SEMI
Siemens
Siemens EDA
software
Sonics
Synopsys
TSMC
verification
Recent CommentsWZIS on Arbitrary Precision DNN Accelerator Controlled by a RISC-V CPU (Ecole Polytechnique Montreal, IBM, Mila, CMC)Rama Chaganti on Growing System Complexity Drives More IP ReuseTL on How Secure Are RISC-V Chips?Frank on The Good And Bad Of Bi-Directional ChargingSandeep Dixit on The Good And Bad Of Bi-Directional ChargingHertz on How Secure Are RISC-V Chips?Andrew on How Software Utilizes CoresAsaf Jivilik on Cybord: Electronic Component TraceabilitySantosh Kurinec on Where All The Semiconductor Investments Are Goingdick freebird on Designing And Securing Chips For Outer SpaceAkshay on Designing And Securing Chips For Outer SpaceRaj on Is UCIe Really Universal?Andrew TAM on How Software Utilizes CoresRiko R on Designing For Multiple DieDan Ganousis on RISC-V Pushes Into The MainstreamIvan Batinic on IC Stresses Affect Reliability At Advanced NodesGiovanni Lostumbo on A Power-First ApproachMohammed Zakir Hussain on Embracing the Challenges Of Cybersecurity In Automotive ApplicationsLaura Peters on Week In Review: Manufacturing, TestAiv on Week In Review: Manufacturing, TestRoss Youngblood on High Voltage Testing Races AheadMark Olivas on Cybord: Electronic Component TraceabilityKarl Stevens on The Drive Toward Virtual PrototypesRon Lavallee on The Politics Of StandardsDenis McCarthy on Hot Trends In Semiconductor Thermal ManagementTom Smith on Are We Too Hard On Artificial Intelligence For Autonomous Driving?Maya F on Where All The Semiconductor Investments Are GoingSaikatm on Balancing Power And Heat In Advanced Chip DesignsDoug L. on Holistic 3D-IC Interposer Analysis In Product DesignsAndy Deng on Post-Quantum And Pre-Quantum Security Issues GrowJohn Dunn on Post-Quantum And Pre-Quantum Security Issues Growmadmax2069 on Chip Design Shifts As Fundamental Laws Run Out Of SteamMatthew Slyman on Chip Design Shifts As Fundamental Laws Run Out Of SteamDouglas MacIntyre on Chip Design Shifts As Fundamental Laws Run Out Of SteamJose on Universal Verification Methodology Running Out Of SteamZhengji Lu on Moving From AMBA ACE to CHI For CoherencyJohn Bennice on A Power-First Approach[email protected] on Chip Design Shifts As Fundamental Laws Run Out Of SteamMatthew on Chip Design Shifts As Fundamental Laws Run Out Of SteamKarthik Krishnamoorthy on AI-Powered VerificationCPlusPlus4Ever on Chip Design Shifts As Fundamental Laws Run Out Of SteamDouglas on Chip Design Shifts As Fundamental Laws Run Out Of SteamBowie Poag on Chip Design Shifts As Fundamental Laws Run Out Of SteamEugene on Startup Funding: October 2022Wesley Sung on Fan-Out And Packaging ChallengesHong Xiao on Chip Design Shifts As Fundamental Laws Run Out Of SteamRobert Anderson on Chip Design Shifts As Fundamental Laws Run Out Of SteamMike Frank on A Power-First ApproachWilliam Ruby on A Power-First ApproachPeter C Salmon on A Power-First ApproachDr. Dev Gupta on Which Foundry Is In The Lead? It Depends.Steve Hoover on A Power-First ApproachDylanP on Which Foundry Is In The Lead? It Depends.Asaf Jivilik on Cybord: Electronic Component TraceabilityChris @ crossPORt on Foundational Changes In Chip ArchitecturesMark Olivas on Cybord: Electronic Component TraceabilityAri ben David on Constraints On The Electricity GridJeff Zika on Auto Safety Tech Adds New IC Design ChallengesJung Yoon on Foundational Changes In Chip ArchitecturesSchrodinger's Cat's Advocate on Foundational Changes In Chip ArchitecturesRigTig on Foundational Changes In Chip ArchitecturesSteve on Foundational Changes In Chip ArchitecturesPrashant Purwar on Why Mask Blanks Are CriticalMostafa Abdelgawwad on Radar For Automotive: How Far Can A Radar See?yieldWerx on Managing Wafer RetestJohn Horner on A Brief History of TestLakshm J on ESD Requirements Are ChangingDr. Dev Gupta on Improving Redistribution Layers for Fan-out Packages And SiPsAkarsh on Better PMIC Design Using Multi-Physics SimulationTodd Bermensolo on Reducing Schedule Slips With Automated Post-Route Verification Of SerDes High Speed Serial LinksLaur Rizzatti on Why Geofencing Will Enable L5Raj Raghuram on The Complex Art Of Handling S-ParametersStevo on CHIPS Act: U.S. Releases New Implementation StrategySantosh Kurinec on Quantum Research Bits: Sept. 12Lewis Sternberg on ML And UVM Share Same FlawsRoger Stierman on L5 Adoption Hinges on 5G/6GMarcel on MicroLEDs Move Toward CommercializationRagu Athreya on Is There A Limit To The Number of Layers In 3D-NAND?Brian Bailey on AI Power Consumption ExplodingDavid S on AI Power Consumption ExplodingMike Cormack on Cryogenic CMOS Becomes CoolLance Harvie on New Uses For AI In ChipsDoc R on Electronics And Its Role In Climate ChangeMagdy Abadir on Is Standardization Required For Security?guest on How Overlay Keeps Pace With EUV PatterningSantosh Kurinec on Week In Review, Manufacturing, Testsravani on Timing Library LVF Validation For Production Design FlowsDr. F on A Sputnik Moment For ChipsGary Dagastine on A Sputnik Moment For ChipsMike Sottak on A Sputnik Moment For ChipsRobert Pearson on A Sputnik Moment For ChipsRaye E. Ward on A Sputnik Moment For ChipsMichael Williams on A Look Inside RF DesignSURESHBABU CHILUGODU on Week In Review: Manufacturing, TestJC Bouzigues, Menta on Customizing ProcessorsSteve Swendrowski on IC Package Illustrations, From 2D To 3DEMV on Hybrid Bonding Moves Into The Fast LaneDr. Appo van der Wiel on Variation Making Trouble In Advanced Packageswang yu on Verification Of Functional SafetyFrederick Chen on High-NA EUV May Be Closer Than It AppearsFact Cheq on The Week In Review: DesignShiwen Huang on E-beam’s Role Grows For Detecting IC DefectsAdele Hars on Wafer Shortage Improvement In Sight For 300mm, But Not 200mmDavid A. Humphreys on IMS2022 Booth Tour: EDA And Measurement Science ConvergeMerritt on Can Analog Make A Comeback?subra ganesan on Meeting Processor Performance And Safety Requirements For New ADAS & Autonomous Vehicle SystemsGeorge on Building A More Secure SoCAmit Garg on A New Breed Of EDA RequiredKarl Stevens on A Minimal RISC-VKarl Stevens on EDA Gaps At The Leading EdgeMicah Forstein MS. on Risks Rise As Robotic Surgery Goes MainstreamDr. Punam Raskar on Who Does Processor Validation?Dr. Dev Gupta on Variation Making Trouble In Advanced PackagesCox on DRAM Thermal Issues Reach Crisis PointDavid Leary on DRAM Thermal Issues Reach Crisis PointGeeeeeee on DRAM Thermal Issues Reach Crisis PointPedro Ferro Laks on SOT-MRAM To Challenge SRAMObviously silly on DRAM Thermal Issues Reach Crisis PointSimon on DRAM Thermal Issues Reach Crisis PointGareth on Energy Harvesting Starting To Gain Traction

 





Selecting The Right RISC-V Core Brian BaileyWeek In Review: Semiconductor Ma... Karen Heyman 










  










About

About us
Contact us
Advertising on SemiEng
Newsletter SignUp



Navigation



Homepage
Special Reports
Systems & Design
Low Power-High Perf
Manufacturing, Packaging & Materials
Test, Measurement & Analytics
Auto, Security & Pervasive Computing




Videos
Jobs
Technical Papers
Events
Webinars
Knowledge Centers

Industry Research
Business & Startups
Newsletters





Connect With Us

Facebook
Twitter  @semiEngineering
LinkedIn
YouTube




Copyright ©2013-2023 SMG   |  Terms of Service  |  Privacy Policy





This site uses cookies. By continuing to use our website, you consent to our Cookies PolicyACCEPT Manage consent




Close






Privacy Overview 
This website uses cookies to improve your experience while you navigate through the website. The cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. We do not sell any personal information.

By continuing to use our website, you consent to our Privacy Policy. If you access other websites using the links provided, please be aware they may have their own privacy policies, and we do not accept any responsibility or liability for these policies or for any personal data which may be collected through these sites. Please check these policies before you submit any personal information to these sites.

 





								Necessary							


Necessary

Always Enabled




									Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.								






								Non-necessary							


Non-necessary





									Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website.								












SAVE & ACCEPT










 

 
 

 
























 








"
https://news.ycombinator.com/rss,A Supermarket Visit Brought Down the Soviet Union,http://beelineblogger.blogspot.com/2016/01/how-supermarket-visit-brought-down.html,Comments,"


















BeeLine: How A Supermarket Visit Brought Down The Soviet Union








































































BeeLine




The Shortest Route To What You Need To Know




















Scott Beeken





BeeLine



View my complete profile








































































Tuesday, January 5, 2016








How A Supermarket Visit Brought Down The Soviet Union





Many point to the fact that the Soviet Union collapse occurred as the Soviets were baited into trying to compete with the defense build-up instituted by Ronald Reagan.

The Soviets just did not have the financial resources to match the United States in defense spending while also tending to the needs of its citizenry. The Soviets spent money on guns rather than butter. Something had to give and the Soviet people were the ones that suffered.

However, a little known visit to a suburban Houston supermarket in 1989 by Boris Yeltsin appears to have been the catalyst that ended up bringing down the Soviet Union.

Yeltsin visited the Johnson Space Center in Houston in September, 1989 to tour mission control and to view a model of the planned International Space Station.

After visiting the Space Center, Yeltsin made an unplanned stop at a local Randall's grocery store that was close by before heading to the airport.

That visit changed the course of history.

At the time, Yeltsin was a newly elected member of the Soviet Parliament and the Supreme Soviet and had been a key ally of the General Secretary of the Communist Party Mikhail Gorbachev, who was initiating reforms but the pace of which was too slow for Yeltsin.

Houston Chronicle reporter Stephanie Asin was with Yeltsin on the visit to the grocery store that day.


Yeltsin, then 58, “roamed the aisles of Randall’s nodding his head in amazement,” wrote Asin. He told his fellow Russians in his entourage that if their people, who often must wait in line for most goods, saw the conditions of U.S. supermarkets, “there would be a revolution.”

“Even the Politburo doesn’t have this choice. Not even Mr. Gorbachev,” he said.

The fact that stores like these were on nearly every street corner in America amazed him. They even offered free cheese samples. According to Asin, Yeltsin didn’t leave empty-handed, as he was given a small bag of goodies to enjoy on his trip.

This is a picture of Yeltsin touring the grocery store.



Credit: Houston Chronicle


This Houston Chronicle story from 2014 fills in the rest of the story.


About a year after the Russian leader left office, a Yeltsin biographer later wrote that on the plane ride to Yeltsin’s next destination, Miami, he was despondent. He couldn’t stop thinking about the plentiful food at the grocery store and what his countrymen had to subsist on in Russia.

In Yeltsin’s own autobiography, he wrote about the experience at Randall’s, which shattered his view of communism, according to pundits. Two years later, he left the Communist Party and began making reforms to turn the economic tide in Russia. 

“When I saw those shelves crammed with hundreds, thousands of cans, cartons and goods of every possible sort, for the first time I felt quite frankly sick with despair for the Soviet people,” Yeltsin wrote. “That such a potentially super-rich country as ours has been brought to a state of such poverty! It is terrible to think of it.”

To give you some perspective on what was available in the Soviet Union at that time, here is a picture of a Russian store from that era.




Credit: Gennady Galperin/Reuters


An aide to Yeltsin later reported that in that visit to the grocery store in Houston “the last vestige of Bolshevism collapsed” inside his boss.

Two years later Yeltsin was elected to the newly created office of President of the Russian Federation after the collapse of the Soviet Union with Gorbachev.

Yeltsin immediately began dismantling the socialist economic system and introducing capitalism to the Russians. In the process he attempted to convert the world's largest command economy into a free-market one. 

The results of that transition were rocky in large part to cronyism in the break-up of many of the large state-owned businesses. In the process, many Russian oligarchs were created and Yeltsin eventually resigned his office in 1999 haunted by charges of corruption and incompetence.

His successor?

Vladimir Putin.

The falling price of oil has put a similar squeeze on Putin and the Russians today. Putin has been popular with the Russian people based on his macho style and nationalistic bombast. However, potential trouble lurks for Putin because of the Russian economy.

The Russian consumer is being squeezed with annual inflation of almost 20% and the average Russian spends about 50% of their income on food.

By comparison, the average American spends only 8% of income on groceries.

Will groceries once again determine the future of Russia?






Posted by



BeeLine




at

8:21 PM
















Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest














42 comments:




andreSeptember 14, 2017 at 4:52 AMobat viagraviagra asliReplyDeleteRepliesUnknownOctober 29, 2021 at 10:14 AMGreat Article Artificial Intelligence Projects Project Center in Chennai JavaScript Training in Chennai JavaScript Training in Chennai Project Centers in Chennai DeleteRepliesReplyReplychegekhanMarch 28, 2018 at 8:11 AMUseful Information, your blog is sharing unique information....Thanks for sharing!!! buy bakery products online south-cbuy branded food online in panganiReplyDeleteRepliesReplyUnknownSeptember 27, 2018 at 7:24 AMThank you for your post. This is excellent information. It is amazing and wonderful to visit your site.buy bakery products online south-c ReplyDeleteRepliesReplyYK AgencyDecember 25, 2018 at 11:30 AMSupermarket in Dubai Great article. Cool.ReplyDeleteRepliesReplyLuck CityFebruary 11, 2019 at 3:50 AMWithin this webpage, you'll see the page, you need to understand this data. https://digitalglobal.comReplyDeleteRepliesReplyjames brownNovember 22, 2019 at 6:45 AMAwesome blog. I enjoyed reading your articles. This is truly a read for me. I have bookmarked it and I am looking forward to reading new articles. Keep up the good work!Kroger customer surveyReplyDeleteRepliesReplyKroger experienceDecember 10, 2019 at 8:05 AMPlease share more like that.Kroger experienceReplyDeleteRepliesReplyAnonymousDecember 30, 2019 at 7:47 PMGreat article! Yeltsin revealed as a realist! I never knew this ...ReplyDeleteRepliesReplyDavid Grant Stewart, Sr., EgyptologistJanuary 1, 2020 at 3:17 PMWho paid you to write this drivel? You are either incredibly gullible or on the take from the USSR propaganda machine. You look and say the Soviet civilian economy is bad. There is no civilian economy in the USSR. The country does not have a war machine. The country is a war machine.ReplyDeleteRepliesReplyNeha UppalJanuary 9, 2020 at 3:33 AMThanks for sharing such beautiful information with us. I hope you will share some more information about best grocery shopping app. Please keep sharing.ReplyDeleteRepliesReplyjames brownFebruary 13, 2020 at 1:15 PMHey There. I found your blog using This is a very well written article. I’ll be sure to bookmark it and come back to read more of your useful info. Thanks for the post. I’ll definitely return.https://krogerexperiencee.com/ReplyDeleteRepliesReplyjames brownFebruary 18, 2020 at 7:24 AMGreat post I would like to thank you for the efforts you have made in writing this interesting and knowledgeable article.https://tellthebelll.usReplyDeleteRepliesReplysurvey monkey usaFebruary 19, 2020 at 7:45 AMGreat things you’ve always with us. Just keep writing this kind of posts.The time which was wasted in traveling for tuition now it can be used for studies.Thankssurvey monkey usaReplyDeleteRepliesReplydanielwilsonnFebruary 19, 2020 at 3:43 PM Thank you again for all the knowledge u distribute,Good post. I was very interested in the article, it's quite inspiring I should admit. I like visiting you site since I always come across interesting articles like this one.Great Job, I greatly appreciate that.Do Keep sharing! Regards,https://krogerfeeedback.us/ReplyDeleteRepliesReplydahliaApril 23, 2020 at 3:21 AM Thanks for sharing this information. I really like your post very much. You have really shared an informative and interesting post with people  TellTheBell ReplyDeleteRepliesReplypatronsurveysApril 24, 2020 at 8:23 AMAfter Kroger and Walmart, I prefer to go to Tesco supermarket. Do you know what? there is quality in products with an extraordinary service tescoviews com offers a platform to complete the Tesco customer satisfaction survey to win £1000 Gift Card & 25 Club Points. Survey site is giving a lot of store surveys at one place to complete.ReplyDeleteRepliesReplyjames brownJuly 14, 2020 at 3:24 AMThis comment has been removed by the author.ReplyDeleteRepliesReplyjames brownJuly 14, 2020 at 3:25 AMIts a great pleasure reading your post.Its full of information I am looking for and I love to post a comment that ""The content of your post is awesome"" Great work.https://krogerexperiencee.com/greatpeople-me-kroger-employee-login-portal/ReplyDeleteRepliesReplyNFL FanSeptember 20, 2020 at 4:46 PMThe official source for NFL news, video highlights, fantasy football, game-day coverage, schedules, stats, scores and more. Ravens FootballReplyDeleteRepliesReplytellthebellSeptember 22, 2020 at 11:42 AM Excellent website you have  so much cool information!..tellthebellReplyDeleteRepliesReplydgcustomerfirstJune 25, 2021 at 6:49 AMTo take an interest in the Dollar general super market survey, it is important to arrange a few things at one of its Branches initially. Visit the authority survey site of Dollar General survey at Dgcustomerfirst.Com and Win A $100 gift voucher. Then, at that point, you need to save the receipt of the store. Then, at that point, go to the authority survey site of dollar general. The Dg survey is accessible in both English or Spanish.ReplyDeleteRepliesReplymybkexperienceAugust 3, 2021 at 2:30 AMDollar general survey is an online platform that collects customers' most recent shopping experiences and overall customer satisfaction. Participate in the survey and be the lucky person to get enlisted in dg customer first winners. ReplyDeleteRepliesReplyarnavharperAugust 30, 2021 at 12:33 PMRemote for Fire TV is designed specifically to control Fire TV, Fire TV Cube and Fire TV Stick. Just connect mobile device and a TV or media player to the Firestick Remote.ReplyDeleteRepliesReplySEOOctober 12, 2021 at 3:37 AMdelta international recruitment agency in pakistanReplyDeleteRepliesReplyHealthandBeautyTipsNovember 6, 2021 at 5:24 PMEmployees can Perform Kroger E-Schedule Login at the Feed Kroger Login Portal once their Schedule Credentials are verified. If you are unable to sign in to Kroger Login then you need to contact the branch manager.Feed KrogerReplyDeleteRepliesReplyUnknownJanuary 27, 2022 at 5:48 AMThe NASA dark Brant IX sounding rocket conveyed the payload to an apogee of 177 miles prior to plunging by parachute and arriving at White Sands. Dgcustomerfirst.com Survey ReplyDeleteRepliesReplydgcustomerfirst.comMay 19, 2022 at 3:35 AMThe dgcustomerfirst.com criticism review permits customers to enter the Dollar General Sweepstakes of Cash $100 in the wake of finishing the overview.dgcustomerfirstwin.shop Survey  ReplyDeleteRepliesReplywww.DGCustomerFirst.comJune 6, 2022 at 10:50 AMDollar general survey is an online platform that collects customers' most recent shopping experiences and overall customer satisfaction. https://idgcustomerfirst.org/ReplyDeleteRepliesReplydgcustomerfirsts.shopJune 10, 2022 at 6:07 AMAlso, you have a superb opportunity to partake in the client criticism overview. DGCustomerFirst 2022 or Dollar General Survey is a study led by Dollar General's authorities for all United States inhabitants.dgcustomerfirsts executed a connected with the WWW review to take an arrangement about dgcustomerfirsts Helpline notwithstanding your support level thereafter visiting service  Click here dgcustomerfirsts ReplyDeleteRepliesReplySteveJune 16, 2022 at 9:58 AMGreat Article, it was very informative. That was such thought-provoking content. I enjoyed reading your content. Every week, I look forward to your column. In my opinion, this one is one of the best articles you have written so far.How to Change Instagram PasswordChange Windows 10 PasswordSubwaylistensHome Depot SurveyDQFanFeedback.com ReplyDeleteRepliesReplyUmairJune 30, 2022 at 6:44 AMirescopk.comReplyDeleteRepliesReplyDgcustomerfirstscom.shopJuly 9, 2022 at 6:51 AMFormerly referred to as J L Turner, Dollar General has numerous subsidiaries viz Dollar General Market, Dollar General Financial, Dollar General Global Sourcing, and lots extra.dgcustomerfirstscom executed a connected with the WWW review to take an arrangement about dgcustomerfirstscom Helpline notwithstanding your support level thereafter visiting service  Click here dgcustomerfirstscom ReplyDeleteRepliesReplyAnonymousJuly 19, 2022 at 1:35 PMLiveTheOrangeLife – Official Portal www.LiveTheOrangeLife.comWalmartOne Login - Walmartone.com Login Guidemyaccountaccess.comonevanillaJCP Associate KioskReplyDeleteRepliesReplySmith AdomJuly 30, 2022 at 12:03 PMTalkToWendys executed a connected with the WWW review to take an arrangement about TalkToWendys Helpline notwithstanding your support level thereafter visiting service  Click here TalkToWendys  It is mandatory to make a purchase at Wendy’s once before being a participant in this survey.ReplyDeleteRepliesReplyInformTarget.comAugust 10, 2022 at 3:38 AMRules are guidelines, and they are set to be accompanied. If you want to participate within the survey effectively, you need to adhere to the rules and policies set apart via the informtarget.Com remarks survey.informtargets executed a connected with the WWW review to take an arrangement about informtargets Helpline notwithstanding your support level thereafter visiting service  Click here informtargets.shop ReplyDeleteRepliesReplyTellBaskinRobbinsAugust 19, 2022 at 5:39 AMThere are some basic rules and requirements of this Baskin Robbins Customer Satisfaction Survey which I even have furnished in this newsletter. tellbaskinrobbins executed a connected with the WWW review to take an arrangement about tellbaskinrobbins Helpline notwithstanding your support level thereafter visiting service  Click here tellbaskinrobbins ReplyDeleteRepliesReplyTalktofoodlionAugust 20, 2022 at 2:37 AMTalktofoodlion The company's full name is general Dollar, and it is offering a $100 incentive to customers who take the time to participate in this little survey. visit here Talktofoodlion ReplyDeleteRepliesReplyFaiz IsrailiAugust 21, 2022 at 5:02 AMThe procedure is requesting information from individuals using a questionnaire, which may be completed offline or online. New technologies, however, are frequently disseminated via digital channels like social media, email, QR codes, or URLs. dgcustomerfirstReplyDeleteRepliesReplyTellBaskinRobbinsSeptember 8, 2022 at 7:11 AMTellBaskinRobbins After responding to the feedback questions, participants are expected to rate their experience shopping at Baskin Robbins.The feedback survey is sponsored by Baskin Robbins in order to better understand its service quality TellBaskinRobbinsReplyDeleteRepliesReplyNikithaOctober 8, 2022 at 1:31 AMmybkexperience customer satisfaction survey which is an online platform to get timely feedback from their customers about the food and services. This can improve their services according to customer’s needs and at the same time, rewards their customer for their time and loyalty towards the restaurant. So you can answer the questions and eat delicious food for free at the same time. Now you might be wondering about how to participate in the survey or what are the requirements and much more.ReplyDeleteRepliesReplySEOOctober 14, 2022 at 5:59 AMtop recruitment agencies in pakistan for saudi arabiaReplyDeleteRepliesReplyAdd commentLoad more...























Newer Post


Older Post

Home




Subscribe to:
Post Comments (Atom)















BeeLine Email Subscription

Get new posts by email:  Subscribe




Follow @BeeLineBlog




Followers











Blog Archive








        ► 
      



2023

(5)





        ► 
      



January

(5)









        ► 
      



2022

(139)





        ► 
      



December

(11)







        ► 
      



November

(10)







        ► 
      



October

(12)







        ► 
      



September

(12)







        ► 
      



August

(13)







        ► 
      



July

(11)







        ► 
      



June

(12)







        ► 
      



May

(12)







        ► 
      



April

(12)







        ► 
      



March

(12)







        ► 
      



February

(10)







        ► 
      



January

(12)









        ► 
      



2021

(131)





        ► 
      



December

(15)







        ► 
      



November

(12)







        ► 
      



October

(9)







        ► 
      



September

(13)







        ► 
      



August

(14)







        ► 
      



July

(11)







        ► 
      



June

(10)







        ► 
      



May

(6)







        ► 
      



April

(10)







        ► 
      



March

(11)







        ► 
      



February

(7)







        ► 
      



January

(13)









        ► 
      



2020

(154)





        ► 
      



December

(11)







        ► 
      



November

(12)







        ► 
      



October

(14)







        ► 
      



September

(11)







        ► 
      



August

(12)







        ► 
      



July

(13)







        ► 
      



June

(14)







        ► 
      



May

(12)







        ► 
      



April

(16)







        ► 
      



March

(16)







        ► 
      



February

(10)







        ► 
      



January

(13)









        ► 
      



2019

(145)





        ► 
      



December

(14)







        ► 
      



November

(11)







        ► 
      



October

(9)







        ► 
      



September

(12)







        ► 
      



August

(13)







        ► 
      



July

(12)







        ► 
      



June

(12)







        ► 
      



May

(14)







        ► 
      



April

(13)







        ► 
      



March

(12)







        ► 
      



February

(10)







        ► 
      



January

(13)









        ► 
      



2018

(139)





        ► 
      



December

(11)







        ► 
      



November

(10)







        ► 
      



October

(10)







        ► 
      



September

(10)







        ► 
      



August

(12)







        ► 
      



July

(13)







        ► 
      



June

(12)







        ► 
      



May

(12)







        ► 
      



April

(13)







        ► 
      



March

(12)







        ► 
      



February

(9)







        ► 
      



January

(15)









        ► 
      



2017

(132)





        ► 
      



December

(9)







        ► 
      



November

(10)







        ► 
      



October

(15)







        ► 
      



September

(9)







        ► 
      



August

(13)







        ► 
      



July

(12)







        ► 
      



June

(9)







        ► 
      



May

(13)







        ► 
      



April

(12)







        ► 
      



March

(10)







        ► 
      



February

(10)







        ► 
      



January

(10)









        ▼ 
      



2016

(119)





        ► 
      



December

(11)







        ► 
      



November

(15)







        ► 
      



October

(15)







        ► 
      



September

(10)







        ► 
      



August

(2)







        ► 
      



July

(9)







        ► 
      



June

(11)







        ► 
      



May

(6)







        ► 
      



April

(9)







        ► 
      



March

(11)







        ► 
      



February

(12)







        ▼ 
      



January

(8)

In the Middle of 5th Avenue
Risky Business
That's for the Birds
An Inconvenient Truth +10
Citizen Cruz
Context on Guns
How A Supermarket Visit Brought Down The Soviet Union
A Humble Servant?










        ► 
      



2015

(71)





        ► 
      



December

(9)







        ► 
      



November

(5)







        ► 
      



October

(2)







        ► 
      



September

(2)







        ► 
      



August

(9)







        ► 
      



July

(7)







        ► 
      



June

(6)







        ► 
      



May

(3)







        ► 
      



April

(10)







        ► 
      



March

(6)







        ► 
      



February

(4)







        ► 
      



January

(8)









        ► 
      



2014

(88)





        ► 
      



December

(5)







        ► 
      



November

(7)







        ► 
      



October

(10)







        ► 
      



September

(9)







        ► 
      



August

(5)







        ► 
      



July

(8)







        ► 
      



June

(7)







        ► 
      



May

(9)







        ► 
      



April

(7)







        ► 
      



March

(6)







        ► 
      



February

(5)







        ► 
      



January

(10)









        ► 
      



2013

(115)





        ► 
      



December

(8)







        ► 
      



November

(5)







        ► 
      



October

(13)







        ► 
      



September

(9)







        ► 
      



August

(7)







        ► 
      



July

(10)







        ► 
      



June

(10)







        ► 
      



May

(9)







        ► 
      



April

(13)







        ► 
      



March

(10)







        ► 
      



February

(7)







        ► 
      



January

(14)









        ► 
      



2012

(139)





        ► 
      



December

(6)







        ► 
      



November

(16)







        ► 
      



October

(16)







        ► 
      



September

(9)







        ► 
      



August

(14)







        ► 
      



July

(13)







        ► 
      



June

(8)







        ► 
      



May

(11)







        ► 
      



April

(9)







        ► 
      



March

(9)







        ► 
      



February

(11)







        ► 
      



January

(17)









        ► 
      



2011

(188)





        ► 
      



December

(12)







        ► 
      



November

(10)







        ► 
      



October

(10)







        ► 
      



September

(10)







        ► 
      



August

(17)







        ► 
      



July

(18)







        ► 
      



June

(11)







        ► 
      



May

(3)







        ► 
      



April

(16)







        ► 
      



March

(19)







        ► 
      



February

(25)







        ► 
      



January

(37)









About Me





BeeLine


Scott Beeken has practiced as an attorney, CPA and has been an officer with two Fortune 500 companies overseeing diverse functions such as Taxation, Employee Benefits, Human Resources, Real Estate Facilities, Risk Management, Corporate Communications, Marketing and Advertising. In addition to writing BeeLine, he is a Keynote Speaker, Author  and Strategic Consultant.

View my complete profile




























Total Pageviews

























Theme images by luoman. Powered by Blogger.

























"
https://news.ycombinator.com/rss,Show HN: Sketch – AI code-writing assistant that understands data content,https://github.com/approximatelabs/sketch,Comments,"








approximatelabs

/

sketch

Public




 

Notifications



 

Fork
    8




 


          Star
 281
  









        AI code-writing assistant that understands data content
      





281
          stars
 



8
          forks
 



 


          Star

  





 

Notifications












Code







Issues
0






Pull requests
0






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







approximatelabs/sketch









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





6
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




bluecoconut

update readme wording




        …
      




        9d567ec
      

Jan 16, 2023





update readme wording


9d567ec



Git stats







133

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github/workflows



remove python 3.7, add tests, remove uneeded code



Dec 15, 2022









sketch



some bug fix and copy addition



Jan 16, 2023









tests



remove python 3.7, add tests, remove uneeded code



Dec 15, 2022









.gitignore



update with edit and work on text2sql



Oct 13, 2022









README.md



update readme wording



Jan 16, 2023









dev-requirements.txt



starting to rebuild sketch



Dec 15, 2022









pyproject.toml



rename to pandas extension, add missing requirement, use base64 encoding



Jan 11, 2023









setup.py



starting to rebuild sketch



Dec 15, 2022




    View code
 


















sketch
Demo
How to use
.sketch.ask
.sketch.howto
.sketch.apply
Sketch currently uses prompts.approx.dev to help run with minimal setup
How it works





README.md




sketch
Sketch is an AI code-writing assistant for pandas users that understands the context of your data, greatly improving the relevance of suggestions. Sketch is usable in seconds and doesn't require adding a plugin to your IDE.
pip install sketch
Demo
Here we follow a ""standard"" (hypothetical) data-analysis workflow, showing a Natural Language interace that successfully navigates many tasks in the data stack landscape.

Data Catalogging:

General tagging (eg. PII identification)
Metadata generation (names and descriptions)


Data Engineering:

Data cleaning and masking (compliance)
Derived feature creation and extraction


Data Analysis:

Data questions
Data visualization








sketch-demo.mp4





Try it out in colab: 
How to use
It's as simple as importing sketch, and then using the .sketch extension on any pandas dataframe.
import sketch
Now, any pandas dataframe you have will have an extension registered to it. Access this new extension with your dataframes name .sketch
.sketch.ask
Ask is a basic question-answer system on sketch, this will return an answer in text that is based off of the summary statistics and description of the data.
Use ask to get an understanding of the data, get better column names, ask hypotheticals (how would I go about doing X with this data), and more.
df.sketch.ask(""Which columns are integer type?"")
.sketch.howto
Howto is the basic ""code-writing"" prompt in sketch. This will return a code-block you should be able to copy paste and use as a starting point (or possibly ending!) for any question you have to ask of the data. Ask this how to clean the data, normalize, create new features, plot, and even build models!
df.sketch.howto(""Plot the sales versus time"")
.sketch.apply
apply is a more advanced prompt that is more useful for data generation. Use it to parse fields, generate new features, and more. This is built directly on lambdaprompt. In order to use this, you will need to set up a free account with OpenAI, and set an environment variable with your API key. OPENAI_API_KEY=YOUR_API_KEY
df['review_keywords'] = df.sketch.apply(""Keywords for the review [{{ review_text }}] of product [{{ product_name }}] (comma separated):"")
df['capitol'] = pd.DataFrame({'State': ['Colorado', 'Kansas', 'California', 'New York']}).sketch.apply(""What is the capitol of [{{ State }}]?"")
Sketch currently uses prompts.approx.dev to help run with minimal setup
In the future, we plan to update the prompts at this endpoint with our own custom foundation model, built to answer questions more accurately than GPT-3 can with its minimal data context.
You can also directly call OpenAI directly (and not use our endpoint) by using your own API key. To do this, set 2 environment variables.
(1) SKETCH_USE_REMOTE_LAMBDAPROMPT=False
(2) OPENAI_API_KEY=YOUR_API_KEY
How it works
Sketch uses efficient approximation algorithms (data sketches) to quickly summarize your data, and feed that information into language models. Right now it does this by summarizing the columns and writing these summary statistics as additional context to be used by the code-writing prompt. In the future we hope to feed these sketches directly into custom made ""data + language"" foundation models to get more accurate results.









About

      AI code-writing assistant that understands data content
    
Topics



  python


  data-science


  data


  ai


  tabular-data


  pandas


  df


  sketches


  dataframe


  copilot


  codex


  ds


  datasketches


  gpt3


  lambdaprompt


  datasketch



Resources





      Readme
 


Stars





281
    stars

Watchers





2
    watching

Forks





8
    forks







    Releases





6
tags







    Packages 0


        No packages published 







        Used by 22
 




























            + 14
          







    Contributors 2








bluecoconut
Justin Waugh

 






jmbiven
Mike Biven

 





Languages










Python
100.0%











"
https://news.ycombinator.com/rss,Rust vs. C++ Formatting,https://brevzin.github.io/c++/2023/01/02/rust-cpp-format/,Comments," Rust vs C++ Formatting | Barry's C++ Blog      Barry's C++ BlogJust a blog about C++   HOME    TAGS    ARCHIVES    ABOUT                     Home   Rust vs C++ Formatting   Post     Cancel Rust vs C++ Formatting  Posted  Jan 2, 2023    Updated  Jan 9, 2023    By  Barry Revzin    35 min readIn Rust, if I want to print some 32-bit unsigned value in hex, with the leading 0x, padded out with zeros, I would write that as:println!(""{:#010x}"", value);
In C++23, if I want to do the same, that’s:std::println(""{:#010x}"", value);
The only difference is the spelling of the name of the thing we’re calling (which is a function template in C++ and a macro in Rust) - otherwise, identical.Nevertheless, there is a surprisingly vast gulf of difference between the two languages in how they handle formatting, at basically every level beneath the user-facing syntax. I thought the differences were pretty interesting and worth going over.Ergonomic FeaturesBefore I go over the differences in the two format libraries, it’s worth starting out by discussing the differences in ergonomic features that are available to users. By an ergonomic feature, what I mean is a feature that doesn’t necessarily add any functionality – it may solve a problem that may otherwise have been solvable – but rather that it makes it easier to get done, with less typing, and thus probably with fewer bugs.The canonical example in C++ of an ergonomic feature might be: lambdas. We could always write function objects by declaring a class or class template somewhere and overloading its operator() - but doing so is very verbose and, if the call operator needs to be a template, it can’t be written locally. Lambdas don’t add functionality, but they are tremendously more user-friendly 1.When it comes to formatting, Rust has two major ergonomic features that have tremendous impact on user experience: #[derive(Debug)] and f-strings.#[derive(Debug)]Rust has multiple different formatting traits (which I’ll get into later), but for now I’ll touch on the distinction between the two most important ones: fmt::Display and fmt::Debug. Debug is, as the name suggests, for debugging purposes and the Rust documentation states that:fmt::Debug implementations should be implemented for all public types. Output will typically represent the internal state as faithfully as possible. The purpose of the Debug trait is to facilitate debugging Rust code. In most cases, using #[derive(Debug)] is sufficient and recommended.Now, the notion of making your types printable for debug purposes is hardly unique to Rust. I do this in C++ all the time. But the key feature in Rust is how much code you have to write to accomplish this:#[derive(Debug)]
struct Point {
    x: i32,
    y: i32,
}

fn main() {
    println!(""p={:?}"", Point { x : 1, y : 2});
}
Which, when run, prints:p=Point { x: 1, y: 2 }
The fact that you have to write one line of code to achieve this is tremendous. And even calling this one line is a bit much, since you’ll typically be deriving many traits (like Eq or Ord or Clone, for a type like this), so we’re effectively just talking about a few characters.Of course, you could implement the Debug trait by hand - it’s not impossible without #[derive]. But over the long run, the ability to do this just adds so much value.f-stringsIn the intro, I showed that in Rust you could write this:println!(""{:#010x}"", value);
But recently (as of Rust 1.58), Rust also added the ability to use f-strings (also called interpolated literals or interpolated strings in some other languages), which allows you to just write:println!(""{value:#010x}"");
In Rust, this was originally implemented (as far as I’m aware) in the fstrings crate, modeled after the Python feature of the same name. This feature exists in a number of other languages (as noted in the Rust RFC): JavaScript, C#, VB, Swift, Ruby, Scala, Perl, and PHP. There are certainly more (like Kotlin).For a simple example like this, using string interpolation doesn’t really make a big difference. Sure, it’s shorter, but only by two characters, which is hardly significant. The value of the feature isn’t how much typing it saves - the value here is that it allows you to put the variables you’re formatting in the location that they’re being formatted. Compare the readability between these two lines:println!(""Point is at (x={}, y={}, z={})"", p.x, p.y, p.z);
println!(""Point is at (x={p.x}, y={p.y}, z={p.z})"");
It is much easier to understand what’s being formatted in the second line and, importantly, it’s easier to ensure that you format all of your arguments in the correct order - since seeing ""y={p.z}"" is clearly wrong.It’s been pointed to me by numerous people that Rust’s interpolation feature doesn’t actually support {p.x} like that. The fstrings crate does support that usage (you can see examples in their docs), but the RFC explicitly rejects this on the basis that once you allow arbitrary expressions, they become too hard to read. Not sure I agree with this line of reasoning - seems like the sort of thing you should leave up to the community:If any expressions beyond identifiers become accepted in format strings, then the RFC author expects that users will inevitably ask “why is my particular expression not accepted?”. This could lead to feature creep, and before long perhaps the following might become valid Rust:println!(""hello { if self.foo { &self.person } else { &self.other_person } }"");
This no longer seems easily readable to the RFC author.As with #[derive], f-strings don’t add any functionality - these two lines really do the same thing. But this is the kind of language feature that once you start using regularly in one language (in my case, Python), you really want to use it everywhere, all at once.See also David Sankel’s Rust Features That I Want In C++ from CppNow 2022.Format String BasicsNow that I got the ergonomics out of the way, let’s talk about the way format strings work - whether in Rust or Python or C++ or a number of other languages. I’m going to use the terms in the C++ grammar for format strings, since that’s what I’m most familiar with.Given a string like:""A string literal with x={} and y={:#08x} and name={:>25}""
What we have are alternating string pieces and replacement fields. Each replacement field consumes one or more trailing (ignoring interpolation) arguments that are passed into the formatting function (or macro). A replacement field is enclosed in braces - the {}, {:#08x} and {:>25} above are all replacement fields.A replacement field consists of an optional argument id followed by an optional format specifier. An argument id allows you to explicitly choose an argument by number ({} will replace with whatever the next argument is, while {0} will explicitly replace with the first argument). In C++ and Python, you cannot mix and match automatic and manual numbering. Manual numbering allows you to format the same argument multiple times, or to format the arguments out of order - which is particularly useful for translation purposes. The format specifier is what tells the library how to specifically format the chosen argument. The format specifier 2 must be introduced by a :.Types typically have a common set of format specifiers they can use:fill and alignmentsignwidthprecisionalternate type representation (e.g. hex for integers, or scientific for floating point)For more details on what these specifiers are and how to use them, check out the C++ fmt docs or the Rust docs or the Python docsThe good news here is that the way format strings work in Rust, Python, and C++ are mostly the same.One interesting distinction I do want to point out is how these languages differ in their handling of dynamic width. In all three, if I want to format a string, right-aligned, in a 25-character wide field, that’s something like this:format(""{:>25}"", s)
But if I want the width to come from a variable instead, I can use this in C++ (or the equivalent in Python):format(""{:>{}}"", s, 25)
But Rust doesn’t let you do {} (i.e. automatic numbering) for dynamic width , you can only provide an explicit index or a named variable – which then must be suffixed with $. Rust’s version is:println!(""{:>1$}"", ""hello"", 25);
Rust here allows a mix of automatic (for the string) and manual (for the width) indexing. Neither C++ nor Python allow this - you can write either {0:>{1}} or {:>{}} in those two languages, but not {:>{1}} or {0:>{}}.I’m not sure why Rust differs here - I think the visual distinction between {:>25} and {:>{}} is quite a bit larger than {:>25} and {:>1$}, since the latter seems like it could easily be misread to be a width of 1.C++ Formatting with {fmt}Let’s talk about the way the core C++ formatting library works – with {fmt} and now std::format. How do you implement formatting for your type?In C++, you have to specialize the type fmt::formatter (now std::formatter) and provide two functions for it: parse() and format().Using a two-dimensional Point example:struct Point {
    int x;
    int y;
};

template <>
struct std::formatter<Point> {
    constexpr auto parse(auto& ctx) {
        // ...
    }

    auto format(Point const& p, auto& ctx) const {
        // ...
    }
};
The job of parse() is to parse and validate the provided format specifier. It should throw an exception (format_error) if the provided format specifier is invalid. The ctx argument gives you access to the format string. What does it mean for a format specifier to be invalid for Point? Interesting question.The job of format() is to use the saved state from parse() (if any) to format the object (p) into the provided output iterator (which you get via ctx.out()).The simplest implementation would be to mandate that no format specifier is provided and then format the Point in some friendly, readable way:template <>
struct std::formatter<Point> {
    constexpr auto parse(auto& ctx) {
        return ctx.begin();
    }

    auto format(Point const& p, auto& ctx) const {
        return std::format_to(ctx.out(), ""(x={}, y={})"", p.x, p.y);
    }
};
Using the standard specifiersLet’s say we don’t want to just format p.x and p.y as regular integers, but also want to support whatever arbitrary format specifiers the ints do: padding, hex, etc. We can do that by deferring to formatter<int> for both the parsing and the formatting logic:template <>
struct fmt::formatter<Point> {
    fmt::formatter<int> f;

    constexpr auto parse(auto& ctx) {
        return f.parse(ctx);
    }

    auto format(Point const& p, auto& ctx) const {
        auto out = fmt::format_to(ctx.out(), ""(x="");
        ctx.advance_to(out);
        out = f.format(p.x, ctx);
        out = fmt::format_to(out, "", y="");
        ctx.advance_to(out);
        out = f.format(p.y, ctx);
        *out++ = ')';
        return out;
    }
};
And with that, we can get arbitrarily complex formatting:fmt::print(""{0}\n{0:#x}\n{0:*^7}\n"", Point{.x=100, .y=200});
which prints:(x=100, y=200)
(x=0x64, y=0xc8)
(x=**100**, y=**200**)
The implementation is mildly tedious because we have to do this two-step:ctx.advance_to(out);
out = f.format(p.x, ctx);
Perhaps a different way of writing it it that’s potentially less error prone would be to not even have a local variable for the output iterator:auto format(Point const& p, auto& ctx) const {
    ctx.advance_to(fmt::format_to(ctx.out(), ""(x=""));
    ctx.advance_to(f.format(p.x, ctx));
    ctx.advance_to(fmt::format_to(ctx.out(), "", y=""));
    ctx.advance_to(f.format(p.y, ctx));
    return fmt::format_to(ctx.out(), "")"");
}
This is because in the C++ model, the format context just has some arbitrary output iterator while the format() function on the formatter takes the format context - these two things need to be kept in sync. If we don’t remember to ctx.advance_to(out) and out happens to be something like char*, then we would just overwrite stuff that we’d already written.It’s hard to really ensure that you did this right because the default iterator in {fmt} is fmt::appender, with which you simply cannot run into this problem. It’s just a std::back_insert_iterator - the kind of output iterator where ++it doesn’t actually do anything since it doesn’t have a notion of position 3. Since all std::back_insert_iterator<Container>s into a given Container have the same state, forgetting to update with advance_to doesn’t matter.Because {fmt} (and std::format) type-erases the provided output iterator, even if you use fmt::format_to(out, ""{}"", p) where out is a char*, this still won’t break if you forget the ctx.advance_to(out). This issue will only surface in the library if you use both compile-time format strings and provide your own iterator:char buf[300] = {};
// without the calls to advance_to(), this will end up writing
// just ""2)"" instead of ""(x=1, y=2)"", but buf[2:3] will still be ""y=""
char* o = fmt::format_to(buf, FMT_COMPILE(""{}""), Point{1, 2});
You can see the impact of the missing advance_to call here.And that still isn’t even completely right. Output iterators in C++20 are allowed to be move-only, so ctx.advance_to(out) might not compile. Like I said, it’s hard to get this right.Outside of remembering this pitfall, this is pretty nifty. We get support for all of this logic in one go.Using custom specifiersWe’re not limited to just supporting the standard specifiers – we can also add our own. Let’s say that instead of supporting the standard integer specifiers, all we care about for our Point type is printing it either in cartesian coordinates, as (x={}, y={}), or in polar coordinates, as (r={}, theta={}).I go over this in more detail in my CppCon 2022 talk, “The Surprising Complexity of Formatting Ranges,” but say we wanted to use the c or r specifiers (for cartesian or rectangular) to format x/y and to use the p specifier (for polar) to format r/theta. We can implement that this way:template <>
struct fmt::formatter<Point> {
    // store additional state during parse()
    enum class Coord {
        cartesian,
        polar
    };
    Coord type = Coord::cartesian;

    constexpr auto parse(auto& ctx) {
        auto it = ctx.begin();
        // if we don't have any specifier, then we're done
        if (it == ctx.end() or *it == '}') {
            return it;
        }

        // otherwise consume the one character that we expect
        switch (*it++) {
        case 'r':
        case 'c':
            type = Coord::cartesian;
            break;
        case 'p':
            type = Coord::polar;
            break;
        default:
            throw fmt::format_error(""invalid specifier"");
        }
        return it;
    }

    auto format(Point const& p, auto& ctx) const {
        // our choice of output is based on our state
        if (type == Coord::cartesian) {
            return fmt::format_to(ctx.out(), ""(x={}, y={})"", p.x, p.y);
        } else {
            return fmt::format_to(ctx.out(), ""(r={:.4}, theta={:.4})"", p.r(), p.theta());
        }
    }
};
In my CppCon talk, I also go over some more complicated things that you can do with specifiers - like how to implement support for dynamic width that I mentioned earlier.Rust Formatting with std::fmtIn Rust, implementing manual formatting for Point looks quite different. There, you have to implement the trait Display, which only has one function for you to implement instead of two. The same, simplest-possible implementation for Point would look like this:impl fmt::Display for Point {
    fn fmt(&self, formatter : &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(formatter, ""(x={}, y={})"", self.x, self.y)
    }
}
&self here is the reference to Point, while formatter holds a similar role to the ctx arguments that were passed to parse() and format() in C++.While this implementation looks superficially similar to the C++ implementation, just being a little shorter as we only had to write one function instead of two, they’re actually quite different.The Formatter objectLet’s start by going over fmt::Formatter. You can find the docs here.In C++, parse() and format() are exposed to the user - to do with as they wish. In Rust, you only get format() (spelled fmt) - the library itself does the parsing for you, and gives you the state in the Formatter object.That state looks like so:pub struct Formatter<'a> {
    flags: u32,
    fill: char,
    align: rt::v1::Alignment,
    width: Option<usize>,
    precision: Option<usize>,

    buf: &'a mut (dyn Write + 'a),
}
The flags here includes the sign choice and whether we’re using the alternative formatting, the other fields are not surprising. buf is an arbitrary type-erased buffer, similar to the way that {fmt} type-erases the output iterator 4.Conceptually, you can think of Rust’s fmt::Display::fmt() for T and C++’s formatter<T>::format() as being fairly equivalent. They even get the same pieces of information:InformationC++Rustthe Tpassed as first parameter&selfthe parse statethe *this object, populated from parse()the Formatter object, populated by librarythe output bufferctx.out()formatter.bufBecause Display::fmt takes a Formatter, the right way to extend our formatter for Point to support the standard specifiers is to pass the Formatter we get into subsequent calls to fmt:impl fmt::Display for Point {
    fn fmt(&self, f : &mut fmt::Formatter<'_>) -> fmt::Result {
        f.write_str(""(x="")?;
        fmt::Display::fmt(&self.x, f)?;
        f.write_str("", y="")?;
        fmt::Display::fmt(&self.y, f)?;
        f.write_str("")"")
    }
}
Note the use of ? for error propagation, which I’d love to have in C++. I’m currently pursing this as P2561This has the same structure as the C++ implementation - we have one mechanism to write the string pieces (in this case f.write_str) and a different mechanism to format the underlying part (in this calling fmt::Display::fmt again). But that formatting is handled internally in a way that ends up being more convenient for the user. No manual buffer manipulation here.With that change, we have parity with our C++ implementation for the standard specifiers:let p = Point { x: 100, y: 200 };
println!(""{}"", p);      // (x=100, y=200)
println!(""{:*^7}"", p);  // (x=**100**, y=**200**)
println!(""{:#x}"", p);   // error
Well… we almost have parity.A Constellation of TraitsThe error we get from the above call is:error[E0277]: the trait bound `Point: LowerHex` is not satisfied
  --> src/main.rs:22:23
   |
22 |     println!(""{:#x}"", p);
   |                       ^ the trait `LowerHex` is not implemented for `Point`
   |
   = help: the following other types implement trait `LowerHex`:
             &T
             &mut T
             NonZeroI128
             NonZeroI16
             NonZeroI32
             NonZeroI64
             NonZeroI8
             NonZeroIsize
           and 21 others
note: required by a bound in `ArgumentV1::<'a>::new_lower_hex`
   = note: this error originates in the macro `$crate::format_args_nl` which comes from the expansion of the macro `arg_new` (in Nightly builds, run with -Z macro-backtrace for more info)

For more information about this error, try `rustc --explain E0277`.
error: could not compile `playground` due to previous error
The key really is just the first line. {} and {:*^7} require fmt::Display, but {:x} (and {:#x}, etc.) don’t go through fmt::Display. They instead go through an entirely different, unrelated trait: fmt::LowerHex.As the name implies, there’s not just fmt::LowerHex for x. There’s also fmt::UpperHex for X. And even that’s not all of them. In total, there are nine formatting traits:TraitKindBinarybDebug?DisplayotherLowerExpeLowerHexxOctaloPointerpUpperExpEUpperHexXWhat this means is that - if we wanted to support printing Point in all the different ways that you can print an i32, we need to implement nine traits. Which all would look exactly the same as what I showed for Display, just substituting the name Display for all the other names.The one odd exception is that implementing Debug doesn’t just give you {:?} but also {:x?} and {:X?} (but that’s it - so you can do debug hex, but not debug exponent?). I’m not sure why that’s the case.Using custom specifiersWith C++, implementing a formatter for T requires writing a parse() function. That parse() gets, basically, a std::string_view and can interpret its contents however it wants.With Rust, implementing any of the formatting traits (let’s just stick with Display) requires just writing fmt, and you get the parsed specifiers ready-made for use. That fmt::Formatter object is what you get. Full stop. Which is quite nice when that’s what you want, but there’s no way to get anything else.The Rust docs have this example:use std::fmt;

#[derive(Debug)]
struct Vector2D {
    x: isize,
    y: isize,
}

impl fmt::Display for Vector2D {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        // The `f` value implements the `Write` trait, which is what the
        // write! macro is expecting. Note that this formatting ignores the
        // various flags provided to format strings.
        write!(f, ""({}, {})"", self.x, self.y)
    }
}

// Different traits allow different forms of output of a type. The meaning
// of this format is to print the magnitude of a vector.
impl fmt::Binary for Vector2D {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        let magnitude = (self.x * self.x + self.y * self.y) as f64;
        let magnitude = magnitude.sqrt();

        // Respect the formatting flags by using the helper method
        // `pad_integral` on the Formatter object. See the method
        // documentation for details, and the function `pad` can be used
        // to pad strings.
        let decimals = f.precision().unwrap_or(3);
        let string = format!(""{:.*}"", decimals, magnitude);
        f.pad_integral(true, """", &string)
    }
}

fn main() {
    let myvector = Vector2D { x: 3, y: 4 };

    println!(""{myvector}"");       // => ""(3, 4)""
    println!(""{myvector:?}"");     // => ""Vector2D {x: 3, y:4}""
    println!(""{myvector:10.3b}""); // => ""     5.000""
}
The example demonstrates different format specifiers producing very different kinds of outputs:{} goes through Display, and just prints (x, y){:?} goes through Debug, which is #[derive]-ed, so you get the type name and then all the members{:b} goes through Binary which… prints the magnitude. Because b for… bagnitude?The example also ends up demonstrating the limitation of having only standard specifiers. That’s simply all that you have available to you, so you have to pick from what’s there.In C++, I showed how you can have a Point that’s printed either cartesian or polar, by providing that specifier, which is fairly straightforward to implement. In Rust, we can use ‘p’ (that’s fmt::Pointer) but you can’t use either c or r - those letters just aren’t available to you.There is technically the ability to use an arbitrary character, but only with alignment. That is:impl fmt::Display for Point {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match f.align().map(|_a| f.fill()).unwrap_or('c') {
            'c' | 'r' => {
                write!(f, ""(x={}, y={})"", self.x, self.y)
            }
            'p' => {
                write!(f, ""(r={:.4}, theta={:.4})"", self.r(), self.theta())
            }
            _ => Err(fmt::Error),
        }
    }
}

fn main() {
    let p = Point { x: 100, y: 200 };
    println!(""{:c^}"", p); // (x=100, y=200)
    println!(""{:p^}"", p); // (r=223.6068, theta=1.1071)
}
This… works, but just seems like an especially weird way to do this, and is highly limited anyway.I used the f.align().map(...) approach above with the goal of parity with the C++ implementation: by default, the format is cartesian. Otherwise, the only allowed characters are c, r, and p. If I just checked f.fill(), the default value is a space. But if I matched on space or c or r, I would allow {: ^}, which I don’t want to.Maybe that’s not a big deal, since I doubt anybody writes this sort of thing anyway.Comparing The Two ModelsNow that we’ve seen a brief introduction to the C++ and Rust formatting models, we can start to meaningfully compare the two.A simpler modelIn Rust, if we stick with the subset of standard specifiers supported by Formatter (basically: fill, align, width, precision, sign), then the Rust approach lets you do a lot of things easier. Because the buffer writing is hidden from you, you don’t have to worry about the keeping the writing in sync - the implementation of Display for Point supporting these specifiers is shorter than the C++ one.The other notable advantage of Rust’s fmt::Formatter is that it provides a lot of helper functions to get the various specifier state out and to use it. The Formatter will tell you the fill, the alignment, the width, the precision, the sign. And it has member functions to help you write things padded.In C++, whether in {fmt} or the standard library, there are no such helpers. You can use fmt::formatter<int>, as I did above, to be able to do more complex formatting - but the only members that type exposes are parse() and format(). So if you want to support these specifiers yourself, you kind of just have to implement all of that logic… yourself.ChronoOn the other hand, the custom specifier support allows for a lot more functionality to be handled by the formatting library itself. Let’s say I want to print today’s date in UTC. I can do that:std::println(""Today is {:%Y-%m-%d}"", std::chrono::system_clock::now());
Chrono basically has its own mini-language of placeholders that you can use to build up its format specifier. That is pretty cool, and very useful. You can’t do that in Rust - since there’s no notion of custom specifier at all, and something this complicated you can’t just hack into the fill character.Rust also has a chrono library, which gives you time points that are printable:let now = chrono::offset::Utc::now();
println!(""{}"", now);    // 2023-01-02 03:18:57.477157070 UTC
println!(""{:?}"", now);  // 2023-01-02T03:18:57.477157070Z
Rust’s chrono crate does support this sort of arbitrary placeholder logic that C++ does, and with the same placeholder syntax too. It’s just that you have to do it differently:let now = chrono::offset::Utc::now();
println!(""Today is {}"", now.format(""%Y-%m-%d"")); // Today is 2023-01-02
This works, and is fine for this simple example. But it doesn’t scale particularly well. Since what happens when we move up to more complex structures. Like…RangesRust has a very different philosophy for formatting ranges than C++ does.In C++, if I try to print a container and then an adapted version of it:auto v = std::vector{1, 2, 3, 4, 5};
std::println(""{}"", v);
std::println(""{}"",
    v | std::views::transform([](int i){
        return i * i;
    }));
That prints:[1, 2, 3, 4, 5]
[1, 4, 9, 16, 25]
Which, I think, is probably what most people want and expect. But Rust’s approach here is quite different:let v = vec![1, 2, 3, 4, 5];
println!(""{:?}"", v);
println!(""{:?}"", v.iter().map(|i| i * i));
That, instead, prints:[1, 2, 3, 4, 5]
Map { iter: Iter([1, 2, 3, 4, 5]) }
Neither Vec<T> nor any of the iterators implement fmt::Display, only fmt::Debug.I quoted from the Rust documentation earlier that, for fmt::Debug:Output will typically represent the internal state as faithfully as possible.So the goal of Debug for Map isn’t necessarily to print the elements that you get out - it’s to represent the internal state of the Map itself. That does make sense on some level.But Map doesn’t implement fmt::Display. None of these types do. So what do I do if I did want the [1, 4, 9, 16, 25] output that I get out of the box in C++? There’s actually no solution in the Rust standard library. We instead of have to turn to the itertools create to add some extensions for us:println!(""[{}]"", v.iter().map(|i| i * i).format("", ""));
The format function (on the Itertools trait) basically returns some new type that itself implements fmt::Display, using the specifier to print each element and the provided string as the delimiter.Now, if I want to format the elements of a range differently from just {}, I can do that in both Rust and C++:std::println(""{::*^5}"", v);
vsprintln!(""[{:*^5}]"", v.iter().format("", ""));
Both of these print:[**1**, **2**, **3**, **4**, **5**]
Though we get there in wildly different ways. In C++, the formatter for ranges uses the underlying type’s formatter to parse the element-specific format specifier (after the second colon), and we use that formatter to print every element. In Rust, the format() hook returns a new object which implements Display by using the provided Formatter to fmt each element, separated by the provided delimiter.Now here’s the question: what if I had a range of dates, and I wanted to print them all with the %Y-%m-%d format? In C++, that’s exactly the same idea as the previous example: we’re just providing the format specifier we want to use for each element:std::println(""{::%Y-%m-%d}"", dates);
But in Rust, for the chrono time point, this isn’t a format specifier. We had to do this whole other function call to get that behavior. So we need a new mechanism to solve this problem. The itertools crate provides this for us under the name format_with:println!(
    ""[{}]"",
    dates
        .iter()
        .format_with("", "", |elt, f| f(&elt.format(""%Y-%m-%d"")))
);
format_with is the most general API and probably lets you do anything you want. But it’s now fairly complicated, and differs quite a lot from the simple case. It may be helpful to place these examples back to back to make this more clear - in both cases, I’m formatting a range of some element type with some particular choice of specifier:// C++
std::println(""{::*^5}"", v);
std::println(""{::%Y-%m-%d}"", dates);
vs// Rust
println!(""[{:*^5}]"", v.iter().format("", ""));
println!(
    ""[{}]"",
    dates
        .iter()
        .format_with("", "", |elt, f| f(&elt.format(""%Y-%m-%d"")))
);
Unsupported SpecifiersOne other notable difference between the two is in their handling of unsupported specifiers. In the very first examples I showed for each language, I demonstrated how to format a Point as (x=1, y=2). The Rust implementation was shorter, but the C++ one wasn’t exactly a leviathan.But the two implementations weren’t exactly equivalent. The C++ approach only supported that case. Attempting to use any other specifier would have been a compile error:std::println(""{}"", p);     // (x=1, y=2)
std::println(""{:*^7}"", p); // error
std::println(""{:+}"", p);   // error
But the Rust approach would allow and simply ignore any specifier that wasn’t used in the implementation:println!(""{}"", p);     // (x=1, y=2)
println!(""{:*^7}"", p); // (x=1, y=2)
println!(""{:+}"", p);   // (x=1, y=2)
The specifiers still have to be valid - Rust would still reject {:7^*}, the thing that I seemingly always want to type instead. But we didn’t use any of specifiers the user provided in our implementation, so the output is the same.Is it better to reject unsupported specifiers, or ignore unsupported specifiers? Good question.Debug RepresentationIn Rust, debug formatting is a first-class citizen via fmt::Debug. And, as I mentioned at the very top of this blog, it’s extremely easy to opt into via #[derive(Debug)].In C++, debug formatting is also a thing, that notably surfaces in formatting ranges: formatting ""hello""s prints hello but formatting vector{""hello""s} prints [""hello""]. Notice the quotes: that comes from the debug representation. It’s not just quoting, the string will also be escaped (e.g. newlines will be formatted as the two-character sequence \n).But while in Rust, ? is just a first-class specifier that everybody can rely on, that simply isn’t the case in C++. Not all C++ types support debug formatting with ?, and whether a type does or not is entirely up to that type author. std::string supports it, but int does not, for instance. Even worse than that, a type could support a ? specifier whose meaning has nothing whatsoever to do with debugging (in the same way my Point example used p in a way that has nothing to do with pointers).That’s why in C++, I had to come up with a completely different mechanism for choosing the debug representation: an optional member function on formatter named set_debug_format(). And this design is still in flux, since originally this was a function that took no arguments, but it may need to change to take a bool (to enable or disable debug formatting). This would simply not be an issue we would have to think about if, in C++, we had any control over specifiers. But we don’t. That’s one of the downsides for allowing users to do anything they want: they can do anything they want.ConclusionOn the usage side, the C++ and Rust models of formatting look very similar. Nearly identical even. But they have a surprising amount of differences.In Rust, {:?} always works for every type and is always some debug-friendly formatting. In C++, ? isn’t special at all, so types support it or not as they see fit. It works for std::string, but not int. Instead, we have a completely different approach to debug representation, which is still in flux.In C++, each type can support whatever specifiers it wants. In Rust, there is one global fixed set of specifiers that is parsed by the implementation. This means that the Rust ecosystem is more consistent and coherent, since there’s only one way of doing things. But it also means that the many times that custom specifiers would prove useful, Rust needs some ad hoc escape hatch, which is just a different kind of inconsistency. It also means that Rust users will just abuse other formatting traits to solve their customization needs (such as the docs themselves demonstrating using Binary to display a magnitude), so you can end up with choices of specifiers that make no sense.In C++, writing wrappers that propagate all specifiers is mildly tedious due to needing to keep the context and output iterators in sync, but otherwise fairly straightforward. In Rust, this seems like it should be less tedious, since the formatter manages all of its state directly, except that you actually have to implement nine traits to do this, which seems… like an odd design decision to me.In C++, the library doesn’t provide any tools to help you parse things like fill, alignment, and width, so you have to implement them yourself. Which is not trivial. In Rust, you don’t need a tool to parse the specifiers since you just get the result of the parse.In C++, ranges are formattable. In Rust, iterators only implement fmt::Debug but in a way that logs their state, not the underlying elements. You need to include the itertools create to actually format ranges, but it ends up being a bit complicated due to the way that custom specifiers end up being handled.Which approach is better? I think different people could react very differently to those paragraphs.On the whole, I was surprised at how different Rust’s and C++’s approaches ended up being to solving the same problem, and I thought it was interesting to really consider the the implications of them.Implementing Rust’s approach with C++As is usually the case with C++ 5, you can implement the Rust model in C++, but you cannot implement the C++ model in Rust.To implement the Rust model, you can write a general formatter object:struct GeneralFormatter {
    uint32_t flags;
    char fill;
    Alignment align;
    std::optional<size_t> width;
    std::optional<size_t> precision;

    constexpr auto parse(auto& ctx) {
        // I am not even going to try
    }

    template <typename T>
    auto format(auto& object, auto& ctx) const {
        // I am not even going to try
    }
};
And then just use that to implement all of your parse() functions and as your helper for all of your format() functions.Improving C++’s approachThere are a few things that make Rust’s approach more user-friendly that I think we can, and should, pursue.The biggest two, overwhelmingly, are the ergonomic features I mentioned earlier. Static reflection will allow us to provide a #[derive(Debug)] equivalent and there is ongoing discussion about how to support interpolated literals in C++ for use with formatting.One of the difficulties (though not the only one) with supporting interpolated literals is precisely this issue of custom specifiers. For instance, how do you make this work:std::println(f""{name:>{width}}"");
Getting the name part is fine, but what do you do with width? If name happens to be a std::string, then >{} means right-aligned using the next format argument as the width. In which case {width} should be interpreted as interpolating width – doing name lookup on width and evaluating it.But if name is some other type, like an acme::Widget, then >{width} could mean anything. It could be a dynamic width like std::string and int and so forth. Or it could be a placeholder syntax similar to chrono, where >{width} is a request to print the Widget’s width, having nothing to do with alignment whatsoever, and certainly not a request for a variable named width. Would interpolation need to be something the user can hook into, so that if they do support dynamic arguments, they can interpolate, otherwise they don’t?The other problem is that, for an arbitrary user-defined type, the format specifier does not need to be balanced between { and }. I mean, it should be balanced, and it seems pretty hostile to try to come up with a reason to not be balanced. But this is C++, so since you can, somebody certainly will. But at least in this case, we can just say that if you play silly games, you win silly prizes: no string interpolation for you. Come back next year with sane bracing.Outside of these two language improvements, one of which really having nothing in particular to do with formatting, specifically, there are a few things that are probably worth thinking about:adding more member functions to format_context to make it more convenient to alternate between what kind of thing we’re formatting, which would also make it less error prone. Perhaps (using Rust’s names, as usual):  auto format(Point const& p, auto& ctx) const {
      return ctx.write_str(""(x="")
                .format(p.x, f)
                .write_str("", y="")
                .format(p.y, f)
                .write_str("")"")
                .out();
  }
adding something akin to the GenericFormatter type I showed earlier to make it easier for users to support common specifiers like padding.To be clear, both of these combined I think provide significantly less value than string interpolation which itself provides significantly less value than static reflection. But I do think these would provide some value, and I definitely think both are much easier problems to solve.Specifiers Are UsefulOutside of the significantly better ergonomics Rust provides, I do think C++’s general approach of allowing arbitrary specifiers is the better one. When used well (as in chrono or in range element formatting), it lets the user do a lot of complicated things quite concisely and consistently: the fact that formatting a range of dates and a range of integers looks the same is a big win.The specifier mini-language does have the potential to turn into line noise very quickly, so it’s certainly not a panacea. But on the whole I’d definitely rather have it than not.Even if I wish they were much terser. ↩Technically, in C++, a replacement-field is an optional arg-id followed by an optional format-specifier, where a format-specifier is a : followed by a format-spec. But while this makes sense grammatically, it’s a bit awkward in English to have “format specifier” and “format spec” be these subtly different things, so I’m going to (hopefully not super confusingly) use “format specifier” to refer to the stuff after the colon, which I think colloquially is how people actually think about this. ↩For those output iterators that aren’t already input iterators, I think the output iterator API is a bit lacking, and I go over this in more detail in my output iterators post ↩The fact that you can just write &dyn Write to get the language to give you a non-owning type-erased object (this is like std::function_ref, not std::function) in Rust is simply spectacular. ↩My CppNow 2021 and CPPP 2021 talks compared, in part, the C++ iterator model to the Rust iterator model. The talk showed how you can implement a Rust iterator with a C++ iterator pair pretty easily, but that you can’t do better than a C++ input iterator from a Rust iterator. ↩  c++ c++23 rust formatting This post is licensed under  CC BY 4.0  by the author. Share          Contents What's so hard about views::enumerate?-Trending Tags c++ c++20 concepts optional ranges c++17 spaceship span c++23 d © 2023 Barry Revzin. Some rights reserved. Powered by Jekyll with Chirpy theme.               
"
https://news.ycombinator.com/rss,Show HN: Cross-Platform GitHub Action,https://github.com/marketplace/actions/cross-platform-action,Comments,"





Marketplace
Actions
Cross Platform Action






play-circle





GitHub Action
Cross Platform Action




v0.9.0
Latest version







    Use latest version
 









play-circle






Cross Platform Action
Provides cross platform runner


Installation
Copy and paste the following snippet into your .yml file.












- name: Cross Platform Action
  uses: cross-platform-actions/action@v0.9.0



          Learn more about this action in cross-platform-actions/action












Choose a version







v0.9.0

                Cross Platform Action 0.9.0
              

 




v0.8.0

                Cross Platform Action 0.8.0
              

 




v0.7.0

                Cross Platform Action 0.7.0
              

 




v0.6.2

                Cross Platform Action 0.6.2
              

 




v0.6.1

                Cross Platform Action 0.6.1
              

 




v0.6.0

                Cross Platform Action 0.6.0
              

 




v0.5.0

                Cross Platform Action 0.5.0
              

 




v0.4.0

                Cross Platform Action 0.4.0
              

 




v0.3.1

                Cross Platform Action 0.3.1
              

 




v0.3.0

                Cross Platform Action 0.3.0
              

 








Cross-Platform GitHub Action
This project provides a GitHub action for running GitHub Action workflows on
multiple platforms. This includes platforms that GitHub Actions doesn't
currently natively support.
Features
Some of the features that are supported include:

Multiple operating system with one single action
Multiple versions of each operating system
Allows to use default shell or Bash shell
Low boot overhead
Fast execution

Usage
Here's a sample workflow file which will setup a matrix resulting in four jobs.
One which will run on FreeBSD 13.1, one which runs OpenBSD 7.2, one which runs
NetBSD 9.2 and one which runs OpenBSD 7.2 on ARM64.
name: CI

on: [push]

jobs:
  test:
    runs-on: ${{ matrix.os.host }}
    strategy:
      matrix:
        os:
          - name: freebsd
            architecture: x86-64
            version: '13.1'
            host: macos-12

          - name: openbsd
            architecture: x86-64
            version: '7.2'
            host: macos-12

          - name: openbsd
            architecture: arm64
            version: '7.2'
            host: ubuntu-latest

          - name: netbsd
            architecture: x86-64
            version: '9.2'
            host: ubuntu-latest

    steps:
      - uses: actions/checkout@v2

      - name: Test on ${{ matrix.os.name }}
        uses: cross-platform-actions/action@v0.9.0
        env:
          MY_ENV1: MY_ENV1
          MY_ENV2: MY_ENV2
        with:
          environment_variables: MY_ENV1 MY_ENV2
          operating_system: ${{ matrix.os.name }}
          architecture: ${{ matrix.os.architecture }}
          version: ${{ matrix.os.version }}
          shell: bash
          run: |
            uname -a
            echo $SHELL
            pwd
            ls -lah
            whoami
            env | sort
Different platforms need to run on different runners, see the
Runners section below.
Inputs
This section lists the available inputs for the action.



Input
Required
Default Value
Description




run
✓
✗
Runs command-line programs using the operating system's shell. This will be executed inside the virtual machine.


operating_system
✓
✗
The type of operating system to run the job on. See Supported Platforms.


version
✓
✗
The version of the operating system to use. See Supported Platforms.


shell
✗
default
The shell to use to execute the commands. Defaults to the default shell for the given operating system. Allowed values are: default, sh and bash


environment_variables
✗
""""
A list of environment variables to forward to the virtual machine. The list should be separated with spaces.



All inputs are expected to be strings. It's important that especially the
version is explicitly specified as a string, using single or double quotes.
Otherwise YAML might interpet the value as a numeric value instead of a string.
This might lead to some unexpected behavior. If the version is specified as
version: 13.0, YAML will interpet 13.0 as a floating point number, drop the
fraction part (because 13 and 13.0 are the same) and the GitHub action will
only see 13 instead of 13.0. The solution is to explicitly state that a
string is required by using quotes: version: '13.0'.
Supported Platforms
This sections lists the currently supported platforms by operating system. Each
operating system will list which versions are supported.
OpenBSD (openbsd)



Version
x86-64
arm64




7.2
✓
✓


7.1
✓
✓


6.9
✓
✓


6.8
✓
✗



FreeBSD (freebsd)



Version
x86-64




13.1
✓


13.0
✓


12.4
✓


12.2
✓



NetBSD (netbsd)



Version
x86-64




9.2
✓



Runners
This section list the different combinations of platforms and on which runners
they can run.



Runner
OpenBSD
FreeBSD
NetBSD




Linux
✓
✓
✓


macos-10.15, macos-11, macos-12
✓
✓
✗



Under the Hood
GitHub Actions currently only support the following platforms: macOS, Linux and
Windows. To be able to run other platforms, this GitHub action runs the
commands inside a virtual machine (VM). If the host platform is macOS the
hypervisor can take advantage of nested virtualization.
The FreeBSD and OpenBSD VMs run on the xhyve hypervisor (on a macOS
host), while the other platforms run on the QEMU hypervisor (on a Linux
host). xhyve is built on top of Apple's Hypervisor
framework. The Hypervisor framework allows to implement hypervisors with
support for hardware acceleration without the need for kernel extensions. xhyve
is a lightweight hypervisor that boots the guest operating systems quickly and
requires no dependencies outside of what's provided by the system. QEMU is a
more general purpose hypervisor that runs on most host platforms and supports
most guest systems. It's a bit slower than xhyve because it's general purpose
and it cannot use nested virtualization on the Linux hosts provided by GitHub.
The VM images running inside the hypervisor are built using Packer.
It's a tool for automatically creating VM images, installing the guest
operating system and doing any final provisioning.
The GitHub action uses SSH to communicate and execute commands inside the VM.
It uses rsync to share files between the guest VM and the host. xhyve
does not have any native support for sharing files. To authenticate the SSH
connection a unique key pair is used. This pair is generated each time the
action is run. The public key is added to the VM image and the private key is
stored on the host. Since xhyve does not support file sharing, a secondar hard
drive, which is backed by a file, is created. The public key is stored on this
hard drive, which is then mounted by the VM. At boot time, the secondary hard
drive will be identified and the public key will be copied to the appropriate
location.
To reduce the time it takes for the GitHub action to start executing the
commands specified by the user, it aims to boot the guest operating systems as
fast as possible. This is achieved in a couple of ways:


By downloading resources, like the hypervisor and a few other
tools, instead of installing them through a package manager


No compression is used for the resources that are downloaded. The size is
small enough anyway and it's faster to download the uncompressed data than
it is to download compressed data and then uncompress it.


It leverages async/await to perform tasks asynchronously. Like
downloading the VM image and other resources at the same time


It performs as much as possible of the setup ahead of time when the VM image
is provisioned


Local Development
Prerequisites

NodeJS
npm
git

Instructions


Install the above prerequisites


Clone the repository by running:
git clone https://github.com/cross-platform-actions/action



Navigate to the newly cloned repository: cd action


Install the dependencies by running: npm install


Run any of the below npm commands


npm Commands
The following npm commands are available:

build - Build the GitHub action
format - Reformat the code
lint - Lint the code
package - Package the GitHub action for distribution and end to end testing
test - Run unit tests
all - Will run all of the above commands

Running End to End Tests
The end to end tests can be run locally by running it through Act. By
default, resources and VM images will be downloaded from github.com. By running
a local HTTP server it's possible to point the GitHub action to local resources.
Prerequisites

Docker
Act

Instructions


Install the above prerequisites


Copy test/workflows/ci.yml.example to
test/workflows/ci.yml


Make any changes you like to test/workflows/ci.yml, this is file ignored by
Git


Build the GitHub action by running: npm run build


Package the GitHub action by running: npm run package


Run the GitHub action by running: act --privileged -W test/workflows


Providing Resources Locally
The GitHub action includes a development dependency on a HTTP server. The
test/http directory contains a skeleton of a directory structure
which matches the URLs that the GitHub action uses to download resources. All
files within the test/http are ignore by Git.


Add resources as necessary to the test/http directory


In one shell, run the following command to start the HTTP server:
./node_modules/http-server/bin/http-server test/http -a 127.0.0.1

The -a flag configures the HTTP server to only listen for incoming
connections from localhost, no external computers will be able to connect.


In another shell, run the GitHub action by running:
act --privileged -W test/workflows --env CPA_RESOURCE_URL=<url>

Where <url> is the URL inside Docker that points to localhost of the host
machine, for macOS, this is http://host.docker.internal:8080. By default,
the HTTP server is listening on port 8080.






Stars

 


          Star
 29
  





Contributors



 

 


Categories


  Continuous integration


  Testing




Links



cross-platform-actions/action
    



Open issues
        1




Pull requests
      1




Report abuse
 

Cross Platform Action is not certified by GitHub. It is provided by a third-party and is governed by separate terms of service, privacy policy, and support documentation.
    




"
https://news.ycombinator.com/rss,"Late Hokusai: Thought, technique, society",https://www.britishmuseum.org/research/projects/late-hokusai-thought-technique-society,Comments,"




405 Not allowed


Error 405 Not allowed
Not allowed
Error 54113
Details: cache-iad-kiad7000064-IAD 1673903211 1393961676

Varnish cache server


"
https://news.ycombinator.com/rss,TAB electronics books,https://worldradiohistory.com/BOOKSHELF-ARH/Bookshelf_TAB.htm,Comments,"




TAB BOOLS LIBRARY: TAB electronics books
























			|
















 






  TAB Books 



														Search:
														Select TAB from Menu 













     Books from the TAB Library 
					in the areas of Broadcasting and Electronics










							TAB Books was originally based in Blue Ridge Summit, 
							Pennsylvania, TAB was founded by Verne M. Ray and 
							Malcolm Parks Jr. in 1964 to publish technically 
							oriented magazines; TAB is an acronym for Technical 
							Author's Bureau. It became TAB Books Inc. in 1980 
							and published books in a wide variety of mostly 
							technical fields. It was acquired by McGraw-Hill in 
							1990, at which time it published books in 12 fields 
							including computing, electronics, aviation, 
							engineering, maritime, and several how-to subjects

















									Tab Books 






 















The Complete Broadcast Antenna Handbook  
									Cunningham 1977

									TAB-72-Oscilloscope-Techniques-1958 

									 

 

									 





 


 













Radio Control Manual
									Safford - 1979

									Handbook of Advanced Robotics
									Safford 1982 

									Electronic Circuit Design Handbook 1975 

Build a Personal Earth Station 1982

									Color TV Troubles and Solutions
									 1972



















 




199 Test & Alignment Procedures

									Principles and Practice of Impedance


									The Mater IC Cookbook

4 Channel Stereo Sessions 1973 

									The Complete Handbook of Robotics
									Safford 1978 





 


 


 



 


 





Audio Systems Handbook 

									Basic Color Television 

									Impedance
									1976

How to Troubleshoot & Repair Electronic 
									Circuits 

									Troubleshooting Microprocessors & Digital 
									Logic 




















									 





Basic 
									Transistor Course

									Oscilloscope Techniques

									How to Design & Build Your Own Custom TV 
									Games

Radio Communications Receivers

									Understanding Electronics
									1989 






 

									 


 



 

									 



Master Handbook of Electronic Tables 

									Giant Handbook of Electronic Circuits 

									Electronic Databook
									3rd Edition

Electronic Components

									Digital Interfacing with an Analog World  
















									Tab  Library courtesy of
									
									ncwalz





















							Service Manual 
	#2
	Supp. 1
















"
https://news.ycombinator.com/rss,"GPU Caching Compared Among AMD, Intel UHD, Apple M1",https://chipsandcheese.com/2022/05/21/igpu-cache-setups-compared-including-m1/,Comments,"




iGPU Cache Setups Compared, Including M1




May 21, 2022
clamchowder

1 Comment



Like CPUs, modern GPUs have evolved to use complex, multi level cache hierarchies. Integrated GPUs are no exception. In fact, they’re a special case because they share a memory bus with CPU cores. The iGPU has to contend with CPUs for limited memory bandwidth, making caching even more important than with dedicated GPUs. 
At the same time, the integrated nature of integrated GPUs provides a lot of interesting cache design options. We’re going to take a look at paths taken by AMD, Intel, and Apple.
Global Memory Latency
GPUs are given a lot of explicit parallelism, so memory latency isn’t as critical as it is for CPUs. Still, latency can play a role. GPUs often don’t run at full occupancy – that is, the amount of parallel work they’re tracking isn’t maximized. We have more on that in another article, so we’ll go right to the data.
Testing latency is also a good way of probing the cache setup. Doing so with bandwidth isn’t as straightforward because requests can be combined at various levels in the memory hierarchy, and defeating that to get clean breaks between cache levels can be surprisingly difficult.


The Ryzen 4800H’s cache hierarchy is exactly what you’d expect from AMD’s well known GCN graphics architecture. Each of the 4800H’s seven GCN-based CUs have a fast 16 KB L1 cache. Then, a larger 1 MB L2 is shared by all of the CUs. AMD’s strategy for dealing with memory bus constraints appears quite simple: use a higher L2 capacity to compute ratio than that of discrete GPUs. A fully enabled Renoir iGPU has 8 CUs, giving 128 KB per CU. Contrast this with AMD’s Vega 64, where 4 MB of L2 gives it 64 KB per CU.

GCN’s simple, two-level cache hierarchy with a 16 KB L1 is instantly recognizable across generations and form factors. Thanks to cha0s for contributing data
Apple’s cache setup is similar, with a fast L1 followed by a large 1 MB L2. Apple’s L1 is half of AMD’s size at 8 KB, but has similar latency. This low latency suggests it’s placed within iGPU cores, though we don’t have a test to directly verify this. Compared to AMD, Apple’s L2 is a bit lower latency, which should help make up for the smaller L1. We also expect to see a 8 MB SLC, but that doesn’t really show up in the latency test. It could be the somewhat lower latency area up to 32 MB.
Then, we have Intel. Compared to AMD and Apple, Intel tends to use a less conventional cache setup. Right off the bat, we’re hitting a large cache shared by all of the GPU’s cores. It’s at least 1.5 MB in size, making it bigger than AMD and Apple’s GPU-level caches. In terms of latency, it’s somewhere between AMD and Apple’s L2 caches. That’s not particularly good, because we don’t see a smaller, faster cache in front of it. But its large size should help Intel keep more memory traffic within the iGPU block. Intel should have smaller, presumably faster caches in front of the large shared iGPU-level cache. But we weren’t able to see them through testing.
Like Apple, Intel has a large, shared chip-level cache that’s very hard to spot on a latency plot. This is strange – our latency test clearly shows the shared L3 on prior generations of Intel integrated graphics.

Looks like the shared chip-level L3 doesn’t help latency much in certain Intel iGPU designs
From this first glance at latency, we can already get a good idea of how each manufacturer approaches caching. Let’s move on to bandwidth now.
Global Memory Bandwidth
Bandwidth is more important to GPUs than to CPUs. Usually, CPUs only see high bandwidth usage in heavily vectorized workloads. For GPUs though, all workloads are vectorized by nature. And bandwidth limitations can show up even when cache hitrates are high.


AMD and Apple’s iGPU private caches have roughly comparable bandwidth. Intel’s is much lower. Part of that is because Alder Lake’s integrated graphics have somewhat different goals. Comparing the GPU configurations makes this quite obvious:
FP32 ALUsClock SpeedFP32 FMA Vector ThroughputAMD Ryzen 4800H, Vega 7448 (out of 512 possible)1.6 GHz1433.6 GFLOPsIntel Core i5-12600K, Xe GT12561.45 GHz742.4 GFLOPsApple M1, 7 Core iGPU896 (out of 1024 possible)1.278 GHz?2290.2 GFLOPs
AMD’s Renoir and Apple’s M1 are designed to provide low end gaming capability to thin and light laptops, where a separate GPU can be hard to fit. But desktop Alder Lake definitely expects to be paired with a discrete GPU for gaming. Understandably, that means Intel’s iGPU is pretty far down on on the priority list when it comes to power and die area allocation. Smaller iGPUs will have less cache bandwidth, so let’s try to level out the comparison by using vector FP32 throughput to normalize for GPU size.

Measured bandwidth values are used. If a cache level isn’t present, the next one down is used. GTX 1070 Max-Q was tested with Vulkan, because the L1 is not used for OpenCL. Intel shows no faster cache level even with Vulkan
Intel’s cache bandwidth now looks better, at least if we compare from L2 onward. Bytes per FLOP is roughly comparable to that of other iGPUs. Its shared chip-level L3 also looks excellent, mostly because its bandwidth is over-provisioned for such a small GPU.
As far as caches are concerned, AMD is the star of the show. Renoir’s Vega iGPU enjoys higher cache bandwidth to compute ratios than Intel or Apple. But its performance will likely be dependent on cache hitrate. L2 misses go directly to memory, because AMD doesn’t have another cache behind it. And Renoir has the weakest memory setup of all the iGPUs here. DDR4 may be flexible and economical, but it’s not winning any bandwidth contests. Apple and Intel both have a stronger memory setup, augmented by a big on-chip cache. 
Local Memory Latency
GPU memory access is more complicated than on CPUs, where programs access a single pool of memory. On GPUs, there’s global memory that works like CPU memory. There’s constant memory, which is read only. And there’s local memory, which acts as a fast scratchpad shared by a small group of threads. Everyone has a different name for this scratchpad memory. Intel calls it SLM (Shared Local Memory), Nvidia calls it Shared Memory, and AMD calls it LDS (Local Data Share). Apple calls it Tile Memory. To keep things simple, we’re going to use OpenCL terminology, and just call it local memory.

Pointer chasing latency with local memory
AMD and Apple take about as long to access local memory as they do to hit their first level caches. Of course, latency isn’t the whole story here. Each of AMD’s GCN CUs has 64 KB of LDS – four times the capacity of its L1D cache. Bandwidth from local memory is likely higher too, though we currently don’t have a test for that. Clinfo on M1 shows 32 KB of local memory, so M1 has at least that much available. That figure likely only indicates the maximum local memory allocation by a group of threads, so the hardware value could be higher.
Intel meanwhile enjoys very fast access to local memory, as does Nvidia, which is here for perspective. Their story is an interesting one too. Prior to Gen10, Intel put their SLM along the iGPU’s L3, outside the the subslices (Intel’s cloest equivalent to GPU cores on Apple and CUs on AMD). For a long time, that meant Intel iGPUs had unimpressive local memory latency.

Good job Intel
Starting with Gen 11, Intel thankfully moved the SLM into the subslice, making the local memory configuration similar to AMD and Nvidia’s. Apple likely does the same (putting “tile memory” within iGPU cores) since local memory latency on Apple’s iGPU is also quite low. 
CPU to GPU Copy Bandwidth
A shared, chip-level cache can bring other benefits. In theory, transfers between CPU and GPU memory spaces can go through the shared cache, basically providing a very high bandwidth link between the CPU and GPU. Due to time and resource constraints, slightly different devices are tested here. But Renoir and Cezanne should be similar, and Intel’s behavior is unlikely to regress from Skylake’s.

Only Intel is able to take advantage of the shared cache to accelerate data movement across different blocks. As long as buffer sizes fit in L3, Skylake handles copies completely within the chip, with performance counters showing very little memory traffic. Larger copies are still limited by memory bandwidth. The Core i7-7700K tested here only has a dual channel DDR4-2400 setup, so that’s not exactly a strong point. 
Apple in theory should be able to do the same. However, we don’t see an improvement for small copy sizes that should fit within M1’s system level cache. There are a couple of explanations. One is that M1 is unable to keep CPU to GPU transfers on-die. Another is that small transfers are kept on-die, but commands to the GPU suffer from very high latency, resulting in poor performance for small copies. Intel’s Haswell iGPU suffers from the same issue, so the second is a very likely explanation. When we get to larger copy sizes, M1’s high bandwidth LPDDR4X setup does a very good job.
AMD’s performance is very easy to understand. There’s no shared cache, so bandwidth between the CPU and GPU is limited by memory bandwidth. 
Finally, it’s worth noting that all of the iGPUs here, as well as modern dedicated GPUs, can theoretically do zero-copy transfers by mapping the appropriate memory on both the CPU and GPU. But we currently don’t have a test written to analyze transfer speeds with mapped memory.
Final Words
GPUs tend to be memory bandwidth guzzlers, and feeding an integrated GPU is particularly challenging. Their memory subsystems are typically not as beefy as those of dedicated GPUs. To make matters worse, the iGPU has to fight with the CPU for memory bandwidth. 
Apple and Intel both tackle this challenge with sophisticated cache hierarchies, including a large on-chip cache that serves the CPU and GPU. The two companies take different approaches to implementing that cache, based on how they’ve evolved their designs. Intel has the most integrated solution. Its L3 cache does double duty. It’s tied very closely to the CPU cores on a high speed ring interconnect, in order to provide low latency for CPU-side accesses. The iGPU is simply another agent on the ring bus, and L3 slices handle iGPU and CPU core requests in the same way.

Intel has a fast, flexible ring bus that can connect a variety of agents. The iGPU is simply another agent on the ring bus
Apple uses more specialized caches instead of trying to optimize one cache for both the CPU and GPU. M1 implements a 12 MB L2 cache within the Firestorm CPU cluster, which fills a similar role to Intel’s L3 from the CPU’s perspective. A separate 8 MB system level cache helps reduce DRAM bandwidth demands from all blocks on the chip, and acts as a last stop before hitting the memory controller. By dividing up responsibilities, Apple can tightly optimize the 12 MB L2 for low latency to the CPU cores. Because the L2 is large enough to absorb the bulk of CPU-side requests, the system level cache’s latency can be higher in order to save power.
M1 still has a bit of room for improvement. Its cache bandwidth to compute ratio could be a touch higher. Transfers between the CPU and GPU could take full advantage of the system level cache to improve bandwidth. But these are pretty minor complaints, and overall Apple has a pretty solid setup.

Apple has a slower, SoC-wide interconnect and a variety of blocks on it. Only the Firestorm cluster and GPU are drawn to save space
AMD’s caching setup is bare-bones in comparison. Renoir (and Cezanne) are basically a CPU and GPU glued together. Extra GPU-side L2 is the only concession made to reduce memory bandwidth requirements. And “extra” here only applies in comparison to discrete GCN cards. 1 MB of L2 isn’t anything special next to Apple and Intel, both of which have 1 MB or larger caches within their iGPUs. If the L2 is missed, AMD goes straight to memory. Memory bandwidth isn’t exactly AMD’s strong point, making Renoir’s lack of cache even worse. Renoir’s CPU-side setup isn’t helping matters either. A L3 setup that’s only 1/4 the size of desktop Zen 2’s will lead to additional memory traffic from CPU cores, putting even more pressure on the memory controller. 

Cezanne has a pretty similar layout, but with Zen 3 cores in a single core complex
AMD’s APU caching setup leaves a lot to be desired. Somehow, AMD’s iGPU still manages to be competitive against Intel’s Tiger Lake iGPU, which speaks to the strength of their GCN graphics architecture. I just wish they took advantage of that potential to deliver a killer APU. After all, AMD has a lot of low hanging fruit to improve with. RDNA 2 based discrete GPUs use a large “Infinity Cache” sitting behind Infinity Fabric to reduce memory bandwidth requirements. Experience gained implementing that cache could trickle down to AMD’s integrated GPUs. 
AMD’s RDNA2 uses a giant 128 MB Infinity Cache to compete with Nvidia while having a lot less memory bandwidth
It’s easy to imagine an Infinity Cache delivering benefits beyond reducing GPU memory bandwidth requirements too. For example, the cache could enable faster copies between GPU and CPU memory. And it could benefit CPU performance, especially since AMD likes to give their APUs less CPU-side L3 compared to desktop chips.
But such a move is unlikely with in the next generation or two. With AMD moving to LP/DDR5, the bandwidth boost along with large architecture changes allowed AMD to double iGPU performance with Rembrandt. Factor in Renoir and Cezanne’s already adequate graphics performance, Intel’s inability to capitalize on their superior cache setup, and Apple’s closed ecosystem, there’s little pressure on AMD to make aggressive moves. 
Infinity cache on an APU will also require significant die area to be effective. Hitrate with a 8 MB system level cache is abysmal:

28% bandwidth reduction implies a 28% hitrate. Source – ARM
Cache hitrate tends to increase with the logarithm of size, so AMD would probably want to start with at least 32 MB of cache to make it worth the effort. That means a bigger die, and unfortunately, I’m not sure if there’s a market for a powerful APU in the consumer x86 realm.
If you like our articles and journalism and you want to support us in our endeavors then consider heading over to our Patreon or our PayPal if you want to toss a few bucks our way or if you would like to talk with the Chips and Cheese staff and the people behind the scenes then consider joining our Discord.
Test Setup
Memory SetupNotesAMD Ryzen 4800H (Renoir)Dual channel DDR4-3200 22-22-22-52Eluktronics RP-15 laptopIntel Core i5-12600K (Alder Lake)2x DDR5-4800 CL40Thanks to Luma for running the testsApple M1On-package LPDDR4XMacbook Air with 7 core GPU, thanks to Longhorn for running tests

Author







clamchowder



View all posts
















Please leave this field empty Don’t miss our articles!

Email Address *






Check your inbox or spam folder to confirm your subscription.

 




Related Posts








Post navigation
← Examining Centaur CHA’s Die and Implementation GoalsGraviton 3: First Impressions →



1 thought on “iGPU Cache Setups Compared, Including M1” 





 Alan says: 

January 16, 2023 at 7:33 pm 


> as modern dedicated GPUs, can theoretically do zero-copy transfers by mapping the appropriate memory on both the CPU and GPU.
Is this true for dgpus? How does this work?

Reply 



Leave a Reply Cancel reply










This site uses Akismet to reduce spam. Learn how your comment data is processed.

"
https://news.ycombinator.com/rss,Granian – a Rust HTTP server for Python applications,https://github.com/emmett-framework/granian,Comments,"








emmett-framework

/

granian

Public







 

Notifications



 

Fork
    9




 


          Star
 415
  









        A Rust HTTP server for Python applications
      
License





     BSD-3-Clause license
    






415
          stars
 



9
          forks
 



 


          Star

  





 

Notifications












Code







Issues
8






Pull requests
0






Discussions







Actions







Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Discussions
 


                  Actions
 


                  Security
 


                  Insights
 







emmett-framework/granian









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











master





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





13
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




gi0baro

Update CI release workflow




        …
      




        b843a90
      

Jan 13, 2023





Update CI release workflow


b843a90



Git stats







132

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github



Update CI release workflow



Jan 13, 2023









benchmarks



Update benchmarks



Jan 13, 2023









docs/spec



Fix typos (#14)



Nov 17, 2022









granian



Follow WSGI spec on response iterable (#29)



Jan 13, 2023









lib/pyo3-asyncio



Bump pyo3-asyncio to 0.17



Oct 25, 2022









src



Code cleanup



Jan 13, 2023









tests



Fix wsgi.input out of spec (close #24)



Jan 12, 2023









.gitignore



first implementation



Apr 15, 2022









Cargo.lock



Add PyPy support



Jan 3, 2023









Cargo.toml



Add PyPy support



Jan 3, 2023









LICENSE



first implementation



Apr 15, 2022









README.md



Update benchmarks results



Dec 24, 2022









build.rs



Add PyPy support



Jan 3, 2023









pyproject.toml



Add PyPy support



Jan 3, 2023









setup.py



review package meta



Apr 18, 2022




    View code
 















Granian
Rationale
Features
Quickstart
Project status
License





README.md




Granian
A Rust HTTP server for Python applications.
Rationale
The main reasons behind Granian design are:

Have a single, correct HTTP implementation, supporting versions 1, 2 (and eventually 3)
Provide a single package for several platforms
Avoid the usual Gunicorn + uvicorn + http-tools dependency composition on unix systems
Provide stable performance when compared to existing alternatives

Features

Supports ASGI/3, RSGI and WSGI interface applications
Implements HTTP/1 and HTTP/2 protocols
Supports HTTPS
Supports Websockets over HTTP/1 and HTTP/2

Quickstart
You can install Granian using pip:
$ pip install granian

Create an ASGI application in your main.py:
async def app(scope, receive, send):
    assert scope['type'] == 'http'

    await send({
        'type': 'http.response.start',
        'status': 200,
        'headers': [
            [b'content-type', b'text/plain'],
        ],
    })
    await send({
        'type': 'http.response.body',
        'body': b'Hello, world!',
    })
and serve it:
$ granian --interface asgi main:app

You can also create an app using the RSGI specification:
async def app(scope, proto):
    assert scope.proto == 'http'

    proto.response_str(
        status=200,
        headers=[
            ('content-type', 'text/plain')
        ],
        body=""Hello, world!""
    )
and serve it using:
$ granian --interface rsgi main:app

Project status
Granian is currently under active development.
Granian is compatible with Python 3.7 and above versions on unix platforms and 3.8 and above on Windows.
License
Granian is released under the BSD License.









About

      A Rust HTTP server for Python applications
    
Topics



  python


  rust


  http


  http-server


  asyncio


  asgi



Resources





      Readme
 
License





     BSD-3-Clause license
    



Stars





415
    stars

Watchers





7
    watching

Forks





9
    forks







    Releases
      13







Granian 0.2.1

          Latest
 
Jan 13, 2023

 

        + 12 releases





Sponsor this project



 

 

 Sponsor
 
Learn more about GitHub Sponsors







    Packages 0


        No packages published 







        Used by 6
 




























    Contributors 4








gi0baro
Giovanni Barillari

 






kianmeng
Kian-Meng Ang

 






shulcsm
Mārtiņš Šulcs

 






cirospaciari
Ciro Spaciari

 





Languages












Rust
83.1%







Python
16.5%







Other
0.4%











"
https://news.ycombinator.com/rss,Adventures in Mastoland,https://searchtodon.social/Adventures-in-Mastoland.html,Comments,"    
 Adventures in Mastoland

We strongly advise you to read up on the myriad of failed experiments in this space.



Great! Where can I find them?



…hm?

This is a retrospective post about my experiment Searchtodon, an attempt at building a privacy conscious personal timeline search tool for Mastodon. I’m intentionally vague about people and projects relevant to this story, to protect the innocent. Titles used are lighthearted pop-culture references and do not semantically reflect on the content or my opinion.
Last updated 2023-01-16.
Introduction
I’ve been online since ~1997, AOL Chat, Forums, AIM, jabber, IRC, the usual, and then Twitter since 2007, did a first Mastodon exploration in 2017 and finally got swept over fully in November 2022. By all accounts, I’m new here, but none of this is new to me.
A few years ago, I built a toy Twitter web client for myself for experimenting with getting more out of my timeline: e.g. don’t show RTs, but show my top 10 RTs for the last 12/24 hours. Something similar I hear Twitter Pro/Blue/whatever has got now, by way of acquiring nuzzle, with top links posted from your timeline.
I’m not a very good frontend web developer, so this didn’t go far, but one thing that this left me convinced with is that there is a distinct lack of things you can do with your timeline. I put ~15 years into meeting lovely folks around the globe and connecting with them over Twitter, but the default experience does not allow me to get the most out of it. E.g. say I follow more people than I can possibly read all posts from, it’d be great if there was a “slow” section, where people that rarely post are listed, so I don’t miss any of their posts while other folks are more busy, or just 12 hours worth of time zones away, without me having to put that together as a list manually.
As an aside, I know I could follow fewer people, but let’s be real, that’s not gonna happen. I have about a dozen of other ideas what useful things can be done with your carefully curated timeline and I believe there is an opportunity for a lot of fun and useful things that can be done with using someone’s social timeline as the data source and better connect people that way.
One of the things I’ve missed on Twitter for the longest time is being able to recall what I’ve seen before. While Twitter has/had search, to my knowledge there is no way to filter by tweets that have shown up in your timeline. This is honestly baffling to me, and I don’t see this getting fixed any time soon. With my custom client, I could have built that, but never got around to, mainly because I feared it not being worth it (the other work having been a nice learning exercise), because Twitter could take it out any time, as shown with the native clients’ debacle just this week. — I’m not interested in building anything for that platform for the time being.
Into the Mastoverse
With taking the plunge into moving most of my social online activity to Mastodon in late 2022, I am intrigued by the possibilities of an open platform. I believe the biggest long-term impact this has is finally getting the social graph into the open, that’s amazing. But with my small demo on Twitter I knew it was feasible to bring some of my ideas over here and I don’t have to worry about losing access because of the whims of the centralised owners of a commercial platform. This is very very appealing for a lot of tinkery-folk like me. If you are reading this, you are probably one of them.
Since Ivory timeline filtering is already mature enough to cover the needs of the client I wrote for Twitter (no boosts, no replies), I had no need to port that over, and instead thought I go after the next most pressing thing: recalling things I have seen in my timeline. In my case, I follow a lot of techies in very different communities (Web dev, JavaScript, Erlang, Python, Rust, macOS, FreeBSD, Databases etc.) and as a result I sweep up a lot of information that I’m generally interested in, but only occasionally dip into more deeply. The effect here is that when I know my macOS folks talk about certain issues for developers a new OS version brings, a while later, the JS community starts running into this issue, and now I can bridge that gap and can help out. But I can’t remember all the little details and references about everything, so I need a way to find things that I’ve seen before.
Yes, there are favs and bookmarks, but if I knew something was gonna be important, I’d have filed it away already otherwise, so that’s not really helping. Plus not all instances enable search on bookmarks.
I fully realise that many people have none of these problems, or are fine using bookmarks, and that’s great, but it doesn’t solve my problem, and I know now that I’m not the only one.
Act Two
I’ll spare you the details of technically getting to a point where I could search my timeline, but let’s just say a little service that runs as an OAuth app dumping plaintext files into a directory on my Mac and then using it’s built-in Spotlight search was done in about an hour or two.
However, it was clear that a “runs on your Mac” solution doesn’t work when your computer is asleep, and with the somewhat exasperating default 400 post timeline limit, not being able to catch up things properly would defeat the usefulness of this. — There is an argument to be made for: well if you are asleep and miss posts, you didn’t see them, so you can’t know to search for them. And while that reasoning is certainly correct, it misses that folks use multiple clients to access Mastodon at different times and that I might see a post in Ivory before going to bed, but after I closed my computer for the day. — I now concede however, that a “really only the posts I’ve seen” indexing would be preferable, but there is no cross-client standard for reporting that anywhere, so I’m not hopeful this will get anywhere. The only other option is: index your entire home timeline.
Attentive readers will point out that at the time of all this, I was on an instance who’s ToS state:

Content on this instance must not be archived or indexed wholesale by automated means by any user or service. Active users may export their data through the export functionality or the API.

When reviewing the feasibility of my project I read “wholesale” as ”everybody’s all the time” and “export their data through […] the API” as “keep a copy of my timeline” as covered as allowed. — I know now that I was mistaken in my, granted, optimistic reading of this, and that I should have reached out to the admins there to double check.
Back to November 2022: when first signing up for that instance I distinctly remember that one of the top configuration options for the profile after sign up was “Opt-out of search engine indexing” and it was checked by default for me (from here on out, I’ll refer to this feature as the noindex flag). I thought that it was really nice that this is such a prominent control feature empowering users and choosing a safe default.
This was my first mistake.
It turns out, not only do few people seem to know about this, almost nobody made a conscious choice here. And if I read things correctly, this flag does not federate, which seems like a tremendous oversight.
I reached out to a few folks that I saw have this enabled and discussed whether what I was building was considered “a search engine” (I didn’t think so), and I learned that a) there are folks that would continue to to use the noindex flag to not get indexed by Google, but they’d be fine being part of a “scoped to a user’s timeline and no one else” type of search (I too fell into this category at the time), and b) on properly considering this setting for the first time, and while agreeing that my thing wasn’t a Google-like public search engine, they’d expect the noindex flag to cause their posts to be excluded. — I did fear that this would diminish the usefulness of the search tool, but eventually came around to this point of view.
While talking to more folks, I was introduced to the #nobot hashtag that accounts use to indicate they don’t want to have anything to do with any bots, which I made to behave like the noindex flag, and I added #nosearch in case folks wanted to be more fine-grained with this.
Threats
Next, I needed to consider if I was adding new vectors of abuse to Mastodon (or strengthening old ones). My reasoning went thus:

if I was a bad actor, my 1-hour-index-to-spotlight experiment would give me all the benefits without anyone knowing about this. I am sadly convinced this is already happening, it is just so simple.

running a custom instance gives you admin rights to all posts that are being read on that instance and those posts live in a database that supports searching. And the ecosystem in general seems to fine with folks running their own instances.


I still don’t think that anything I did would make any of this easier for bad actors or worse for the community.
While considerng  other prior experiences that could inform this, I thought of Twitter’s third party thread unrollers that rehost tweets with advertising next to them. They were genuinely useful before Twitter fixed its rendering of threads, but it was annoying to me. Eventually, the services added a feature where the thread-poster could block the unroll-account and unrolling would no longer work. While annoyed that I had to do that four times, I could live with the power balance here. 

Mastodon isn’t Twitter, but I also thought this set enough of a precedent that honouring noindex, #nobot and #nosearch would be a decent enough equivalent.
This was my second mistake.
Prior Art
One of the biggest volume of feedback on this was that I should have looked at the search projects that came before. If those folks had taken the time to read the associated website, they’d have seen that I did, alas.
My calculus was:

noindex & friends are well established
no additional threat vector is introduced
indexing is limited to (mostly) what a human has seen
and searching is limited only ever to that one human
open to feedback & suggestions, especially critical
and promise to shut it down with enough negative signal

This is substantially different from the other “fediverse crawlers” (as later confirmed by some of the most fierce critics) that I saw, technically and in framing. I didn’t think this even qualifies as a crawler, but I did get some early feedback that folks would consider this a crawler, but I was optimistic that this would at least bring out valuable feedback for future iterations.
Finally, for the quick demo to validate the user experience and basic functionality to work, I put this together as an OAuth app that runs as web service. That way you can just sign into the UI and start using Mastodon as before, without having to run anything yourself. To give this an honest shot, I believed this needed to be easy to get started with.
A consequence of this is that, for the multiple hours this experiment ran, all indexing happened on one of my servers. Given the framing of this as an experiment with a direct goal to inform operational overhead for Mastodon operators (more on this below), and given that I generally know what I’m doing running web services, I thought that was a valuable trade-off to make for now.
That was my third mistake.
The Experiment
From the get-go, I framed Searchtodon as an experiment with three hypotheses to validate (or not). The quotes are from the Searchtodon website.

1. User experience: can private search for Mastodon be done in a functional way and will folks find this useful?

First, quantities.
The post announcing Searchtodon has received more interactions than I could have imagined:

Comments: 129
Boosts: 700
Favs: 938

The number of signups at the end of the experiment:

304

While not all boosts should be counted as support, as some folks are just interested in the experiment, this still proves to me that a feature that lets you search through your home timeline would be popular enough. I hope the folks at Mastodon see this as encouragement in this direction.
A lot of acceptance surely comes from using an excellent open source web client, as the “Home Search” addition done by me was minimal, and limited in functionality.
But numbers aren’t everything, what about the qualitative angle? — From the folks interested in the functionality, the feedback was predominantly positive. It didn’t work for a few folks, and their feedback was understanding of the early stage of the project.
A good number of people didn’t quite understand the difference between this and what Mastodon already supports. I could clearly have explained things a little better.
Finally, some folks wrote in that they have no need for this as they don’t have this problem, or the regularly provided tools are enough for them. This was entirely expected. It is good to keep in mind that this is not something “for everybody” when arguing for its general acceptance. But enough people liked it.
Hypothesis one: confirmed. Enough people find this useful.

2. Operational feasibility: index data costs storage, search costs compute, etc. Even if the Searchtodon stack is slightly different from Mastodon, it plays in the same ballpark. In case premise #1 comes back positive, we can learn what additional resources will instance operators need to provide private search.

I’m happy to report that things look entirely feasible.
The way things worked, I stored the full post JSON with inline account and reblog (with its own reblog.account) as individual entries in CouchDB. This made the indexing and retrieving side extremely simple, but duplicated quite a bit of data. If two users received the same post, that was saved twice in strictly separate database files, to make account deletions easy.
From that base storage, I created Lucene search indexes partitioned by user timeline.
There are three obvious ways to improve on this:

deduplicate data, all those account records could have been exfiltrated into their own representations.
deduplicate posts across all users.
or forego the additional JSON store by side-caring this to Mastodon directly and only add the search indexes (Lucene or Postgres should both work. Given everything else, this is the probably best way forward).

That said, here’s the data usage as implemented:

~ 1GB JSON + index storage per 100 users per 24 hours. The optimisations outlined above could reduce this need consevatively by 50% – 80%

used ZFS block level lz4 compression, with compressratio ranging between 3.5x – 4.3x . One could go with a higher compression level with, say gzip-5 (higher gzip settings are not worth the CPU/space trade-off). Additional space savings from experience with gzip-5 could be another ~4x.


Hypothesis two: Doable. I’ve since found some demo implementations using Postgres for search in Mastodon, but no serious take-up as far as I can tell. Worth watching though. — Running this as a free-forever not-part-of-Mastodon service would likely not be sustainable, and it remains unclear if folks would pay for storage costs over multiple years (or if the index should go back that far, for that matter).
Additional learnings: Some Mastodon admins are usually not computing resource constrained, but person-time constrained. They just run the bare minimum Mastodon setup and that’s already enough work for a team of two admins. If those admins encounter optional pieces of infrastructure, they will skip them, as each piece adds to the complexity of the system operated with regards to software updates and interoperability. The only way forward here would be having this feature available in the Mastodon core distribution, using the Postgres full text search feature.

3. Community (most important): is private search for Mastodon actually something that can be done in a way that gels with the community rather than against it? — The folks behind Searchtodon do not wanna fight anyone and if there is enough negative signal, we’ll shut it down. — Open questions include: how to handle reporting, defederation after the fact of instance-to-instance trust, are noindex, #nobot & #nosearch good enough account markers or will the Mastodon community have to invent more mechanics around delegating trust from a user’s timeline to tools operating on their timeline?

Hypothesis three: Nope. It is safe to conclude that as implemented, Searchtodon does not “gel with the community”. This warrants its own section:
The Feedback
As outlined above, the support was tremendous and encouraging. I genuinely wanted to help out folks with a problem and Searchtodon found an audience.
Among the supporters were folks that said they liked the idea, but wouldn’t want to sign up for an experimental service run by one person, which is more than fair, this is me most of the time these days.
The next biggest caveat mentioned here however was the implementation choice of making this a standalone service. As that has two distinct consequences:

data that is previously known and trusted to only exist in one instance now lives in a second place that needs to be trusted to keep that data safe.

folks’ posts that were stored and indexed didn’t opt into that (c.f. noindex & friends above).


It also highlighted a distinct lack of mechanics around this in the Mastodon ecosystem, more on that later.
From here on out it got a little more erratic, as folks commented and criticised things that didn’t match reality, or only confirmed their preconceived notions. Some folks have a reflex to react negatively when hearing “mastodon” and “search” in the same sentence without actually evaluating what is proposed. Names were called.
After the Shutdown
The post-shutdown feedback also came in a bunch of relevant flavours:

thanks for taking this down

thanks for listening, this is rare

thanks for paving the way, I’ll rethink my approach (good!)

thanks for trying this, I’ll find a better loophole (don’t do this)

thanks for trying this, I hope this problem will be solved eventually


Lessons Learned

Enough folks of the current Fediverse want opt-in experimenting rather than opt-out experimenting.

There is a difference between what a thing is vs. what people perceive that thing to be. For success, it is important for whoever makes the thing to respect both of these positions.
Profile hashtags for opt in/out are limited in usefulness:

many experiments are not viable if there are not enough posts to operate and reaching a critical mass of opt-ins limits progress. Some folks think this is good.

with account metadata limits, this can only go so far. Adding more and more hashtags is not a scalable solution. And forcing new users through a gauntlet of choices with hard-to-grasp consequences is not gonna make this place more popular.

maybe the Mastodon onboarding experience could include a “allow my posts to participate in experiments” like operating systems ask to “send crash logs to the developer” on signup/upgrade. Then we could formulate a set of guidelines that experiments must adhere to. Maybe some more differentiation like CC-BY-NC where folks can say “you can do whatever you want”, “experimentation is okay, but no commercial exploitation (ads etc) of my posts”, or “nope, nothing” (the default), or anything in between.



Folks don’t trust a central service and/or closed source (and rightfully so). Ideally start as open source.

Folks value the trust relationship they have with their instance admins and the admins of their followers. Things like OAuth apps that take a delegation of that trust, especially without an opt-in process, are suspect under certain circumstances.

I think this area benefits most from clarification in the larger ecosystem. Consider a web or native client that uses the Mastodon REST API and OAuth to authenticate a user.

They get a copy of all the data that the users have access to and usually present them in an ephemeral manner (old content gets eventually pushed out by new posts coming in). This seems to be a largely accepted use of posts, APIs and terms of service.

To offer a good application experience, the client app, in both the web and native scenario, keeps a local cache of all the data it is currently accessing, so restarting the app is instantaneous and no new data has to be downloaded from the internet. This still is largely a reality today and appears to be largely accepted.

Depending on the implementation, it could be that an app not merely caches the data, but stores it permanently, which makes it part of the device’s backup. These backups more often than not are Google or Apple cloud backups under control by, admittedly very capable, admins that the original poster has no trust relationship with, and no option to opt in or out. This too seems to be generally accepted.

Next is a client that just keeps all data stored it’s ever seen. I’m not aware of a public client that does this, but there are a few folks looking to experiment with this (not me). This would probably violate some instance’s ToS, but I haven’t seen this as grounds for defederation yet. I’m relatively certain, someone is doing this privately today. — I’ve seen responses go in either direction here, with folks saying that seems fair and absolutely not. For people in the second camp, Mastodon has no effective way to protect them, but I predict will need one before long.

The one exception of this is single-user instances, which are commonly accepted, it seems. Unless the person running one doesn’t fall out of anyone’s good graces, nobody will even know what they are doing with the data. A few people have privately reached out to me that they used this exact loophole to find things in their timeline in the past (before you look, none of them are in my follower/followee lists). I like the idea of single-user instances, and I’d prefer them not to get ostracised because some of them might be bad actors.


Next is a server-side project, like Searchtodon. We don’t have to rehash that this implementation didn’t meet the community’s standards, but changes could be made:

make this an open source tool that anyone can choose to run

there would have to be an easy way to blanket opt out of these. Maybe a good-faith list that an operator can add themselves to that dissenting instances or users then can defederate or block automatically. Bad actors wouldn’t do this of course, but that’s nothing new.

and/or there could be a “this user just signed up for service X” message going out to all followers and giving them a choice to opt-in. Services (especially new ones) sending messages on the user’s behalf has always been icky on Twitter, but maybe this is a good enough cause to establish this practice here.





And then: build this into Mastodon. I think for timeline search, this is the inevitable future, IF this feature gets demanded enough. This would solve any trust issues, consent could be handled on the regular instance-to-instance trust/defederation network, and operationally, the experiment (above) showed that the resource overhead is manageable.

there are details to be discussed about how long these indexes should go back. Realistically I think 3 – 6 months probably addresses ~80% – 90% of my needs, so maybe that’s an acceptable convention (same for permacaching clients).




Finally, we can choose not do to any of these things and leave everything as is in Mastoland, but I’m certain the Fediverse at large will move on eventually. And from some of the responses I’ve got is that is what old-school folks here desperately wish for: get the new influx of users move on as fast as possible.




The Way Forward
I am still optimistic about Mastodon’s potential to become a lot better for a lot more people and I hope this write-up helps folks to not make the same mistakes again. But I’m also a bit disheartened by the response to this endeavour. I think Mastodon culture needs to become better at handling these kinds of things. Not for my sake, but for the culmination of these two factors:

If a big enough “bad actor” joins here, and we are on the brink of this with major platforms having announced upcoming ActivityPub interop, whatever they do will happen regardless of the consent given by folks who think they can federate content but restrict where it goes. I’m not suggesting this as a justification to just make away with people’s data, but realistically this is going to happen eventually and I’d rather Mastodon has tooling and conventions available to deal with this rather than “we always hoped this would never happen”. The only alternative to this is further hard splintering of communities that have more in common than they don’t and I really hope we can avoid that.

The Fediverse at large will only get more and more popular and mainstream. This means that these new people’s needs will want addressing, and I’d rather have a system in place where we can experiment safely to help those people rather than “pressuring the good actors out until only the bad ones are left” (paraphrasing a few responses I got). And while not claiming to be one or the other, I agree with this sentiment.


—
I regret my mistakes and I apologise to everyone harmed by this experiment.
Thank you for reading.
❧"
https://news.ycombinator.com/rss,The Cloud Conundrum: S3 Encryption,https://www.secwale.com/p/encryption,Comments,"























The Cloud Conundrum: S3 Encryption - by Aditya Patel











Security WaleSubscribeSign inShare this postThe Cloud Conundrum: S3 Encryptionwww.secwale.comCopy linkTwitterFacebookEmailThe Cloud Conundrum: S3 EncryptionAWS will now encrypt all new data in its Amazon S3 storage service by default. Huge announcement, secure default for the win, sure, but it gives a false sense of security. Here’s how.Aditya Patel3 hr ago1Share this postThe Cloud Conundrum: S3 Encryptionwww.secwale.comCopy linkTwitterFacebookEmailRiver Landscape with Cows, 1645/1650 by Aelbert Cuyp👋 Dear reader: Hope you’re staying safe, and going strong with your new year resolutions. This is first part of a series of posts I wish to write on peculiar cloud security challenges. In this post, I will cover:Encryption at rest in cloudAmazon S3 and its encryption optionsHow cloud’s server side encryption can give a false sense of security, and what you can do about itEncryption is a tricky concept. It’s simple at the surface, but dig a level deeper and it unravels like Game of Thrones subplots.Let’s take AWS' recent announcement that all new objects in Amazon S3 (Simple Storage Service) will now be encrypted by default.Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 will be automatically encrypted at no additional cost and with no impact on performance. (Source: AWS docs)What was earlier a 1-click setup, is now zero-click. AWS, and its S3 service specially, operate at a mind boggling scale. There are 280 trillion objects in S3, averaging over 100 million requests per second. To be able to support transparent encryption on all new objects, while not breaking any existing functionality, dependencies and applications - is impressive to say the least. Kudos to the engineering teams.But.. does the default SSE-S3 encryption provide any real security? And, does it protect from one of the primary causes of CISO migraines, i.e., data leakage via intentional or accidental public S3 buckets?Short answer: No.Let’s dive in to understand why and what can be done instead.Cryptography - a quick primerEncryption converts readable data to a random looking blob. It is the reason we can watch dog videos in private, or bitch about things on group chats (well, mostly), or buy toilet paper online securely. On a serious note, encryption is a fundamental tool for cybersecurity to the extent that it can be an enabler of human rights, by allowing freedom of speech through end-to-end encryption. It is a big deal to get it right.Technically, encryption is the process of converting plaintext data into ciphertext. There are 2 types of encryption: symmetric and asymmetric. Symmetric encryption uses the same key to encrypt and decrypt the data. Asymmetric encryption uses separate keys to do the same, and is also called public key cryptography. We will limit this blog post to symmetric encryption of data at rest, which is the data stored on disks. encrypt(plaintext, key) → ciphertextThe efficacy of a good encryption scheme depends upon the strength of encryption algorithm (the lock) and the encryption key (key).🔒 First is the lock, i.e., the algorithm itself. There are many encryption algorithms available, the most common type today is Advanced Encryption Standard with 256 bit key (AES-256). There is enough evidence, backed by gory mathematical proofs, to safely assume that the AES-256 encryption algorithm is not broken, for now. From one estimate, if we use the combined compute power of every PC on earth (estimated 2 billion PCs), it’d take 13,689 trillion trillion trillion trillion years to brute force AES-256. For comparison, the age of the universe is a meager 14 billion years. Quantum computers might change the equation sooner than later, but for now AES-256 is considered a quantum resistant algorithm. Moral of the story here is to trust the researchers and don’t invent your crypto.🔑 Then comes the key. As a wise man once said: if a thief has your key, no lock is strong enough. That’s why protecting the key is the most important part of a secure encryption scheme. So in theory, it’s pretty simple to protect your data. Choose a vetted encryption algorithm, and protect the key. In practice, things are more complex.Any modern production application usually has many different data sources, and hence many encryption schemes and keys. For instance, Pinterest currently stores and manages 1 exabyte of data on AWS. Nope - that’s not a typo, that is one frickin’ exabyte, or 1 billion gigabytes of data, which needs to be protected.This humongous amount of data is unlikely to be in a single data store. So now, you need to manage encryption across all the applicable data stores, equating to potentially thousands of encryption keys. Add to it the security best practice of rotating the keys periodically, or in case of an incident, deleting a whole bunch of keys. Doing this on your own is a nightmare. Ask Harry Potter.Luckily, you don’t have to. There are key management services both for on premise and cloud. In AWS, the service is called AWS Key Management Service (AWS KMS).This brings us to the types of encryption choices available in S3.Encryption in Amazon S3You can either do Server Side Encryption (SSE), in which Amazon S3 encrypts your data as it writes it to disks in its data centers and decrypts it for you when you access it. With server side encryption, there are 3 broad ways to manage your encryption keys.One option is for S3 to fully manage the encryption keys (SSE-S3). This option places the most trust in AWS, and is the reason I’m writing this post. A second option is for customers to use a key that is managed by the Amazon Key Management Service (SSE-KMS). This option gives customers control and transparency over access to their keys with strong auditing. Spoiler: this is my recommendation for most use-cases. Third option is for the customer to provide and manage the key, but have S3 perform the actual encryption and decryption (SSE-C). This gives customers a level of separation between themselves and AWS; do note that there’s a small window where the encryption key will be present on AWS servers to do encryption and decryption. Using either of these 3 ways, you can choose to give all the encryption, decryption and associated compute headaches to AWS.Or, you can say hey AWS, I don’t trust you, I will do the Client Side Encryption (CSE), in which you encrypt your data locally and pass it to the Amazon S3 service for storage and retrieval. You’ve 2 further options here: Use a key stored in AWS Key Management Service (AWS KMS). Or, use a key that you store within your application.Encryption-at-rest options in Amazon S3Security is a tradeoff problem. Your security decisions may come at the cost of convenience or performance or a higher spend. If you can safely create and manage your own keys in your applications for instance, you, and only you, will have access to the unencrypted material (assuming your access controls are rock solid). Choose client side encryption for highly regulated industries, business critical and the most paranoid of use cases. For the rest, the tradeoff problem may lean towards using the other option.As per an AWS blog, server side encryption may be the way to go. And I agree.While client-side encryption still has an important role in security and data protection, two of its disadvantages are that it depends on clients having a secure source of randomness, which is not always easy, and it is CPU intensive on the client. For more simplicity and efficiency, our services also offer server-side encryption.(Source: AWS blog)Now, let’s go back to the news announcement, that AWS now encrypts all new object uploads with SSE-S3 server side encryption. So does it provide any meaningful security?So does it?No, the SSE-S3 server side encryption does not provide any meaningful security assurance.It is mostly a checkbox exercise. This may appease some auditors but not all (disclaimer: nothing against auditors, I work very closely with them at Amazon, and they understand security better than most). For example, SSE-S3 meets PCI DSS’ encryption requirement but not the segregation of duties requirement.Next, at best SSE-S3 adds a defense in depth protection against a physical loss, theft or confiscation of an AWS hard drive storing your data. Think crazy scenarios like a tornado or fire, followed by more chaos and somehow the AWS hard drive landing at Goodwill. If the data on it is unencrypted, game over. As you can imagine, the likelihood of this happening is about the same as that of the United States winning a cricket world cup.More importantly, in SSE-S3, since S3 encrypts and decrypts the data transparently to anyone with access to the bucket, it will not protect leaked S3 buckets’ contents from being read. This is unfortunately still a very common scenario. Why does AWS even provide this option then? For one, some encryption is better than no encryption. Few compliance attestations will be happy with it, since it gives you a defense in depth option. It also provides some practical benefits for AWS to wipe out the hard drives more easily and securely (delete the key and you get crypto shredding). Also worth noting, there are no additional costs for using SSE-S3.What should you do instead? My suggestion is to go with any of the other options. At a minimum, go with the server side encryption with KMS keys, SSE-KMS.SSE-KMS provides a good balance between security and usability. For reading and writing contents of S3, it requires users to have access to both the object and the key. Enter multiple permission policies at IAM, S3 and KMS level, and hence segregation of duties. Now if a bucket is made public, and if it’s encrypted with SSE-KMS, it’s a very low likelihood that its contents will be world readable. Win!TakeawaysIf you’re new to AWS, you might be wondering, wow this shit is complicated. It is, and I didn’t even cover all the scenarios. Here’re the takeaways for meaningfully doing encryption at rest in the cloud.Don’t invent your crypto. Choose a cryptographic algorithm vetted by academia and industry such as AES-256.Outsource key generation and management. Prefer not to create and manage your own cryptographic keys if it’s not your core competency. Use the cloud service provider’s key management service instead.SSE-KMS for the win. That means, in AWS for your data in S3, prefer the server side encryption with KMS keys (SSE-KMS) for most use cases.SSE-S3 is misleading. Server side encryption with S3 keys (SSE-S3) shows AWS’ commitment to security, but to customers it doesn’t provide any real security benefit beyond a compliance checkbox.To wrap it up, here’s a relevant quote, attributed to Amazon CTO Werner Vogels: “Dance like nobody's watching. Encrypt like everyone is.”📕 Security Wale is a blog about cloud, cybersecurity, and in between - written by Aditya Patel. This is a passion project, where Aditya shares his learnings, opinions and rants from over a decade of working in the IT industry in United States. For a living, currently, he protects ☁️ cloudy things at Amazon/AWS. Earlier, Aditya has done software security consulting, masters in Information Security from Johns Hopkins, and computer science engineering. To support this effort, consider subscribing (it’s free) and spreading the word.Share this postThe Cloud Conundrum: S3 Encryptionwww.secwale.comCopy linkTwitterFacebookEmailPreviousCommentsTopNewNo postsReady for more?Subscribe© 2023 Aditya PatelPrivacy ∙ Terms ∙ Collection notice Start WritingGet the appSubstack is the home for great writing









        This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts
    



"
https://news.ycombinator.com/rss,GPT-3 is the best journal I’ve used,https://every.to/superorganizers/gpt-3-is-the-best-journal-you-ve-ever-used,Comments,"


GPT-3 Is the Best Journal I've Ever Used - Superorganizers - Every






































Subscribe





≡


About
Founders‘ Letter
Publications
Collections

Contact Us
Become a Sponsor
Login











Superorganizers




          GPT-3 Is the Best Journal I’ve Ever Used
        
My slow and steady progression to living out the plot of the movie 'Her'

by Dan Shipper
January 13, 2023
♥ 226





Listen







This is a joke, but it's not entirely wrong either.





Sponsor Every

Do you run a software company looking to reach an audience of early-adopters? Consider sponsoring our smart long-form essays on tech, AI, and productivity:

﻿Sponsor Every﻿


Want to hide ads? Become a subscriber

For the past few weeks, I’ve been using GPT-3 to help me with personal development. I wanted to see if it could help me understand issues in my life better, pull out patterns in my thinking, help me bring more gratitude into my life, and clarify my values.I’ve been journaling for 10 years, and I can attest that using AI is journaling on steroids. To understand what it’s like, think of a continuum plotting levels of support you might get from different interactions:Talking to GPT-3 has a lot of the same benefits of journaling: it creates a written record, it never gets tired of listening to you talk, and it’s available day or night. If you know how to use it correctly and you want to use it for this purpose, GPT-3 is pretty close, in a lot of ways, to being at the level of an empathic friend:If you know how to use it right, you can even push it toward some of the support you’d get from a coach or therapist. It’s not a replacement for those things, but given its rate of improvement, I could see it being a highly effective adjunct to them over the next few years. People who have been using language models for much longer than I have seem to agree:
Nick@nickcammarata

Replying to @nickcammarata

@krismartens I'm afraid of seeming hyperbolic, but also don't want to lie or hide information. GPT-3 is really just an incredible therapist, and is able to uncover complex patterns in my thinking and distill clean narratives that helps me a lot. It's also a lot warmer than most therapists

July 17th 2020, 4:55am EST

6 Retweets36 Likes

(Nick is a researcher at OpenAI. He’s also into meditation and is generally a great follow on Twitter.)It sounds wild and weird, but I think language models can have a productive, supportive role in any personal development practice. Here’s why I think it works.Why chatbots are great for journalingJournaling is already an effective personal development practice. It can help you get your thoughts out of your head, rendering them less scary. It shows you patterns in your thinking, which increases your self-awareness and makes it easier for you to change.It creates a record of your journey through life, which can tell you who you are at crucial moments. It can help you create a new narrative or storyline for life events so that you can make meaning out of them.It can also guide your focus toward emotional states like gratitude, or directions you want your life to go in, rather than letting you get swept up in whatever is currently going on in your life. But journaling has a few problems. For one, it’s sometimes hard to sit down and do it. It can be difficult to stare at a blank page and know what to write. For another, sometimes it feels a little silly—is summarizing my day really worth something?Once you get over those hurdles, as a practice it tends to get stale. You don’t read through your old entries that often, so the act of writing down your thoughts and experiences doesn’t compound in the way that it should. The prompts you use often get old: one like, “What are you grateful for today?” might work for the first few weeks, but after a while you need something fresh in order for the question to feel genuine.You want your journal to feel like an intimate friend that you can confide in—someone who’s seen you in different situations and can reflect back to you what’s important in crucial moments. You want your journal to be personal to you, and the act of journaling to feel fresh and full of hope and possibility every time you do.Unfortunately, paper isn’t great at those things. But GPT-3 is. Journaling in GPT-3 feels more like a conversation, so you don’t have to stare at a blank page or feel silly because you don’t know what to say. The way it reacts to you depends on what you say to it, so it’s much less likely to get stale or old. (Sometimes it does repeat itself, which is annoying but I think long-term solvable.) It can summarize things you’ve said to it in new language that helps you look at yourself in a different light and reframe situations more effectively. In this way, GPT-3 is a mashup of journaling and more involved forms of support like talking to a friend. It becomes a guide through your mind—one that shows unconditional positive regard and acceptance for whatever you’re feeling. It asks thoughtful questions, and doesn’t judge. It’s around 24/7, it never gets tired or sick, and it’s not very expensive.Let me tell you about how I use it, what its limitations are, and where I think it might be going.How I started with GPT-3 journalingI didn’t think of using GPT-3 in this way myself. I saw Nick Cammarata’s tweets about it over the years first. My initial reaction was a lot of skepticism mixed with some curiosity. After we launched Lex and I got more interested in AI, I remembered those tweets and decided to play around for myself. I started in the OpenAI playground—a text box where you input a prompt that tells GPT-3 how you want it to behave, and then interact with it:I had a bunch of ideas to start. I tried one from a Facebook PM, Mina Fahmi, whom I met at the AI hackathon I wrote about a few weeks ago. He suggested telling GPT-3 to take on a persona, and told me that he’d had great results asking it to be Socrates.GPT-3 as famous compassionate figureI started experimenting with prompts like this:The green messages are responses from GPT-3. I tried Socrates, the Buddha, Jesus, and a few others, and found I liked Socrates the best (apologies to my Christian and Buddhist readers). The GPT-3 version of him is effective at driving toward the root of an issue and helping you figure out small steps to take to resolve it.There’s a long tradition in various religions of visualizing and interacting with a divine, compassionate figure as a way of getting support—and this was a surprisingly successful alternative route to a similar experience.After a while, though, I became a little bored of Socrates. I’m a verified therapy nerd, so the obvious next step was to try asking GPT-3 to do interactions based on various therapy modalities.GPT-3 as therapy modality expertI tried asking GPT-3 to become a bot that’s well-versed in Internal Family Systems—a style of therapy that emphasizes the idea that the self is composed of many different parts or sub-personalities, and that a lot of growth comes from learning to understand and integrate those parts. It turns out, GPT-3 isn’t bad at that:﻿I also tried asking it to be a psychoanalyst and a cognitive behavioral therapist, both of which were interesting and useful. I even asked it to do Jungian dream interpretation:I don’t know what to make of the efficacy of dream interpretation in general, nor do I know what an actual Jungian might say about this interpretation. But I have found that having dreams reflected back to me in this way can help me understand some of what I’ve been feeling day to day but haven’t been able to put into words. GPT-3 as gratitude journalAnother thing I tried is asking GPT-3 to help me increase my sense of gratitude and joy—like a better gratitude journal:You’ll notice it starts by acting like a normal gratitude journal, asking me to list three things I’m grateful for. But once I respond, it probes about details of what you’re grateful for to get you past your stock answers and into the emotional experience of gratitude. GPT-3 as values coachOne of my favorite therapy modalities is ACT—acceptance and commitment therapy—because I love its focus on values. ACT emphasizes helping people understand what’s most important to them and uses that knowledge to help them navigate difficult emotions and experiences in their lives.Values work is challenging because sometimes it’s hard to connect your day-to-day experiences to your values. So I wanted to see if GPT-3 could help. This is one of the experiments I tried:This works well, and one of the cool things about it is how the prompt works. I took a sample therapy dialog from an ACT-focused values book that I love, Values in Therapy, and asked GPT-3 to generalize from that dialog to learn how to talk to me about values.It worked—successfully guiding our conversation toward talking about what was most important to me. It’s not perfect, but it suggests interesting possibilities for things to try going forward.Problems and limitationsWhile I liked these early experiments, they had a few significant problems.First, the OpenAI playground isn’t designed to facilitate chats, so it’s hard to use. Second, it doesn’t record inputs between sessions, so I ended up having to re-explain myself every time I started a new session. Third, it sometimes gets repetitive and asks the same questions.These are solvable, though. I know because I built a solution: a web app with a chatbot interface that remembers what I say in every session so I never have to repeat myself.  The bot lets me select a persona—like Socrates or an Internal Family Systems therapist—which corresponds to the prompts above. Then I can have a conversation with it. It will help me work through something I’m dealing with, or set goals, or bring my attention to something I’m grateful for. It can even output and save a summary of the session to help me notice patterns in my thinking over time. It’s still early and there are a lot of problems to fix, but I find myself gravitating toward it every day. I feel like I’m building up a record of myself and my patterns over time, and the more I write in it, the more it compounds.I’ll be releasing the bot soon for paying Every members, so if you want access, make sure to subscribe. What’s nextHere’s what I’ve learned so far through all of these experiments with GPT-3 as a journaling tool.There is something innately appealing about  building a relationship with an empathetic friend that you can talk to any time. It’s comforting to know that it’s available, and it’s exciting to think about all of the different prompts you can experiment with to help it support you in the way you need.There is also something weird about all of this. Spilling your guts to a robot somehow cheapens the experience because it doesn’t cost much for a robot to tell you it understands you. This mix of feelings is reflected in this Twitter thread by Rob Morris, the founder of a peer-to-peer support app called Koko:
Rob Morris@RobertRMorris

We provided mental health support to about 4,000 people — using GPT-3. Here’s what happened 👇

January 6th 2023, 2:50pm EST

1k Retweets6k Likes

When people were using GPT-3 to help them provide support to peers, their responses were rated significantly more highly than responses that were generated by humans alone:
Rob Morris@RobertRMorris

Replying to @RobertRMorris

Messages composed by AI (and supervised by humans) were rated significantly higher than those written by humans on their own (p < .001). Response times went down 50%, to well under a minute.

January 6th 2023, 2:50pm EST

66 Retweets785 Likes

But they had to stop using the GPT-3 integration because people felt like getting a response from GPT-3 wasn’t genuine and ruined the experience. Those feelings are understandable, but whether or not they ruin the experience depends on how the interaction is framed to you, and how familiar you are with these tools.I don’t think these objections will last over time for most people. It’s more likely a temporary result of contact with new technology. When you see a movie that you loved, does it cheapen the experience to know that you were touched by a set of pixels moving in the correct sequence over the course of a few hours? Obviously not, but if I had to bet, when movies were first introduced many people probably felt it was a cheaper version of a live performance experience.As these kinds of bots get more common, and we learn to interact with them and depend on them for different parts of our lives, we’ll be less likely to feel that our interactions with them are cheap or stilted.(None of this, by the way, means that in-person interactions aren’t valuable anymore—just that there’s probably more room for bot interactions in your life than you might realize.)If you're someone that's journaled for a long time, you'll find a lot of value in trying GPT-3 out as an alternative to your day-to-day practice. And if you've never journaled before this might be a good way to get started.I’ll be experimenting with this a lot more over the coming weeks and months, and I’ll be sharing everything I learn with you here. I’m excited for what’s next.




What did you think of this post?

Amazing
Good
Meh
Bad





Send Privately

      Your feedback has been saved anonymously. If you want it to be attributed to you, login or sign up.
    



Like this?Become a subscriber.
Subscribe →
Or, learn more.




Read this next:








Superorganizers


How Josh Kaufman Does Research 
The author of The Personal MBA shares his process for finding answers hiding in plain sight

♥ 184
🔒 
          Aug 20, 2020
      










Superorganizers


The Fall of Roam
I don’t use Roam anymore. Why?

♥ 208

          Feb 12, 2022
          by Dan Shipper











Superorganizers


The End of Organizing
How GPT-3 will turn your notes into an *actual* second brain

♥ 160

          Jan 6, 2023
          by Dan Shipper










The End of Moore’s Law?
As chips are getting smaller, prices are going up

♥ 91

          Jan 9, 2023
          by Anna-Sofia Lesiv











The Sunday Digest


Toward the Unoptimized Life, GPT-3 Journaling, and More
Everything we published this week.

♥ 12

          Jan 15, 2023
      






Comments







Post









Post



    You need to login before you can comment.
    Don't have an account? Sign up!














@nattaliehartwig
1 day ago


Loved this, are any of your projects available to try?



♡ 0

      ·
      Reply










✕
Thanks for reading Every!
Sign up for our daily email featuring the most interesting thinking (and thinkers) in tech.
Subscribe
Already a subscriber? Login




Contact Us ·
            Become a Sponsor ·
            Search ·
            Terms

©2023 Every Media, Inc





"
https://news.ycombinator.com/rss,The IAB loves tracking users but hates users tracking them,https://shkspr.mobi/blog/2023/01/the-iab-loves-tracking-users-but-it-hates-users-tracking-them/,Comments,"The IAB loves tracking users. But it hates users tracking them.By 
@edent

 on 
2023-01-16
advertising email privacy13 comments550 words
The Interactive Advertising Bureau (IAB) is a standards development group for the advertising industry. Their members love tracking users. They want to know where you are, who you're with, what you're buying, and what you think. All so they can convince you to spend slightly more on toothpaste.  Or change your political opinions. Either way, they are your adversaries.The IAB's tech lab is working on a system called UID2. It's a more advanced way to track you no matter what you do and no matter what steps you take to avoid it.
UID2 is a framework that enables deterministic identity for advertising opportunities on the open internet for many participants across the advertising ecosystem. The UID2 framework enables logged-in experiences from publisher websites, mobile apps, and Connected TV (CTV) apps to monetize through programmatic workflows.Basically, they tie your email address to everything you do. Signed in to watch a TV show? Better sell that info to the advertisers so when you sign in to a different site they can send you targetted messages. Yuck.One of the ways privacy conscious users normally avoid this is by subtly altering their email addresses for each service they use.  For example, GMail ignores any dots in your username. So if you are Han.Solo@gmail.com you can also use H.ansolo@gmail.com or ha.ns.ol.o@gmail.com.  A user might sign up to a service and use a specifically ""dotted"" email address.  If they later start receiving spam to that address, they know the service has leaked or sold their info.You can go one step further and use plus addressing.  For example han.solo+amazon@gmail.com and han.solo+github@gmail.com. They both will appear in your normal inbox, but are unique for every service you use. Again, this is great for making sure that someone hasn't sold your email address to spammers.The IAB hates this.As part of the UID2 API they specifically describe how an advertiser must ""normalise"" their users' email addresses.This means h.a.n.solo+iab@gmail.com becomes plain old hansolo@gmail.comI think this is pretty shitty behaviour. If someone has deliberately set their email address in this form it is because the user does not want their identities to be commingled.Last year, I asked them to respect users' privacy and reverse this change.  They finally responded:
Thank you for your input, we thought long about this update and ultimately as it stands today it is not a change we would like to add.So, there you have it. If you want to take even the smallest step to preserve your privacy - tough.
If you want to track which IAB members are using your data - tough.
If you want to track users even if they don't want to be tracked - the IAB is happy to help.If you want to opt out of this - and you trust the IAB to handle your data safely - you can submit your email address and phone number to https://transparentadvertising.org/.Personally, I recommend installing the uBlock advert blocker on all devices which support it.Share the love:MastodonTwitterFacebookLinkedInRedditHackerNewsLobstersEmailPocketWhatsAppTelegramMore posts from around the site:
13 thoughts on “The IAB loves tracking users. But it hates users tracking them.”

2023-01-16 12:38

 Ian Betteridge says:@Edent I’ve noticed several brands now blocking services like iCloud’s relay, which lets you sign up with a random email address that’s not related to yours. Firefox relays ducks around that by letting you use your own domain, which makes it much harder for them to block sign-ups, but that’s obviously only applicable to a few users.Reply

2023-01-16 12:55

 Gabor says:I've been loving fastmail's masked email functionality, which gives you a random email alias like ""salty.hotdog8233@fastmail.com"", plus it has 1password integration, so signing up to places is fairly straightforward if you use 1p.Reply

2023-01-16 13:00

 That Privacy Guy says:I just read this and the solution I use is my own instance of AnonAddy - and I create a new and unique email address for every site/service I use. If you don't want to run it yourself there is a SaaS version - plus it is FOSS.


Reply

2023-01-16 13:40

 HackerNewsTop10 says:The IAB loves tracking users. But it hates users tracking them
Link: shkspr.mobi/blog/2023/01/t…
Comments: news.ycombinator.com/item?id=344000…Reply

2023-01-16 13:42

 Kazaii says:@Edent wow, that's rather unsettling. Thanks for shedding light on this.Reply

2023-01-16 13:49

 That Privacy Guy says:I have been using my own installation of AnonAddy for a couple of years now. I used to just have a catchall in my mail server which would forward anything which was sent to a non existing email address to a delegated account



Reply

2023-01-16 13:55

 Fazal Majid says:The plus convention is not specific to GMail (Sendmail, MS Exchange, Postfix and other email software have it), but they only require stripping it for @gmail.com domains. I have my own dedicated domain for vendors so I won't be impacted, and Apple's email masking feature will do the same, along with competing offerings from DuckDuckGo et al.Hashing PII like an email is also PII and this proposal is a blatant violation of GDPR, of course.Reply

2023-01-16 13:55

 Nikki says:Personally my opinion of anyone involved in advertising is so poor that I'd probably not be allowed to express it here. I can easily imagine a world without advertising as the web allows you to find anything you want without having someone trying to force it down your throat. Also the idea that many parts of the web could not exist without advertising support is facile. It's a bit like saying that free and open parks cannot exist without employing pick pockets to gather funds to pay for maintenance. If there are any parts of the web that really can not exist without advertising, they must be so bankrupt of alternatives ideas that their services could not be trusted to be useful.Reply

2023-01-16 15:09

 Anonymous says:A link says uBlock but points to uBlock Origin. uBlock is different from uBlock Origin: https://github.com/gorhill/uBlock/wiki/uBlock-Origin-is-completely-unrelated-to-the-web-site-ublock.orgReply

2023-01-16 15:21

 Oli says:I’m a big fan of Fastmail’s masked addresses for this reason.Word dot word four digit number at my own domain, goes in the password manager, never thought about again!Reply

2023-01-16 15:44

 Privacy Matters says:Hi @IABTechLab  What is the legal basis relied on  to alter the email identities of individuals who will be targeted by those using UID2?Oh, & I note domain reg details for transparentadvertising.org are redacted for privacy reasons. Who owns the domain pls?

Reply

2023-01-16 15:56

 trinity says:I own my name dot [tld] so I can do slingshit@me.com. Looks like I'm still gonna be doing alright. Cloudflare's mail forwarding works well for this, before that I used ImprovMX. Both just point the proper DNS records from your site to someone's mail server for quick relay+disposal. I imagine having all mail filter through a magic box is technically A Bit Troublesome but it's still better than Google Mail!Reply

2023-01-16 20:19

 Duane Johnson :verified: says:@Edent The issue at the core of privacy is dignity--to hide or reveal parts of ourselves as we create relationships. In this case, advertisers want a kind of ""forced intimacy"" with all of humankind--to prevent people from hiding parts of themselves--so they can offer goods and services. The difference between a friend recommended something--because they know you well--and the IAB or others advertising to you, is that a friend actually has your long-term best interest in mind.ReplyLeave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment *Name * Email * Website  Notify me of follow-up comments by email. Notify me of new posts by email. 
Δ
To respond on your own website, enter the URL of your response which should contain a link to this post's permalink URL. Your response will then appear (possibly after moderation) on this page. Want to update or remove your response? Update or delete your post and re-enter your post's URL again. (Learn More)



"
https://news.ycombinator.com/rss,"Why Alexa won't wake up when hearing ""Alexa"" in Amazon's Super Bowl ad (2019)",https://www.amazon.science/blog/why-alexa-wont-wake-up-when-she-hears-her-name-in-amazons-super-bowl-ad,Comments,"


Conversational AI / Natural-language processing


        Why Alexa won't wake up when she hears her name in Amazon's Super Bowl ad
    



By Mike Rodehorst

        January 31, 2019
    







Share



Share




Copy link




Email



Twitter



LinkedIn


 
Facebook


 
Line

 
Reddit

 
QZone

 
Sina Weibo

 
WeChat

 
WhatsApp






                        分享到微信
                    
x




















This Sunday's Super Bowl between the New England Patriots and the Los Angeles Rams is expected to draw more than 100 million viewers, some of whom will have Alexa-enabled devices within range of their TV speakers. When Amazon's new Alexa ad airs, and Forest Whitaker asks his Alexa-enabled electric toothbrush to play his podcast, how will we prevent viewers’ devices from mistakenly waking up?












                  Related content
              
How AWS scientists help create the NFL’s Next Gen Stats
In its collaboration with the NFL, AWS contributes cloud computing technology, machine learning services, business intelligence services — and, sometimes, the expertise of its scientists.






With the Super Bowl ad — as with thousands of other media mentions of Alexa tracked by our team — we teach Alexa what individual recorded instances of her name sound like, so she will know to ignore them. We can also apply this technique, known as acoustic fingerprinting, on the fly to recognize when multiple devices from different households are hearing the same command at around the same time. This is crucial to preventing Alexa from responding to pranks on TV, references to people named Alexa, or other instances of her name in broadcast media that we don't know about in advance.












                  Related content
              
Audio watermarking algorithm is first to solve ""second-screen problem"" in real time
Audio watermarking is the process of adding a distinctive sound pattern — undetectable to the human ear — to an audio signal to make it identifiable to a computer. It’s one of the ways that video sites recognize copyrighted recordings that have been posted illegally. To identify a watermark, a computer usually converts a digital file into an audio signal, which it processes internally.






Our approach to matching audio recordings is based on classic acoustic-fingerprinting algorithms like that of Haitsma and Kalker in their 2002 paper “A Highly Robust Audio Fingerprinting System”. Such algorithms are designed to be robust to audio distortion and interference, such as those introduced by TV speakers, the home environment, and our microphones.To produce an acoustic fingerprint, we first derive a grid of log filter-bank energies (LFBEs) for the acoustic signal, which represent the amounts of energy in multiple overlapping frequency bands in a series of overlapping time windows. The algorithm steps through the grid in two-by-two blocks and adds and subtracts the measurements in the grid cells in a standardized way. (Technically, it computes the 2-D gradient of each block.) The sign of the result — positive or negative — provides a one-bit summary of the values in the block. The summaries of all the blocks in the grid constitute the acoustic fingerprint, and two fingerprints are deemed to match if the fraction of bits that are different (the “bit error rate”) is small enough.






An illustration of how fingerprints are used to match audio. Different instances of Alexa’s name result in a bit error rate of about 50% (random bit differences). A bit error rate significantly lower than 50% indicates two recordings of the same instance of Alexa’s name.

When we have audio samples in advance — as we do with the Super Bowl ad — we fingerprint the entire sample and store the result. With audio that’s streaming to the cloud from Alexa-enabled devices, we build up fingerprints piecemeal, repeatedly comparing them to other fingerprints as they grow.If a match is found, the incoming request is ignored. Noisy audio may yield a match, but it requires the accumulation of more data (a larger fingerprint) than clean audio does.Using this matching algorithm, we have built a system with multiple layers to protect customers at multiple stages:On-device: On most Echo devices, every time the wake word “Alexa” is detected, the audio is checked against a small set of known instances where Alexa is mentioned in commercials. Due to the limits of device CPU, this set is generally restricted to commercials we expect to be currently airing on TV.In the cloud: Every audio request to Alexa that starts with a wake word is checked in two ways:Known media: the audio is checked against a large set of fingerprints for known instances of “Alexa” and other wake words in commercials and other media. These fingerprints can also make use of the audio that follows the wake word.Unknown media: the audio is checked against a fraction of other Alexa requests arriving at around the same time. If the audio of a request matches that of requests from at least two other customers, we identify it as a media event. We also check incoming audio against a small cache of fingerprints discovered on the fly (the cached fingerprints are averages of the fingerprints that were declared matches). The cache allows Alexa to continue to ignore spurious wake words even when they no longer occur simultaneously.Ideally, a device will identify media audio using locally stored fingerprints, so it does not wake up at all. If it does wake up, and we match the media event in the cloud, the device will quickly and quietly turn back off.In addition to tracking new media mentions of Alexa’s name and updating our library of fingerprints accordingly, our team works continuously to improve the accuracy and efficiency of the fingerprinting system. We’re also exploring complementary technologies, such as machine learning systems that can distinguish media audio more generally from live human speech.Acknowledgments: Joe Wang, Aaron Challenner, Mike Peterson, Michael Rudeen, Naresh Narayanan, Liangwei Guo, and the rest of the team






        Research areas
    


Conversational AI / Natural-language processing







        Tags
    


Alexa


Keyword spotting


Signal processing




About the Author




Mike Rodehorst


                    Mike Rodehorst is a machine learning scientist in the Alexa Speech group.
                




"
https://news.ycombinator.com/rss,Brad Feld: I Don't Hate Crypto,https://feld.com/archives/2023/01/i-dont-hate-crypto/,Comments,"   Back to Blog I Don’t Hate Crypto Jan 16, 2023  Category   Investments  #crypto #ponziWell, that was interesting.I get many more private emails in response to blog posts than comments. Yesterday, in response to Reflecting on Ponzi Schemes, I got a few that said anyone under 35 needs a net native currency, and that’s crypto. A few others said some versions of all governments are Ponzi schemes. And I got a few that implied I hated crypto.Earlier last year, one of my partners told me that I’d developed a reputation with other VCs (presumably our partner funds) that I hate crypto. At the time, I deflected and said that I didn’t hate crypto; I just thought there was considerable Ponzi-like behavior in crypto. I’m regularly cynical about things on our internal Slack channel and periodically post about big blowups, including in crypto.I realize that I’m conflating speculation vs. investment. The part of crypto I don’t like is the rampant speculation. This morning, a friend of mine sent me an email about some money I owed him for a thing we are doing together. He said, “If you paypal me I’ll buy some bitcoin with it. Looks like it’s starting to firm up.”Here were the bitcoin prices when he sent me the email and when I Paypalled him the money ($1,456.42).1/15/23 9:51 PM MT: $21,158.551/16/23 7:28 PM MT: $20,879.14That’s a 1.33% difference. It cost me nothing to Paypal him the money. It would have cost me $19.37 to pay him via Bitcoin just because of the timing difference. That has nothing to do with the transaction cost. It’s entirely a result of speculative activity.I mean, c’mon. Yeah, I know credit cards have fees, and endless payment rails in the system extract money along the way. But there are also ACH and Debit Cards. And free checking accounts, although I guess it would cost me $0.60 for a stamp. Wait, $0.60 for a stamp? The last time I bought a stamp, they were $0.29. And yes, I know some of you out there have never bought a stamp.It’s hard for me to hate crypto. It’s been economically very good to me. I accidentally bought twice as many bitcoins as I needed for an online programming course I took in 2013 for about $100 each. I sold the FIL I got from investing in their SAFT as it vested (daily) and was amazed at how much money resulted. The Helium that I earned, which seemed to have no functional utility whatsoever, generated a nice multiple on the cost of all the routers I bought, even though today I earn nothing because of whatever algorithm changes they’ve made, so the network is now functionally and economically worthless. And, the crypto funds we have invested in have done exceptionally well … mostly.I regularly hear to be patient. It’s like the Internet was in 1999 – ahead of its time. The builders are building, and it’ll take over everything in the future.Ok. That’s cool. Just beware of the Ponzi schemes. Previous Post"
https://news.ycombinator.com/rss,mRNA vaccines induce higher long-term IgG4 response versus adenovirus vaccines,https://www.frontiersin.org/articles/10.3389/fimmu.2022.1020844/full?s=09#B75,Comments,"








                                ORIGINAL RESEARCH article
                            



                            Front. Immunol., 12 January 2023Sec. Vaccines and Molecular Therapeutics

                                
https://doi.org/10.3389/fimmu.2022.1020844



mRNA vaccines against SARS-CoV-2 induce comparably low long-term IgG Fc galactosylation and sialylation levels but increasing long-term IgG4 responses compared to an adenovirus-based vaccineJana Sophia Buhre1†, Tamas Pongracz2†, Inga Künsting1, Anne S. Lixenfeld1, Wenjun Wang2, Jan Nouta2, Selina Lehrian1, Franziska Schmelter3, Hanna B. Lunding1, Lara Dühring1, Carsten Kern1, Janina Petry1, Emily L. Martin1, Bandik Föh3, Moritz Steinhaus1,4, Vera von Kopylow1, Christian Sina3, Tobias Graf5, Johann Rahmöller1,4, Manfred Wuhrer2*‡ and Marc Ehlers1,6*‡1Laboratories of Immunology and Antibody Glycan Analysis, Institute of Nutritional Medicine, University of Lübeck and University Medical Center Schleswig-Holstein, Lübeck, Germany2Center for Proteomics and Metabolomics, Leiden University Medical Center, Leiden, Netherlands3Institute of Nutritional Medicine, University of Lübeck and University Medical Center Schleswig-Holstein, Lübeck, Germany4Department of Anesthesiology and Intensive Care, University of Lübeck and University Medical Center Schleswig-Holstein, Lübeck, Germany5Medical Department 2, University Heart Center of Schleswig-Holstein, Lübeck, Germany6Airway Research Center North (ARCN), University of Lübeck, German Center for Lung Research (DZL), Lübeck, GermanyBackground: The new types of mRNA-containing lipid nanoparticle vaccines BNT162b2 and mRNA-1273 and the adenovirus-based vaccine AZD1222 were developed against SARS-CoV-2 and code for its spike (S) protein. Several studies have investigated short-term antibody (Ab) responses after vaccination.Objective: However, the impact of these new vaccine formats with unclear effects on the long-term Ab response – including isotype, subclass, and their type of Fc glycosylation – is less explored.Methods: Here, we analyzed anti-S Ab responses in blood serum and the saliva of SARS-CoV-2 naïve and non-hospitalized pre-infected subjects upon two vaccinations with different mRNA- and adenovirus-based vaccine combinations up to day 270.Results: We show that the initially high mRNA vaccine-induced blood and salivary anti-S IgG levels, particularly IgG1, markedly decrease over time and approach the lower levels induced with the adenovirus-based vaccine. All three vaccines induced, contrary to the short-term anti-S IgG1 response with high sialylation and galactosylation levels, a long-term anti-S IgG1 response that was characterized by low sialylation and galactosylation with the latter being even below the corresponding total IgG1 galactosylation level. Instead, the mRNA, but not the adenovirus-based vaccines induced long-term IgG4 responses – the IgG subclass with inhibitory effector functions. Furthermore, salivary anti-S IgA levels were lower and decreased faster in naïve as compared to pre-infected vaccinees. Predictively, age correlated with lower long-term anti-S IgG titers for the mRNA vaccines. Furthermore, higher total IgG1 galactosylation, sialylation, and bisection levels correlated with higher long-term anti-S IgG1 sialylation, galactosylation, and bisection levels, respectively, for all vaccine combinations.Conclusion: In summary, the study suggests a comparable “adjuvant” potential of the newly developed vaccines on the anti-S IgG Fc glycosylation, as reflected in relatively low long-term anti-S IgG1 galactosylation levels generated by the long-lived plasma cell pool, whose induction might be driven by a recently described TH1-driven B cell response for all three vaccines. Instead, repeated immunization of naïve individuals with the mRNA vaccines increased the proportion of the IgG4 subclass over time which might influence the long-term Ab effector functions. Taken together, these data shed light on these novel vaccine formats and might have potential implications for their long-term efficacy.IntroductionThe rapid spread of severe acute respiratory syndrome coronavirus type 2 (SARS-CoV-2), the cause of Coronavirus Disease 2019 (COVID-19), has led to a global health threat (1). The virus employs a transmembrane spike (S) protein that interacts through its receptor-binding domain (RBD) with the host membrane-bound angiotensin-converting enzyme 2 (ACE2) to enter cells of the respiratory tract (2). A range of different intramuscularly administered vaccines inducing an immune response against the S-protein have been developed, led by novel mRNA-containing lipid nanoparticle (LNP) vaccines such as the BNT162b2 and the mRNA-1273 vaccines from BioNTech/Pfizer (3) and Moderna (4), respectively, as well as replication-deficient adenovirus-based vaccines such as the ChAdOx1 nCoV-19 (AZD1222) vaccine from AstraZeneca (5).Neutralizing anti-S IgG and IgA antibodies (Abs) inhibiting the interaction of the viral S protein with ACE2 have been identified both in blood and in the respiratory tract after SARS-CoV-2 infection (6–9). Persisting neutralizing Abs in the respiratory tract likely constitute the first line of defense that protect from a subsequent SARS-CoV-2 re-infection and spreading (8, 10).Although both mRNA vaccines induce strong initial neutralizing anti-S IgG and IgA Ab responses in the blood, all three vaccines seem to be only moderately or temporarily protective against SARS-CoV-2 infection and spreading (3, 7, 9–15). Moreover, better vaccine-induced protection from infection and spreading has been described for previously (pre)-infected vaccinees (with SARS-CoV-2 history) as compared to naïve vaccinees (without SARS-CoV-2 history) (16, 17). However, long-term anti-S IgG and IgA levels in the blood and the respiratory tract have hardly been investigated in naïve and pre-infected subjects upon vaccination against SARS-CoV-2.Nevertheless, all three vaccines seem to induce high protection from severe disease conditions in the next weeks after a second immunization (5, 18, 19), assuming a robust long-term systemic T and B cell response – also against non-RBD parts of the virus and virus escape variants (15). However, the influence of the different new vaccine formats with unclear co-stimulatory/”adjuvant” effects on the long-term B cell and Ab Fc response remains unknown.IgG Fc-mediated effector functions are influenced by the induced IgG subclass and the IgG Fc N-glycosylation pattern. Human IgG1 and IgG3 subclasses have been described to convey the highest potential to activate immune cells via classical activating Fcγ receptors (FcγRs) and the classical complement pathway via C1q (20–24). These IgG subclasses can form hexamers, thereby facilitating the interaction with the six-arm C1q molecule (21, 25–29). IgG2 hardly interacts with classical FcyRs and C1q and its effector function-inducing capacity needs further investigation (20, 22, 23). In contrast, IgG4 shows higher affinity to the classical IgG inhibitory receptor FcyRIIB than to classical activating FcyRs (20, 22, 23). Furthermore, IgG4 cannot activate C1q but instead is able to disturb the hexamer formation of the C1q-activating IgG subclasses (21). Furthermore, IgG4 can generate Fab arm-exchange, meaning that heavy chains with different specificities can dimerize resulting in bispecific Abs, which reduces their ability to form immune-complexes (30). Thus, IgG1 and IgG3 are the IgG subclasses with the highest potential to activate the immune system, whereas IgG4 has less activating potential and can even inhibit the effector functions of IgG1 and IgG3.Both SARS-CoV-2 infection and vaccination initially induce the IgG1 and IgG3 subclasses against the S protein (5, 31–35).Another factor known to influence IgG Fc-mediated effector functions is the type of IgG Fc N-glycosylation. The highly conserved glycosylation site at Asn297 in the Fc moiety of IgG carries a complex type N-glycan characterized by a core structure, that can be further modified with a core fucose, a bisecting N-acetylglucosamine (GlcNAc) as well as one or two galactose residues, each of which can further be capped by a sialic acid (36–38) (Table S2).IgG Abs lacking fucose are known to have an increased affinity to activating FcyRIIIa and are linked to enhanced tumor-fighting potential as well as protection against HIV and malaria infection (38–43).Agalactosylated (G0) IgG Abs have been linked to severe conditions in inflammatory (auto-) immune diseases, whereas IgG sialylation has been associated with a decreased affinity of IgG to classical activating FcyRs and lower or anti-inflammatory effects (21, 24, 36, 37, 44–52). The functional analysis of differently glycosylated IgG Abs is complex because single terminal glycan residues may in addition interact with glycan binding receptors, such as galectins, siglecs, and C-type lectin receptors (37, 44, 53–55). In vivo, immune inhibitory functions have been described for sialylated as well as terminally galactosylated antigen-specific and total IgG Abs (36, 44–48, 52, 54, 56). IgG Fc bisection is often increased in inflammatory autoimmune diseases (57). However, the biological significance of IgG bisection is less clear and remains to be investigated.In the context of SARS-CoV-2 infection, transient afucosylated and more persistent agalactosylated anti-S IgG (1) Abs have been linked to pro-inflammatory disease conditions in COVID-19 patients without (non-ICU), but in particular in those with the need of intensive care unit (ICU) admission (32–35, 58–61). Noteworthy, severe COVID-19 conditions have also been linked to the appearance of broad autoreactive IgG Abs (62). It remains unclear whether a combination of inflammatory Fc glycosylation patterns and broad auto-reactivity of the induced IgG Abs contributes to the severe inflammatory complications of COVID-19. However, it has rather been investigated whether the inflammatory IgG glycosylation patterns contribute to the inflammatory complications than to the elimination of the virus. Afucosylated as well as agalactosylated anti-S IgG Abs may also strengthen the anti-viral response (63).A recent mouse vaccination study with different adjuvants showed that the early, likely extrafollicularly induced IgG Abs were characterized by high Fc galactosylation and sialylation levels (64). Over time, the potential of each adjuvant/co-stimulus to induce an IFNγ- and IL-17-producing T follicular helper (TFH1 and TFH17) cell-dependent germinal center (GC) B cell and Ab response became visible that correlated with lower long-term IgG Fc galactosylation and sialylation levels (64). A recent vaccination study against simian immunodeficiency virus in rhesus macaques with two different adjuvant formats found comparable results. In that study, the adjuvant, which correlated with better protection, also correlated with lower induced IgG galactosylation and sialylation levels (65).The new mRNA vaccines against SARS-CoV-2 generate an initial anti-S and -RBD IgG (1) Ab response with transient afucosylation, but high galactosylation and sialylation levels (32, 34, 35).However, little is known about repeated immunizations and the potential of the different new vaccine formats with unclear “adjuvant” effects on the long-term IgG subclass and IgG Fc glycosylation response.Here, we present a comprehensive analysis of serum-derived and salivary anti-S1 IgG (subclass) and IgA Ab responses as well as anti-S serum IgG1 Fc N-glycosylation patterns of SARS-CoV-2 naïve and pre-infected individuals vaccinated with different vaccine combinations over time to compare and characterize the long-term Ab responses up to day 270 post-immunization.Materials and methodsStudy cohortSARS-CoV-2 naïve (with no known SARS-CoV-2 history) and non-hospitalized pre-infected (with past SARS-CoV-2 infection history) subjects were recruited at the University of Lübeck and the University Medical Center Schleswig-Holstein (Lübeck, Germany) since December 2020. The regimens for six vaccination cohorts varied as follows: (i) 48 naïve individuals received two doses of the BioNTech/Pfizer vaccine BNT162b2 (each 30 µg) (3); (ii) 25 naïve individuals received two doses of the Moderna vaccine mRNA-1273 (each 100 µg) (4); (iii) 14 naïve individuals received two doses of the adenovirus-based vaccine ChAdOx1 nCoV-19 (AZD1222) from AstraZeneca (each 5x1010 virus particle with not less than 2.5x108 infectious units) (5); (iv) 12 naïve individuals received one dose of AZD1222 and subsequently one dose of BNT162b2; (v) 44 naïve individuals received one dose of AZD1222 and subsequently one dose of mRNA-1273; and (vi) 14 non-hospitalized pre-infected individuals received one or two doses of BNT162b2 (Table 1 and Table S1).TABLE 1Table 1 Vaccination study groups.The mRNA and adenovirus vaccinees received their second dose between day 21 and 45 or between day 70 and 84 (except for five vaccinees with AZD1222 that received their second dose between day 35 and 61), respectively, and were sampled (blood serum and/or saliva) once or multiple times up to 270 days after the first immunization.In addition to pre-vaccination samples of the pre-infected subjects, 2 further non-hospitalized pre-infected non-vaccinated individuals as positive controls, and 8 non-vaccinated naïve subjects as negative controls were recruited.No selection criteria were used and participants as well as repeated sampling were selected at random. However, the preferred vaccination strategies at the University Medical Center Schleswig-Holstein and the University of Lübeck were vaccination with two doses of BNT162b2 or the first vaccination with AZD1222 followed by a booster injection of mRNA-1273, respectively, explaining the comparably high numbers of individuals/samples in these two groups.The identification of pre-infected individuals was limited by their low incidence in the catchment area when the project was started in December 2020 and the number of vaccinated pre-infected individuals was even lower because of unclear recommendations regarding vaccination after infection.All recruited pre-infected individuals have had a mild pre-infection meaning that they neither had to go to the hospital (non-hospitalized) nor showed signs of shortness of breath or abnormal chest imaging during infection. Furthermore, most recruited pre-infected individuals were vaccinated with BNT162b2 and not with mRNA-1273 or AZD1222. So, we included only BNT162b2-vaccinated pre-infected in this study. Most of these pre-infected individuals were tested positive for SARS-CoV-2 in a narrow time window between 200 and 150 days before the first vaccination (only three individuals were infected earlier). This is why we decided not to investigate the influence of this period on the vaccine-induced Ab response.To verify and recognize pre-infected individuals, previous positive SARS-CoV-2 PCR results were considered, together with anti-viral nucleocapsid protein (anti-NCP) and anti-S1 serum IgG responses (Figure S1F).Blood samples and saliva were collected after obtaining written informed consent according to the Declaration of Helsinki in accordance with the local ethics board-approved protocol 20-123 (Ethics Committee of the University of Lübeck, Germany).Blood serum and saliva antibody detectionBlood samples were collected as described earlier (31). Salivary samples were collected with the Saliva Collection system-ORACOL Plus S14 (Malvern Medical Developments, United Kingdom) and frozen before usage.Enzyme-linked Immunosorbent Assays (ELISA) were used to detect always anti-S1 (the extracellular part of S containing the RBD) Abs. The EUROIMMUN SARS-CoV-2 S1 IgG (EUROIMMUN, Lübeck, Germany; #EI 2606-9601-2 G), the EUROIMMUN SARS-CoV-2 S1 IgA (#EI 2606-9601-2 A), and the EUROIMMUN SARS-CoV-2-NCP IgG (#EI 2606-9601-2 G) ELISA were performed according to manufacturer’s instructions; serum dilution: 1/101. A ratio to reference value was calculated by dividing the sample OD (450 nm) value by the OD (450 nm) value of a reference sample provided by the manufacturer.Alternatively, 96-well ELISA plates were coated with 4 µg/mL of SARS-CoV-2-S1 antigen (ACROBiosystems, Newark, DE 19711, USA; #S1N-C52H3) to identify anti-S1 serum IgG and IgG1-4 (Hansestadt Lübeck (HL)-1 ELISA), or anti-S1 salivary IgG and IgA (HL-2 ELISA) levels as recently described (31), or J-chain-coupled salivary Abs (HL-2 ELISA). These HL ELISA protocols were established in-house. Briefly (HL-1 and HL-2 ELISA), the plates were washed with 0.05% Tween 20 in PBS to remove unbound antigens. In case of anti-S1 salivary IgG and IgA detection additional blocking (HL-2 ELISA) was performed with 0.05% Tween 20, and 3% BSA in PBS. Subsequently, serum (diluted 1/1000 for IgG and IgG1, 1/100 for IgG2-4, and in addition 1/10 for IgG1 (Figure S2B) detection) or saliva (diluted 1/10 for IgG and IgA detection) in 0.05% Tween 20, 3% BSA in PBS were added. Bound Abs were detected with horseradish peroxidase (HRP)-coupled polyclonal goat anti-human IgG Fc (#A80-104P) or IgA (#A80-102P)-specific Abs purchased from Bethyl Laboratories (Montgomery, TX, USA) or monoclonal anti-human IgG1 (clone HP-6001), IgG2 (clone HP-6014), IgG3 (clone HP-6050), or IgG4 (clone HP-6025)-specific Abs purchased from Southern Biotech (Birmingham, AL, USA) or anti-J-chain Ab (clone F-12) obtained from Santa Cruz Biotechnology (Dallas, TX, USA; #sc-133177) in 0.05% Tween 20, 3% BSA in PBS. After incubation with the 3,3′,5,5′-tetramethylbenzidine (TMB) substrate (BD Biosciences, San Diego, CA, USA), the optical density (OD) was measured at 450 nm. Secondary Ab specificity was verified recently (31). OD (450 nm) values are shown or alternatively, a ratio to reference value was calculated by dividing the sample OD (450 nm) value through the OD (450 nm) value of an internal reference sample of an individual with a historic non-hospitalized SARS-CoV-2 infection.IgG Fc glycosylation analysisTotal IgG Abs were affinity-captured from sera using Protein G Sepharose 4 Fast Flow beads (GE Healthcare, Uppsala, Sweden) in a 96-well filter plate (Millipore Multiscreen, Amsterdam, Netherlands), as described (60, 66). Eluates from total IgG affinity-purification were dried by vacuum centrifugation and subjected to tryptic cleavage followed by liquid chromatography (LC)-mass spectrometry (MS) analysis according to established procedures (60). Using this method, IgG1 glycoforms were assigned based on accurate mass and specific migration position in LC, excluding the possible glycopeptide-level interference of IgG3 with IgG2 and IgG4 (66).LC-MS data processingRaw LC-MS spectra were converted to mzXML files. LaCyTools, an in-house developed software was used for the alignment and targeted extraction of raw data (67). Alignment was performed based on the average retention time of at least three highly abundant glycoforms. The analyte list for targeted extraction of the 2+ and 3+ charge states was based on manual annotation as well as on literature reports (60, 64).The inclusion of an analyte for the final data analysis was based on quality criteria including signal-to-noise (higher than 9), isotopic pattern quality (less than 25% deviation from the theoretical isotopic pattern), and mass error (within ±20 parts per million range) leading to a final analyte list (Table S2). The relative intensity of each glycan form in the final analyte list was calculated by normalizing it to the sum of their total glycoform areas. Normalized intensities were used to calculate fucosylation, bisection, galactosylation, and sialylation (Tables S2, S3). Serum samples with low anti-S Ab levels shortly after the first vaccination did not always result in sufficient signal strengths and hence were excluded from the analysis.Anti-S IgG1 Fc N-glycosylation patterns from unvaccinated, hospitalized non-ICU and ICU SARS-CoV-2 patients were used for comparison. Therefore, anti-S IgG1 Fc N-glycopeptide raw data of our previous study (60) were used to calculate the glycosylation traits based on the analyte list described above (Tables S2, S3).Statistical analysisStatistical analyses were performed using GraphPad Prism v6.0 and v9.0 (GraphPad, La Jolla, CA), and MatLab (The MathWorks Inc., Massachusetts, NE). The smoothed mean curves shown in scatter plots were created by polynomial regression. Confidence bands were plotted by using a confidence level of 95%. Data in bar graphs were presented as mean values ± SD. Differences between two groups were assessed with the Mann-Whitney U test. Differences between more than two groups were assessed with the Kruskal-Wallis test. Pearson correlation was done to measure the strength of the linear relationship between two variables. p-values < 0.05 were considered significant as follows: *, **, ***, ****: p-value < 0.05, 0.01, 0.001, and 0.0001 respectively. Principal component analyses (PCA) were performed in GraphPad Prism v9.0, while the partial least square-discriminant analyses (PLS-DA) were performed in PLS-Toolbox (Eigenvector Research Inc., Wenatchee, WA) in MatLab. For cross-validation, venetian blinds were used and the area under the receiver operating characteristic curve (AUROC) was calculated.ResultsSix study groupsNaïve and non-hospitalized SARS-CoV-2 pre-infected individuals were recruited in Lübeck, Germany, that received one of the six mRNA and adenovirus-based SARS-CoV-2 vaccine combinations shown in Table 1, and sampled (blood serum and/or saliva) once or multiple times up to 270 days after the first immunization.Anti-S1 serum and salivary IgG Ab responsesFirst, we analyzed anti-S1 serum IgG Ab levels by commercially available (EUROIMMUN) and in-house developed (HL) ELISA methods (Figures 1A–D, S1). Early anti-S1 serum IgG levels of naïve and pre-infected vaccinees were similar to those described earlier (9, 31, 34, 68). All three vaccines induced higher anti-S1 serum IgG levels after the second as compared to the first vaccination indicating a re-activation of memory B cells (Figures 1A, S1). Two vaccinations with an mRNA vaccine induced higher anti-S1 serum IgG levels than two vaccinations with the adenovirus-based vaccine AZD1222 (Figures 1A, S1). Among the mRNA vaccines, mRNA-1273 appeared to induce higher anti-S1 IgG titers than BNT162b2 (Figures 1A, S1). Individuals who received the first immunization with the adenovirus-based vaccine and the second with either one or the other mRNA vaccine reached IgG levels comparable to the levels induced with two mRNA vaccinations (Figures 1B, S1). High anti-S1 serum IgG levels were observed early on after the first vaccination with BNT162b2 in pre-infected individuals (Figures 1C, S1).FIGURE 1Figure 1 Anti-S1 serum and salivary IgG levels. (A-C) Anti-S1 serum IgG (HL-1 ELISA) levels (ratios to reference value) of the indicated six vaccination groups. (D) Color legend of the six study groups. (E–G) Anti-S1 salivary IgG levels (OD 450 nm values) of the indicated six groups. Gray bars: time windows of the second shot after the first shot with an mRNA (between day 21 and 45) or adenovirus-based (between day 70 and 84) vaccine. Dashed (— (C)) and dotted (…) lines indicate the corresponding average anti-S1 IgG levels of pre-infected individuals without/before vaccination or non-vaccinated healthy (negative) controls, respectively. (H, I) Pearson correlations between anti-S1 serum IgG (HL-1) levels and anti-S1 salivary IgG levels (y-axis as in (E)) of all paired samples from once and twice BNT162b2-vaccinated naïve and pre-infected individuals. p-values of the indicated correlations are shown.Subsequently, however, the high mRNA-induced anti-S1 serum IgG levels of naïve and pre-infected individuals waned and approached over time the long-term levels observed following two adenovirus-based vaccinations (Figures 1A–C, S1). Furthermore, anti-S1 IgG time courses were similar for and highly correlated between serum and saliva (Figures 1E–I, S1) assuming passive transfer of IgG between blood and lumen/mucosa of the respiratory tract (8, 69).Anti-S1 serum IgG subclass responsesNext, we analyzed anti-S1 serum IgG subclass abundances over time (Figures 2, S2, S3). Our observations confirmed recent findings describing that the mRNA vaccines initially induce anti-S1 IgG1 followed by IgG3 and IgG2 and hardly any IgG4 responses (31, 32, 34), whereas vaccination with AZD1222 mainly results in anti-S IgG1 and IgG3, but hardly any IgG2 and IgG4 (5) in the first weeks after immunization of naïve individuals (Figures 2, S2, S3).FIGURE 2Figure 2 Anti-S1 serum IgG1-4 subclass levels. Anti-S1 serum (A–C) IgG1, (F–H) IgG3, (K–M) IgG4, and (R, S) IgG2 levels (ratios to reference values) of the indicated six vaccination groups. The A+B and A+M groups were not analyzed for IgG2. The used color codes are identical to Figure 1D. Gray bars: time windows of the second shot after the first shot with an mRNA (between day 21 and 45) or adenovirus-based (between day 70 and 84) vaccine. Dotted lines indicate the corresponding average anti-S1 IgG1, IgG3, IgG4, or IgG2 levels of non-vaccinated healthy (negative) controls. (D, E, I, J, N–Q, T, U) Pearson correlations between anti-S1 serum IgG levels (HL-1) and anti-S1 serum (D, E) IgG1, (I, J) IgG3, (N–Q) IgG4, or (T, U) IgG2 levels (y-axis as in (A, F, K or R, respectively)) of all paired samples from one and two-times BNT162b2-vaccinated (D, I, N, T) naïve and (E, J, O, U) pre-infected individuals, and (P, Q) two-times mRNA-1273-vaccinated naïve individuals. The IgG to IgG4 correlation in (Q) was only done with paired long-term samples of two times mRNA-1273-vaccinated naïve individuals collected between day (d) 209 and 256 upon the first immunization. p-values of the indicated correlations are shown.Furthermore, we observed more intense differences in early anti-S1 IgG1 than in early IgG3 levels between the naïve mRNA and AZD1222 groups, with the high initial mRNA-induced anti-S1 IgG response dominated by IgG1 (Figures 2A, F, S2). Over time, anti-S1 IgG1 and IgG3 levels became comparable between the different naïve vaccination groups (Figures 2A, B, F, G, S2).Interestingly, the two times mRNA-1273 vaccination group and to a lesser extent also the two times BNT162b2 vaccination group generated long-term IgG4 responses (Figures 2K, S3). This long-term IgG4 response also developed in vaccinees receiving a combination of AZD1222 and mRNA-1273 as well as AZD1222 and BNT162b2 but to a lesser extent (Figures 2L, S3). Notably, the two times AZD1222 vaccination group did not show this long-term IgG4 response (Figures 2K, L, S3). The initial IgG2 response after mRNA vaccination kept higher over time as compared to the adenovirus-based vaccination in naïve individuals (Figures 2R, S, S3).In pre-infected individuals mostly an increase of IgG1 levels was observed after vaccination with BNT162b2. Although this group showed comparable long-term IgG subclass levels when compared to naïve individuals immunized with BNT162b2 (Figures 2C, H, M, S, S2, S3), their IgG4 response seemed not to be or barely induced (Figures 2M, O).The anti-S1 serum IgG1-3 levels of naïve and the IgG1 levels of pre-infected vaccinees positively correlated with their total anti-S1 serum IgG levels, whereas the IgG2-4 levels of pre-infected vaccinees did not show such a significant positive correlation (Figures 2D, E, I, J, T, U, S2, S3). In contrast, the anti-S1 serum IgG4 levels of naïve individuals vaccinated with mRNA vaccines showed a significant or in tendency negative correlation with their total anti-S1 serum IgG levels (Figures 2N–P, S3). However, anti-S1 serum IgG4 levels of naïve individuals vaccinated twice with mRNA-1273 significantly correlated with their total anti-S1 serum IgG levels, when only long-term samples (between day 209-256 upon the first immunization) were considered (Figures 2Q, S3).Thus, two immunizations or at least a second immunization with an mRNA vaccine generated detectable long-term IgG4 responses in naïve individuals. Notably, the mRNA-1273 vaccine showed a higher potential to generate such a late IgG4 response than the BNT162b2 vaccine.Anti-S1 serum and salivary IgA Ab responsesIn naïve individuals, two vaccinations with mRNA-1273 induced higher anti-S1 IgA levels in the serum and saliva than BNT162b2, and both mRNA vaccines induced higher levels than two doses with AZD1222 (Figures 3A, E, D, S4). In contrast to the anti-S1 IgG levels, the anti-S1 IgA levels in serum and saliva of naïve individuals seemed to be boosted less and decreased faster (Figures 3A, E, S4). Further, the mRNA vaccines induced only a reduced anti-S1 IgA response as compared to the anti-S1 IgG response when the individuals were first vaccinated with AZD1222 (Figures 3B, F, S4). Over time, the mRNA vaccine-induced IgA levels gradually approached the rather low IgA levels resulting from two AZD1222 vaccinations (Figures 3A, E, S4).FIGURE 3Figure 3 Anti-S1 serum and salivary IgA levels. (A-C) Anti-S1 serum IgA levels (ratios to reference value) of the indicated six vaccination groups. (D) Color legend of the six study groups. (E-G) Anti-S1 salivary IgA levels (OD 450 nm values) of the indicated six groups. Gray bars: time windows of the second shot after the first shot with an mRNA (between day 21 and 45) or adenovirus-based (between day 70 and 84) vaccine. Dashed and dotted lines indicate the corresponding anti-S1 IgA average levels of pre-infected individuals without/before vaccination or non-vaccinated healthy (negative) controls, respectively. (H, I) Pearson correlations between anti-S1 serum IgA and salivary IgA levels (y-axis as in (E)) of all paired samples from once and twice BNT162b2-vaccinated naïve and pre-infected individuals. p-values of the indicated correlations are shown.In contrast, pre-infected individuals vaccinated with BNT162b2 reached and maintained higher anti-S1 IgA levels both in serum and saliva over time compared to naïve individuals vaccinated with BNT162b2 (Figures 3C, G, S4). Anti-S1 saliva IgA levels correlated in pre-infected as well as naïve vaccinees with anti-S1 saliva J-chain levels (Figure S4) suggesting that the anti-S1 salivary IgA is mostly dimeric J-chain-coupled secretory (s)IgA in all groups. While anti-S1 salivary IgA levels correlated with anti-S1 serum IgA levels in naïve vaccinees, they did not display such a significant correlation in pre-infected ones (Figures 3H, I, S4). The findings suggest a proper, but more decoupled re-activation of local respiratory and systemic S1-reactive IgA+ B cells in pre-infected vaccinees.Anti-S serum IgG1 Fc N-glycosylationFinally, we analyzed the Fc N-glycosylation patterns of anti-S and total serum IgG1 over time up to day 270 by LC-MS (Figures 4, 5, S5, S6 and Tables S2, S3). The analysis resulted in the identification of 12 IgG1 Fc glycopeptide species (the six major glycan species are schematically shown in Figure 4A), from which glycosylation traits of fucosylation, bisection, sialylation, and galactosylation were calculated (Tables S2, S3). The development of the anti-S IgG1 glycosylation from the vacinees were compared to the anti-S IgG1 glycosylation from unvaccinated, hospitalized non-ICU and ICU SARS-CoV-2 patients investigated in the context of our previous study (60) (Figures 4E, I, 5D, H).FIGURE 4Figure 4 Anti-S serum IgG1 fucosylation and bisection. (A) The six major IgG Fc N-glycans attached to Asn 297 of IgG1 with an average relative abundance of more than 3% (Table S2): Galactose: G, yellow circle; sialic acid: S, purple diamond; fucose: F, red triangle; mannose: green circle; N-acetylglucosamine: GlcNAc and bisecting GlcNAc, N, blue square. (B–D) Anti-S serum IgG1 Fc N-fucosylation and (F–H) anti-S serum IgG1 Fc N-bisection of the indicated six vaccination groups. The used color codes are identical to Figure 1D. Gray bars: time windows of the second shot after the first shot with an mRNA (between day 21 and 45) or adenovirus-based (between day 70 and 84) vaccine. Dashed lines indicate the average level of total IgG1 Fc fucosylation or bisection, respectively (Figure S6). (E) Anti-S serum IgG1 Fc N-fucosylation and (I) anti-S serum IgG1 Fc N-bisection of unvaccinated, hospitalized non-ICU and ICU SARS-CoV-2 patients from our previous study (60) for comparison.FIGURE 5Figure 5 Anti-S serum IgG1 sialylation and galactosylation. (A–C) Anti-S serum IgG1 Fc N-sialylation and (E–G) anti-S serum IgG1 Fc N-galactosylation of the indicated six vaccination groups. The used color codes are identical to Figure 1D. Gray bars: time windows of the second shot after the first shot with an mRNA (between day 21 and 45) or adenovirus-based (between day 70 and 84) vaccine. Dashed lines indicate the average level of total IgG1 Fc sialylation and galactosylation, respectively (Figure S6). (D) Anti-S serum IgG1 Fc N-sialylation and (H) anti-S serum IgG1 Fc N-galactosylation of unvaccinated, hospitalized non-ICU and ICU SARS-CoV-2 patients from our previous study (60) for comparison. (I, J) Comparison of anti-S and total (Figure S6) IgG1 (I) sialylation and (J) galactosylation levels of samples from mRNA (B/M+B/M; B: blue; M: pink) or adenovirus (A+A) vaccinated individuals collected late between day (d) 190 and 256 upon first immunization. The green and red lines indicate the mean of the anti-S IgG1 sialylation and galactosylation levels from the hospitalized non-ICU and ICU patients, respectively, upon day 50 post-infection (D, H).FucosylationIn line with recent findings, the first immunization with BNT162b2 induced an initial anti-S IgG1 response with a temporarily afucosylated glycosylation pattern, with fucosylation levels as low as 80% (34) (Figures 4B, S5). The fucosylation levels steadily increased in the next few weeks to about 98%, surpassing the corresponding total IgG1 fucosylation levels (92-94%), and stagnated over the study period (Figures 4B, S5, S6). A comparable longitudinal fucosylation pattern was observed upon one and two mRNA-1273 immunizations (Figures 4B, S5, S6). On the other hand, one immunization with AZD1222 seemed to induce a less pronounced early afucosylated anti-S IgG1 response, only down to 95% fucosylation, that also increased upon a booster immunization, irrespective of the vaccine type, to about 98% (Figures 4B, C, S5, S6).In contrast, BNT162b2-vaccinated non-hospitalized pre-infected individuals maintained stable anti-S IgG1 fucosylation levels over time (96-97%), which was lower than the long-term anti-S IgG1 fucosylation level of naïve vaccinees and might have had developed already before vaccination as hypothesized previously (34) (Figures 4D, S5, S6). In comparison, the early, very low anti-S IgG1 fucosylation levels (down to 70% fucosylation) of the unvaccinated hospitalized patients vastly increased and reached fucosylation levels higher than 95% already 50 days post-infection (60) (Figure 4E).BisectionFollowing booster immunization with an mRNA vaccine, anti-S IgG1 bisection levels had fallen, then slightly increased, but remained below their total counterpart in naïve individuals throughout the study period (34) (Figures 4F, S5, S6).One dose of AZD1222 induced higher early anti-S IgG1 bisection levels than one dose of an mRNA vaccine (Figures 4F, S5) but after a second immunization irrespective of the vaccine type the levels became comparable to the levels upon two mRNA doses (Figures 4G, S5, S6). However, the two times AZD1222 vaccination group showed a slightly higher long-term anti-S IgG1 bisection level than the other vaccine combinations throughout the study period (Figures 4F, G, S5). Upon day 100, the anti-S IgG1 bisection levels of pre-infected vaccinees surpassed those of naïve individuals upon two BNT162b2 vaccinations (Figures 4H, S5, S6). The unvaccinated, hospitalized non-ICU and ICU patients showed very low early anti-S IgG1 bisection levels that increased over time (60) (Figure 4I).Galactosylation and sialylationBoth one and two immunizations of naïve individuals with an mRNA vaccine led to initial anti-S IgG1 sialylation as well as galactosylation levels higher than their corresponding total IgG1 sialylation and galactosylation levels, a finding consistent with our recent report (34) (Figures 5A, E, S5, S6). However, these anti-S IgG1 sialylation and galactosylation levels decreased with the passage of time and the anti-S1 galactosylation levels even fell below their corresponding total IgG1 levels, but still remained above the very low anti-S1 IgG1 galactosylation levels of the unvaccinated, hospitalized ICU patients that prevailed upon day 50 post-infection (60) (Figures 5A, E, D, H, I, J, S5, S6).Although the initial anti-S IgG1 galactosylation and sialylation levels were lower after one AZD1222 dose compared to one mRNA vaccine dose, the AZD1222-induced levels upon a second dose with AZD1222, BNT162b2, or mRNA-1273 became comparable to the mRNA-induced ones over time up to day 270 (Figure 5, S5, S6). BNT162b2-vaccinated, pre-infected individuals showed a comparable anti-S IgG1 sialylation and a slightly higher anti-S IgG1 galactosylation course to naïve individuals vaccinated with BNT162b2 (Figures 5C, G, S5, S6).Potential predictive parameters and long-term differences in the antibody response upon vaccinationNext, we explored associations between the data at baseline and at later time points to identify potential predictive correlations. Interestingly, age negatively correlated with the long-term anti-S1 serum IgG levels between day 190 and 256 in naïve individuals after two mRNA vaccinations but not after two AZD1222 vaccinations (Figures 6A, B). Furthermore, total IgG1 galactosylation, sialylation, and bisection levels positively correlated significantly or in tendency with the long-term anti-S IgG1 galactosylation, sialylation, and bisection levels, respectively, between day 190 and 256 of individuals from all groups (Figure 6C), suggesting an influence of the total IgG galactosylation, sialylation, and bisection levels on the induced anti-S T and B cell responses in all vaccination groups.FIGURE 6Figure 6 Investigation of potential predictive parameters. (A) Negative correlations between age and late anti-S1 IgG titers (samples collected between day (d) 190 and 256 upon first vaccination) (Left: HL-1 ELISA data; Right: EUROIMMUN (EI) ELISA data) of the indicated vaccination groups. (B) Color legend of the six naïve groups; the mRNA group includes both naïve B+B- and M+M-vaccinated individuals. (C) Positive correlations between total and anti-S IgG1 galactosylation, sialylation, and bisection levels from samples collected late between day (d) 190 and 256.Finally, we verified long-term outcomes between the vaccination groups by performing principal component analyses (PCA) and partial least square-discriminant analyses (PLS-DA) (Figures 7, S7). Conversely to PCA, PLS-DA considers the initial separation in vaccination groups. Both analyses resulted in comparable parameter-dependent separations, albeit with slightly stronger separations with the PLS-DA analysis (Figures 7, S7).FIGURE 7Figure 7 Partial least squares-discriminant analysis (PLS-DA) of the study groups (A) PLS-DA of data collected from the naïve and pre-infected B+(B) vaccination cohorts between day (d) 100 and 170 upon first vaccination. Latent variable (LV) scores and loadings are shown. The area under the receiver operating characteristic curve (AUROC) for cross-validation is 0.8810 for both groups. (B) Color legend of the six vaccination groups. (C) PLS-DA of data collected from all naïve vaccination cohorts between day (d) 209 and 256 upon first vaccination. The AUROC values for cross-validation are B+B: 0.8304, M+M: 0.8449, A+A: 0.7416, A+B: 0.8773, and A+M: 0.9094.Naïve and pre-infected vaccinees that received two BNT162b2 immunizations were separated between day 100 and 170 as follows: pre-infected vaccines showed higher anti-S1 serum and saliva IgA as well as anti-S IgG1 bisection and galactosylation levels, whereas naïve vaccines showed higher anti-S1 IgG4 and anti-S IgG1 fucosylation levels (Figures 7A, B, S7).Naïve individuals that received two AZD1222 immunizations were separated between day 209 and 256 by higher anti-S IgG1 bisection levels and lower anti-S1 IgG4 titers from two times mRNA vaccinated individuals (Figures 7C, S7). AZD1222 vaccinees that received an mRNA booster were separated between day 209 and 256 by higher anti-S1 Ab titers and anti-S IgG1 galactosylation and sialylation levels, likely because of later boosting (Figures 7C, S7).DiscussionOur study shows that the mRNA-containing LNP vaccines BNT162b2 and mRNA-1273 induce high anti-S1 IgG and IgA levels in the blood as well as in the saliva, but these Ig levels steadily decrease over time and approach levels that are comparable to the long-term levels induced by two immunizations with the adenovirus-based vaccine AZD1222. In the long run, such pronounced anti-S1 IgG and (s)IgA reductions in the saliva likely reflect the declining protection against infection and from spreading in the respiratory tract of naïve individuals (16, 17). On the other hand, the observed stronger anti-S1 (s)IgA response in the saliva of previously infected vaccinees – likely generated by re-activation of infection-induced local (s)IgA+ memory B cells – might explain their recently described higher protection from infection and spreading (16, 17, 70).Neutralizing mucosal Abs play a crucial role in preventing infections of the respiratory tract. Therefore, optimized vaccination strategies against pathogens of the respiratory tract should enhance local antigen-specific long-lived PC and memory B cell responses for generating an improved, long-lasting Ab frontline defense response in the mucosa of the respiratory tract (71–73).Both mRNA- and adenovirus-based vaccines generate comparable long-term anti-S1 IgG1 and IgG3 levels up to day 270; whereas the IgG2 levels remained higher after mRNA vaccination. Very interestingly, two mRNA immunizations as well as one AZD1222 immunization with an mRNA booster, in particular with the mRNA-1273 vaccine, induced long-term anti-S1 IgG4 responses – the IgG subclass with inhibitory effector functions – in naïve subjects. In contrast, we could not observe such an increase upon two immunizations with the AZD1222 vaccine in naïve individuals up to day 270, suggesting that only mRNA vaccines generate detectable long-term IgG4 responses at least until day 270.Supporting evidence stems from another study, where comparable results were observed, additionally with a further increase of IgG4 after the third vaccination with BNT162b2 (74). Similarly to our results, no IgG4 response was detected after two AZD1222 vaccinations. The authors observed a slight IgG4 response after immunization with AZD1222 and boosting with BNT162b2, while the effects of the mRNA-1273 vaccine were not analyzed.In a setting of HIV vaccination, a study compared repeated immunizations with two related HIV vaccine formats. The authors described that the protection of one vaccine composition correlated with the induction of IgG1 and IgG3 Abs, whereas the other vaccine composition hardly showed any protection, which correlated with the generation of IgG4 upon repeated immunizations instead (75). Interestingly, repeated immunizations by allergen-specific immunotherapy also induce IgG4 responses over time (50). Thus, the vaccine composition might influence the IgG4-inducing capacity upon repeated immunization.The mRNA-1273 vaccine (100 µg) contained higher amounts of mRNA than the BNT162b2 vaccine (30 µg) that will influence the amount and probably also the duration of S protein expression. One or both circumstances might contribute to the higher induction of anti-S1 IgG4 by the mRNA-1273 vaccine. Furthermore, the induced IgG4 class switching might occur directly from IgM or via other IgG subclass intermediates, as the Igg4 gene locus is the most downstream of the IgG subclass gene loci.Notably, in pre-infected individuals no IgG4 responses were observed upon one or two doses of BNT162b2. However, since the case number and observation period for pre-infected individuals were limited, further studies involving more long-term samples are needed to verify this observation. In the future, the potential non-neutralizing effects of IgG4 on the elimination of SARS-CoV-2 require further investigation.Two mRNA vaccinations and also an AZD1222 vaccination with an mRNA booster induce massive, but temporary anti-S1 IgG(1) responses, presumably generated by short-term PCs. Accordingly, IgG Abs generated by long-term PC responses become noticeable only after several months, when the short-lived PC responses had already faded. In contrast, the adenovirus-based vaccine induced a weaker short-term Ab response, and Abs generated by long-term PC subsets likely become dominant more rapidly.Early and late IgG responses may convey functionally divergent roles. Recent studies suggest that highly galactosylated and sialylated short-term IgG responses might facilitate antigen-delivery to GC reactions in a sialylation-dependent manner for aiding affinity maturation and therewith the induction of IgG Abs with high neutralizing potential (76, 77). In contrast, long-term IgG Ab responses with lower galactosylation and sialylation levels might thereby induce a stronger immune cell activation, also, after subsequent infection with emerging SARS-CoV-2 variants, when neutralizing capacities of the existing Abs are diminished due to their potentially reduced RBD-specificities (15).Although the fighting potential of differently galactosylated and sialylated IgG Abs against pathogens has to be further investigated, it is important to comprehend how vaccine compositions influence IgG Fc galactosylation and sialylation levels in both short- and long-term PC responses (78).A recent mouse immunization study has shown that the (inflammatory) potential of an adjuvant (co-stimulus) reflects the qualitative potential of a vaccine composition to determine the IgG Fc galactosylation and sialylation levels during the GC response and thereby the GC-derived long-term IgG response (64). In summary, the results suggested distinct adjuvant-specific (inflammatory) potentials of different adjuvants to induce GC-driven antigen-specific IgG Abs with low galactosylation and sialylation levels: CFA (complete Freund’s adjuvant; water-in-oil adjuvant+M.tb.) > IFA, Montanide (both water-in-oil adjuvants) > Alum (aluminum hydroxide) > AddaVax (similar to MF59; squalene-based), Toll-like-receptor ligands (64).Furthermore, the adjuvant Alum induced better protection from subsequent SIV infection than MF59, correlating with Alum-induced anti-gp120 IgG Abs with lower galactosylation and sialylation levels than MF59 (65).Mechanistically, the induction of antigen-specific IgG Abs with low galactosylation and sialylation levels in the GC (e.g. with IFA) was linked to the induction of IL-6-dependent IFNγ-producing T follicular helper TFH1 cells. CFA-induced very low IgG galactosylation and sialylation levels have further been linked to the additional induction of (inflammatory) IL-17-producing TFH17 cells (64).Strong inflammatory immune responses induced in ICU-admitted SARS-CoV-2-infected patients were characterized by high IL-6 and IL-17 levels (79), CCR6+ circulating (c)TFH17 cells in the blood (80, 81), and anti-S IgG1 Abs with very low galactosylation levels that prevailed upon day 50 post-infection (60) (Figure 5H). In contrast, non-hospitalized SARS-CoV-2 patients were characterized by CXCR3+ cTFH1 cells (82), and relatively higher anti-S IgG1 galactosylation and sialylation levels (34).The mRNA and adenovirus-based vaccines induced comparable, relatively low long-term anti-S IgG1 galactosylation levels both in naïve vaccinees and vaccinees with past infection. These anti-S IgG1 galactosylation levels were lower than their corresponding total IgG1 galactosylation levels, but not as low as the presumably pro-inflammatory anti-S IgG1 galactosylation levels of ICU patients that were prevailing upon day 50 post-infection (60).These findings suggest that the adenovirus-based and the two mRNA-containing LNP vaccine formats have a comparable, strong potential to influence the quality of the long-term anti-S IgG (1) response, as reflected by the low long-term anti-S IgG1 Fc galactosylation levels.The stimulatory “adjuvant” potential” of the adenovirus-based vaccine might be induced by the activation of pattern-recognition receptors (PRRs) on immune cells, due to its adenovirus-inherent activation nature. Although the mRNAs in the mRNA vaccines have been modified to reduce their interaction with PRRs, residual activation of PRRs cannot be excluded and potentially co-stimulate immune cells. In addition, it has been described that LNPs can also have a co-stimulatory adjuvant effect (83, 84).Recent mouse studies have shown that an mRNA-containing LNP vaccine is inducing rather a TFH1- than an IL-4-producing TFH2-driven GC response, whereas an RBD protein-AddaVax vaccination rather induced a TFH2-driven GC response (85). Furthermore, both mRNA vaccines have induced GC B cells and TFH1 > TFH2, but hardly TFH17 cells in human lymph nodes (86) as well as cTFH1 cells in the blood (82). For the adenovirus-based vaccine, a strong TH1 response has been described (87).Altogether, these human data suggest, comparable to the murine data described above (64), that the induction of (c)TFH17 > (c)TFH1 > (c)TFH2 cells might correlate with lower long-term antigen-specific IgG Fc galactosylation and sialylation levels upon vaccination. Accordingly, the moderate/relatively low long-term anti-S IgG galactosylation levels in the three vaccination groups seem to correlate with the recently described (c)TFH1-driven B cell response for all three vaccines.Predictively, age correlated with lower long-term IgG Ab titers for the mRNA vaccines. Furthermore, total IgG1 galactosylation, sialylation, and bisection levels correlated with higher long-term anti-S IgG1 sialylation, galactosylation, and bisection levels, respectively, for all vaccine combinations suggesting that total IgG Fc glycosylation patterns might influence the glycosylation patterns of antigen-specific immune responses upon vaccination.In summary, the data indicate that the high initial mRNA vaccine-induced anti-S1 IgG(1) and IgA responses decrease over time and approach levels induced with the adenovirus-based vaccine up to day 270. Higher and more stable anti-S1 (s)IgA levels in the saliva of pre-infected vaccinees might explain their higher protection from infection and spread of SARS-CoV-2.Intriguingly, the mRNA vaccines, and in particular the mRNA-1273 vaccine, induced increasing long-term anti-S1 serum IgG4 levels in naïve individuals with hitherto unclear influences on the fight against the pathogen. Naïve individuals vaccinated with the adenovirus-based vaccine did not show such long-term anti-S1 IgG4 response at least after two vaccinations until day 270.Instead, both the mRNA-containing LNP and adenovirus-based vaccines induced comparable anti-S IgG1 glycosylation responses over time up to day 270 as reflected in relatively low anti-S IgG(1) galactosylation levels. This low galactosylation level might reflect the stimulatory “adjuvant” potential of the new vaccine formats and a previously described, primarily TH1-driven B cell response, which overall may contribute to the described efficient protection from severe infections. Understanding the long-term adjuvant effects of mRNA and adenovirus-based vaccinations against SARS-CoV-2 will have potential implications on future vaccine designs.Data availability statementThe original contributions presented in the study are included in the article/Supplementary Material. Further inquiries can be directed to the corresponding authors.Ethics statementThe studies involving human participants were reviewed and approved by Ethics Committee of the University of Lübeck, Germany. The patients/participants provided their written informed consent to participate in this study.Author contributionsOrganization of blood and saliva sampling: JB, IK, AL, BF, EM, SL, MS, VK, JR, CS, TG, and ME. Serum and saliva ELISA analysis: JB, IK, AL, EM, SL, HL, CK, LD, JP, and JR. IgG glycosylation analysis: TP, WW, JN, and MW. Statistical analysis: JB, FS, CS, JR, TP, and ME. Supervision: MW and ME. Writing - original draft: JB, TP, MW, and ME. Initial submission: JB. All authors contributed to the article and approved the submitted version.FundingThis project received funding from the Deutsche Forschungsgemeinschaft ((DFG, German Research Foundation): grants 398859914 (EH 221/10-1); 400912066 (EH 221/11-1); 429175970 (RTG 2633); and 390884018 (Germany`s Excellence Strategies - EXC 2167, Precision Medicine in Chronic Inflammation (PMI)) (ME), the Federal State Schleswig-Holstein, Germany (“COVID-19 Research Initiative Schleswig-Holstein”): grant DOI4-Nr. 3 (ME) and the European Union’s Horizon 2020 research and innovation program H2020-MSCA-ITN: grant 721815) (TP). We acknowledge financial support by Land Schleswig-Holstein within the funding program Open Access Publication Fond. JB was a PhD student of the RTG 2633.Conflict of interestThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.Publisher’s noteAll claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.Supplementary materialThe Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/fimmu.2022.1020844/full#supplementary-materialReferences 1. Acter T, Uddin N, Das J, Akhter A, Choudhury TR, Kim S. Evolution of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) as coronavirus disease 2019 (COVID-19) pandemic: A global health emergency. Sci Total Environ (2020) 730:138996. doi: 10.1016/j.scitotenv.2020.138996PubMed Abstract | CrossRef Full Text | Google Scholar 2. Letko M, Marzi A, Munster V. Functional assessment of cell entry and receptor usage for SARS-CoV-2 and other lineage b betacoronaviruses. Nat Microbiol (2020) 5(4):562–9. doi: 10.1038/s41564-020-0688-yPubMed Abstract | CrossRef Full Text | Google Scholar 3. Polack FP, Thomas SJ, Kitchin N, Absalon J, Gurtman A, Lockhart S, et al. C4591001 clinical trial group. safety and efficacy of the BNT162b2 mRNA covid-19 vaccine. N Engl J Med (2020) 383(27):2603–15. doi: 10.1056/NEJMoa2034577PubMed Abstract | CrossRef Full Text | Google Scholar 4. Jackson LA, Anderson EJ, Rouphael NG, Roberts PC, Makhene M, Coler RN, et al. mRNA-1273 study group. an mRNA vaccine against SARS-CoV-2 - preliminary report. N Engl J Med (2020) 383(20):1920–31. doi: 10.1056/NEJMoa2022483PubMed Abstract | CrossRef Full Text | Google Scholar 5. Barrett JR, Belij-Rammerstorfer S, Dold C, Ewer KJ, Folegatti PM, Gilbride C, et al. Oxford COVID vaccine trial group. phase 1/2 trial of SARS-CoV-2 vaccine ChAdOx1 nCoV-19 with a booster dose induces multifunctional antibody responses. Nat Med (2021) 27(2):279–88. doi: 10.1038/s41591-021-01372-zPubMed Abstract | CrossRef Full Text | Google Scholar 6. Sterlin D, Mathian A, Miyara M, Mohr A, Anna F, Claër L, et al. IgA dominates the early neutralizing antibody response to SARS-CoV-2. Sci Transl Med (2021) 13(577):eabd2223. doi: 10.1126/scitranslmed.abd2223PubMed Abstract | CrossRef Full Text | Google Scholar 7. Seow J, Graham C, Merrick B, Acors S, Pickering S, Steel KJA, et al. Longitudinal observation and decline of neutralizing antibody responses in the three months following SARS-CoV-2 infection in humans. Nat Microbiol (2020) 5(12):1598–607. doi: 10.1038/s41564-020-00813-8PubMed Abstract | CrossRef Full Text | Google Scholar 8. Isho B, Abe KT, Zuo M, Jamal AJ, Rathod B, Wang JH, et al. Persistence of serum and saliva antibody responses to SARS-CoV-2 spike antigens in COVID-19 patients. Sci Immunol (2020) 5(52):eabe5511. doi: 10.1126/sciimmunol.abe5511PubMed Abstract | CrossRef Full Text | Google Scholar 9. Wang Z, Schmidt F, Weisblum Y, Muecksch F, Barnes CO, Finkin S, et al. mRNA vaccine-elicited antibodies to SARS-CoV-2 and circulating variants. Nature (2021) 592(7855):616–22. doi: 10.1038/s41586-021-03324-6PubMed Abstract | CrossRef Full Text | Google Scholar 10. Krammer F. SARS-CoV-2 vaccines in development. Nature (2020) 586:516–27. doi: 10.1038/s41586-020-2798-3PubMed Abstract | CrossRef Full Text | Google Scholar 11. Dagan N, Barda N, Kepten E, Miron O, Perchik S, Katz MA, et al. BNT162b2 mRNA covid-19 vaccine in a nationwide mass vaccination setting. N Engl J Med (2021) 384(15):1412–23. doi: 10.1056/NEJMoa2101765PubMed Abstract | CrossRef Full Text | Google Scholar 12. Shrotri M, Navaratnam AMD, Nguyen V, Byrne T, Geismar C, Fragaszy E, et al. Virus Watch Collaborative. Spike-antibody waning after second dose of BNT162b2 or ChAdOx1. Lancet (2021) 398(10298):385–7. doi: 10.1016/S0140-6736(21)01642-1PubMed Abstract | CrossRef Full Text | Google Scholar 13. Mizrahi B, Lotan R, Kalkstein N, Peretz A, Perez G, Ben-Tov A, et al. Correlation of SARS-CoV-2-breakthrough infections to time-from-vaccine. Nat Commun (2021) 12(1):6379. doi: 10.1038/s41467-021-26672-3PubMed Abstract | CrossRef Full Text | Google Scholar 14. Levine-Tiefenbrun M, Yelin I, Alapi H, Katz R, Herzel E, Kuint J, et al. Viral loads of delta-variant SARS-CoV-2 breakthrough infections after vaccination and booster with BNT162b2. Nat Med (2021) 27(12):2108–10. doi: 10.1038/s41591-021-01575-4PubMed Abstract | CrossRef Full Text | Google Scholar 15. Bartsch YC, Tong X, Kang J, Avendaño MJ, Serrano EF, García-Salum T, et al. Omicron variant spike-specific antibody binding and fc activity is preserved in recipients of mRNA or inactivated COVID-19 vaccines. Sci Trans Med (2022) 14(642):eabn9243. doi: 10.1126/scitranslmed.abn9243CrossRef Full Text | Google Scholar 16. Petráš M, Lesná IK, Večeřová L, Nyčová E, Malinová J, Klézl P, et al. The effectiveness of post-vaccination and post-infection protection in the hospital staff of three Prague hospitals: A cohort study of 8-month follow-up from the start of the COVID-19 vaccination campaign (COVANESS). Vaccines (Basel) (2021) 10(1):9. doi: 10.3390/vaccines10010009PubMed Abstract | CrossRef Full Text | Google Scholar 17. Abu-Raddad LJ, Chemaitelly H, Ayoub HH, Yassine HM, Benslimane FM, Al Khatib HA, et al. Association of prior SARS-CoV-2 infection with risk of breakthrough infection following mRNA vaccination in Qatar. JAMA (2021) 326(19):1930–9. doi: 10.1001/jama.2021.19623PubMed Abstract | CrossRef Full Text | Google Scholar 18. Tenforde MW, Self WH, Adams K, Gaglani M, Ginde AA, McNeal T, et al. Influenza and other viruses in the acutely ill (IVY) network. association between mRNA vaccination and COVID-19 hospitalization and disease severity. JAMA (2021) 326(20):2043–54. doi: 10.1001/jama.2021.19499PubMed Abstract | CrossRef Full Text | Google Scholar 19. Andrews N, Tessier E, Stowe J, Gower C, Kirsebom F, Simmons R, et al. Duration of protection against mild and severe disease by covid-19 vaccines. Engl J Med (2022) 386(4):340–50. doi: 10.1056/NEJMoa2115481CrossRef Full Text | Google Scholar 20. Nimmerjahn F, Ravetch JV. Fcgamma receptors as regulators of immune responses. Nat Rev Immunol (2008) 8(1):34–47. doi: 10.1038/nri2206PubMed Abstract | CrossRef Full Text | Google Scholar 21. Lilienthal GM, Rahmöller J, Petry J, Bartsch YC, Leliavski A, Ehlers M. Potential of murine IgG1 and human IgG4 to inhibit the classical complement and fcγ receptor activation pathways. Front Immunol (2018) 9:958. doi: 10.3389/fimmu.2018.00958PubMed Abstract | CrossRef Full Text | Google Scholar 22. Bruhns P, Iannascoli B, England P, Mancardi DA, Fernandez N, Jorieux S, et al. Specificity and affinity of human fcgamma receptors and their polymorphic variants for human IgG subclasses. Blood (2009) 113(16):3716–25. doi: 10.1182/blood-2008-09-179754PubMed Abstract | CrossRef Full Text | Google Scholar 23. Wang Y, Kreímer V, Iannascoli B, Goff OR, Mancardi DA, Ramke L, et al. Specificity of mouse and human fcgamma receptors and their polymorphic variants for IgG subclasses of different species. Eur J Immunol (2022) 52(5):753–9. doi: 10.1002/eji.202149766PubMed Abstract | CrossRef Full Text | Google Scholar 24. Buhre JS, Becker M, Ehlers M. IgG subclass and fc glycosylation shifts are linked to the transition from pre- to inflammatory autoimmune conditions. Front Immunol (2022) 13:1006939. doi: 10.3389/fimmu.2022.1006939PubMed Abstract | CrossRef Full Text | Google Scholar 25. Diebolder CA, Beurskens FJ, de Jong RN, Koning RI, Strumane K, Lindorfer MA, et al. Complement is activated by IgG hexamers assembled at the cell surface. Science (2014) 343:1260–3. doi: 10.1126/science.1248943PubMed Abstract | CrossRef Full Text | Google Scholar 26. Melis JP, Strumane K, Ruuls SR, Beurskens FJ, Schuurman J, Parren PW. Complement in therapy and disease: regulating the complement system with antibody-based therapeutics. Mol Immunol (2015) 67:117–30. doi: 10.1016/j.molimm.2015.01.028PubMed Abstract | CrossRef Full Text | Google Scholar 27. Wang G, de Jong RN, van den Bremer ET, Beurskens FJ, Labrijn AF, Ugurlar D, et al. Molecular basis of assembly and activation of complement component C1 in complex with immunoglobulin G1 and antigen. Mol Cell (2016) 63:135–45. doi: 10.1016/j.molcel.2016.05.016PubMed Abstract | CrossRef Full Text | Google Scholar 28. de Jong RN, Beurskens FJ, Verploegen S, Strumane K, van Kampen MD, Voorhorst M, et al. A novel platform for the potentiation of therapeutic antibodies based on antigen-dependent formation of IgG hexamers at the cell surface. PloS Biol (2016) 14:e1002344. doi: 10.1371/journal.pbio.1002344PubMed Abstract | CrossRef Full Text | Google Scholar 29. Cook EM, Lindorfer MA, van der Horst H, Oostindie S, Beurskens FJ, Schuurman J, et al. Antibodies that efficiently form hexamers upon antigen binding can induce complement-dependent cytotoxicity under complement-limiting conditions. J Immunol (2016) 197:1762–75. doi: 10.4049/jimmunol.1600648PubMed Abstract | CrossRef Full Text | Google Scholar 30. van der Neut Kolfschoten M, Schuurman J, Losen M, Bleeker WK, Martınez-Mart í ınez P, Vermeulen E, et al. Anti-in í flammatory activity of human IgG4 antibodies by dynamic fab arm exchange. Science (2007) 317(5844):1554–7. doi: 10.1126/science.1144603PubMed Abstract | CrossRef Full Text | Google Scholar 31. Lixenfeld AS, Künsting I, Martin EL, von Kopylow V, Lehrian S, Lunding HB, et al. The BioNTech/Pfizer vaccine BNT162b2 induces class-switched SARS-CoV-2-specific plasma cells and potential memory b cells as well as IgG and IgA serum and IgG saliva antibodies upon the first immunization. MedRxiv (2021) 2021:03. doi: 10.1101/2021.03.10.21252001CrossRef Full Text | Google Scholar 32. Farkash I, Feferman T, Cohen-Saban N, Avraham Y, Morgenstern D, Mayuni G, et al. Anti-SARS-CoV-2 antibodies elicited by COVID-19 mRNA vaccine exhibit a unique glycosylation pattern. Cell Rep (2021) 37(11):110114. doi: 10.1016/j.celrep.2021.110114PubMed Abstract | CrossRef Full Text | Google Scholar 33. Chakraborty S, Gonzalez J, Edwards K, Mallajosyula V, Buzzanco AS, Sherwood R, et al. Proinflammatory IgG fc structures in patients with severe COVID-19. Nat Immunol (2021) 22(1):67–73. doi: 10.1038/s41590-020-00828-7PubMed Abstract | CrossRef Full Text | Google Scholar 34. van Coillie V, Pongracz T, Rahmöller J, Chen H-JGeyer C, van Vlught LA, Buhre JS, et al. The BNT162b2 mRNA SARS-CoV-2 vaccine induces transient afucosylated IgG1 in naïve but not antigen-experienced vaccines. eBioMedicine (2023) 87:104408. doi: 10.1016/j.ebiom.2022.104408CrossRef Full Text | Google Scholar 35. Chakraborty S, Gonzalez JC, Sievers BL, Mallajosyula V, Chakraborty S, Dubey M, et al. Early non-neutralizing, afucosylated antibody responses are associated with COVID-19 severity. Sci Transl Med (2022) 14(635):eabm7853. doi: 10.1126/scitranslmed.abm7853PubMed Abstract | CrossRef Full Text | Google Scholar 36. Kaneko Y, Nimmerjahn F, Ravetch JV. Anti-inflammatory activity of immunoglobulin G resulting from fc sialylation. Science (2006) 313:670–3. doi: 10.1126/science.1129594PubMed Abstract | CrossRef Full Text | Google Scholar 37. Arnold JN, Wormald MR, Sim RB, Rudd PM, Dwek RA. The impact of glycosylation on the biological function and structure of human immunoglobulins. Annu Rev Immunol (2007) 25:21–50. doi: 10.1146/annurev.immunol.25.022106.141702PubMed Abstract | CrossRef Full Text | Google Scholar 38. Nimmerjahn F, Ravetch JV. Anti-inflammatory actions of intravenous immunoglobulin. Annu Rev Immunol (2008) 26:513–33. doi: 10.1146/annurev.immunol.26.021607.090232PubMed Abstract | CrossRef Full Text | Google Scholar 39. Shields RL, Lai J, Keck R, O’Connell LY, Hong K, Meng YG, et al. Lack of fucose on human IgG1 n-linked oligosaccharide improves binding to human FcγRIII and antibody-dependent cellular toxicity. J Biol Chem (2002) 277:26733–40. doi: 10.1074/jbc.M202069200PubMed Abstract | CrossRef Full Text | Google Scholar 40. Ferrara C, Grau S, Jäger C, Sondermann P, Brünker P, Waldhauer I, et al. Unique carbohydrate-carbohydrate interactions are required for high affinity binding between FcγRIII and antibodies lacking core fucose. PNAS (2011) 108:12669–74. doi: 10.1073/pnas.1108455108PubMed Abstract | CrossRef Full Text | Google Scholar 41. Ackerman ME, Crispin M, Yu X, Baruah K, Boesch AW, Harvey DJ, et al. Natural variation in fc glycosylation of HIV-specific antibodies impacts antiviral activity. J Clin Invest (2013) 123(5):2183–92. doi: 10.1172/JCI65708PubMed Abstract | CrossRef Full Text | Google Scholar 42. Larsen MD, Lopez-Perez M, Dickson EK, Ampomah P, Tuikue Ndam N, Nouta J, et al. Afucosylated plasmodium falciparum-specific IgG is induced by infection but not by subunit vaccination. Nat Commun (2021) 12(1):5838. doi: 10.1038/s41467-021-26118-wPubMed Abstract | CrossRef Full Text | Google Scholar 43. Bharadwaj P, Shrestha S, Pongracz T, Concetta C, Sharma S, Le Moine A, et al. Afucosylation of HLA-specific IgG1 as a potential predictor of antibody pathogenicity in kidney transplantation. medRxiv (2022). doi: 10.1101/2022.03.09.22272152CrossRef Full Text | Google Scholar 44. Anthony RM, Kobayashi T, Wermeling F, Ravetch JV. Intravenous gammaglobulin suppresses inflammation through a novel T(H)2 pathway. Nature (2011) 475(7354):110–3. doi: 10.1038/nature10134PubMed Abstract | CrossRef Full Text | Google Scholar 45. Oefner CM, Winkler A, Hess C, Lorenz AK, Holecska V, Huxdorf M, et al. Tolerance induction with T cell-dependent protein antigens induces regulatory sialylated IgGs. J Allergy Clin Immunol (2012) 129(6):1647–55. doi: 10.1016/j.jaci.2012.02.037PubMed Abstract | CrossRef Full Text | Google Scholar 46. Hess C, Winkler A, Lorenz AK, Holecska V, Blanchard V, Eiglmeier S, et al. T Cell-independent b cell activation induces immunosuppressive sialylated IgG antibodies. J Clin Invest (2013) 123(9):3788–96. doi: 10.1172/JCI65938PubMed Abstract | CrossRef Full Text | Google Scholar 47. Pincetic A, Bournazos S, DiLillo DJ, Maamary J, Wang TT, Dahan R, et al. Type I and type II fc receptors regulate innate and adaptive immunity. Nat Immunol (2014) 15(8):707–16. doi: 10.1038/ni.2939PubMed Abstract | CrossRef Full Text | Google Scholar 48. Ohmi Y, Ise W, Harazono A, Takakura D, Fukuyama H, Baba Y, et al. Sialylation converts arthritogenic IgG into inhibitors of collagen-induced arthritis. Nat Commun (2016) 7:11205. doi: 10.1038/ncomms11205PubMed Abstract | CrossRef Full Text | Google Scholar 49. Pfeifle R, Rothe T, Ipseiz N, Scherer HU, Culemann S, Harre U, et al. Regulation of autoantibody activity by the IL-23-TH17 axis determines the onset of autoimmune disease. Nat Immunol (2017) 18(1):104–13. doi: 10.1038/ni.3579PubMed Abstract | CrossRef Full Text | Google Scholar 50. Epp A, Hobusch J, Bartsch YC, Petry J, Lilienthal G-M, Koeleman CAM, et al. Sialylation of IgG antibodies inhibits IgG-mediated allergic reactions. J Allergy Clin Immunol (2018) 141(1):399–402.e8. doi: 10.1016/j.jaci.2017.06.021PubMed Abstract | CrossRef Full Text | Google Scholar 51. Bartsch YC, Rahmöller J, Mertes MMM, Eiglmeier S, Lorenz FKM, Stoehr AD, et al. Sialylated autoantigen-reactive IgG antibodies attenuate disease development in autoimmune mouse models of lupus nephritis and rheumatoid arthritis. Front Immunol (2018) 9:1183. doi: 10.3389/fimmu.2018.01183PubMed Abstract | CrossRef Full Text | Google Scholar 52. Petry J, Rahmöller J, Dühring L, Lilienthal G-M, Lehrian S, Buhre JS, et al. Enriched blood IgG sialylation attenuates IgG-mediated and IgGcontrolled-IgE-mediated allergic reactions. J Allergy Clin Immunol (2021) 147(2):763–7. doi: 10.1016/j.jaci.2020.05.056PubMed Abstract | CrossRef Full Text | Google Scholar 53. Banda NK, Wood AK, Takahashi K, Levitt B, Rudd PM, Royle L, et al. Initiation of the alternative pathway of murine complement by immune complexes is dependent on n-glycans in IgG antibodies. Arthritis Rheumatol (2008) 58:3081–9. doi: 10.1002/art.23865CrossRef Full Text | Google Scholar 54. Karsten CM, Pandey MK, Figge J, Kilchenstein R, Taylor PR, Rosas M, et al. Anti-inflammatory activity of IgG1 mediated by fc galactosylation and association of FcγRIIB and dectin-1. Nat Med (2012) 18(9):1401–6. doi: 10.1038/nm.2862PubMed Abstract | CrossRef Full Text | Google Scholar 55. van Osch TLJ, Nouta J, Derksen NIL, van Mierlo G, van der Schoot CE, Wuhrer M, et al. Fc galactosylation promotes hexamerization of human IgG1, leading to enhanced classical complement activation. J Immunol (2021) 207(6):1545–54. doi: 10.4049/jimmunol.2100399PubMed Abstract | CrossRef Full Text | Google Scholar 56. Ito K, Furukawa J, Yamada K, Tran NL, Shinohara Y, Izui S. Lack of galactosylation enhances the pathogenic activity of IgG1 but not IgG2a anti-erythrocyte autoantibodies. J Immunol (2014) 192(2):581–8. doi: 10.4049/jimmunol.1302488PubMed Abstract | CrossRef Full Text | Google Scholar 57. Flevaris K, Kontoravdi C. Immunoglobulin G n-glycan biomarkers for autoimmune diseases: Current state and a glycoinformatics perspective. Int J Mol Sci (2022) 23(9):5180. doi: 10.3390/ijms23095180PubMed Abstract | CrossRef Full Text | Google Scholar 58. Hoepel W, Chen HJ, Geyer CE, Allahverdiyeva S, Manz XD, de Taeye SW, et al. High titers and low fucosylation of early human anti-SARS-CoV-2 IgG promote inflammation by alveolar macrophages. Sci Transl Med (2021) 13(596):eabf8654. doi: 10.1126/scitranslmed.abf8654PubMed Abstract | CrossRef Full Text | Google Scholar 59. Larsen MD, de Graaf EL, Sonneveld ME, Plomp HR, Nouta J, Hoepel W, et al. Afucosylated IgG characterizes enveloped viral responses and correlates with COVID-19 severity. Science (2021) 371(6532):eabc8378. doi: 10.1126/science.abc8378PubMed Abstract | CrossRef Full Text | Google Scholar 60. Pongracz T, Nouta J, Wang W, van Meijgaarden KE, Linty F, Vidarsson G, et al. Immunoglobulin G1 fc glycosylation as an early hallmark of severe COVID-19. EBioMedicine (2022) 78:103957. doi: 10.1016/j.ebiom.2022.103957PubMed Abstract | CrossRef Full Text | Google Scholar 61. Vicente MM, Alves I, Gaifem J, Rodrigues CS, Fernandes Â, Dias AM, et al. Altered IgG glycosylation at COVID-19 diagnosis predicts disease severity. Eur J Immunol (2022) 52(6):946–57. doi: 10.1002/eji.202149491PubMed Abstract | CrossRef Full Text | Google Scholar 62. Woodruff MC, Ramonell RP, Saini AS, Haddad NS, Anam FA, Rudolph ME, et al. Relaxed peripheral tolerance drives broad de novo autoreactivity in severe COVID-19. medRxiv (2021) 2020:10. doi: 10.1101/2020.10.21.20216192CrossRef Full Text | Google Scholar 63. de Jong SE, Selman MH, Adegnika AA, Amoah AS, van Riet E, Kruize YC, et al. IgG1 fc n-glycan galactosylation as a biomarker for immune activation. Sci Rep (2016) 6:28207. doi: 10.1038/srep28207PubMed Abstract | CrossRef Full Text | Google Scholar 64. Bartsch YC, Eschweiler S, Leliavski A, Lunding HB, Wagt S, Petry J, et al. IgG fc sialylation is regulated during the germinal center reaction following immunization with different adjuvants. J Allergy Clin Immunol (2020) 146(3):652–666.e11. doi: 10.1016/j.jaci.2020.04.059PubMed Abstract | CrossRef Full Text | Google Scholar 65. Vaccari M, Gordon SN, Fourati S, Schifanella L, Liyanage NP, Cameron M, et al. Adjuvant-dependent innate and adaptive immune signatures of risk of SIVmac251 acquisition. Nat Med (2016) 22(7):762–70. doi: 10.1038/nm1016-1192aPubMed Abstract | CrossRef Full Text | Google Scholar 66. Falck D, Jansen BC, de Haan N, Wuhrer M. High-throughput analysis of IgG fc glycopeptides by LC-MS. Methods Mol Biol (2017) 1503:31–47. doi: 10.1007/978-1-4939-6493-2_4PubMed Abstract | CrossRef Full Text | Google Scholar 67. Jansen BC, Falck D, De Haan N, Ederveen ALH, Razdorov G, Lauc G, et al. LaCyTools: A targeted liquid chromatography-mass spectrometry data processing package for relative quantitation of glycopeptides. J Proteome Res (2016) 15:2198–210. doi: 10.1021/acs.jproteome.6b00171PubMed Abstract | CrossRef Full Text | Google Scholar 68. Adjobimey T, Meyer J, Sollberg L, Bawolt M, Berens C, Kovačević P, et al. Comparison of IgA, IgG and neutralizing antibody responses following immunization with moderna, BioNTech, AstraZeneca, Sputnik-V, Johnson and Johnson, and sinopharm’s COVID-19 vaccines. Res Square (2021) 13. doi: 10.21203/rs.3.rs-1197023/v1CrossRef Full Text | Google Scholar 69. Hettegger P, Huber J, Paßecker K, Soldo R, Kegler U, Nöhammer C, et al. High similarity of IgG antibody profiles in blood and saliva opens opportunities for saliva based serology. PloS One (2019) 14(6):e0218456. doi: 10.1371/journal.pone.0218456PubMed Abstract | CrossRef Full Text | Google Scholar 70. Azzi L, Dalla Gasperina D, Veronesi G, Shallak M, Ietto G, Iovino D, et al. Mucosal immune response in BNT162b2 COVID-19 vaccine recipients. EBioMedicine (2022) 75:103788. doi: 10.1016/j.ebiom.2021.103788PubMed Abstract | CrossRef Full Text | Google Scholar 71. Hopkins S, Kraehenbuhl JP, Schödel F, Potts A, Peterson D, de Grandi P, et al. A recombinant salmonella typhimurium vaccine induces local immunity by four different routes of immunization. Infect Immun (1995) 63(9):3279–86. doi: 10.1128/iai.63.9.3279-3286.1995PubMed Abstract | CrossRef Full Text | Google Scholar 72. Kantele A, Häkkinen M, Moldoveanu Z, Lu A, Savilahti E, Alvarez RD. Differences in immune responses induced by oral and rectal immunizations with salmonella typhi Ty21a: Evidence for compartmentalization within the common mucosal immune system in humans. Infect Immun (1998) 66(12):5630–5. doi: 10.1128/IAI.66.12.5630-5635.1998PubMed Abstract | CrossRef Full Text | Google Scholar 73. Eriksson K, Quiding-Järbrink M, Osek J, Möller A, Björk S, Holmgren J, et al. Specific-antibody-secreting cells in the rectums and genital tracts of nonhuman primates following vaccination. Infect Immun (1998) 66(12):5889–96. doi: 10.1128/IAI.66.12.5889-5896.1998PubMed Abstract | CrossRef Full Text | Google Scholar 74. Irrgang P, Gerling J, Kocher K, Lapuente D, Steininger P, Wytopil M, et al. Class switch towards non-inflammatory IgG isotypes after repeated SARS-CoV-2 mRNA vaccination. medRxiv (2022). doi: 10.1101/2022.07.05.22277189CrossRef Full Text | Google Scholar 75. Chung AW, Ghebremichael M, Robinson H, Brown E, Choi I, Lane S, et al. Polyfunctional fc-effector profiles mediated by IgG subclass selection distinguish RV144 and VAX003 vaccines. Sci Transl Med (2014) 6(228):228ra38. doi: 10.1126/scitranslmed.3007736PubMed Abstract | CrossRef Full Text | Google Scholar 76. Wang TT, Maamary J, Tan GS, Bournazos S, Davis CW, Krammer F, et al. Anti-HA glycoforms drive b cell affinity selection and determine influenza vaccine efficacy. Cell (2015) 162(1):160–9. doi: 10.1016/j.cell.2015.06.026PubMed Abstract | CrossRef Full Text | Google Scholar 77. Lofano G, Gorman MJ, Yousif AS, Yu WH, Fox JM, Dugast AS, et al. Antigen-specific antibody fc glycosylation enhances humoral immunity via the recruitment of complement. Sci Immunol (2018) 3(26):eaat7796. doi: 10.1126/sciimmunol.aat7796PubMed Abstract | CrossRef Full Text | Google Scholar 78. Alter G, Ottenhoff THM, Joosten SA. Antibody glycosylation in inflammation, disease and vaccination. Semin Immunol (2018) 39:102–10. doi: 10.1016/j.smim.2018.05.003PubMed Abstract | CrossRef Full Text | Google Scholar 79. Ling L, Chen Z, Lui G, Wong CK, Wong WT, Ng RWY, et al. Longitudinal cytokine profile in patients with mild to critical COVID-19. Front Immunol (2021) 12:763292. doi: 10.3389/fimmu.2021.763292PubMed Abstract | CrossRef Full Text | Google Scholar 80. Park JH, Lee HK. Delivery routes for COVID-19 vaccines. Vaccines (Basel). (2021) 9(5):524. doi: 10.3390/vaccines9050524PubMed Abstract | CrossRef Full Text | Google Scholar 81. Biswas B, Chattopadhyay S, Hazra S, Hansda AK, Goswami R. COVID-19 pandemic: the delta variant, T-cell responses, and the efficacy of developing vaccines. Inflammation Res (2022) 71(4):377–96. doi: 10.1007/s00011-022-01555-5CrossRef Full Text | Google Scholar 82. Koutsakos M, Lee WS, Wheatley AK, Kent SJ, Juno JA. T Follicular helper cells in the humoral immune response to SARS-CoV-2 infection and vaccination. J Leukoc Biol (2022) 111(2):355–65. doi: 10.1002/JLB.5MR0821-464RPubMed Abstract | CrossRef Full Text | Google Scholar 83. Alameh MG, Tombácz I, Bettini E, Lederer K, Sittplangkoon C, Wilmore JR, et al. Lipid nanoparticles enhance the efficacy of mRNA and protein subunit vaccines by inducing robust T follicular helper cell and humoral responses. Immunity (2021) 54(12):2877–2892.e7. doi: 10.1016/j.immuni.2021.11.001PubMed Abstract | CrossRef Full Text | Google Scholar 84. Bettini E, Locci M. SARS-CoV-2 mRNA vaccines: Immunological mechanism and beyond. Vaccines (Basel) (2021) 9(2):147. doi: 10.3390/vaccines9020147PubMed Abstract | CrossRef Full Text | Google Scholar 85. Lederer K, Castaño D, Gómez Atria D, Oguin TH 3rd, Wang S, Manzoni TB, et al. SARS-CoV-2 mRNA vaccines foster potent antigen-specific germinal center responses associated with neutralizing antibody generation. Immunity (2020) 53(6):1281–1295.e5. doi: 10.1016/j.immuni.2020.11.009PubMed Abstract | CrossRef Full Text | Google Scholar 86. Lederer K, Bettini E, Parvathaneni K, Painter MM, Agarwal D, Lundgreen KA, et al. Germinal center responses to SARS-CoV-2 mRNA vaccines in healthy and immunocompromised individuals. Cell (2022) 185(6):1008–1024.e15. doi: 10.1016/j.cell.2022.01.027PubMed Abstract | CrossRef Full Text | Google Scholar 87. Swanson PA 2nd, Padilla M, Hoyland W, McGlinchey K, PA F, Bibi S, et al. AstraZeneca/Oxford/VRC study Group.AZD1222/ChAdOx1 nCoV-19 vaccination induces a polyfunctional spike protein-specific TH1 response with a diverse TCR repertoire. Sci Transl Med (2021) 13(620):eabj7211. doi: 10.1126/scitranslmed.abj7211PubMed Abstract | CrossRef Full Text | Google ScholarKeywords: vaccination, antibody, SARS-CoV-2, COVID-19, IgG, IgG subclass, IgG glycosylation, IgACitation: Buhre JS, Pongracz T, Künsting I, Lixenfeld AS, Wang W, Nouta J, Lehrian S, Schmelter F, Lunding HB, Dühring L, Kern C, Petry J, Martin EL, Föh B, Steinhaus M, von Kopylow V, Sina C, Graf T, Rahmöller J, Wuhrer M and Ehlers M (2023) mRNA vaccines against SARS-CoV-2 induce comparably low long-term IgG Fc galactosylation and sialylation levels but increasing long-term IgG4 responses compared to an adenovirus-based vaccine. Front. Immunol. 13:1020844. doi: 10.3389/fimmu.2022.1020844Received: 16 August 2022; Accepted: 09 December 2022;Published: 12 January 2023.Edited by:Shikha Shrivastava, Pfizer, United StatesReviewed by:Lela Kardava, National Institute of Allergy and Infectious Diseases (NIH), United StatesJames Drew Brien, Saint Louis University, United StatesCopyright © 2023 Buhre, Pongracz, Künsting, Lixenfeld, Wang, Nouta, Lehrian, Schmelter, Lunding, Dühring, Kern, Petry, Martin, Föh, Steinhaus, von Kopylow, Sina, Graf, Rahmöller, Wuhrer and Ehlers. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.*Correspondence: Manfred Wuhrer, m.wuhrer@lumc.nl; Marc Ehlers, marc.ehlers@uksh.de†These authors share first authorship‡These authors share last authorship 



This article is part of the Research Topic



                    Current Landscape of Adjuvants for Optimizing the Immunogenicity of Vaccines
                

                        View all
4                        Articles 











"
https://news.ycombinator.com/rss,MSI's (in)Secure Boot,https://dawidpotocki.com/en/2023/01/13/msi-insecure-boot/,Comments,"

MSI's (in)Secure Boot


      posted on 2023-01-13 (UTC)
    

I guess I have found a reason to write my first blog post.
Before we start, maybe I will quickly explain what Secure Boot is. It is
a security feature, which allows our computer to decline booting
operating systems that have not been signed by a key that the firmware
trusts.
On 2022-12-11, I decided to setup Secure Boot on my new desktop with a
help of sbctl. Unfortunately I have
found that my firmware was… accepting every OS image I gave it, no
matter if it was trusted or not. It wasn't the first time that I have
been self-signing Secure Boot, I wasn't doing it wrong.
As I have later discovered on 2022-12-16, it wasn't just broken
firmware, MSI had changed their Secure Boot defaults to allow booting on
security violations(!!).
This can be changed by going to the place where the settings are for
Secure Boot on your motherboard. In my case it's in Security\Secure Boot. From this place, we can see a menu called ""Image Execution
Policy"", which is the culprit.

When we enter the menu, we can see the disappointing default settings.
It's doing no verification. It's useless. It's just there to satisfy
Windows 11 requirements. OS has no idea that Secure Boot is doing
nothing, it just knows that it's ""enabled"".

To change the settings to something saner, we have to change ""Always
Execute"" to ""Deny Execute"" for ""Removable Media"" and ""Fixed Media"".
What's funny is that ""Allow Execute"" and ""Query User"" options
are breaking UEFI specification,
though I'm not really sure what's the difference between ""Allow Execute""
and ""Always Execute"".
We can also change ""Option ROM"", about which you can read more about
here:

https://github.com/Foxboron/sbctl/wiki/FAQ#option-rom
https://learn.microsoft.com/en-us/windows-hardware/manufacture/desktop/uefi-validation-option-rom-validation-guidance


Case closed, everyone can move on, right?
Well, not really. I needed to figure out if this is only affecting my
motherboard or also other models and maybe even other vendors. And also
we need to document this, because even if I know this, there is probably
a lot of people that are not aware about this issue.
I had asked 2 users of B450 TOMAHAWK MAX (thanks Sage Hane and Daniel
Nathan Gray) to check their firmware and what? Unsurprisingly, it's also
there. We were able to determine that version 7C02v3C from 2022-01-18
introduced this issue.
Is it mentioned in the changelog? Hah, nope.

I have also received information from a user of B550-A PRO (CEC) (thanks
Joseph Richey) that they have this issue from 7C56vH1 (2021-12-20)
onwards.
While I was able to extrapolate this information to guess which versions
for other boards have introduced this issue, that isn't really enough.
We need to go deeper.
I have tried extracting some information from MSI's binary firmware
files, but to no avail. I tried using binwalk, UEFITool and others, but
I didn't really find what I wanted. Until one day I have found out that
UEFI has a thing called ""UEFI Internal Form Representation"" or in short
""IFR"". It's a way to describe firmware configuration options. This is
exactly what I need to look for! Now, what do I do with this knowledge?
Once we extract files from the firmware using UEFIExtract from
UEFITool project, we can find a
file called
Section_PE32_image_899407D7-99FE-43D8-9A21-79EC328CAC21_Setup_body.bin.
It contains most of UEFI GUI stuff and seems to be available on all
firmware from all major desktop motherboard makers, though ASUS decided
to remove ""Setup"" from the name for some reason or maybe it has to do
something to do with the UEFIExtract, not sure.
Now once we have this file, we have to extract IFR data from it, to do
it we can use IFRExtractor RS.
Funnily enough, it's made by the same people as UEFITool. Thanks guys
for your hard work, otherwise I would have to do it myself ;p.
Now with IFR extracted, we have what we wanted. We can see all the
UEFI settings available, including ""Image Execution Policy"".
Form FormId: 0x2A79, Title: ""Image Execution Policy""
	Text Prompt: ""Internal FV"", Help: """", Text: ""Always Execute""
	OneOf Prompt: ""Option ROM"", Help: ""Image Execution Policy on Security Violation per Device Path"", QuestionFlags: 0x10, QuestionId: 0x1116, VarStoreId: 0x28, VarOffset: 0x4, Flags: 0x10, Size: 8, Min: 0x0, Max: 0x5, Step: 0x0
		Default DefaultId: 0x0 Value: 0
		OneOfOption Option: ""Always Execute"" Value: 0
		OneOfOption Option: ""Always Deny"" Value: 1
		OneOfOption Option: ""Allow Execute"" Value: 2
		OneOfOption Option: ""Defer Execute"" Value: 3
		OneOfOption Option: ""Deny Execute"" Value: 4
		OneOfOption Option: ""Query User"" Value: 5
	End
	OneOf Prompt: ""Removable Media"", Help: ""Image Execution Policy on Security Violation per Device Path"", QuestionFlags: 0x10, QuestionId: 0x1117, VarStoreId: 0x28, VarOffset: 0x5, Flags: 0x10, Size: 8, Min: 0x0, Max: 0x5, Step: 0x0
		Default DefaultId: 0x0 Value: 0
		OneOfOption Option: ""Always Execute"" Value: 0
		OneOfOption Option: ""Always Deny"" Value: 1
		OneOfOption Option: ""Allow Execute"" Value: 2
		OneOfOption Option: ""Defer Execute"" Value: 3
		OneOfOption Option: ""Deny Execute"" Value: 4
		OneOfOption Option: ""Query User"" Value: 5
	End
	OneOf Prompt: ""Fixed Media"", Help: ""Image Execution Policy on Security Violation per Device Path"", QuestionFlags: 0x10, QuestionId: 0x1118, VarStoreId: 0x28, VarOffset: 0x6, Flags: 0x10, Size: 8, Min: 0x0, Max: 0x5, Step: 0x0
		Default DefaultId: 0x0 Value: 0
		OneOfOption Option: ""Always Execute"" Value: 0
		OneOfOption Option: ""Always Deny"" Value: 1
		OneOfOption Option: ""Allow Execute"" Value: 2
		OneOfOption Option: ""Defer Execute"" Value: 3
		OneOfOption Option: ""Deny Execute"" Value: 4
		OneOfOption Option: ""Query User"" Value: 5
	End
End
I have checked if other vendors (ASRock, ASUS, Biostar, EVGA, Gigabyte
and NZXT) have the same thing and I wasn't able to find anything like
that in their IFR. Also MSI's laptops are not affected by this issue.
I'm gonna assume that they figured that Microsoft wouldn't approve it
and/or that they had less tickets from people about Secure Boot related
issues for their laptops.
Now, doing this manually would be kinda annoying, so I made a small
little shell script which checks if ""Image Execution Policy"" menu is
available and if any of the three options are set to ""Always Execute"".
#!/bin/sh

if [ ! -d ""$1"" ]; then
	[ ! -f ""$1.zip"" ] && curl ""https://download.msi.com/bos_exe/mb/$1.zip"" -O -#
	bsdtar xf ""$1.zip""
fi

cd ""$1"" || exit
UEFIExtract ./*MS.* unpack 1>/dev/null
ifrextractor ./*.dump/Section_PE32_image_899407D7-99FE-43D8-9A21-79EC328CAC21_Setup_body.bin 1>/dev/null
output=""$(grep -A1 -E 'OneOf Prompt: ""(Option ROM|Removable Media|Fixed Media)"", Help: ""Image Execution Policy' ./*.dump/Section_PE32_image_899407D7-99FE-43D8-9A21-79EC328CAC21_Setup_body.bin.*ifr.txt)""
clear

if echo ""$output"" | grep -q ""DefaultId: 0x0""; then
	printf ""\033[1;31m%s: Bad\033[0m\n"" ""$1""
else
	printf ""\033[1;32m%s: Good\033[0m\n"" ""$1""
fi
Now this is where the fun part ends. Now I had to check firmware for
MSI's somewhat recent boards.
While we can get most of the firmware just by going to motherboard's
support page, MSI usually only lists stable firmware and the newest
beta. The problem is that I need to figure out the earliest affected
version of the firmware for each board, which means that I have to guess
what the betas were called (if they even existed). At least MSI doesn't
remove most of its beta firmware from their servers, so they are still
accessible if you know the link.
For some AMD boards, I have found a list of beta firmware on some German
forum. Thankfully, I didn't have to read any German, because contrary to
the popular belief, I don't know German or Russian, I'm Polish.
This… took forever. I checked every motherboard for:

AMD: TRX40, X670, X570, X470, X370, B650, B550, B450, B350, A520, A320
Intel: X399, X299, Z790, Z690, Z590, Z490, Z390, Z370, B760, B660, B560, B460, B360, H670, H510, H410, H370, H310

It's… a lot of motherboards. For a full list of affected motherboards
and their firmware versions, visit
sbctl#181.
And now it's time for some ""fun"" statistic:
# The amount of times I ran the script
$ history 0 | grep ""  msi "" | wc -l
1989
According to Wikipedia in 1989:

The first commercial Internet service providers surfaced in this year,
as well as the first written proposal for the World Wide Web and New
Zealand, Japan and Australia's first Internet connections.

Well, that was a mistake.
You could ask me, why didn't I automate it? The reason is… well… some of
it is not really easy to as some beta names have arbitrary suffixes
which was faster for me to guess than having a script bruteforce its way
in. Also some boards weren't listed on their motherboard list page.
Now, after doing all the work for MSI, I think I should bill them, that
or they should give me a lifetime supply of their motherboards.
If you are curious, yes, I have tried contacting MSI about this issue,
but they ignored my emails and other forms of communication I have
tried.

Conclusion

Don't trust that whatever security features you enabled are working,
TEST THEM! Somehow I was the first person to document this, even though
it has been first introduced somewhere in 2021 Q3.

Quiz time!

What's the difference between these 3 boards:

MSI B360 GAMING ARCTIC
MSI B360 GAMING PLUS
MSI B360-A PRO

Heatsink and PCB colours! They are the same board and share the same
firmware! But hey, the red and white one is only for gamers but black is
only suitable for ""professionals"".

"
https://news.ycombinator.com/rss,Project Mage is an effort to build a power-user environment in Common Lisp,https://project-mage.org,Comments,"




Project Mage













Project Mage
Home
About
Campaign
Code
Contact
RSS









nil


The Power of Structure →



Hi, and welcome! Project Mage is an effort to build a power-user environment and a set of applications in Common Lisp. To get an overview, see The Power of Structure. Otherwise, the essays listed below may be read in any order.








The Power of Structure (12 January 2023) Emacs is Not Enough (12 January 2023) On Flexibility and Software Temples (12 January 2023) Overcoming the Print-Statement (12 January 2023) Data-Supplied Web as a Model of Free Web (12 January 2023) Isn't It Obvious That C Programmers Wrote Git? (12 January 2023) Epilogue (12 January 2023) [Appendix] Why Common Lisp for This Project (12 January 2023) [Appendix] All Else is Not Enough (12 January 2023)







...proudly created, delivered and presented to you by: some-mthfka. Mthfka!



"
https://news.ycombinator.com/rss,Quitting the rat race,https://seanbarry.dev/posts/quitting-the-rat-race/,Comments,"Quitting the Rat Race - Seán BarryAll ArticlesQuitting the Rat RaceSome readers may be familiar with the cartoon “Happiness” by Steve Cutts. If you’re not, it’s embedded below. Please take four minutes to give it a watch if you haven’t already:
  
Around the 30 second mark is a scene where the protaganist rat is waiting for a train to arrive at a packed platform. I recently had a sobering realisation while standing waiting at the platform for the Waterloo & City Line: this had become my life.
I grew up in one of the least densely populated parts of the United Kingdom. My childhood was a happy one: full of trees, hills, mountains, lakes, beaches, and all of the adventures to be found in such places. I now live in one of the most nature depleted parts of an already nature depleted country. 
There’s so much noise in the city around me: planes approaching Heathrow, scooters whizzing around to deliver bags of fast food, cars revving while breaking the speed limit to get to the next red light, people playing music out loud while travelling and rowdy groups heading home from the pub.
When I cycle I need to cycle through kilometre after kilometre of urban jungle before I see anything resembling a field. When I run I have to stop frequently to cross dangerous roads. I’m constantly aware that the air I’m breathing is polluted and the water I drink has been recycled many times.
I’m currently working at a top tier investment bank as a software engineer. I’m an insignificant cog in a machine that skims the cream from the milk. I’m earning the most money I’ve ever made and yet I’m the least fulfilled I’ve ever been. 
Almost everything around me is designed to addict me. Every storefront specifically engineered to attract me inside with gimics like flashing lights. There are countless places I can go to buy experiences - simulations supposed to release some chemicals in my brain and give me a thrill.
The truth is: nothing I’ve done or experienced in this place has given me any experience comparable to walking along the ridge between two mountains. Nothing has made me feel alive like getting in to freezing cold water despite my body screaming at me not to. Nothing has made me feel anything like that feeling when you summit a mountain after 2 hours of solid climbing in the rain, and the clouds part to reveal the most spectacular and breathtaking view you’ve ever seen.
The best part about those things is that there is no booking system. There is no door security choosing who gets in because there is no door. It’s all there, ready to be experienced, and free.
Right now in the UK and across the world, things are uncertain. Companies are laying workers off, there’s a cost of living and energy crisis. I’m excruciatingly lucky to have been in the right place at the right time to develop the skillset I’ve got. In times like these every signal is telling me to stay on the path I’m on, enjoying the comfort and safety of a high paying job. 
I’m not going to listen. I’m quitting and I’m leaving this place. I’ll see you in the mountains.



If this resonated with you - reach out to me on Twitter - @SeanBarryUK.Get notified when I post a new articleYour email address will never be shared or spammed. You'll receive a notification when I post a new article and that's it.SubscribePublished 16 January 2023PersonalBlogI'm a front office Software Engineer, building risk systems in London. I've been coding since 2013 and have worked in a number of startups/scaleups.Seán Barry on Twitter"
https://news.ycombinator.com/rss,Major standard library changes in Go 1.20,https://blog.carlmjohnson.net/post/2023/golang-120-arenas-errors-responsecontroller/,Comments,"What’s New in Go 1.20, Part II: Major Standard Library ChangesTuesday, January 17, 2023In the first part of What’s New in Go 1.20, we looked at language changes. For part two, I would like to introduce three changes to the standard library that address problems that the community has been thinking about and debating solutions to for years.First of all, a whole new package has been added. But you can’t import it by default, and you probably shouldn’t be using it at all. It’s the new experimental arena package.The arena package was proposed by Dan Scales and has been added to the Go standard library in 1.20. But if you just try to add import ""arena"" to a program, you get the following, somewhat cryptic error message:imports arena: build constraints exclude all Go files in GOROOT/src/arena
To opt into using arenas, you need to set GOEXPERIMENT=arenas when calling the go tool, like GOEXPERIMENT=arenas go build ..So what are arenas and why is the Go team trying so hard to keep you from using them? I asked ChatGPT, and this is what it said (this is the equivalent of quoting Webster’s Dictionary for the 21st century):Memory arenas are a memory management technique used in some programming languages and libraries to allocate and deallocate large blocks of memory efficiently. They are typically used in situations where the program needs to frequently allocate and deallocate a large number of small objects. By allocating and deallocating memory in large blocks, rather than individually for each object, memory arenas can reduce the overhead associated with memory management and improve performance.If you want to go more in-depth, Uptrace has a nice guide to the arena package (presumably written by a human, but who knows nowadays), but I’ll try to just give a basic overview here.As you probably know, Go is a garbage collected language. This means that when you refer to a variable, the compiler and the runtime automatically keep track of the uses of that variable to see when it comes into use and when it is no longer being used. Once a variable is no longer used, it is “garbage” waiting to be collected.For many kinds of applications, garbage comes in waves. For example, if you have a web server, it may allocate a lot of memory in order to build up a response to some user request, but once it responds, it no longer needs any of the memory that it allocated, so it can all be returned to the system at once. Another example is a game might want to free all of the objects created for a level once the level is over. The arena package lets Gophers opt into this approach to memory management in performance critical code. Instead of having the garbage collector start a root and then travel down to “mark and sweep” the live memory and return the dead objects, the whole arena can be marked as dead all at once. The release notes for Go 1.20 claim thatWhen used appropriately, [using package arena] has the potential to improve CPU performance by up to 15% in memory-allocation-heavy applications.This is highly efficient, but also highly dangerous. What if the programmer makes a mistake, and for example, adds some strings to a logger call that outlives the request? The log might be overwritten by a subsequent request and the string become replaced with junk data, leading to crashes or worse—security exploits.To mitigate the risk of these kinds of bugs, the arena package will deliberately cause a panic if can detect someone reusing memory after it has been freed. Dan Scales explains,Each arena A uses a distinct range in the 64-bit virtual address spaceA.Free unmaps the virtual address range for arena AThe physical pages for the arena can then be reused by the operating system for other arenas.If a pointer to an object in arena A still exists and is dereferenced, it will get a memory access fault, which will cause the Go program to terminate. Because the implementation knows the address ranges of arenas, it can give an arena-specific error message during the termination.There is a similar comment in the Go runtime package that implements memory arenas:// What makes the arenas here safe is that once they are freed, accessing the
// arena's memory will cause an explicit program fault, and the arena's address
// space will not be reused until no more pointers into it are found. There's one
// exception to this: if an arena allocated memory that isn't exhausted, it's placed
// back into a pool for reuse. This means that a crash is not always guaranteed.
So, it is still possible to write buggy code with arenas, but hopefully, the bugs will translate into simple crashes rather than full blown memory corruption or security exploits.The arena package has a fairly simple API. Here’s some example code from arenas_test.go:a := arena.NewArena()
defer a.Free()

tt := arena.New[T1](a)
tt.n = 1

ts := arena.MakeSlice[T1](a, 99, 100)
// …
There is also an arena.Clone function for when you want to move an object out of an arena and onto the regular Go memory heap.With luck, the arena experiment will succeed, and we will see it introduced as a regular package in a future version of Go.While most Go programmers probably will never need to use the arena package directly, I suspect virtually all Go programmers will have some occasion to use a different new feature in Go 1.20: multierrors.The concept of multierrors in Go is not new. Hashicorp’s go-multierror package goes all the way back to 2014 and there was at least one proposal to add multierrors to the standard library by 2017.Multierrors also exist in other languages. Python added exception groups to Python 3.11, for example. In the case of Python, while there was a popular third party MultiError class, it ultimately needed to be added to the language for full operability:Changes to the language are required in order to extend support for exception groups in the style of existing exception handling mechanisms. At the very least we would like to be able to catch an exception group only if it contains an exception of a type that we choose to handle. Exceptions of other types in the same group need to be automatically reraised, otherwise it is too easy for user code to inadvertently swallow exceptions that it is not handling.Unlike Python, in Go, errors are just values, so it was easy enough to create your own multierror type and expose it using errors.As. Indeed, I wrote my own multierror package that worked this same way. This was clearly an idea that was being created and recreated by the community, so did it really need to be solved at the level of the standard library?Suppose I have some code like this:a := errors.New(""a"")
b := errors.New(""b"")
c := join(a, b)
d := fmt.Errorf(""more context: %w"", c)
e := errors.New(""e"")
f := join(d, e)
If join flattens the error list, then d (“more context”) will be lost from the resulting multierror. Inside of f will be a, b, and e, but not d.afbeIf you are just using your own multierror package in your own application, this is basically a theoretical concern, because you can make sure not to add context around a multierror wherever it might be lost. But if multierrors are part of the standard library and can be expected to be used regularly, then losing d is a real problem that could pop up at unwanted times and places. This problem is what sunk one of the more recent multierror proposals.The reason that Damien Neil’s latest multierror proposal has succeeded where other multierror proposals did not is that it creates a tree of errors, rather than a slice. The accepted errors.Join code instead represents the error hierarchy like this:adcfbeThis means that all of the nodes in the tree are still available for inspection using errors.Is or errors.As.In Go 1.20, multierrors can be created either with errors.Join(errs ...error) error or by using multiple %w verbs with fmt.Errorf like fmt.Errorf(""%w: %w"", notFoundErr, dbErr). Once created, errors.Is and errors.As can extract any of the values in the tree, whether they are in a leaf like a, b and e or a branch like d. Users can also create their own multierror types by adding an Unwrap() []error method to their custom error type.One wrinkle in the implementation is that for now at least, there is no errors.Split(error) []error function. This is a deliberate omission, since that would flatten the tree. Instead, if you need to inspect every node in a tree (for example, for logging purposes), you are encouraged to traverse the tree yourself. I suspect that if we ever see a generic iterator type in Go, something that automatically walks the tree might be added then.Finally, lets look at http.ResponseController.Interfaces are one Go’s defining features as a language. With an interface, you can specify that your code can accept any value that has a certain method, no matter what its concrete type is.From the early days of Go, interfaces have also been used for what Chris Siebenmann has called “interface smuggling”:In interface smuggling, the actual implementation is augmented with additional well known APIs, such as io.ReaderFrom and io.WriterTo. Functions that want to work more efficiently when possible, such as io.Copy(), attempt to convert the io.Reader or io.Writer they obtained to the relevant API and then use it if the conversion succeeded:if wt, ok := src.(WriterTo); ok {
   return wt.WriteTo(dst)
}
if rt, ok := dst.(ReaderFrom); ok {
   return rt.ReadFrom(src)
}
[... do copy ourselves ...]
I call this interface smuggling because we are effectively smuggling a different, more powerful, and broader API through a limited one. In the case of types supporting io.WriterTo and io.ReaderFrom, io.Copy completely bypasses the nominal API; the .Read() and .Write() methods are never actually used, at least directly by io.Copy (they may be used by the specific implementations of .WriteTo() or .ReadFrom(), or more interface smuggling may take place).Russ Cox deemed this pattern the somewhat less pejorative sounding “extension interface pattern”. Whatever you call it, this pattern can be a great way to expose a simple API and still leave room for adding more complicated extensions later.Extension interfaces are useful, but not without their pitfalls. There can be be cases where it would be good to implement an extended interface, but an implementation author isn’t aware of it, so they fail to implement it. This can be addressed with documentation, but it’s not as clear as using the type system directly.What if you want to create a wrapper a simple interface that you do know might also implement an extended interface? How to do this depends on what exactly you’re wrapping and why. If you were creating a new version of io.LimitedReader, it wouldn’t make sense to add a WriteTo method. The whole point is to put a cap on how much can be read from the source reader, not bypass it and hook it up to the writer directly. As an implementation author, you need to think carefully about how extended interfaces interact with your type.Another problem is if the extended interface can only be tested for through a type assertion. In that case, as the author of a wrapper, you need to provide two wrapping types: one that has the extended interface and passes calls through to the underlying type, and one that doesn’t have the method, so it won’t spuriously trigger the type assertion. Worse still, if there are multiple extended interfaces you want to be able to wrap, you need to provide a 2N number of types to provide for every variation of extended interfaces coexisting! This may seem absurd, but this is exactly the situation that library authors who wanted to wrap http.ResponseWriter found themselves in.http.ResponseWriter is a fairly simple interface used for HTTP servers in the Go standard library:type ResponseWriter interface {
    Header() Header
    WriteHeader(statusCode int)
    io.Writer
}
You can set the headers on a response. You can set the status code on the response (which also causes the headers to be written on the wire). And you can write the body of a response. Simple!But of course, the Go http package also supports a number of extended interfaces for ResponseWriter. These are http.Flusher (which lets you flush an in progress write to its clients), http.Pusher (which lets you do HTTP/2 server push requests), http.Hijacker (which provides access to the underlying net.Conn), and io.ReaderFrom (which allows for nice things like automatic sendfile support). As a result, the go-chi project has six different types to implement its WrapResponseWriter interface type. (This is cut down from 24 to 23 on the theory that anything which implements HTTP/2 server push must be maximally fancy.)So then all the way back in 2016, Filippo Valsorda opened an issue about setting timeouts in an http.Handler. This was clearly a real need the Go community had, but it was hard to see how to make it work while retrofitting it into the existing http.ResponseWriter interface. It would be great to set server timeouts for a client based on what we know about that client, but how can this functionality be exposed? Do we just go straight from 3 to 5 optional methods defined in the http package?Despite a lot of careful thought about the problem, this was the situation until Go 1.20, when Damien Neil successfully landed the http.ResponseController proposal. As he wrote,A problem is that we have no good place at the moment to add functions that adjust these timeouts. We might add methods to the ResponseWriter implementation and access them via type assertions (as is done with the existing Flush and Hijack methods), but this proliferation of undiscoverable magic methods scales poorly and does not interact well with middleware which wraps the ResponseWriter type.The solution he came up with based on prior discussions was to add a new concrete type, http.ResponseController:// NewResponseController creates a ResponseController for a request.
//
// The ResponseWriter should be the original value passed to the Handler.ServeHTTP method,
// or have an Unwrap method returning the original ResponseWriter.
//
// If the ResponseWriter implements any of the following methods, the ResponseController
// will call them as appropriate:
//
//  Flush()
//  FlushError() error // alternative Flush returning an error
//  Hijack() (net.Conn, *bufio.ReadWriter, error)
//  SetReadDeadline(deadline time.Time) error
//  SetWriteDeadline(deadline time.Time) error
//
// If the ResponseWriter does not support a method, ResponseController returns
// an error matching ErrNotSupported.
func NewResponseController(rw ResponseWriter) *ResponseController
There are two aspects of http.ResponseController that fix the problems with the earlier extension interfaces. One, the addition of an Unwrap() http.ResponseWriter method allows middleware to easily wrap a ResponseWriter without needing to provide all the extended interface methods. Two, the addition of ErrNotSupported makes it easy for types that do have extended methods to signal to their callers when the types they wrap don’t have the same extended methods they do. These are best practices for extended interfaces that have emerged from experience using them. If you provide an extended interface, also provide an escape hatch for wrapper types that don’t know if their wrapped types will have the extended types or not.This is my blog, so I will immodestly mention that I had a proposal for adding an Unwrap method to ResponseWriter back in 2020, but my proposal didn’t have a concrete type to handle the unwrapping or ErrNotSupported, and I’m not sufficiently in the weeds of the http package to have been able to implement read/write deadlines if I had known to suggest it as a motivating problem. The 3 line long http.MaxBytesHandler is about the limit of my ability to contribute. 😆For all three of these changes written about above, the moral of the story is that when it comes to the Go standard library, it can take sometimes years for all the pieces of a good solution to come together in one place, but when they do, it can solve a longstanding problem in a way that makes things easier for everyone involved going forward. Even the authors of Go didn’t know what idiomatic Go code looked like when they wrote the standard library, but working together today we can evolve the standard library in a way that adds new capabilities while preserving backwards compatibility, so that everyone benefits.See more in this series…ChatGPTChris SiebenmannDamien NeilFilippo ValsordagolangprogrammingRuss CoxUptraceRelated ArticlesWhat’s New in Go 1.20, Part I: Language ChangesWhat’s new in Go 1.19?Three Minor Features in Go 1.18Adding Some Func to Go’s Flag PackageEven More Minor Features in Go 1.18
SubscribeComments"
https://news.ycombinator.com/rss,TI-Basic interpreter written in JavaScript,https://www.davidtorosyan.com/ti-js/,Comments,"


















TI-JS | TI-Basic interpreter written in JavaScript


















TI-JS

TI-Basic interpreter written in JavaScript

View on GitHub











What is this?
If you’re familiar with the TI-84 graphing calculator,
you might know that you can program it with code that looks like this:

Input

2->X
Disp X+3
Well, this project aims to implement that language in JavaScript
so that it can run in the browser.
Try changing the input above to see this output change:

Output



ERR: Looks like JavaScript is disabled. As this is a JS library, the demo won't work!


But why?
I wrote a lot of programs in TI-Basic a long time ago,
and I’d like to be able to preserve and look back on them.
You can see my collection at
ti84-entertainment;
I hope to show all of those off here some day.
And it works?
Kinda. Check out the tests to see what’s supported,
or experiment in the playground.
Can I use this?
Sure, check out the README on GitHub
for instructions.
Note that it’s still in prerelease and isn’t even versioned yet.
Also maybe reach out and tell me why you want to!
What’s next?
I’ve got some of TI-Basic implemented, but there’s more to be done.
As certain milestones are hit I’ll update this page.
Anything else?
See the rest of my projects at davidtorosyan.com




"
https://news.ycombinator.com/rss,Intel Core i9-13900T CPU benchmarks show faster than 12900K 125W performance,https://wccftech.com/intel-core-i9-13900t-cpu-benchmarks-show-faster-than-12900k-125w-performance-at-35w/,Comments,"

HardwareReport
Intel Core i9-13900T CPU Benchmarks Show Faster Than 12900K 125W Performance at 35W

Hassan Mujtaba •
Jan 14, 2023 02:44 PM EST

•
Copy Shortlink
























Intel recently introduced brand new 13th Gen T-series chips which feature the Core i9-13900T that operates at a 35W TDP. The new chip has been benchmarked within Geekbench 5 and showcases impressive performance given its limited power budget.
Intel's 13th Gen Core i9-13900T 35W CPU Beats The 125W Core i9-12900K In Geekbench 5 Benchmark
Starting with the specifications, the Intel Core i9-13900T is a variation of the Core i9-13900 series that comes with a limited TDP design. While the standard chips boast 125W TDP in the unlocked and 65W TDP on the Non-K SKUs, the T-series chip is limited to a 35W TDP.  The Unlocked CPU is rated at up to 253W, the Non-K is rated at up to 219W while the T-series chip is rated at up to 106 Watts which is less than half the power budget of its higher-end siblings.
Related StoryHassan MujtabaIntel Core i9-13900KS, World’s First 6 GHz CPU, Now Available For $699 USThe Intel Core i9-13900T retains the same core configuration with 24 cores that are made up of 8 P-Cores and 16 E-Cores with 32 threads, a base clock of 1.10 GHz, a boost of up to 5.30 GHz & 68 MB of cache (L2+L3). The CPU also comes at a slightly lower price point of $549.00 US. Now the CPU is tested within the Geekbench 5 benchmark using an ASUS TUF Gaming B660M-PLUS WIFI board and coupled with 64 GB of DDR5 memory.

The CPU scored 2178 points in the single-core and 17339 points in the multi-core tests. We used the Intel Core i9-12900K for comparison which scores 1901 points in single-core and 17272 points in multi-core tests. This puts the Intel Core i9-13900T up to 15% faster in single-core and slightly faster in multi-threaded tests which is very impressive considering the Core i9-12900K also has a higher 125W base TDP (3.58x higher) and a peak TDP rating of 241W (2.27x higher).

Intel Core i9-13900KS Single-Thread CPU Benchmark (Geekbench 5)

Single-Core


050010001500200025003000




050010001500200025003000





Core i9-13900KS

2.3k


Core i9-13900K

2.2k


Ryzen 9 7900X

2.2k


Ryzen 9 7950X

2.2k


Ryzen 7 7700X

2.2k


Core i9-13900T

2.2k


Ryzen 5 7600X

2.2k


Ryzen 9 7900

2.1k


Core i9-13900

2.1k


Ryzen 7 7700

2.1k


Core i9-12900KS

2.1k


Core i9-13900HX

2k


Ryzen 5 7600

2k


Core i7-13700K

2k


Core i5-13600K

1.9k


Core i9-12900K

1.9k


Core i7-12700K

1.9k


M2 Max

1.9k


M1 Max

1.8k


Core i5-12600K

1.7k


Ryzen 9 5950X

1.7k


Ryzen 7 5800X

1.7k


Ryzen 9 5900X

1.7k


Ryzen 5 5600X

1.6k







Intel Core i9-13900KS Multi-Thread CPU Benchmark (Geekbench 5)

Multi-Core


050001000015000200002500030000




050001000015000200002500030000





Core i9-13900KS

26.8k


Core i9-13900K

24.3k


Ryzen 9 7950X

24.4k


Core i9-13900HX

20.9k


Core i9-13900

20.1k


Core i7-13700K

19.8k


Ryzen 9 7900X

19.3k


Core i9-12900KS

19k


Ryzen 9 7900

18.6k


Core i9-13900T

17.3k


Core i9-12900K

17.3k


Ryzen 9 5950X

16.5k


Core i5-13600K

16.1k


M2 Max

14.6k


Core i7-12700K

14.1k


Ryzen 7 7700X

14.1k


Ryzen 9 5900X

14k


Ryzen 7 7700

12.7k


M1 Max

12.3k


Core i5-12600K

11.6k


Ryzen 5 7600X

11.4k


Ryzen 5 7600

11.3k


Ryzen 7 5800X

10.3k


Ryzen 5 5600X

8.2k






This goes off to show the immense efficiency that Intel's 10nm ESF process node and the new hybrid architecture packs and we will also get to see some similar results with the mobility lineup, especially the 13th Gen HX parts which are going to ship in enthusiast-grade gaming laptops in the coming months. AMD also introduced its brand new 65W Ryzen 7000 Non-X CPUs which have been showcasing some impressive efficiency feats on their own with the Zen 4 core architecture.
News Source: Benchleaks
				
				Share this story
				 Facebook
 Twitter






Deal of the Day











Further Reading




 AMD Ryzen 9 7950X3D CPU Shown To Beat Intel Core i9-13900K In Games With Up To 24% Lead


 Intel Core i9-13980HX CPU Powered MSI Raider GE78HX Laptop Matches High-End Desktop CPUs In Performance


 Intel Core i9-13980HX Flagship Raptor Lake-HX CPU Spotted In ASUS’s Next-Gen ROG STRIX Laptop


 It’s Over 9000! Intel Core i9-13900KS Becomes The First CPU To Achieve 9 GHz Frequency World Record













Comments




Please enable JavaScript to view the comments.










Trending Stories


NASA Captures Star Eaten By Black Hole 300 Million Light Years Away



				86 Active Readers



SpaceX’s Rockets Split Up In Mid Air For Rare & Stunning Views At 5,000 Km/h+



				78 Active Readers



Apple To Bring microLED Technology To All Products Eventually, Starting With Apple Watch Ultra; Complete Transition May Take 10 Years



				54 Active Readers



PlayStation 5 Vertical Orientation Issue Clarified by Technician; Issue Happens on “Unopened” Consoles



				41 Active Readers



Intel Core i9-13900T CPU Benchmarks Show Faster Than 12900K 125W Performance at 35W



				30 Active Readers








Popular Discussions


AMD Radeon RX 7900 XTX Failure Rates Reportedly At 11%, RMA’s Piling Up But Users Not Receiving Cards



				3100 Comments



AMD Radeon RX 6000 GPUs Mysteriously Start Dying, German Repair Shop Receives 48 Cards With Cracked Chips



				3020 Comments



Intel Lunar Lake To Feature A Brand New CPU Architecture Built From The Ground-Up, Perf/Watt Focused at Mobile



				2757 Comments



AMD To Give The Love of 3D V-Cache This Valentines With Its Ryzen 7000 X3D CPUs Launch



				2021 Comments



Intel Arc A770 Performs Above AMD & NVIDIA In DirectStorage 1.1 Performance Benchmark



				1802 Comments








	 







"
https://news.ycombinator.com/rss,Show HN: Sketch – AI code-writing assistant that understands data content,https://github.com/approximatelabs/sketch,Comments,"








approximatelabs

/

sketch

Public




 

Notifications



 

Fork
    9




 


          Star
 346
  









        AI code-writing assistant that understands data content
      





346
          stars
 



9
          forks
 



 


          Star

  





 

Notifications












Code







Issues
0






Pull requests
0






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







approximatelabs/sketch









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





6
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




bluecoconut

update readme wording




        …
      




        9d567ec
      

Jan 16, 2023





update readme wording


9d567ec



Git stats







133

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github/workflows



remove python 3.7, add tests, remove uneeded code



Dec 15, 2022









sketch



some bug fix and copy addition



Jan 16, 2023









tests



remove python 3.7, add tests, remove uneeded code



Dec 15, 2022









.gitignore



update with edit and work on text2sql



Oct 13, 2022









README.md



update readme wording



Jan 16, 2023









dev-requirements.txt



starting to rebuild sketch



Dec 15, 2022









pyproject.toml



rename to pandas extension, add missing requirement, use base64 encoding



Jan 11, 2023









setup.py



starting to rebuild sketch



Dec 15, 2022




    View code
 


















sketch
Demo
How to use
.sketch.ask
.sketch.howto
.sketch.apply
Sketch currently uses prompts.approx.dev to help run with minimal setup
How it works





README.md




sketch
Sketch is an AI code-writing assistant for pandas users that understands the context of your data, greatly improving the relevance of suggestions. Sketch is usable in seconds and doesn't require adding a plugin to your IDE.
pip install sketch
Demo
Here we follow a ""standard"" (hypothetical) data-analysis workflow, showing a Natural Language interace that successfully navigates many tasks in the data stack landscape.

Data Catalogging:

General tagging (eg. PII identification)
Metadata generation (names and descriptions)


Data Engineering:

Data cleaning and masking (compliance)
Derived feature creation and extraction


Data Analysis:

Data questions
Data visualization








sketch-demo.mp4





Try it out in colab: 
How to use
It's as simple as importing sketch, and then using the .sketch extension on any pandas dataframe.
import sketch
Now, any pandas dataframe you have will have an extension registered to it. Access this new extension with your dataframes name .sketch
.sketch.ask
Ask is a basic question-answer system on sketch, this will return an answer in text that is based off of the summary statistics and description of the data.
Use ask to get an understanding of the data, get better column names, ask hypotheticals (how would I go about doing X with this data), and more.
df.sketch.ask(""Which columns are integer type?"")
.sketch.howto
Howto is the basic ""code-writing"" prompt in sketch. This will return a code-block you should be able to copy paste and use as a starting point (or possibly ending!) for any question you have to ask of the data. Ask this how to clean the data, normalize, create new features, plot, and even build models!
df.sketch.howto(""Plot the sales versus time"")
.sketch.apply
apply is a more advanced prompt that is more useful for data generation. Use it to parse fields, generate new features, and more. This is built directly on lambdaprompt. In order to use this, you will need to set up a free account with OpenAI, and set an environment variable with your API key. OPENAI_API_KEY=YOUR_API_KEY
df['review_keywords'] = df.sketch.apply(""Keywords for the review [{{ review_text }}] of product [{{ product_name }}] (comma separated):"")
df['capitol'] = pd.DataFrame({'State': ['Colorado', 'Kansas', 'California', 'New York']}).sketch.apply(""What is the capitol of [{{ State }}]?"")
Sketch currently uses prompts.approx.dev to help run with minimal setup
In the future, we plan to update the prompts at this endpoint with our own custom foundation model, built to answer questions more accurately than GPT-3 can with its minimal data context.
You can also directly call OpenAI directly (and not use our endpoint) by using your own API key. To do this, set 2 environment variables.
(1) SKETCH_USE_REMOTE_LAMBDAPROMPT=False
(2) OPENAI_API_KEY=YOUR_API_KEY
How it works
Sketch uses efficient approximation algorithms (data sketches) to quickly summarize your data, and feed that information into language models. Right now it does this by summarizing the columns and writing these summary statistics as additional context to be used by the code-writing prompt. In the future we hope to feed these sketches directly into custom made ""data + language"" foundation models to get more accurate results.









About

      AI code-writing assistant that understands data content
    
Topics



  python


  data-science


  data


  ai


  tabular-data


  pandas


  df


  sketches


  dataframe


  copilot


  codex


  ds


  datasketches


  gpt3


  lambdaprompt


  datasketch



Resources





      Readme
 


Stars





346
    stars

Watchers





4
    watching

Forks





9
    forks







    Releases





6
tags







    Packages 0


        No packages published 







        Used by 22
 




























            + 14
          







    Contributors 2








bluecoconut
Justin Waugh

 






jmbiven
Mike Biven

 





Languages










Python
100.0%











"
https://news.ycombinator.com/rss,The hardest part of being a junior developer,https://rachsmith.com/the-hardest-part-of-being-a-jnr/,Comments,"



The hardest part of being a Junior Developer


Added:
        January 16, 2023
      

Tags:

  work


  development





Even though it was 12 years ago, I still remember what the most stressful part of being a junior developer was. I was reminded of it when I saw this meme in my Twitter timeline yesterday:

In my first developer job, I had no previous experience - just some basic front-end coding and design classes at Uni. I had been hired by an advertising agency to animate flash banners and code marketing emails. The work atmosphere was fast-paced but I enjoyed the challenge. There was a lot to learn but I was loving everything I was learning. A lot of the people who worked there - ‘ad people’ were intense personalities but I grew used to working with them.
I loved that job, and couldn’t believe I was being paid more to do it than in any hospitality role I had worked prior. There was just one part that stressed me out: knowing how long I should work on figuring out something by myself vs. asking for help. I had no context for how long anything should take. I didn’t want to be judged harshly for asking too many dumb questions, but I also didn’t want to appear to be slow. I had the added pressure of feeling like I was representing my entire gender with my performance.
My manager at the time must have noticed my anxiety around this, and he did a wonderful thing. He made the timing aspect of it super explicit for me. He would do things like give me my next task and then say “try and get this done, if at any point you have spent an hour without making any progress, come and ask for help”. Then I knew how long was too long to spin my wheels on something on my own. Most of the time, I could keep figuring things out on my own, but when I couldn’t I could approach my manager without worrying whether I was bothering him “too soon”.
By the time I moved on from that job, with the help of that manager, I had enough experience to be able to judge when it is time to ask for help on something and didn’t have to worry about it again.
As I’ve always worked on small teams, I haven’t had the opportunity to manage a green developer that needs a lot of direction. But I thought I would just share this in case it is useful to anyone who does. Try and be very specific when you share your expectations around timing, it might help them out. It certainly helped me.



Thanks for reading! If you'd like to share your thoughts you can leave a comment, send me an email, Tweet at me, or add an issue on GitHub.



  Comments



Please enable Javascript to view comments.

          The Comments system is powered by a third party service - Talkyard. Sometimes they don't load 😞. If you're having trouble leaving a comment you can send me an email.
        



"
https://news.ycombinator.com/rss,Show HN: Cross-Platform GitHub Action,https://github.com/marketplace/actions/cross-platform-action,Comments,"





Marketplace
Actions
Cross Platform Action






play-circle





GitHub Action
Cross Platform Action




v0.9.0
Latest version







    Use latest version
 









play-circle






Cross Platform Action
Provides cross platform runner


Installation
Copy and paste the following snippet into your .yml file.












- name: Cross Platform Action
  uses: cross-platform-actions/action@v0.9.0



          Learn more about this action in cross-platform-actions/action












Choose a version







v0.9.0

                Cross Platform Action 0.9.0
              

 




v0.8.0

                Cross Platform Action 0.8.0
              

 




v0.7.0

                Cross Platform Action 0.7.0
              

 




v0.6.2

                Cross Platform Action 0.6.2
              

 




v0.6.1

                Cross Platform Action 0.6.1
              

 




v0.6.0

                Cross Platform Action 0.6.0
              

 




v0.5.0

                Cross Platform Action 0.5.0
              

 




v0.4.0

                Cross Platform Action 0.4.0
              

 




v0.3.1

                Cross Platform Action 0.3.1
              

 




v0.3.0

                Cross Platform Action 0.3.0
              

 








Cross-Platform GitHub Action
This project provides a GitHub action for running GitHub Action workflows on
multiple platforms. This includes platforms that GitHub Actions doesn't
currently natively support.
Features
Some of the features that are supported include:

Multiple operating system with one single action
Multiple versions of each operating system
Allows to use default shell or Bash shell
Low boot overhead
Fast execution

Usage
Here's a sample workflow file which will setup a matrix resulting in four jobs.
One which will run on FreeBSD 13.1, one which runs OpenBSD 7.2, one which runs
NetBSD 9.2 and one which runs OpenBSD 7.2 on ARM64.
name: CI

on: [push]

jobs:
  test:
    runs-on: ${{ matrix.os.host }}
    strategy:
      matrix:
        os:
          - name: freebsd
            architecture: x86-64
            version: '13.1'
            host: macos-12

          - name: openbsd
            architecture: x86-64
            version: '7.2'
            host: macos-12

          - name: openbsd
            architecture: arm64
            version: '7.2'
            host: ubuntu-latest

          - name: netbsd
            architecture: x86-64
            version: '9.2'
            host: ubuntu-latest

    steps:
      - uses: actions/checkout@v2

      - name: Test on ${{ matrix.os.name }}
        uses: cross-platform-actions/action@v0.9.0
        env:
          MY_ENV1: MY_ENV1
          MY_ENV2: MY_ENV2
        with:
          environment_variables: MY_ENV1 MY_ENV2
          operating_system: ${{ matrix.os.name }}
          architecture: ${{ matrix.os.architecture }}
          version: ${{ matrix.os.version }}
          shell: bash
          run: |
            uname -a
            echo $SHELL
            pwd
            ls -lah
            whoami
            env | sort
Different platforms need to run on different runners, see the
Runners section below.
Inputs
This section lists the available inputs for the action.



Input
Required
Default Value
Description




run
✓
✗
Runs command-line programs using the operating system's shell. This will be executed inside the virtual machine.


operating_system
✓
✗
The type of operating system to run the job on. See Supported Platforms.


version
✓
✗
The version of the operating system to use. See Supported Platforms.


shell
✗
default
The shell to use to execute the commands. Defaults to the default shell for the given operating system. Allowed values are: default, sh and bash


environment_variables
✗
""""
A list of environment variables to forward to the virtual machine. The list should be separated with spaces.



All inputs are expected to be strings. It's important that especially the
version is explicitly specified as a string, using single or double quotes.
Otherwise YAML might interpet the value as a numeric value instead of a string.
This might lead to some unexpected behavior. If the version is specified as
version: 13.0, YAML will interpet 13.0 as a floating point number, drop the
fraction part (because 13 and 13.0 are the same) and the GitHub action will
only see 13 instead of 13.0. The solution is to explicitly state that a
string is required by using quotes: version: '13.0'.
Supported Platforms
This sections lists the currently supported platforms by operating system. Each
operating system will list which versions are supported.
OpenBSD (openbsd)



Version
x86-64
arm64




7.2
✓
✓


7.1
✓
✓


6.9
✓
✓


6.8
✓
✗



FreeBSD (freebsd)



Version
x86-64




13.1
✓


13.0
✓


12.4
✓


12.2
✓



NetBSD (netbsd)



Version
x86-64




9.2
✓



Runners
This section list the different combinations of platforms and on which runners
they can run.



Runner
OpenBSD
FreeBSD
NetBSD




Linux
✓
✓
✓


macos-10.15, macos-11, macos-12
✓
✓
✗



Under the Hood
GitHub Actions currently only support the following platforms: macOS, Linux and
Windows. To be able to run other platforms, this GitHub action runs the
commands inside a virtual machine (VM). If the host platform is macOS the
hypervisor can take advantage of nested virtualization.
The FreeBSD and OpenBSD VMs run on the xhyve hypervisor (on a macOS
host), while the other platforms run on the QEMU hypervisor (on a Linux
host). xhyve is built on top of Apple's Hypervisor
framework. The Hypervisor framework allows to implement hypervisors with
support for hardware acceleration without the need for kernel extensions. xhyve
is a lightweight hypervisor that boots the guest operating systems quickly and
requires no dependencies outside of what's provided by the system. QEMU is a
more general purpose hypervisor that runs on most host platforms and supports
most guest systems. It's a bit slower than xhyve because it's general purpose
and it cannot use nested virtualization on the Linux hosts provided by GitHub.
The VM images running inside the hypervisor are built using Packer.
It's a tool for automatically creating VM images, installing the guest
operating system and doing any final provisioning.
The GitHub action uses SSH to communicate and execute commands inside the VM.
It uses rsync to share files between the guest VM and the host. xhyve
does not have any native support for sharing files. To authenticate the SSH
connection a unique key pair is used. This pair is generated each time the
action is run. The public key is added to the VM image and the private key is
stored on the host. Since xhyve does not support file sharing, a secondar hard
drive, which is backed by a file, is created. The public key is stored on this
hard drive, which is then mounted by the VM. At boot time, the secondary hard
drive will be identified and the public key will be copied to the appropriate
location.
To reduce the time it takes for the GitHub action to start executing the
commands specified by the user, it aims to boot the guest operating systems as
fast as possible. This is achieved in a couple of ways:


By downloading resources, like the hypervisor and a few other
tools, instead of installing them through a package manager


No compression is used for the resources that are downloaded. The size is
small enough anyway and it's faster to download the uncompressed data than
it is to download compressed data and then uncompress it.


It leverages async/await to perform tasks asynchronously. Like
downloading the VM image and other resources at the same time


It performs as much as possible of the setup ahead of time when the VM image
is provisioned


Local Development
Prerequisites

NodeJS
npm
git

Instructions


Install the above prerequisites


Clone the repository by running:
git clone https://github.com/cross-platform-actions/action



Navigate to the newly cloned repository: cd action


Install the dependencies by running: npm install


Run any of the below npm commands


npm Commands
The following npm commands are available:

build - Build the GitHub action
format - Reformat the code
lint - Lint the code
package - Package the GitHub action for distribution and end to end testing
test - Run unit tests
all - Will run all of the above commands

Running End to End Tests
The end to end tests can be run locally by running it through Act. By
default, resources and VM images will be downloaded from github.com. By running
a local HTTP server it's possible to point the GitHub action to local resources.
Prerequisites

Docker
Act

Instructions


Install the above prerequisites


Copy test/workflows/ci.yml.example to
test/workflows/ci.yml


Make any changes you like to test/workflows/ci.yml, this is file ignored by
Git


Build the GitHub action by running: npm run build


Package the GitHub action by running: npm run package


Run the GitHub action by running: act --privileged -W test/workflows


Providing Resources Locally
The GitHub action includes a development dependency on a HTTP server. The
test/http directory contains a skeleton of a directory structure
which matches the URLs that the GitHub action uses to download resources. All
files within the test/http are ignore by Git.


Add resources as necessary to the test/http directory


In one shell, run the following command to start the HTTP server:
./node_modules/http-server/bin/http-server test/http -a 127.0.0.1

The -a flag configures the HTTP server to only listen for incoming
connections from localhost, no external computers will be able to connect.


In another shell, run the GitHub action by running:
act --privileged -W test/workflows --env CPA_RESOURCE_URL=<url>

Where <url> is the URL inside Docker that points to localhost of the host
machine, for macOS, this is http://host.docker.internal:8080. By default,
the HTTP server is listening on port 8080.






Stars

 


          Star
 40
  





Contributors



 

 


Categories


  Continuous integration


  Testing




Links



cross-platform-actions/action
    



Open issues
        1




Pull requests
      1




Report abuse
 

Cross Platform Action is not certified by GitHub. It is provided by a third-party and is governed by separate terms of service, privacy policy, and support documentation.
    




"
https://news.ycombinator.com/rss,Will Floating Point 8 Solve AI/ML Overhead?,https://semiengineering.com/will-floating-point-8-solve-ai-ml-overhead/,Comments,"












Will Floating Point 8 Solve AI/ML Overhead?

























































































 
 
 












 










 


Search for:



 Subscribe

中文 English 


















Home
Systems & Design
Low Power - High Performance
Manufacturing, Packaging & Materials
Test, Measurement & Analytics
Auto, Security & Pervasive Computing




Special Reports

Business & Startups
Jobs
Knowledge Center
Technical Papers 

Home';
				AI/ML/DLArchitecturesAutomotiveCommunication/Data MovementDesign & VerificationLithographyManufacturingMaterialsMemoryOptoelectronics / PhotonicsPackagingPower & PerformanceQuantumSecurityTest & AnalyticsTransistorsZ-End Applications


Events & Webinars 

Events
Webinars



Videos & Research

Videos
Industry Research



Newsletters





MENU 

Home
Special Reports
Systems & Design
Low Power-High Performance
Manufacturing, Packaging & Materials
Test, Measurement & Analytics
Auto, Security & Pervasive Computing
Knowledge Center
Videos
Startup Corner
Business & Startups
Jobs
Technical Papers 
Events
Webinars
Industry Research
Special Reports







































Home >
                                                                    Low Power-High Performance >
                                                                Will Floating Point 8 Solve AI/ML Overhead?                                                    

















Low Power-High Performance

Will Floating Point 8 Solve AI/ML Overhead?









Less precision equals lower power, but standards are required to make this work.





								January 12th, 2023 - 

								By: Karen Heyman








While the media buzzes about the Turing Test-busting results of ChatGPT, engineers are focused on the hardware challenges of running large language models and other deep learning networks. High on the ML punch list is how to run models more efficiently using less power, especially in critical applications like self-driving vehicles where latency becomes a matter of life or death.
AI already has led to a rethinking of computer architectures, in which the conventional von Neumann structure is replaced by near-compute and at-memory floorplans. But novel layouts aren’t enough to achieve the power reductions and speed increases required for deep learning networks. The industry also is updating the standards for floating-point (FP) arithmetic.
“There is a great deal of research and study on new data types in AI, as it is an area of rapid innovation,” said David Bell, product marketing director, Tensilica IP at Cadence. “Eight-bit floating-point (FP8) data types are being explored as a means to minimize hardware — both compute resources and memory — while preserving accuracy for network models as their complexities grow.”
As part of that effort, researchers at Arm, Intel, and Nvidia published a white paper proposing “FP8 Formats for Deep Learning.” [1]
“Bit precision has been a very active topic of debate in machine learning for several years,” said Steve Roddy, chief marketing officer at Quadric. “Six or eight years ago when models began to explode in size (parameter count), the sheer volume of shuffling weight data into and out of training compute (either CPU or GPU) became the performance limiting bottleneck in large training runs. Faced with a choice of ever more expensive memory interfaces, such as HBM, or cutting bit precision in training, a number of companies experimented successfully with lower-precision floats. Now that networks have continued to grow exponentially in size, the exploration of FP8 is the next logical step in reducing training bandwidth demands.”
How we got here
Floating-point arithmetic is a kind of scientific notation, which condenses the number of digits needed to represent a number. This trick is pulled off by an arithmetic expression first codified by IEEE working group 754 in 1986, when floating-point operations generally were performed on a co-processor.
IEEE 754 describes how the radix point (more commonly known in English as the “decimal” point) doesn’t have a fixed position, but rather “floats” where needed in the expression. It allows numbers with extremely long streams of digits (whether originally to the left or right of a fixed point) to fit into the limited bit-space of computers. It works in either base 10 or base 2, and it’s essential for computing, given that binary numbers extend to many more digits than decimal numbers (100 = 1100100).
 

Fig. 1: 12.345 as a base-10 floating-point number. Source: Wikipedia
 
While this is both an elegant solution and the bane of computer science students worldwide, its terms are key to understanding how precision is achieved in AI. The statement has three parts:

A sign bit, which determines whether the number is positive (0) or negative (1);
An exponent, which determines the position of the radix point, and
A mantissa, or significand, which represents the most significant digits of the number.


Fig. 2: IEEE 754 floating-point scheme. Source: WikiHow
As shown in figure 2, while the exponent gains 3 bits in a 64-bit representation, the mantissa jumps from 32 bits to 52 bits. Its length is key to precision.
IEEE 754, which defines FP32 bit and FP64, was designed for scientific computing, in which precision was the ultimate consideration. Currently, IEEE working group P3109 is developing a new standard for machine learning, aligned with the current (2019) version of 754. P3109 aims to create a floating-point 8 standard.
Precision tradeoffs
Machine learning often needs less precision than a 32-bit scheme. The white paper proposes two different flavors of FP8: E4M3 (4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa).
“Neural networks are a bit strange in that they are actually remarkably tolerant to relatively low precision,” said Richard Grisenthwaite, executive vice president and chief architect at Arm. “In our paper, we showed you don’t need 32 bits of mantissa for precision. You can use only two or three bits, and four or five bits of exponent will give you sufficient dynamic range. You really don’t need the massive precision that was defined in 754, which was designed for finite element analysis and other highly precise arithmetic tasks.”
Consider a real-world example: A weather forecast needs the extreme ranges of 754, but a self-driving car doesn’t need the fine-grained recognition of image search. The salient point is not whether it’s a boy or girl in the middle of the road. It’s just that the vehicle must immediately stop, with no time to waste on calculating additional details. So it’s fine to use a floating point with a smaller exponent and much smaller mantissa, especially for edge devices, which need to optimize energy usage.
“Energy is a fundamental quantity and no one’s going to make it go away as an issue,” said Martin Snelgrove, CTO of Untether AI. “And it’s also not a narrow one. Worrying about energy means you can’t afford to be sloppy in your software or your arithmetic. If doing a 32-bit floating point makes everything easier, but massively more power consuming, you just can’t do it. Throwing an extra 1,000 layers at something makes it slightly more accurate, but the value for power isn’t there. There’s an overall discipline about energy — the physics says you’re going to pay attention to this, whether you like it or not.”
In fact, to save energy and performance overhead, many deep learning networks had already shifted to an IEEE-approved 16-bit floating point and other formats, including mantissa-less integers. [2]
“Because compute energy and storage is at a premium in devices, nearly all high-performance device/edge deployments of ML always have been in INT8,” Quadric’s Roddy said. “Nearly all NPUs and accelerators are INT-8 optimized. An FP32 multiply-accumulate calculation takes nearly 10X the energy of an INT8 MAC, so the rationale is obvious.”
Why FP8 is necessary
The problem starts with the basic design of a deep learning network. In the early days of AI, there were simple, one-layer models that only operated in a feedforward manner. In 1986, David Rumelart, Geoffrey Hinton, and Ronald Williams published a breakthrough paper on back-propagation [3] that kicked off the modern era of AI. As their abstract describes, “The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units, which are not part of the input or output, come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units.”
In other words, they created a system in which better results could be achieved by adding more and more layers into a model, which would be improved by incorporating “learned” adjustments. Decades later, their ideas so vastly improved machine translation and transcription that college professors remain unsure whether undergraduates’ essays have been written by bots.
But additional layers require additional processing power. “Larger networks with more and more layers were found to be progressively more successful at neural networks tasks, but in certain applications this success came with an ultimately unmanageable increase in memory footprint, power consumption, and compute resources. It became imperative to reduce the size of the data elements (activations, weights, gradients) from 32 bits, and so the industry started using 16-bit formats, such as Bfloat16 and IEEE FP16,” according to the paper jointly written by Arm/Intel/Nvidia.
“The tradeoff fundamentally is with an 8-bit floating-point number compared to a 32-bit one,” said Grisenthwaite. “I can have four times the number of weights and activations in the same amount of memory, and I can get far more computational throughput as well. All of that means I can get much higher performance. I can make the models more involved. I can have more weights and activations at each of the layers. And that’s proved to be more useful than each of the individual points being hyper-accurate.”
Behind these issues are the two basic functions in machine learning, training and inference. Training is the first step in which, for example, the AI learns to classify features in an image by reviewing a dataset. With inference, the AI is given novel images outside of the training set and asked to classify them. If all goes as it should, the AI should distinguish that tails and wings are not human features, and at finer levels, that airplanes do not have feathers and a tube with a tail and wings is not a bird.
“If you’re doing training or inference, the math is identical,” said Ron Lowman, strategic marketing manager for IoT at Synopsys. “The difference is you do training over a known data set thousands of times, maybe even millions of times, to train what the results will be. Once that’s done, then you take an unknown picture and it will tell you what it should be. From a math perspective, a hardware perspective, that’s the big difference. So when you do training, you want to do that in parallel, rather than doing it in a single hardware implementation, because the time it takes to do training is very costly. It could take weeks or months, or even years in some cases, and that just costs too much.”
In industry, training and inference have become separate specialties, each with its own dedicated teams.
“Most companies that are deploying AI have a team of data scientists that create neural network architectures and train the networks using their datasets,” said Bob Beachler, vice president of product at Untether AI. “Most of the autonomous vehicle companies have their own data sets, and they use that as a differentiating factor. They train using their data sets on these novel network architectures that they come up with, which they feel gives them better accuracy. Then that gets taken to a different team, which does the actual implementation in the car. That is the inference portion of it.”
Training requires a wide dynamic range for the continual adjustment of coefficients that is the hallmark of backpropagation. The inference phase is computing on the inputs, rather than learning, so it needs much less dynamic range. “Once you’ve trained the network, you’re not tweaking the coefficients, and the dynamic range required is dramatically reduced,” explained Beachler.
For inference, continuing operations in FP32 or FP16 is just unnecessary overhead, so there’s a quantization step to shift the network down to FP8 or Integer 8 (Int8), which has become something of a de facto standard for inference, driven largely by TensorFlow.
“The idea of quantization is you’re taking all the floating point 32 bits of your model and you’re essentially cramming it into an eight-bit format,” said Gordon Cooper, product manager for Synopsys’ Vision and AI Processor IP. “We’ve done accuracy tests and for almost every neural network-based object detection. We can go from 32-bit floating point to Integer 8 with less than 1% accuracy loss.”
For quality/assurance, there’s often post-quantization retraining to see how converting the floating-point value has affected the network, which could iterate through several passes.
This is why training and inference can be performed using different hardware. “For example, a common pattern we’ve seen is accelerators using NVIDIA GPUs, which then end up running the inference on general purpose CPUs,” said Grisenthwaite.
The other approach is chips purpose-built for inference.
“We’re an inference accelerator. We don’t do training at all,” says Untether AI’s Beachler. “We place the entire neural network on our chip, every layer and every node, feed data at high bandwidth into our chip, resulting in each and every layer of the network computed inside our chip. It’s massively parallelized multiprocessing. Our chip has 511 processors, each of them with single instruction multiple data (SIMD) processing. The processing elements are essentially multiply/accumulate functions, directly attached to memory. We call this the Energy Centric AI computing architecture. This Energy Centric AI Computing architecture results in a very short distance for the coefficients of a matrix vector to travel, and the activations come in through each processing element in a row-based approach. So the activation comes in, we load the coefficients, do the matrix mathematics, do the multiply/accumulate, store the value, move the activation to the next row, and move on. Short distances of data movement equates to low power consumption.”
In broad outline, AI development started with CPUs, often with FP co-processors, then moved to GPUs, and now is splitting into a two-step process of GPUs (although some still use CPUs) for training and CPUs or dedicated chips for inference.
The creators of general-purpose CPU architectures and dedicated inference solutions may disagree on which approach will dominate. But they all agree that the key to a successful handoff between training and inference is a floating-point standard that minimizes the performance overhead and risk of errors during quantization and transferring operations between chips. Several companies, including NVIDIA, Intel, and Untether, have brought out FP8-based chips.
“It’s an interesting paper,” said Cooper. “8-bit floating point, or FP8, is more important on the training side. But the benefits they’re talking about with FP8 on the inference side is that you possibly can skip the quantization. And you get to match the format of what you’ve done between training and inference.”
Nevertheless, as always, there are still many challenges still to consider.
“The cost is one of model conversion — FP32 trained model converted to INT8. And that conversion cost is significant and labor intensive,” said Roddy. “But if FP8 becomes real, and if the popular training tools begin to develop ML models with FP8 as the native format, it could be a huge boon to embedded inference deployments. Eight-bit weights take the same storage space, whether they are INT8 or FP8. The energy cost of moving 8 bits (DDR to NPU, etc.) is the same, regardless of format. And a Float8 multiply-accumulate is not significantly more power consumptive than an INT8 MAC. FP8 would rapidly be adopted across the silicon landscape.  But the key is not whether processor licensors would rapidly adopt FP8. It’s whether the mathematicians building training tools can and will make the switch.”
Conclusion
As the quest for lower power continues, there’s debate about whether there might even be a FP4 standard, in which only 4 bits carry a sign, an exponent, and mantissa. People who follow a strict neuromorphic interpretation have even discussed binary neural networks, in which the input functions like an axon spike, just 0 or 1.
“Our sparsity level is going to go up,” said Untether’s Snelgrove. “There are hundreds of papers a day on new neural net techniques. Any one of them could completely revolutionize the field. If you talk to me in a year, all of these words could mean different things.”
At least at the moment, it’s hard to imagine that lower FPs or integer schemes could contain enough information for practical purposes. Right now, various flavors of FP8 are undergoing the slow grind towards standardization. For example, Graphcore, AMD, and Qualcomm have also brought a detailed FP8 proposal to the IEEE. [4]
“The advent of 8-bit floating point offers tremendous performance and efficiency benefits for AI compute,” said Simon Knowles, CTO and co-founder of Graphcore. “It is also an opportunity for the industry to settle on a single, open standard, rather than ushering in a confusing mix of competing formats.”
Indeed, everyone is optimistic there will be a standard — eventually. “We’re involved in IEEE P3109, as are many, many companies in this industry,” said Arm’s Grisenthwaite. “The committee has looked at all sorts of different formats. There are some really interesting ones out there. Some of them will stand the test of time, and some of them will fall by the wayside. We all want to make sure we’ve got complete compatibility and don’t just say, ‘Well, we’ve got six different competing formats and it’s all a mess, but we’ll call it a standard.”
References 

Micikevicius, P., et al. FP8 Formats for Deep Learning. Last revised Sep 29 2022 arXiv:2209.05433v2. https://doi.org/10.48550/arXiv.2209.05433
Sapunov, G. FP64, FP32, FP16, BFLOAT16, TF32, and other members of the ZOO. Medium. May 16, 2020. https://moocaholic.medium.com/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407
Rumelhart, D., Hinton, G. & Williams, R. Learning representations by back-propagating errors. Nature 323, 533–536 (1986). https://doi.org/10.1038/323533a0
Noune, B. 8-bit Numerical Formats for Deep Neural Networks. Submitted June 6 2022 arXiv:2206.02915 https://doi.org/10.48550/arXiv.2206.02915

Additional Reading:
How to convert a number from decimal to IEEE 754 Floating Point Representation.
Number Representation and Computer Arithmetic
https://web.ece.ucsb.edu/~parhami/pubs_folder/parh02-arith-encycl-infosys.pdf
Computer Representation of Numbers and Computer Arithmetic
https://people.cs.vt.edu/~asandu/Courses/CS3414/comp_arithm.pdf











 Tags: 8-bit floating point AI AI models AI/ML/DL AMD ARM base 10 base 2 BFLOAT16 Cadence Cadence Design Systems ChatGPT deep learning E4M3 E5M2 edge inference floating point FP16 FP32 FP64 FP8 Graphcore IEEE 754 inference INT8 Integer 8 Intel machine learning mantissa ML training neural network Nvidia P3109 Quadric Quadric.io Qualcomm Synopsys training Untether AI





Karen Heyman   (all posts)


							Karen Heyman is a technology editor at Semiconductor Engineering.
						





Leave a Reply Cancel replyComment * Name*(Note: This name will be displayed publicly)
Email*(This will not be displayed publicly) 
 

Δ 




 Knowledge Centers Blogs 
Spiking Neural Network (SNN)

Published on June 23, 2021



Recurrent Neural Network (RNN)

Published on June 25, 2019



Edge Computing

Published on April 4, 2019



Neural Networks

Published on July 25, 2017



Architectures

Published on 



Machine Learning (ML)

Published on May 10, 2017



Convolutional Neural Network (CNN)

Published on 



Artificial Intelligence (AI)

Published on 




Technical Papers
Manycore-FPGA Architecture Employing Novel Duet Adapters To Integrate eFPGAs in a Scalable, Non-Intrusive, Cache-Coherent Manner (Princeton) January 16, 2023 by Technical Paper LinkHardware Trojan Detection Case Study Based on 4 Different ICs Manufactured in Progressively Smaller CMOS Process Technologies January 11, 2023 by Technical Paper LinkQuantum Computing Architecture Enabling  Communication Between Superconducting Quantum Processors (MIT) January 11, 2023 by Technical Paper LinkArbitrary Precision DNN Accelerator Controlled by a RISC-V CPU (Ecole Polytechnique Montreal, IBM, Mila, CMC) January 10, 2023 by Technical Paper LinkTechnique For Printing Electronic Circuits Onto Curved & Corrugated Surfaces Using Metal Nanowires (NC State) January 10, 2023 by Technical Paper Link 

  Trending Articles

RISC-V Pushes Into The Mainstream

Open-source processor cores are beginning to show up in heterogeneous SoCs and packages.


by Marie C. Baca and Ed Sperling



How Secure Are RISC-V Chips?

Open source by itself doesn’t guarantee security. It still comes down to the fundamentals of design.


by Jeff Goldman



Will Floating Point 8 Solve AI/ML Overhead?

Less precision equals lower power, but standards are required to make this work.


by Karen Heyman



RISC-V decoupled Vector Processing Unit (VPU) For HPC



by Technical Paper Link



Startup Funding: December 2022

Wafer manufacturing and GPUs draw investment; 106 companies raise $2.8B.


by Jesse Allen






Knowledge Centers Entities, people and technologies explored
Learn More



Related Articles

Foundational Changes In Chip Architectures

New memory approaches and challenges in scaling CMOS point to radical changes — and potentially huge improvements — in semiconductor designs. 


by Brian Bailey



How Memory Design Optimizes System Performance

Changes are steady in the memory hierarchy, but how and where that memory is accessed is having a big impact.


by John Koon



Will Floating Point 8 Solve AI/ML Overhead?

Less precision equals lower power, but standards are required to make this work.


by Karen Heyman



Startup Funding: October 2022

113 startups raise $3.5B; batteries, AI, and new architectures top the list.


by Jesse Allen



Startup Funding: November 2022

127 startups raise $2.6B; data center connectivity, quantum computing, and batteries draw big funding.


by Jesse Allen



IC Stresses Affect Reliability At Advanced Nodes

Thermal mismatch in heterogeneous designs, different use cases, can impact everything from accelerated aging to warpage and system failures.


by Ann Mutschler



3D-IC Reliability Degrades With Increasing Temperature

Electromigration and other aging factors become more complicated along the z axis.


by Ann Mutschler



On-Chip Power Distribution Modeling Becomes Essential Below 7nm

Why and when it’s needed, and what tools and technologies are required.


by Ann Mutschler











Sponsors






























Advertise with us





Advertise with us





Advertise with us





Newsletter Signup



Popular Tags2.5D
5G
7nm
advanced packaging
AI
ANSYS
Apple
Applied Materials
ARM
Atrenta
automotive
business
Cadence
EDA
eSilicon
EUV
finFETs
GlobalFoundries
Google
IBM
imec
Intel
IoT
IP
Lam Research
machine learning
memory
Mentor
Mentor Graphics
MIT
Moore's Law
Nvidia
NXP
Qualcomm
Rambus
Samsung
security
SEMI
Siemens
Siemens EDA
software
Sonics
Synopsys
TSMC
verification
Recent CommentsWZIS on Arbitrary Precision DNN Accelerator Controlled by a RISC-V CPU (Ecole Polytechnique Montreal, IBM, Mila, CMC)Rama Chaganti on Growing System Complexity Drives More IP ReuseTL on How Secure Are RISC-V Chips?Frank on The Good And Bad Of Bi-Directional ChargingSandeep Dixit on The Good And Bad Of Bi-Directional ChargingHertz on How Secure Are RISC-V Chips?Andrew on How Software Utilizes CoresAsaf Jivilik on Cybord: Electronic Component TraceabilitySantosh Kurinec on Where All The Semiconductor Investments Are Goingdick freebird on Designing And Securing Chips For Outer SpaceAkshay on Designing And Securing Chips For Outer SpaceRaj on Is UCIe Really Universal?Andrew TAM on How Software Utilizes CoresRiko R on Designing For Multiple DieDan Ganousis on RISC-V Pushes Into The MainstreamIvan Batinic on IC Stresses Affect Reliability At Advanced NodesGiovanni Lostumbo on A Power-First ApproachMohammed Zakir Hussain on Embracing the Challenges Of Cybersecurity In Automotive ApplicationsLaura Peters on Week In Review: Manufacturing, TestAiv on Week In Review: Manufacturing, TestRoss Youngblood on High Voltage Testing Races AheadMark Olivas on Cybord: Electronic Component TraceabilityKarl Stevens on The Drive Toward Virtual PrototypesRon Lavallee on The Politics Of StandardsDenis McCarthy on Hot Trends In Semiconductor Thermal ManagementTom Smith on Are We Too Hard On Artificial Intelligence For Autonomous Driving?Maya F on Where All The Semiconductor Investments Are GoingSaikatm on Balancing Power And Heat In Advanced Chip DesignsDoug L. on Holistic 3D-IC Interposer Analysis In Product DesignsAndy Deng on Post-Quantum And Pre-Quantum Security Issues GrowJohn Dunn on Post-Quantum And Pre-Quantum Security Issues Growmadmax2069 on Chip Design Shifts As Fundamental Laws Run Out Of SteamMatthew Slyman on Chip Design Shifts As Fundamental Laws Run Out Of SteamDouglas MacIntyre on Chip Design Shifts As Fundamental Laws Run Out Of SteamJose on Universal Verification Methodology Running Out Of SteamZhengji Lu on Moving From AMBA ACE to CHI For CoherencyJohn Bennice on A Power-First Approach[email protected] on Chip Design Shifts As Fundamental Laws Run Out Of SteamMatthew on Chip Design Shifts As Fundamental Laws Run Out Of SteamKarthik Krishnamoorthy on AI-Powered VerificationCPlusPlus4Ever on Chip Design Shifts As Fundamental Laws Run Out Of SteamDouglas on Chip Design Shifts As Fundamental Laws Run Out Of SteamBowie Poag on Chip Design Shifts As Fundamental Laws Run Out Of SteamEugene on Startup Funding: October 2022Wesley Sung on Fan-Out And Packaging ChallengesHong Xiao on Chip Design Shifts As Fundamental Laws Run Out Of SteamRobert Anderson on Chip Design Shifts As Fundamental Laws Run Out Of SteamMike Frank on A Power-First ApproachWilliam Ruby on A Power-First ApproachPeter C Salmon on A Power-First ApproachDr. Dev Gupta on Which Foundry Is In The Lead? It Depends.Steve Hoover on A Power-First ApproachDylanP on Which Foundry Is In The Lead? It Depends.Asaf Jivilik on Cybord: Electronic Component TraceabilityChris @ crossPORt on Foundational Changes In Chip ArchitecturesMark Olivas on Cybord: Electronic Component TraceabilityAri ben David on Constraints On The Electricity GridJeff Zika on Auto Safety Tech Adds New IC Design ChallengesJung Yoon on Foundational Changes In Chip ArchitecturesSchrodinger's Cat's Advocate on Foundational Changes In Chip ArchitecturesRigTig on Foundational Changes In Chip ArchitecturesSteve on Foundational Changes In Chip ArchitecturesPrashant Purwar on Why Mask Blanks Are CriticalMostafa Abdelgawwad on Radar For Automotive: How Far Can A Radar See?yieldWerx on Managing Wafer RetestJohn Horner on A Brief History of TestLakshm J on ESD Requirements Are ChangingDr. Dev Gupta on Improving Redistribution Layers for Fan-out Packages And SiPsAkarsh on Better PMIC Design Using Multi-Physics SimulationTodd Bermensolo on Reducing Schedule Slips With Automated Post-Route Verification Of SerDes High Speed Serial LinksLaur Rizzatti on Why Geofencing Will Enable L5Raj Raghuram on The Complex Art Of Handling S-ParametersStevo on CHIPS Act: U.S. Releases New Implementation StrategySantosh Kurinec on Quantum Research Bits: Sept. 12Lewis Sternberg on ML And UVM Share Same FlawsRoger Stierman on L5 Adoption Hinges on 5G/6GMarcel on MicroLEDs Move Toward CommercializationRagu Athreya on Is There A Limit To The Number of Layers In 3D-NAND?Brian Bailey on AI Power Consumption ExplodingDavid S on AI Power Consumption ExplodingMike Cormack on Cryogenic CMOS Becomes CoolLance Harvie on New Uses For AI In ChipsDoc R on Electronics And Its Role In Climate ChangeMagdy Abadir on Is Standardization Required For Security?guest on How Overlay Keeps Pace With EUV PatterningSantosh Kurinec on Week In Review, Manufacturing, Testsravani on Timing Library LVF Validation For Production Design FlowsDr. F on A Sputnik Moment For ChipsGary Dagastine on A Sputnik Moment For ChipsMike Sottak on A Sputnik Moment For ChipsRobert Pearson on A Sputnik Moment For ChipsRaye E. Ward on A Sputnik Moment For ChipsMichael Williams on A Look Inside RF DesignSURESHBABU CHILUGODU on Week In Review: Manufacturing, TestJC Bouzigues, Menta on Customizing ProcessorsSteve Swendrowski on IC Package Illustrations, From 2D To 3DEMV on Hybrid Bonding Moves Into The Fast LaneDr. Appo van der Wiel on Variation Making Trouble In Advanced Packageswang yu on Verification Of Functional SafetyFrederick Chen on High-NA EUV May Be Closer Than It AppearsFact Cheq on The Week In Review: DesignShiwen Huang on E-beam’s Role Grows For Detecting IC DefectsAdele Hars on Wafer Shortage Improvement In Sight For 300mm, But Not 200mmDavid A. Humphreys on IMS2022 Booth Tour: EDA And Measurement Science ConvergeMerritt on Can Analog Make A Comeback?subra ganesan on Meeting Processor Performance And Safety Requirements For New ADAS & Autonomous Vehicle SystemsGeorge on Building A More Secure SoCAmit Garg on A New Breed Of EDA RequiredKarl Stevens on A Minimal RISC-VKarl Stevens on EDA Gaps At The Leading EdgeMicah Forstein MS. on Risks Rise As Robotic Surgery Goes MainstreamDr. Punam Raskar on Who Does Processor Validation?Dr. Dev Gupta on Variation Making Trouble In Advanced PackagesCox on DRAM Thermal Issues Reach Crisis PointDavid Leary on DRAM Thermal Issues Reach Crisis PointGeeeeeee on DRAM Thermal Issues Reach Crisis PointPedro Ferro Laks on SOT-MRAM To Challenge SRAMObviously silly on DRAM Thermal Issues Reach Crisis PointSimon on DRAM Thermal Issues Reach Crisis PointGareth on Energy Harvesting Starting To Gain Traction

 





Manycore-FPGA Architecture Emplo... Technical Paper LinkSelecting The Right RISC-V Core Brian Bailey 










  










About

About us
Contact us
Advertising on SemiEng
Newsletter SignUp



Navigation



Homepage
Special Reports
Systems & Design
Low Power-High Perf
Manufacturing, Packaging & Materials
Test, Measurement & Analytics
Auto, Security & Pervasive Computing




Videos
Jobs
Technical Papers
Events
Webinars
Knowledge Centers

Industry Research
Business & Startups
Newsletters





Connect With Us

Facebook
Twitter  @semiEngineering
LinkedIn
YouTube




Copyright ©2013-2023 SMG   |  Terms of Service  |  Privacy Policy





This site uses cookies. By continuing to use our website, you consent to our Cookies PolicyACCEPT Manage consent




Close






Privacy Overview 
This website uses cookies to improve your experience while you navigate through the website. The cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. We do not sell any personal information.

By continuing to use our website, you consent to our Privacy Policy. If you access other websites using the links provided, please be aware they may have their own privacy policies, and we do not accept any responsibility or liability for these policies or for any personal data which may be collected through these sites. Please check these policies before you submit any personal information to these sites.

 





								Necessary							


Necessary

Always Enabled




									Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.								






								Non-necessary							


Non-necessary





									Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website.								












SAVE & ACCEPT










 

 
 

 
























 








"
https://news.ycombinator.com/rss,Microsoft returns to the Altair,https://hackaday.com/2023/01/15/microsoft-returns-to-the-altair/,Comments,"


Microsoft Returns To The Altair


                18 Comments            

by:
Al Williams



January 15, 2023















Title:


Copy

Short Link:


Copy






The Altair 8800 arguably launched Microsoft. Now [Dave Glover] from Microsoft offers an emulated and potentially cloud-based Altair emulation with CP/M and Microsoft Basic. You can see a video of the project below. One thing that makes it a bit odd compared to other Altair clones we’ve seen is that the emulator runs in a Docker environment and is fully cloud-enabled. You can interact with it via a PCB front panel, or a terminal running in a web browser.
The core emulator is MIT-licensed and seems like it would run nearly everywhere. We were a little surprised there wasn’t an instance in the Azure cloud that you could spin up to test drive. Surely a few hundred Altairs running at once wouldn’t even make a dent in a modern CPU.

There are plenty of Altair emulators and even replicas with authentic CPUs out there. But we have to admit the Wiki documentation on this one is uncommonly well done. Even if you don’t want to use this emulator, you might find the collection of data about the Altair useful.
Don’t know how to use a computer front panel? Learn on the Altair or a PDP/8, even if you don’t have a real one. For simulated hardware, the project that turns an Arduino Due into an Altair works well. If you just want to play Zork, you can do that in your browser, for sure.





 



















Posted in RetrocomputingTagged altair 8800 


Post navigation

← A Number Maze For Younger HackersA Flex Sensor For A Glove Controller Using An LDR → 






            18 thoughts on “Microsoft Returns To The Altair”        





paulvdh says: 



							January 15, 2023 at 1:24 am						




@05:42 “I don’t have the source code for CP/M, I’m not even sure it exists anymore.
https://hackaday.com/2014/10/06/cpm-source-code-released/
This video is apparently only made as entertainment, just some guys blabbing and not caring about facts, That may be all right for some, it makes me wonder what else is off and  I lost interest.


Report comment 
Reply 





MG says: 



							January 15, 2023 at 1:51 am						




Even just off the top of my head, the fact that it’s MIT-licensed is nothing special; the entire core of MAME and a significant number of machine drivers are all MIT/3-clause BSD, which includes the Intel 8080A core that one would need to spin up an Altair 8800 emulator as well.


Report comment 
Reply 





stappers says: 



							January 15, 2023 at 3:59 am						




Thanks for the click bait warning.


Report comment 
Reply 





me says: 



							January 16, 2023 at 2:33 am						




fortunately i read here, so i re-read witho more attention the initial post, so I avoid to lose some minutes of my life looking at another-not-wanted-at-all-cliclbite video. Thanks yo you, dude!


Report comment 
Reply 







Joshua says: 



							January 15, 2023 at 1:58 pm						




What’s also kind of forgotten:
CP/M was thoroughly understood.
It was cloned and derived a dozen times, if not more.
People who worked with assembly language could literally “read” how CP/M worked and flowed through the system.
My father, among many others, wrote his own floppy controller routines for CP/M.
It’s kind of sad to see how incompetent the matter is being handled these days. Such a blunder. *sigh*
Back in the 70s and 80s, it was just natural to know about the internals of the Z-80 and/or CP/M. 
There were hundreds of books being written about all the deepest mechanisms of them.


Report comment 
Reply 





bruceeifler@gmail.com says: 



							January 15, 2023 at 6:15 pm						




“How to program the z80.  Roger Zaks


Report comment 
Reply 





Mr Name Required says: 



							January 15, 2023 at 11:14 pm						




Rodnay Zaks, with the ‘a’. A well-thumbed book for me.


Report comment 
Reply 











daveboltman says: 



							January 15, 2023 at 4:50 am						




Has Bill Gates really got over himself now?
https://en.wikipedia.org/wiki/An_Open_Letter_to_Hobbyists


Report comment 
Reply 





Ken says: 



							January 15, 2023 at 7:37 am						




You must not have been involved in the hobby “back in the day” – the copying of commercial products like Microsoft BASIC was rampant, almost celebrated in the community.
This was before Microsoft had even started licensing ROM BASIC to the major home/personal computer manufacturers, so stolen paper tapes were a serious hit to MS revenues.


Report comment 
Reply 





stappers says: 



							January 15, 2023 at 7:53 am						




And the problem of lack of wallet-voting still exists.  Might be because of the same word for libre and gratuit.
Where I do agree with https://justforfunnoreally.dev/  I also do agree with young Bill Gates that people should do some kind of payment for what they want.


Report comment 
Reply 







Joshua says: 



							January 15, 2023 at 1:48 pm						




Out of context. 
I’m no expert of American copyright law, but as far as I understand:
Material released before 1977 was considered Public Domain, unless the copyright is/was explicitly expressed.
I assume this covers material/works of engineers, students, universities etc. which didn’t mention the copyright in their papers. 
Go double check yourself, if you wish. I’m lazy right now. 😁


Report comment 
Reply 





The Commenter Formerly Known As Ren says: 



							January 15, 2023 at 4:29 pm						




No, he hasn’t, he is now a James Bond level super villain.


Report comment 
Reply 







Sok Puppette says: 



							January 15, 2023 at 6:39 am						




If it cannot play tunes on an AM radio, it is not a satisfying Altair emulator


Report comment 
Reply 





Ken says: 



							January 15, 2023 at 8:13 am						




Neat project, I too am impressed by the level of documentation. While I’m not too interested in running weather/climate change analysis programs on an emulated 50 year-old system, I realize it was important to inject some cloud activity to jazz up the project.
Interesting to note that you need to install Linux on a windows machine to run the code.
Neat how the sense hat reduces the need for ‘blinkin lights’ to its most basic elements, like the computer prop in the bat cave or any 1960s sci-fi movie/tv show…


Report comment 
Reply 





paulvdh says: 



							January 15, 2023 at 9:25 am						




At 5:42 he says he does not know whether the CP/M source code still exists…
He clearly has not checked Hackaday lately…
https://hackaday.com/2014/10/06/cpm-source-code-released/


Report comment 
Reply 





Joshua says: 



							January 15, 2023 at 1:51 pm						




Not just CP/M, also MP/M, the power user version of CP/M. The real thing, so to say. :)


Report comment 
Reply 







SayWhat? says: 



							January 15, 2023 at 12:16 pm						




I remember hacking “protected” MS Basic programs back in the day. All it took was finding the one byte in memory  that set the code to be not listable LOL.


Report comment 
Reply 





demon256 says: 



							January 15, 2023 at 3:45 pm						




Heh, my first computer ran CP/M.


Report comment 
Reply 



Leave a Reply					Cancel reply










Please be kind and respectful to help make the comments section excellent. (Comment Policy)This site uses Akismet to reduce spam. Learn how your comment data is processed.







Search

Search for:



Never miss a hack
Follow on facebook
Follow on twitter
Follow on youtube
Follow on rss
Contact us
Subscribe










If you missed it







AI-Controlled Twitch V-Tuber Has More Followers Than You


 39 Comments				








Ask Hackaday: What’s Your Worst Repair Win?


 173 Comments				








The Surprisingly Simple Way To Steal Cryptocurrency


 65 Comments				








Excuse Me, Your Tie Is Unzipped


 100 Comments				








All About USB-C: Resistors And Emarkers


 16 Comments				



More from this category





Our Columns







2022 FPV Contest: Congratulations To The Winners!


 1 Comment				








Machining With Electricity Hack Chat


 5 Comments				








Hackaday Links: January 15, 2023


 16 Comments				








Too Many Pixels


 34 Comments				








Hackaday Podcast 201: Faking A Transmission, Making Nuclear Fuel, And A Slidepot With A Twist


 2 Comments				



More from this category

"
https://news.ycombinator.com/rss,How my brother's iCloud account was stolen,https://7c0h.com/blog/new/stolen_iphone.html,Comments,"






Martin's blog - How my brother's iCloud account was stolen


















Martin's blog 



april_2020
april_2022
august_2015
august_2018
august_2020
december_2018
december_2020
december_2021
december_2022
february_2017
february_2018
february_2020
february_2022
january_2016
january_2020
january_2021
january_2022
january_2023
july_2015
july_2016
july_2019
july_2020
july_2021
july_2022
june_2015
june_2018
june_2020
june_2021
march_2015
march_2018
march_2020
march_2021
may_2015
may_2016
may_2020
may_2021
misc
november_2017
november_2019
november_2020
november_2021
november_2022
october_2015
october_2020
october_2021
october_2022
september_2015
september_2016
september_2021
september_2022






7c0h

Home
About me
Blog
Projects
Research





Martin's blog







RSS


Archive


Mastodon












How my brother's iCloud account was stolen

				Posted on 2023-01-16 under 					iphone
support




My brother got his iPhone stolen at gunpoint. This is the story of how he lost
control of his iCloud account first (along with years of priceless memories,
including my nephew's first steps) and how this couldn't have happened without
Apple's incompetent support.
But before that, a plea for help: if you know of anyone who can help us get
the account back (or rather, the priceless photos locked inside) please
get in touch with the links at the end of the article.
Part 1: Locked out of iCloud
The first thing the thieves did (after running away, of course) was changing
the phone number associated with the iCloud account. I do not know how they
did this - it has been suggested that Apple will send a code to your phone,
which the thieves obviously had. In any case, as soon as my brother tried to
log into ""Find my Phone"" he was faced with a screen asking him to verify
the phone number associated with the account, which was set to a number we
do not recognize.
It didn't matter that we still had the proper password for the iCloud account,
nor that we still have control of the e-mail associated with the account.
As far as Apple is concerned, if you don't know the phone number (which, again,
the thieves changed) you cannot access your iCloud account.
This is a known issue with iCloud security.
Next we got in touch with Apple, both via phone and Twitter.
Phone support was useless, as all they would say was that they couldn't help
us unless we knew the phone number. The closest I got to a victory was
getting the phone representative to say out loud that she wanted me to provide
the phone number the thieves gave, but that's about it.
Even if we have the old phone number, the iCloud e-mail, the iCloud
password, a police report and multiple IDs, Apple will not budge.
Twitter support was even worse: after repeatedly asking them to read what I
said two messages ago, I ended up getting this pointless, infuriating response:

We completely understand the concern with how important it is to have this resolved as soon as possible, and we were able to locate the cases from when you had previously reached out. Based on the information that you've provided, and the steps you and your brother have gone through, if your brother is unable to regain access to his Apple ID we would be unable to provide any additional methods to regain access to the account, and we would be unable to change the trusted phone number on the account. If you have any other questions or concerns regarding this issue, the best option would be to reach back out to our Apple ID experts at the link provided in our previous message (Note: that's the phone we called
before).

Part 2: Losing iCloud for good
But the story gets even worse (better?). While we were stuck in phone support
hell, my brother got a phishing SMS. He didn't recognize it as such, and lost
the phone for good. The trick works like this: once you get a new SIM card
(which the thieves can tell because the old one stops working) the
thieves send you a phishing e-mail pretending to be from Apple.
You follow the link, give your iCloud username and password,
and now the thieves can unlock your iPhone and resell it.
Crucially, this step only works because thieves know that Apple support will
not help you: if Apple had been of ANY help then we would have recovered
access long before the SMS and my brother wouldn't have followed the link.
According to Gizmodo (in Spanish) the next step would have been
a phishing call with spoofed caller ID. But we will never know.
Sidenote: I reported both URLs (https://apple.iforgot-ip.info and
https://apple.located-maps.info) to their hosting providers.
Results have been mixed. PublicDomainRegistry.com (which belongs to
Webhostbox) will not take down the hosting addresses unless they can see the
phishing attack themselves (they won't check logs), but good luck getting
a one-time link to work twice. UnifiedLayer.com was helpful,
and I believe that GoDaddy revoked their domains.
Part 3: No more Apple
The iCloud website promises
""all your stuff — photos, files, notes, and more — is safe, up to date, and available wherever you are"".
You have now seen what ""safe"" means in this context: it will be in the cloud,
yes, but that doesn't mean you'll have access to it.
The question now is: would my brother choose Apple again?
An iPhone is not cheap in general, and in Argentina less so. The current price
for an iPhone 13 is ca. 400.000 ARS, which roughly translates to 2200 USD
or 1300 USD at the unofficial rate (it's complicated). With an average
monthly salary of 427 USD (according to Numbeo) you can see that getting a new
iPhone is not a choice to take lightly.
What will my brother do? Paraphrasing from a conversation we had:
""if I could get my account back I could consider getting a new iPhone.
But if I have to start from scratch then it doesn't make sense.
If I can't get my data back I'll probably get an Android phone instead"".
Part 4: Conclusion and next steps
I have not entirely given up, but I'm not keeping my hopes up either. We are
currently looking into whether my brother's wife can get access to his files
(they had some kind of shared access), whether his iWatch can be of any use
(it was logged into the iCloud account), whether small claims court is likely to help (I know
it would work in the EU but
Argentina is trickier), and whether anyone in my extended network can reach
someone at Apple who is not an AI (thanks for nothing, LinkedIn).
As for next steps, I will be gifting my family access to some cloud storage,
but unless I can get a service with competent tech support
I may end up setting up a cloud of my own.
Hopefully the loss of my nephew's first steps will not be in vain.
Do you have any ideas? Do you work for Apple? Then send
me an e-mail or get in
touch in Mastodon or even
Twitter.








Pure.css
Pelican






					All content in this blog is licensed under a
Creative Commons Attribution-ShareAlike 4.0 International License






"
https://news.ycombinator.com/rss,Marriage rules in Minoan Crete revealed by ancient DNA analysis,https://phys.org/news/2023-01-marriage-minoan-crete-revealed-ancient.html,Comments,"400 Bad request
Your browser sent an invalid request.

"
https://news.ycombinator.com/rss,Boris Yeltsin's visit to a suburban Houston supermarket in 1989,http://beelineblogger.blogspot.com/2016/01/how-supermarket-visit-brought-down.html,Comments,"


















BeeLine: How A Supermarket Visit Brought Down The Soviet Union








































































BeeLine




The Shortest Route To What You Need To Know




















Scott Beeken





BeeLine



View my complete profile








































































Tuesday, January 5, 2016








How A Supermarket Visit Brought Down The Soviet Union





Many point to the fact that the Soviet Union collapse occurred as the Soviets were baited into trying to compete with the defense build-up instituted by Ronald Reagan.

The Soviets just did not have the financial resources to match the United States in defense spending while also tending to the needs of its citizenry. The Soviets spent money on guns rather than butter. Something had to give and the Soviet people were the ones that suffered.

However, a little known visit to a suburban Houston supermarket in 1989 by Boris Yeltsin appears to have been the catalyst that ended up bringing down the Soviet Union.

Yeltsin visited the Johnson Space Center in Houston in September, 1989 to tour mission control and to view a model of the planned International Space Station.

After visiting the Space Center, Yeltsin made an unplanned stop at a local Randall's grocery store that was close by before heading to the airport.

That visit changed the course of history.

At the time, Yeltsin was a newly elected member of the Soviet Parliament and the Supreme Soviet and had been a key ally of the General Secretary of the Communist Party Mikhail Gorbachev, who was initiating reforms but the pace of which was too slow for Yeltsin.

Houston Chronicle reporter Stephanie Asin was with Yeltsin on the visit to the grocery store that day.


Yeltsin, then 58, “roamed the aisles of Randall’s nodding his head in amazement,” wrote Asin. He told his fellow Russians in his entourage that if their people, who often must wait in line for most goods, saw the conditions of U.S. supermarkets, “there would be a revolution.”

“Even the Politburo doesn’t have this choice. Not even Mr. Gorbachev,” he said.

The fact that stores like these were on nearly every street corner in America amazed him. They even offered free cheese samples. According to Asin, Yeltsin didn’t leave empty-handed, as he was given a small bag of goodies to enjoy on his trip.

This is a picture of Yeltsin touring the grocery store.



Credit: Houston Chronicle


This Houston Chronicle story from 2014 fills in the rest of the story.


About a year after the Russian leader left office, a Yeltsin biographer later wrote that on the plane ride to Yeltsin’s next destination, Miami, he was despondent. He couldn’t stop thinking about the plentiful food at the grocery store and what his countrymen had to subsist on in Russia.

In Yeltsin’s own autobiography, he wrote about the experience at Randall’s, which shattered his view of communism, according to pundits. Two years later, he left the Communist Party and began making reforms to turn the economic tide in Russia. 

“When I saw those shelves crammed with hundreds, thousands of cans, cartons and goods of every possible sort, for the first time I felt quite frankly sick with despair for the Soviet people,” Yeltsin wrote. “That such a potentially super-rich country as ours has been brought to a state of such poverty! It is terrible to think of it.”

To give you some perspective on what was available in the Soviet Union at that time, here is a picture of a Russian store from that era.




Credit: Gennady Galperin/Reuters


An aide to Yeltsin later reported that in that visit to the grocery store in Houston “the last vestige of Bolshevism collapsed” inside his boss.

Two years later Yeltsin was elected to the newly created office of President of the Russian Federation after the collapse of the Soviet Union with Gorbachev.

Yeltsin immediately began dismantling the socialist economic system and introducing capitalism to the Russians. In the process he attempted to convert the world's largest command economy into a free-market one. 

The results of that transition were rocky in large part to cronyism in the break-up of many of the large state-owned businesses. In the process, many Russian oligarchs were created and Yeltsin eventually resigned his office in 1999 haunted by charges of corruption and incompetence.

His successor?

Vladimir Putin.

The falling price of oil has put a similar squeeze on Putin and the Russians today. Putin has been popular with the Russian people based on his macho style and nationalistic bombast. However, potential trouble lurks for Putin because of the Russian economy.

The Russian consumer is being squeezed with annual inflation of almost 20% and the average Russian spends about 50% of their income on food.

By comparison, the average American spends only 8% of income on groceries.

Will groceries once again determine the future of Russia?






Posted by



BeeLine




at

8:21 PM
















Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest














42 comments:




andreSeptember 14, 2017 at 4:52 AMobat viagraviagra asliReplyDeleteRepliesUnknownOctober 29, 2021 at 10:14 AMGreat Article Artificial Intelligence Projects Project Center in Chennai JavaScript Training in Chennai JavaScript Training in Chennai Project Centers in Chennai DeleteRepliesReplyReplychegekhanMarch 28, 2018 at 8:11 AMUseful Information, your blog is sharing unique information....Thanks for sharing!!! buy bakery products online south-cbuy branded food online in panganiReplyDeleteRepliesReplyUnknownSeptember 27, 2018 at 7:24 AMThank you for your post. This is excellent information. It is amazing and wonderful to visit your site.buy bakery products online south-c ReplyDeleteRepliesReplyYK AgencyDecember 25, 2018 at 11:30 AMSupermarket in Dubai Great article. Cool.ReplyDeleteRepliesReplyLuck CityFebruary 11, 2019 at 3:50 AMWithin this webpage, you'll see the page, you need to understand this data. https://digitalglobal.comReplyDeleteRepliesReplyjames brownNovember 22, 2019 at 6:45 AMAwesome blog. I enjoyed reading your articles. This is truly a read for me. I have bookmarked it and I am looking forward to reading new articles. Keep up the good work!Kroger customer surveyReplyDeleteRepliesReplyKroger experienceDecember 10, 2019 at 8:05 AMPlease share more like that.Kroger experienceReplyDeleteRepliesReplyAnonymousDecember 30, 2019 at 7:47 PMGreat article! Yeltsin revealed as a realist! I never knew this ...ReplyDeleteRepliesReplyDavid Grant Stewart, Sr., EgyptologistJanuary 1, 2020 at 3:17 PMWho paid you to write this drivel? You are either incredibly gullible or on the take from the USSR propaganda machine. You look and say the Soviet civilian economy is bad. There is no civilian economy in the USSR. The country does not have a war machine. The country is a war machine.ReplyDeleteRepliesReplyNeha UppalJanuary 9, 2020 at 3:33 AMThanks for sharing such beautiful information with us. I hope you will share some more information about best grocery shopping app. Please keep sharing.ReplyDeleteRepliesReplyjames brownFebruary 13, 2020 at 1:15 PMHey There. I found your blog using This is a very well written article. I’ll be sure to bookmark it and come back to read more of your useful info. Thanks for the post. I’ll definitely return.https://krogerexperiencee.com/ReplyDeleteRepliesReplyjames brownFebruary 18, 2020 at 7:24 AMGreat post I would like to thank you for the efforts you have made in writing this interesting and knowledgeable article.https://tellthebelll.usReplyDeleteRepliesReplysurvey monkey usaFebruary 19, 2020 at 7:45 AMGreat things you’ve always with us. Just keep writing this kind of posts.The time which was wasted in traveling for tuition now it can be used for studies.Thankssurvey monkey usaReplyDeleteRepliesReplydanielwilsonnFebruary 19, 2020 at 3:43 PM Thank you again for all the knowledge u distribute,Good post. I was very interested in the article, it's quite inspiring I should admit. I like visiting you site since I always come across interesting articles like this one.Great Job, I greatly appreciate that.Do Keep sharing! Regards,https://krogerfeeedback.us/ReplyDeleteRepliesReplydahliaApril 23, 2020 at 3:21 AM Thanks for sharing this information. I really like your post very much. You have really shared an informative and interesting post with people  TellTheBell ReplyDeleteRepliesReplypatronsurveysApril 24, 2020 at 8:23 AMAfter Kroger and Walmart, I prefer to go to Tesco supermarket. Do you know what? there is quality in products with an extraordinary service tescoviews com offers a platform to complete the Tesco customer satisfaction survey to win £1000 Gift Card & 25 Club Points. Survey site is giving a lot of store surveys at one place to complete.ReplyDeleteRepliesReplyjames brownJuly 14, 2020 at 3:24 AMThis comment has been removed by the author.ReplyDeleteRepliesReplyjames brownJuly 14, 2020 at 3:25 AMIts a great pleasure reading your post.Its full of information I am looking for and I love to post a comment that ""The content of your post is awesome"" Great work.https://krogerexperiencee.com/greatpeople-me-kroger-employee-login-portal/ReplyDeleteRepliesReplyNFL FanSeptember 20, 2020 at 4:46 PMThe official source for NFL news, video highlights, fantasy football, game-day coverage, schedules, stats, scores and more. Ravens FootballReplyDeleteRepliesReplytellthebellSeptember 22, 2020 at 11:42 AM Excellent website you have  so much cool information!..tellthebellReplyDeleteRepliesReplydgcustomerfirstJune 25, 2021 at 6:49 AMTo take an interest in the Dollar general super market survey, it is important to arrange a few things at one of its Branches initially. Visit the authority survey site of Dollar General survey at Dgcustomerfirst.Com and Win A $100 gift voucher. Then, at that point, you need to save the receipt of the store. Then, at that point, go to the authority survey site of dollar general. The Dg survey is accessible in both English or Spanish.ReplyDeleteRepliesReplymybkexperienceAugust 3, 2021 at 2:30 AMDollar general survey is an online platform that collects customers' most recent shopping experiences and overall customer satisfaction. Participate in the survey and be the lucky person to get enlisted in dg customer first winners. ReplyDeleteRepliesReplyarnavharperAugust 30, 2021 at 12:33 PMRemote for Fire TV is designed specifically to control Fire TV, Fire TV Cube and Fire TV Stick. Just connect mobile device and a TV or media player to the Firestick Remote.ReplyDeleteRepliesReplySEOOctober 12, 2021 at 3:37 AMdelta international recruitment agency in pakistanReplyDeleteRepliesReplyHealthandBeautyTipsNovember 6, 2021 at 5:24 PMEmployees can Perform Kroger E-Schedule Login at the Feed Kroger Login Portal once their Schedule Credentials are verified. If you are unable to sign in to Kroger Login then you need to contact the branch manager.Feed KrogerReplyDeleteRepliesReplyUnknownJanuary 27, 2022 at 5:48 AMThe NASA dark Brant IX sounding rocket conveyed the payload to an apogee of 177 miles prior to plunging by parachute and arriving at White Sands. Dgcustomerfirst.com Survey ReplyDeleteRepliesReplydgcustomerfirst.comMay 19, 2022 at 3:35 AMThe dgcustomerfirst.com criticism review permits customers to enter the Dollar General Sweepstakes of Cash $100 in the wake of finishing the overview.dgcustomerfirstwin.shop Survey  ReplyDeleteRepliesReplywww.DGCustomerFirst.comJune 6, 2022 at 10:50 AMDollar general survey is an online platform that collects customers' most recent shopping experiences and overall customer satisfaction. https://idgcustomerfirst.org/ReplyDeleteRepliesReplydgcustomerfirsts.shopJune 10, 2022 at 6:07 AMAlso, you have a superb opportunity to partake in the client criticism overview. DGCustomerFirst 2022 or Dollar General Survey is a study led by Dollar General's authorities for all United States inhabitants.dgcustomerfirsts executed a connected with the WWW review to take an arrangement about dgcustomerfirsts Helpline notwithstanding your support level thereafter visiting service  Click here dgcustomerfirsts ReplyDeleteRepliesReplySteveJune 16, 2022 at 9:58 AMGreat Article, it was very informative. That was such thought-provoking content. I enjoyed reading your content. Every week, I look forward to your column. In my opinion, this one is one of the best articles you have written so far.How to Change Instagram PasswordChange Windows 10 PasswordSubwaylistensHome Depot SurveyDQFanFeedback.com ReplyDeleteRepliesReplyUmairJune 30, 2022 at 6:44 AMirescopk.comReplyDeleteRepliesReplyDgcustomerfirstscom.shopJuly 9, 2022 at 6:51 AMFormerly referred to as J L Turner, Dollar General has numerous subsidiaries viz Dollar General Market, Dollar General Financial, Dollar General Global Sourcing, and lots extra.dgcustomerfirstscom executed a connected with the WWW review to take an arrangement about dgcustomerfirstscom Helpline notwithstanding your support level thereafter visiting service  Click here dgcustomerfirstscom ReplyDeleteRepliesReplyAnonymousJuly 19, 2022 at 1:35 PMLiveTheOrangeLife – Official Portal www.LiveTheOrangeLife.comWalmartOne Login - Walmartone.com Login Guidemyaccountaccess.comonevanillaJCP Associate KioskReplyDeleteRepliesReplySmith AdomJuly 30, 2022 at 12:03 PMTalkToWendys executed a connected with the WWW review to take an arrangement about TalkToWendys Helpline notwithstanding your support level thereafter visiting service  Click here TalkToWendys  It is mandatory to make a purchase at Wendy’s once before being a participant in this survey.ReplyDeleteRepliesReplyInformTarget.comAugust 10, 2022 at 3:38 AMRules are guidelines, and they are set to be accompanied. If you want to participate within the survey effectively, you need to adhere to the rules and policies set apart via the informtarget.Com remarks survey.informtargets executed a connected with the WWW review to take an arrangement about informtargets Helpline notwithstanding your support level thereafter visiting service  Click here informtargets.shop ReplyDeleteRepliesReplyTellBaskinRobbinsAugust 19, 2022 at 5:39 AMThere are some basic rules and requirements of this Baskin Robbins Customer Satisfaction Survey which I even have furnished in this newsletter. tellbaskinrobbins executed a connected with the WWW review to take an arrangement about tellbaskinrobbins Helpline notwithstanding your support level thereafter visiting service  Click here tellbaskinrobbins ReplyDeleteRepliesReplyTalktofoodlionAugust 20, 2022 at 2:37 AMTalktofoodlion The company's full name is general Dollar, and it is offering a $100 incentive to customers who take the time to participate in this little survey. visit here Talktofoodlion ReplyDeleteRepliesReplyFaiz IsrailiAugust 21, 2022 at 5:02 AMThe procedure is requesting information from individuals using a questionnaire, which may be completed offline or online. New technologies, however, are frequently disseminated via digital channels like social media, email, QR codes, or URLs. dgcustomerfirstReplyDeleteRepliesReplyTellBaskinRobbinsSeptember 8, 2022 at 7:11 AMTellBaskinRobbins After responding to the feedback questions, participants are expected to rate their experience shopping at Baskin Robbins.The feedback survey is sponsored by Baskin Robbins in order to better understand its service quality TellBaskinRobbinsReplyDeleteRepliesReplyNikithaOctober 8, 2022 at 1:31 AMmybkexperience customer satisfaction survey which is an online platform to get timely feedback from their customers about the food and services. This can improve their services according to customer’s needs and at the same time, rewards their customer for their time and loyalty towards the restaurant. So you can answer the questions and eat delicious food for free at the same time. Now you might be wondering about how to participate in the survey or what are the requirements and much more.ReplyDeleteRepliesReplySEOOctober 14, 2022 at 5:59 AMtop recruitment agencies in pakistan for saudi arabiaReplyDeleteRepliesReplyAdd commentLoad more...























Newer Post


Older Post

Home




Subscribe to:
Post Comments (Atom)















BeeLine Email Subscription

Get new posts by email:  Subscribe




Follow @BeeLineBlog




Followers











Blog Archive








        ► 
      



2023

(5)





        ► 
      



January

(5)









        ► 
      



2022

(139)





        ► 
      



December

(11)







        ► 
      



November

(10)







        ► 
      



October

(12)







        ► 
      



September

(12)







        ► 
      



August

(13)







        ► 
      



July

(11)







        ► 
      



June

(12)







        ► 
      



May

(12)







        ► 
      



April

(12)







        ► 
      



March

(12)







        ► 
      



February

(10)







        ► 
      



January

(12)









        ► 
      



2021

(131)





        ► 
      



December

(15)







        ► 
      



November

(12)







        ► 
      



October

(9)







        ► 
      



September

(13)







        ► 
      



August

(14)







        ► 
      



July

(11)







        ► 
      



June

(10)







        ► 
      



May

(6)







        ► 
      



April

(10)







        ► 
      



March

(11)







        ► 
      



February

(7)







        ► 
      



January

(13)









        ► 
      



2020

(154)





        ► 
      



December

(11)







        ► 
      



November

(12)







        ► 
      



October

(14)







        ► 
      



September

(11)







        ► 
      



August

(12)







        ► 
      



July

(13)







        ► 
      



June

(14)







        ► 
      



May

(12)







        ► 
      



April

(16)







        ► 
      



March

(16)







        ► 
      



February

(10)







        ► 
      



January

(13)









        ► 
      



2019

(145)





        ► 
      



December

(14)







        ► 
      



November

(11)







        ► 
      



October

(9)







        ► 
      



September

(12)







        ► 
      



August

(13)







        ► 
      



July

(12)







        ► 
      



June

(12)







        ► 
      



May

(14)







        ► 
      



April

(13)







        ► 
      



March

(12)







        ► 
      



February

(10)







        ► 
      



January

(13)









        ► 
      



2018

(139)





        ► 
      



December

(11)







        ► 
      



November

(10)







        ► 
      



October

(10)







        ► 
      



September

(10)







        ► 
      



August

(12)







        ► 
      



July

(13)







        ► 
      



June

(12)







        ► 
      



May

(12)







        ► 
      



April

(13)







        ► 
      



March

(12)







        ► 
      



February

(9)







        ► 
      



January

(15)









        ► 
      



2017

(132)





        ► 
      



December

(9)







        ► 
      



November

(10)







        ► 
      



October

(15)







        ► 
      



September

(9)







        ► 
      



August

(13)







        ► 
      



July

(12)







        ► 
      



June

(9)







        ► 
      



May

(13)







        ► 
      



April

(12)







        ► 
      



March

(10)







        ► 
      



February

(10)







        ► 
      



January

(10)









        ▼ 
      



2016

(119)





        ► 
      



December

(11)







        ► 
      



November

(15)







        ► 
      



October

(15)







        ► 
      



September

(10)







        ► 
      



August

(2)







        ► 
      



July

(9)







        ► 
      



June

(11)







        ► 
      



May

(6)







        ► 
      



April

(9)







        ► 
      



March

(11)







        ► 
      



February

(12)







        ▼ 
      



January

(8)

In the Middle of 5th Avenue
Risky Business
That's for the Birds
An Inconvenient Truth +10
Citizen Cruz
Context on Guns
How A Supermarket Visit Brought Down The Soviet Union
A Humble Servant?










        ► 
      



2015

(71)





        ► 
      



December

(9)







        ► 
      



November

(5)







        ► 
      



October

(2)







        ► 
      



September

(2)







        ► 
      



August

(9)







        ► 
      



July

(7)







        ► 
      



June

(6)







        ► 
      



May

(3)







        ► 
      



April

(10)







        ► 
      



March

(6)







        ► 
      



February

(4)







        ► 
      



January

(8)









        ► 
      



2014

(88)





        ► 
      



December

(5)







        ► 
      



November

(7)







        ► 
      



October

(10)







        ► 
      



September

(9)







        ► 
      



August

(5)







        ► 
      



July

(8)







        ► 
      



June

(7)







        ► 
      



May

(9)







        ► 
      



April

(7)







        ► 
      



March

(6)







        ► 
      



February

(5)







        ► 
      



January

(10)









        ► 
      



2013

(115)





        ► 
      



December

(8)







        ► 
      



November

(5)







        ► 
      



October

(13)







        ► 
      



September

(9)







        ► 
      



August

(7)







        ► 
      



July

(10)







        ► 
      



June

(10)







        ► 
      



May

(9)







        ► 
      



April

(13)







        ► 
      



March

(10)







        ► 
      



February

(7)







        ► 
      



January

(14)









        ► 
      



2012

(139)





        ► 
      



December

(6)







        ► 
      



November

(16)







        ► 
      



October

(16)







        ► 
      



September

(9)







        ► 
      



August

(14)







        ► 
      



July

(13)







        ► 
      



June

(8)







        ► 
      



May

(11)







        ► 
      



April

(9)







        ► 
      



March

(9)







        ► 
      



February

(11)







        ► 
      



January

(17)









        ► 
      



2011

(188)





        ► 
      



December

(12)







        ► 
      



November

(10)







        ► 
      



October

(10)







        ► 
      



September

(10)







        ► 
      



August

(17)







        ► 
      



July

(18)







        ► 
      



June

(11)







        ► 
      



May

(3)







        ► 
      



April

(16)







        ► 
      



March

(19)







        ► 
      



February

(25)







        ► 
      



January

(37)









About Me





BeeLine


Scott Beeken has practiced as an attorney, CPA and has been an officer with two Fortune 500 companies overseeing diverse functions such as Taxation, Employee Benefits, Human Resources, Real Estate Facilities, Risk Management, Corporate Communications, Marketing and Advertising. In addition to writing BeeLine, he is a Keynote Speaker, Author  and Strategic Consultant.

View my complete profile




























Total Pageviews

























Theme images by luoman. Powered by Blogger.

























"
https://news.ycombinator.com/rss,"Show HN: Vento, Screen Recorder that lets you rewind and re-record over mistakes",https://vento.so,Comments,"ventoNew RecordingLog inStress-free Screen Recording Constantly restarting your screen recordings? With Vento, just pause, rewind, and carry on instead - keeping calm helps too :)Come back on a desktop computer to try us out!Install Chrome ExtensionStart Recording“To record great videos, one must first rewind.” - Ghandi, probably.Video Not Availablevento© 2023 Vento. All rights reserved.Say hello! We don’t bite. Well, maybe one of us does.hello@vento.so"
https://news.ycombinator.com/rss,Granian – a Rust HTTP server for Python applications,https://github.com/emmett-framework/granian,Comments,"








emmett-framework

/

granian

Public







 

Notifications



 

Fork
    9




 


          Star
 475
  









        A Rust HTTP server for Python applications
      
License





     BSD-3-Clause license
    






475
          stars
 



9
          forks
 



 


          Star

  





 

Notifications












Code







Issues
7






Pull requests
0






Discussions







Actions







Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Discussions
 


                  Actions
 


                  Security
 


                  Insights
 







emmett-framework/granian









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











master





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





14
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




gi0baro

Refactor response build in RSGI protocol




        …
      




        e5139e2
      

Jan 16, 2023





Refactor response build in RSGI protocol


e5139e2



Git stats







136

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github



Update CI release workflow



Jan 13, 2023









benchmarks



Update benchmarks



Jan 13, 2023









docs/spec



Fix typos (#14)



Nov 17, 2022









granian



wsgi: ensure to close the app response iterator (#29)



Jan 16, 2023









lib/pyo3-asyncio



Bump pyo3-asyncio to 0.17



Oct 25, 2022









src



Refactor response build in RSGI protocol



Jan 16, 2023









tests



Fix wsgi.input out of spec (close #24)



Jan 12, 2023









.gitignore



first implementation



Apr 15, 2022









Cargo.lock



Upgrade cargo dependencies



Jan 16, 2023









Cargo.toml



Upgrade cargo dependencies



Jan 16, 2023









LICENSE



first implementation



Apr 15, 2022









README.md



Update benchmarks results



Dec 24, 2022









build.rs



Add PyPy support



Jan 3, 2023









pyproject.toml



Add PyPy support



Jan 3, 2023









setup.py



review package meta



Apr 18, 2022




    View code
 















Granian
Rationale
Features
Quickstart
Project status
License





README.md




Granian
A Rust HTTP server for Python applications.
Rationale
The main reasons behind Granian design are:

Have a single, correct HTTP implementation, supporting versions 1, 2 (and eventually 3)
Provide a single package for several platforms
Avoid the usual Gunicorn + uvicorn + http-tools dependency composition on unix systems
Provide stable performance when compared to existing alternatives

Features

Supports ASGI/3, RSGI and WSGI interface applications
Implements HTTP/1 and HTTP/2 protocols
Supports HTTPS
Supports Websockets over HTTP/1 and HTTP/2

Quickstart
You can install Granian using pip:
$ pip install granian

Create an ASGI application in your main.py:
async def app(scope, receive, send):
    assert scope['type'] == 'http'

    await send({
        'type': 'http.response.start',
        'status': 200,
        'headers': [
            [b'content-type', b'text/plain'],
        ],
    })
    await send({
        'type': 'http.response.body',
        'body': b'Hello, world!',
    })
and serve it:
$ granian --interface asgi main:app

You can also create an app using the RSGI specification:
async def app(scope, proto):
    assert scope.proto == 'http'

    proto.response_str(
        status=200,
        headers=[
            ('content-type', 'text/plain')
        ],
        body=""Hello, world!""
    )
and serve it using:
$ granian --interface rsgi main:app

Project status
Granian is currently under active development.
Granian is compatible with Python 3.7 and above versions on unix platforms and 3.8 and above on Windows.
License
Granian is released under the BSD License.









About

      A Rust HTTP server for Python applications
    
Topics



  python


  rust


  http


  http-server


  asyncio


  asgi



Resources





      Readme
 
License





     BSD-3-Clause license
    



Stars





475
    stars

Watchers





7
    watching

Forks





9
    forks







    Releases
      14







Granian 0.2.2

          Latest
 
Jan 16, 2023

 

        + 13 releases





Sponsor this project



 

 

 Sponsor
 
Learn more about GitHub Sponsors







    Packages 0


        No packages published 







        Used by 6
 




























    Contributors 4





 



 



 



 







Languages












Rust
83.1%







Python
16.5%







Other
0.4%











"
https://news.ycombinator.com/rss,"Late Hokusai: Thought, technique, society",https://www.britishmuseum.org/research/projects/late-hokusai-thought-technique-society,Comments,"




405 Not allowed


Error 405 Not allowed
Not allowed
Error 54113
Details: cache-sjc10059-SJC 1673915968 1469586448

Varnish cache server


"
https://news.ycombinator.com/rss,Apple TV prompt requires another Apple device,https://twitter.com/hugelgupf/status/1614794516309118977,Comments,"














JavaScript is not available.
We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.
Help Center

Terms of Service
Privacy Policy
Cookie Policy
Imprint
Ads info
      © 2023 Twitter, Inc.
    

Something went wrong, but don’t fret — let’s give it another shot."
https://news.ycombinator.com/rss,Show HN: Otterkit – COBOL compiler for .NET,https://github.com/otterkit/otterkit,Comments,"








otterkit

/

otterkit

Public




 

Notifications



 

Fork
    1




 


          Star
 32
  









        Otterkit COBOL Compiler
      





otterkit.com


License





     Apache-2.0 license
    






32
          stars
 



1
          fork
 



 


          Star

  





 

Notifications












Code







Issues
1






Pull requests
0






Discussions







Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Discussions
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







otterkit/otterkit









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








4
branches





0
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




KTSnowy

[Update]: Added SourceId, LevelStack and FileIndex comments




        …
      




        2365003
      

Jan 16, 2023





[Update]: Added SourceId, LevelStack and FileIndex comments


2365003



Git stats







175

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github/ISSUE_TEMPLATE



[Update]: Update issue templates and security policy notice



Nov 19, 2022









Assets



[Update]: Added dotnet tool and project templates Nuget packages



Dec 24, 2022









OtterkitTemplatePack



[Update]: Update Otterkit.Templates version number



Dec 24, 2022









libotterkit @ a0256a7



[Update/Fix]: Apply Roslyn Analyzers fixes and change suggestions



Jan 4, 2023









src



[Update]: Added SourceId, LevelStack and FileIndex comments



Jan 16, 2023









.gitattributes



Create .gitattributes



Oct 13, 2022









.gitignore



[Update]: Added dotnet tool and project templates Nuget packages



Dec 24, 2022









.gitmodules



[Update]: Added libotterkit as a git submodule



Nov 2, 2022









LICENSE



[Update]: Added year and name to the LICENSE file



Dec 19, 2022









README.md



[Update]: Add project icon to the README header



Jan 11, 2023









SECURITY.md



[Update]: Change ""runtime"" to ""compiler"" in the security policy



Nov 19, 2022









THIRD-PARTY-LICENSES



[Update]: Added libmpdec license notice



Oct 23, 2022




    View code
 















 Otterkit COBOL Compiler
About
Installation
Quick Install
Build from Source
Standard Acknowledgement





README.md




 Otterkit COBOL Compiler
Otterkit is a free and open source compiler for the COBOL Programming Language on the .NET platform.
Warning: The project is currently in pre-release, so not all of the standard has been implemented.
About
COBOL was created in 1959 by the CODASYL Committee (With Rear Admiral Grace Hopper as a technical consultant to the committee), its design follows Grace Hopper's belief that programs should be written in a language that is close to English. It prioritizes readability, reliability, and long-term maintenance. The language has been implemented throughout the decades on many platforms with many dialects, and the Otterkit COBOL compiler is a free and open source implementation of the ISO COBOL 2022 Standard on the .NET platform.
Installation
Quick Install
Otterkit is available to install on the Nuget package manager (.NET 7 is required). To install, type into the command line:
dotnet tool install --global Otterkit --version 1.0.15-alpha

Build from Source
First, clone the git repo from https://github.com/otterkit/otterkit.git to get the source code. To access the libotterkit submodule inside, use the --recurse-submodules --remote-submodules flag on the clone command. To run, navigate into the src folder (for the compiler, not libotterkit) and then type dotnet run into the command line.
Standard Acknowledgement
Any organization interested in reproducing the COBOL standard and specifications in whole or in part,
using ideas from this document as the basis for an instruction manual or for any other purpose, is free
to do so. However, all such organizations are requested to reproduce the following acknowledgment
paragraphs in their entirety as part of the preface to any such publication (any organization using a
short passage from this document, such as in a book review, is requested to mention ""COBOL"" in
acknowledgment of the source, but need not quote the acknowledgment):
COBOL is an industry language and is not the property of any company or group of companies, or of any
organization or group of organizations.
No warranty, expressed or implied, is made by any contributor or by the CODASYL COBOL Committee
as to the accuracy and functioning of the programming system and language. Moreover, no
responsibility is assumed by any contributor, or by the committee, in connection therewith.
The authors and copyright holders of the copyrighted materials used herein:

FLOW-MATIC® (trademark of Sperry Rand Corporation), Programming for the 'UNIVAC® I and
II, Data Automation Systems copyrighted 1958,1959, by Sperry Rand Corporation;
IBM Commercial Translator Form No F 28-8013, copyrighted 1959 by IBM;
FACT, DSI 27A5260-2760, copyrighted 1960 by Minneapolis-Honeywell

Have specifically authorized the use of this material in whole or in part, in the COBOL specifications.
Such authorization extends to the reproduction and use of COBOL specifications in programming
manuals or similar publications.









About

      Otterkit COBOL Compiler
    





otterkit.com


Topics



  compiler


  dotnet


  cobol



Resources





      Readme
 
License





     Apache-2.0 license
    

Security policy





      Security policy
    



Stars





32
    stars

Watchers





2
    watching

Forks





1
    fork







    Releases

No releases published






    Contributors 3








KTSnowy
Gabriel Gonçalves

 






TriAttack238
Sean Vo

 






gabrielesilinic
Gabriele Silingardi

 





Languages










C#
100.0%











"
https://news.ycombinator.com/rss,Pretty Good House,https://www.prettygoodhouse.org,Comments,"






Pretty Good House























































the Pretty Good House



the Pretty Good House


Read More


PGH 2.0


Certificates


links


the BS*+Beer Show











the Pretty Good House



the Pretty Good House/
Read More/
PGH 2.0/
Certificates/
links/
the BS*+Beer Show/













the Pretty Good House
A Guide to Creating Better Homes



The Pretty Good House Approach to Building
How to Design and Build a Pretty (darn) Good House or Renovation







the Pretty Good House



the Pretty Good House/
Read More/
PGH 2.0/
Certificates/
links/
the BS*+Beer Show/












Pretty Good House provides a framework and guidelines to focus on the core issues that should be front and center when designing and building a high quality home or renovation.
 
About the name…. In southern Maine, there’s a small group of building professionals that get together monthly to discuss building science.  It sounds boring, I know, but truly it isn’t.  Food is brought in, beer is consumed, a blackboard is present and a topic is brought up for discussion.  (topics  like, high performance windows, venting cathedral ceilings, moisture migration in wall assemblies, slab on grade details, etc.) Since we are all peers, and colleagues (with egos, attitude, and a good sense of humor) the conversation is usually lively, informal and frankly all over the place.  There’s usually a moderator to keep everyone reined in and on topic.  Every once in a while I have the pleasure of being that moderator, but that is only when builder, Dan Kolbert can’t attend.  Dan while on a rant once, expressed frustration with the status quo, and even more frustration with the various rating systems that had him jumping through many hoops to prove that his work was in fact “green” and energy efficient.  As many can attest, these systems can be very costly in time and money, and in the case of many green projects, they aren’t doing anything to advance the project.  Rather they are simply certifying them.  So Dan simply stated, “I just want to build a pretty good house.”- Chris Briley
 



  












We’ve been talking about it for some time, but now the Pretty Good House book is real!It is available directly through the Taunton store, local bookshops, and on Amazon. The link below allows you to order the book.Updates on the book, cover, and release will be all over our social media channels. We do not plan to make an email list with updates at this time.Click HERE to order your copy!Meet the authors this fall:Midwest Building Science Symposium Kansas City - Sept 14th & 15th - Mike & EmilyCommon Ground FairUnity, ME Sept 23-25th - Dan & MikeSoutheast Building Science SymposiumChattanooga, TN - Oct 12th - 13th - Emily The Entre Architect Community Annual MeetingAustin, TX - Nov 1-3rd - Mike & Emily South Bend City Housing SymposiumSouth Bend, IN - Dates TBD (Late Nov / Early Dec) Dan & Chris
 
Guidelines for Designing and Building a Pretty Good HouseBuilding a house or doing a major renovation is a daunting task.  This diagram shows how the PGH pros think about things!  Scroll down a little bit further for explanation of the main points. 















Featured

























Economics






Key to the PGH approach is balancing expenditures and gains. Where other programs use specific energy-use targets or other criteria, and the building code establishes a baseline (“the worst house you can legally build”), a PGH goes above code until it stops making financial sense. read more....
Read More →


























The Team Approach






Choosing the right team members can make a big difference on project going smoothly.read more….
Read More →


























Climate - Where are you?






Where you live plays an important part in the design of a PGH.read more…
Read More →


























Design






Design is much more than floor plans and making pretty things. The whole process of construction is designed as well.read more…
Read More →


























The Building Shell - Envelope basics






Every house needs to keep rain and cold (or hot) air out, and conditioned air inside. In a Pretty Good House it’s easiest and best to identify dedicated control layersread more….
Read More →


























Envelope Details






Basic physics. This is where the rubber meets the road. A PGH builder or architect is also something of a science geek.read more….
Read More →


























Windows and Doors






Even the best windows and doors on the market perform much worse than even an average wall, but they can also allow solar energy into the house, and of course light and views are important. Selection of specific brands and models of windows and doors comes into the conversation very early in the process.read more…
Read More →


























Materials






Local is good. While we can get materials from nearly anywhere on earth, buying locally supports the local economy, typically keeps the carbon footprint smaller, and makes a house a product of its environment instead of a cookie-cutter house that could be anywhere. read more…
Read More →


























Mechanical Systems






Mechanical systems include the equipment needed to heat, cool and control humidity in a house, and also the plumbing system. The primary reason for mechanical systems is occupant comfort. A PGH allows mechanicals to be smaller, less expensive and generally simpler.read more…
Read More →


























Electrical and Lighting






Lighting can be divided into two categories: natural and artificial. Daylighting, using windows and other glazing, reduces the need for artificial lighting during the day. Wiring in a PGH must include a well thought out strategy for sealing all penetrations in the building shell.read more…
Read More →


























Verification






There are many ways to check your results. Third party verification systems such as Energy Star or Passive house may influence bank financing but are not the focus of a Pretty Good House.read more….
Read More →


























Owner / Occupant






Occupants can be creative in the ways they break the rules so it’s important for them to be educated on how to operate their Pretty Good home.read more…
Read More →












If you feel that you have designed and built a Pretty Good House, please feel free to download, fill out and display a Pretty Good Certificate, signed by the Pretty Good House authors—available in two flavors, plain and fancy! Just click on the image and you can download a PDF. 

























Relevant organizations and references the BS* + Beer Show a weekly (for now) Zoom based meeting of building science nerds from around the country - links to local chapters coming soonTo watch previous episodes of The BS* and Beer Show - Find the YouTube Channel HEREGreen Building Advisor - This is where many of us initially heard the term “Pretty Good House”  The original article is HEREBuilding Science Corporation provides training, research and can provide a deep dive into building scienceFine Homebuilding The original magazine that we all know and love has a ton of information on high performance building including some great podcastsJournal of Light Construction has really stepped up to the plate with lots of informationPro Trade Craft - Best practices and great videos. This is a great place for builders to learn.E3 Podcast - Emily Mottram interviews various people with a focus on Energy and EfficiencyGreen Architects’ Lounge - A podcast covering many topics relevant to Green Building, high performance building and Pretty Good House infoBuilding Green - industry watchdog and long time provider of product vetting and technical expertise to the profession.NESEA the Northeast Sustainable Energy Association is a resource for builders and homeowners - they put on symposiums in Boston and NYC, builder training, home tours etc.Maine Indoor Air Quality Council - information, links and training.  The quality of the air inside a Pretty Good House is paramount!
 
Mike Maines and Robert Swinburne presented the Pretty Good House at the 2020 Fine Homebuilding Summit












    



















  





    Contact site administation
  


Terms and ConditionsThe Pretty Good House website copyright 2019 by the Pretty Good House Crew



 


 






 




"
https://news.ycombinator.com/rss,MSI's (in)Secure Boot,https://dawidpotocki.com/en/2023/01/13/msi-insecure-boot/,Comments,"

MSI's (in)Secure Boot


      posted on 2023-01-13 (UTC)
    

I guess I have found a reason to write my first blog post.
Before we start, maybe I will quickly explain what Secure Boot is. It is
a security feature, which allows our computer to decline booting
operating systems that have not been signed by a key that the firmware
trusts.
On 2022-12-11, I decided to setup Secure Boot on my new desktop with a
help of sbctl. Unfortunately I have
found that my firmware was… accepting every OS image I gave it, no
matter if it was trusted or not. It wasn't the first time that I have
been self-signing Secure Boot, I wasn't doing it wrong.
As I have later discovered on 2022-12-16, it wasn't just broken
firmware, MSI had changed their Secure Boot defaults to allow booting on
security violations(!!).
This can be changed by going to the place where the settings are for
Secure Boot on your motherboard. In my case it's in Security\Secure Boot. From this place, we can see a menu called ""Image Execution
Policy"", which is the culprit.

When we enter the menu, we can see the disappointing default settings.
It's doing no verification. It's useless. It's just there to satisfy
Windows 11 requirements. OS has no idea that Secure Boot is doing
nothing, it just knows that it's ""enabled"".

To change the settings to something saner, we have to change ""Always
Execute"" to ""Deny Execute"" for ""Removable Media"" and ""Fixed Media"".
What's funny is that ""Allow Execute"" and ""Query User"" options
are breaking UEFI specification,
though I'm not really sure what's the difference between ""Allow Execute""
and ""Always Execute"".
We can also change ""Option ROM"", about which you can read more about
here:

https://github.com/Foxboron/sbctl/wiki/FAQ#option-rom
https://learn.microsoft.com/en-us/windows-hardware/manufacture/desktop/uefi-validation-option-rom-validation-guidance


Case closed, everyone can move on, right?
Well, not really. I needed to figure out if this is only affecting my
motherboard or also other models and maybe even other vendors. And also
we need to document this, because even if I know this, there is probably
a lot of people that are not aware about this issue.
I had asked 2 users of B450 TOMAHAWK MAX (thanks Sage Hane and Daniel
Nathan Gray) to check their firmware and what? Unsurprisingly, it's also
there. We were able to determine that version 7C02v3C from 2022-01-18
introduced this issue.
EDIT: I have noticed that some websites have misreported about this
issue because they have not fully read my article. This firmware version
only affects B450 TOMAHAWK MAX, other motherboards have different
versions. 7C02 in the version is the codename for B450 TOMAHAWK MAX.
More information is available down below.
Is it mentioned in the changelog? Hah, nope.

I have also received information from a user of B550-A PRO (CEC) (thanks
Joseph Richey) that they have this issue from 7C56vH1 (2021-12-20)
onwards.
While I was able to extrapolate this information to guess which versions
for other boards have introduced this issue, that isn't really enough.
We need to go deeper.
I have tried extracting some information from MSI's binary firmware
files, but to no avail. I tried using binwalk, UEFITool and others, but
I didn't really find what I wanted. Until one day I have found out that
UEFI has a thing called ""UEFI Internal Form Representation"" or in short
""IFR"". It's a way to describe firmware configuration options. This is
exactly what I need to look for! Now, what do I do with this knowledge?
Once we extract files from the firmware using UEFIExtract from
UEFITool project, we can find a
file called
Section_PE32_image_899407D7-99FE-43D8-9A21-79EC328CAC21_Setup_body.bin.
It contains most of UEFI GUI stuff and seems to be available on all
firmware from all major desktop motherboard makers, though ASUS decided
to remove ""Setup"" from the name for some reason or maybe it has to do
something to do with the UEFIExtract, not sure.
Now once we have this file, we have to extract IFR data from it, to do
it we can use IFRExtractor RS.
Funnily enough, it's made by the same people as UEFITool. Thanks guys
for your hard work, otherwise I would have to do it myself ;p.
Now with IFR extracted, we have what we wanted. We can see all the
UEFI settings available, including ""Image Execution Policy"".
Form FormId: 0x2A79, Title: ""Image Execution Policy""
	Text Prompt: ""Internal FV"", Help: """", Text: ""Always Execute""
	OneOf Prompt: ""Option ROM"", Help: ""Image Execution Policy on Security Violation per Device Path"", QuestionFlags: 0x10, QuestionId: 0x1116, VarStoreId: 0x28, VarOffset: 0x4, Flags: 0x10, Size: 8, Min: 0x0, Max: 0x5, Step: 0x0
		Default DefaultId: 0x0 Value: 0
		OneOfOption Option: ""Always Execute"" Value: 0
		OneOfOption Option: ""Always Deny"" Value: 1
		OneOfOption Option: ""Allow Execute"" Value: 2
		OneOfOption Option: ""Defer Execute"" Value: 3
		OneOfOption Option: ""Deny Execute"" Value: 4
		OneOfOption Option: ""Query User"" Value: 5
	End
	OneOf Prompt: ""Removable Media"", Help: ""Image Execution Policy on Security Violation per Device Path"", QuestionFlags: 0x10, QuestionId: 0x1117, VarStoreId: 0x28, VarOffset: 0x5, Flags: 0x10, Size: 8, Min: 0x0, Max: 0x5, Step: 0x0
		Default DefaultId: 0x0 Value: 0
		OneOfOption Option: ""Always Execute"" Value: 0
		OneOfOption Option: ""Always Deny"" Value: 1
		OneOfOption Option: ""Allow Execute"" Value: 2
		OneOfOption Option: ""Defer Execute"" Value: 3
		OneOfOption Option: ""Deny Execute"" Value: 4
		OneOfOption Option: ""Query User"" Value: 5
	End
	OneOf Prompt: ""Fixed Media"", Help: ""Image Execution Policy on Security Violation per Device Path"", QuestionFlags: 0x10, QuestionId: 0x1118, VarStoreId: 0x28, VarOffset: 0x6, Flags: 0x10, Size: 8, Min: 0x0, Max: 0x5, Step: 0x0
		Default DefaultId: 0x0 Value: 0
		OneOfOption Option: ""Always Execute"" Value: 0
		OneOfOption Option: ""Always Deny"" Value: 1
		OneOfOption Option: ""Allow Execute"" Value: 2
		OneOfOption Option: ""Defer Execute"" Value: 3
		OneOfOption Option: ""Deny Execute"" Value: 4
		OneOfOption Option: ""Query User"" Value: 5
	End
End
I have checked if other vendors (ASRock, ASUS, Biostar, EVGA, Gigabyte
and NZXT) have the same thing and I wasn't able to find anything like
that in their IFR. Also MSI's laptops are not affected by this issue.
I'm gonna assume that they figured that Microsoft wouldn't approve it
and/or that they had less tickets from people about Secure Boot related
issues for their laptops.
Now, doing this manually would be kinda annoying, so I made a small
little shell script which checks if ""Image Execution Policy"" menu is
available and if any of the three options are set to ""Always Execute"".
#!/bin/sh

if [ ! -d ""$1"" ]; then
	[ ! -f ""$1.zip"" ] && curl ""https://download.msi.com/bos_exe/mb/$1.zip"" -O -#
	bsdtar xf ""$1.zip""
fi

cd ""$1"" || exit
UEFIExtract ./*MS.* unpack 1>/dev/null
ifrextractor ./*.dump/Section_PE32_image_899407D7-99FE-43D8-9A21-79EC328CAC21_Setup_body.bin 1>/dev/null
output=""$(grep -A1 -E 'OneOf Prompt: ""(Option ROM|Removable Media|Fixed Media)"", Help: ""Image Execution Policy' ./*.dump/Section_PE32_image_899407D7-99FE-43D8-9A21-79EC328CAC21_Setup_body.bin.*ifr.txt)""
clear

if echo ""$output"" | grep -q ""DefaultId: 0x0""; then
	printf ""\033[1;31m%s: Bad\033[0m\n"" ""$1""
else
	printf ""\033[1;32m%s: Good\033[0m\n"" ""$1""
fi
Now this is where the fun part ends. Now I had to check firmware for
MSI's somewhat recent boards.
While we can get most of the firmware just by going to motherboard's
support page, MSI usually only lists stable firmware and the newest
beta. The problem is that I need to figure out the earliest affected
version of the firmware for each board, which means that I have to guess
what the betas were called (if they even existed). At least MSI doesn't
remove most of its beta firmware from their servers, so they are still
accessible if you know the link.
For some AMD boards, I have found a list of beta firmware on some German
forum. Thankfully, I didn't have to read any German, because contrary to
the popular belief, I don't know German or Russian, I'm Polish.
This… took forever. I checked every motherboard for:

AMD: TRX40, X399, X670, X570, X470, X370, B650, B550, B450, B350, A520, A320
Intel: X299, Z790, Z690, Z590, Z490, Z390, Z370, B760, B660, B560, B460, B360, H670, H510, H410, H370, H310

It's… a lot of motherboards. For a full list of affected motherboards
and their firmware versions, visit
sbctl#181.
And now it's time for some ""fun"" statistic:
# The amount of times I ran the script
$ history 0 | grep ""  msi "" | wc -l
1989
According to Wikipedia in 1989:

The first commercial Internet service providers surfaced in this year,
as well as the first written proposal for the World Wide Web and New
Zealand, Japan and Australia's first Internet connections.

Well, that was a mistake.
You could ask me, why didn't I automate it? The reason is… well… some of
it is not really easy to as some beta names have arbitrary suffixes
which was faster for me to guess than having a script bruteforce its way
in. Also some boards weren't listed on their motherboard list page.
Now, after doing all the work for MSI, I think I should bill them, that
or they should give me a lifetime supply of their motherboards.
If you are curious, yes, I have tried contacting MSI about this issue,
but they ignored my emails and other forms of communication I have
tried.

Conclusion

Don't trust that whatever security features you enabled are working,
TEST THEM! Somehow I was the first person to document this, even though
it has been first introduced somewhere in 2021 Q3.

Quiz time!

What's the difference between these 3 boards:

MSI B360 GAMING ARCTIC
MSI B360 GAMING PLUS
MSI B360-A PRO

Heatsink and PCB colours! They are the same board and share the same
firmware! But hey, the red and white one is only for gamers but black is
only suitable for ""professionals"".

"
https://news.ycombinator.com/rss,Programmer salaries in the age of LLMs,https://milkyeggs.com/?p=303,Comments,"



Milky Eggs  » Blog Archive   » Programmer salaries in the age of LLMs










































 













Milky Eggs






Home
About
All Posts
Links




Programmer salaries in the age of LLMs

What happens to the distribution of programmer salaries in the age of LLMs? I argue they will separate bimodally, much like what happened to lawyers’ salaries in the 1990s due to the rise of the Internet.
Dan Luu has previously written about lawyers’ salaries. I will paste some graphs from his article. First, lawyers in 1991:


Next, lawyers in 2000:


What happened? Well, if you’re an elite lawyer in the modern age, you probably command a large team of lower-tier lawyers, cheap paralegals, and so on, who are very good at using the Internet to look up case law and draft your opinions and so on. In contrast, if you’re an elite lawyer in the year 1990, it’s much more valuable for you specifically to have an encyclopedic knowledge of all the relevant case law, and it’s much harder for you to delegate modular pieces of work off to less competent people.
Indeed Cowen makes a similar point about lawyers in Who gains and loses from the new AI? where he writes:

The returns to factual knowledge are falling, continuing a trend that started with databases, search engines and Wikipedia. It is no longer so profitable to be a lawyer who knows a large amount of accumulated case law. Instead, the skills of synthesis and persuasion are more critical for success.

I claim that the trend which AI/ML continues for lawyers is one that it starts for programmers. Just like how a partner at Cravath likely sketches an outline of how they want to approach a particular case and swarms of largely replaceable lawyers fill in the details, we are perhaps converging to a future where a FAANG L7 can just sketch out architectural details and the programmer equivalent of paralegals will simply query the latest LLM and clean up the output. Note that querying LLMs and making the outputted code conform to specifications is probably a lot easier than writing the code yourself ー and other LLMs can also help you fix up the code and integrate the different modules together!
More generally, the farther such technologies advance, the more existing technical professions will undergo a sort of “paralegalization” and bifurcation of the talent distribution.
Somewhat-above-average programmers (including myself) had a decent run of it, but it may be over for us before too long.

January 16th, 2023  | Posted in Technology



Leave a Reply


Name (required)

Mail (will not be published) (required)

Website





Δ












			 Subscribe to Milky Eggs via RSS here.
		




"
https://news.ycombinator.com/rss,"Show HN: Vento, a screen recorder that lets you rewind and record over mistakes",https://vento.so,Comments,"ventoNew RecordingLog inStress-free Screen Recording Constantly restarting your screen recordings? With Vento, just pause, rewind, and carry on instead - keeping calm helps too :)Come back on a desktop computer to try us out!Install Chrome ExtensionStart Recording“To record great videos, one must first rewind.” - Ghandi, probably.Video Not Availablevento© 2023 Vento. All rights reserved.Say hello! We don’t bite. Well, maybe one of us does.hello@vento.so"
https://news.ycombinator.com/rss,Legit Startups – AI Generated Startup Websites,https://legit-startups.com/,Comments,"

503 Service Unavailable

Service Unavailable
The server is temporarily unable to service your
request due to maintenance downtime or capacity
problems. Please try again later.

"
https://news.ycombinator.com/rss,Scaling Bevy Game Engine Development,https://bevyengine.org/news/scaling-bevy-development/,Comments,"



    Scaling Bevy Development
  
Posted on January 14, 2023 by Carter Anderson
    
    
      (
      
      
@cart


@cart_cart



cartdev

      
      )
    




The Bevy community has grown a lot over the past couple of years. We've had over 3,642 pull requests, 599 contributors, 357,917 downloads, and 21,200 github stars. Together we've built the most popular, most used Rust game engine on the market and the second most popular game engine on GitHub. Up until now, in the interest of maintaining a consistent vision and quality bar, I've kept our decision making process as small and centralized as possible. And I scaled out our leadership roles as slowly as possible. I believe this was the right call for Bevy's early days, but we are overdue for changes that bias toward more trust and development agility.
The Current State Of Things
#

I have been slowly delegating responsibility over time, and each time I placed trust in someone it yielded massive benefits to both the project and my personal well being. We now have 3 more fantastic Maintainers: Alice (@alice-i-cecile), François (@mockersf), and Rob (@superdump). And their scope has grown over time.
But even today, I must personally approve every ""controversial"" change to Bevy, where ""controversial"" is basically anything that meaningfully expands our features, user experience, or scope. With the volume of controversial changes we're seeing now, and the number of things I would still like to personally design and build, this is untenable. I have long since reached the limits of my bandwidth and the community has felt those limits for long enough.
When Bevy was younger and the community was newer, this level of conservativeness made sense. I knew what I wanted to build, how the pieces fit together, and what my capabilities were. What I didn't know as well was what everyone else wanted to build, what their capabilities were, and how that all fit into my personal vision.
I can now happily report that the situation has changed. We have proven technical experts in a variety of areas. They regularly design and build huge pieces of Bevy in a way that aligns with our collective vision for the future. Sometimes we have different ideas about what the future should look like, but we almost always reach consensus eventually. They have done outstanding technical work, built trust, and made Bevy amazing. They deserve a seat at the table.
With that preamble out of the way, I am excited to announce two major changes to the Bevy Organization.
A New Role: Subject Matter Expert
#

Subject Matter Experts (SME for short) are Bevy developers that have consistently demonstrated technical expertise and synchronized vision within a given ""subject area"" (ex: Rendering, ECS, Reflection, Animation). They must have contributed and reviewed significant pieces of Bevy within their area. These people have the ability to vote on controversial pull requests in their subject area (both code changes and RFCs). SMEs are also great people to reach out to if you have questions about a given subject area in Bevy.
If two SMEs within a given subject area approve a ""controversial"" PR, a maintainer can now merge it. We are intentionally keeping the number of SMEs within a subject area small to make establishing consensus and enforcing a consistent vision easier. For now, 2 SMEs is the bare minimum to allow voting to occur, 3 is the ""sweet spot"", and 4 will be allowed under some circumstances, but is the ""upper limit"".
As Project Lead, I can still merge controversial PRs. Consider 2 SME approvals as equivalent to a Project Lead approval. As a last line of defense for cohesion, the Project Lead maintains final say on changes. While I will by default defer to the SMEs, if they approve a PR that I firmly believe is the wrong direction for Bevy, I will still block or revert the change. Preserving consistent vision and quality is critically important to me. But I intend to bias toward trust and consensus as much as possible.
It is the job of SMEs to strive for consensus amongst themselves, the wider Bevy community, and Project Leads. They can merge controversial changes without me, but they still must do their best to anticipate my reaction to those changes (and discuss the changes with me ahead of time when that feels relevant). Likewise, I will try to establish consensus with SMEs and the wider community before making changes myself.
We are rolling out SMEs slowly so we can tweak the process over time and avoid changing too much too quickly. We've largely started with subject areas that have the most activity and the clearest subject matter experts. Join me in welcoming our initial set of SMEs!

Rendering: @superdump (Rob Swain), @robtfm (Rob Macdonald)
ECS: @BoxyUwU (Boxy), @james7132 (James Liu), @maniwani (Joy)
Reflection: @MrGVSV (Gino Valente), @jakobhellermann (Jakob Hellermann)
Animation: @james7132 (James Liu), @mockersf (François Mockers)

Hopefully there aren't too many surprises here. These people have been building fantastic things in their areas for a long time now.
We will be rolling out more subject areas (and the SMEs inside them) as SMEs prove themselves within the Bevy project and express interest. New areas are largely defined by the experts doing work inside them. We expect areas like UI, Editor, and Audio to be populated in reasonably short order.
We have also left spots open in each of the subject areas above. If you believe you meet our SME criteria for any current or proposed subject area and have interest in the role, don't hesitate to reach out to myself or any of the other maintainers. We will consider new candidates regularly. Just because you weren't included in this first batch doesn't mean we don't think you would be a good fit!
A New Maintainer
#

We also want to improve our velocity for merging uncontroversial pull requests. And there will be new maintainership load associated with facilitating the SME process. The current maintainers and I have unanimously agreed that it is time to bring on one more maintainer.
Join me in welcoming James Liu (@james7132) as our latest maintainer! James has proven themselves to be a technical expert across many of Bevy's systems (especially ECS, animation, parallelization / task scheduling, and optimization). You may have noticed that they also have the SME role for ECS and Animation. They have contributed huge volumes of code changes, provided solid reviews, are easy to work with, and have a vision for Bevy's future that aligns with ours.
A New Bevy Organization Document
#

We have a new Bevy Organization Document, which describes how the Bevy Organization will work going forward. It outlines the functionality of each role, as well as the expectations we have for them. The previously existing roles (Project Lead, Maintainer) still work the same way, but their definition and scope have been made much clearer.
The biggest changes to the organization are the new SME role and an initial description of ""role rotation"":
Roles like Project Lead, Maintainer, and SME are intentionally kept in limited supply to ensure a cohesive project vision. However these roles can be taxing, sometimes other aspects of life need to take priority, and qualified motivated people deserve a chance to lead. To resolve these issues, we plan on building in ""role rotation"". What this looks like hasn't yet been determined (as this issue hasn't come up yet and we are still in the process of scaling out our team), but we will try to appropriately balance the needs and desires of both current and future leaders, while also ensuring consistent vision and continuity for Bevy.
The Bevy People Page
#

Bevy is a community-driven project. It makes sense for the people behind Bevy and the roles they fill to be easily discoverable.
To make that happen, François (@mockersf) and I built a new Bevy People page. ""Bevy people"" can opt-in to listing their name and/or pseudonym, their social information (GitHub, Discord, Mastodon, Twitter, personal website, itch.io, etc), a sponsorship link, and a personal bio describing who they are and what they work on.
It also displays the current Bevy Organization roles these people occupy. You could call it an ""org chart"" if you wanted to, but my anti-bureaucracy reflexes prevent me from doing so. The Bevy community will always be as flat and ""people first"" as possible.
It is open to anyone (both Bevy Organization members and the wider Bevy community). If you would like to see yourself on this page, create a pull request here.
This is what it looks like!

Looking Forward
#

The next Bevy release (Bevy 0.10) is roughly a month away and I'm very excited for the changes we have in the pipeline. I can't guarantee all of these will make it in, but they're all shaping up nicely:

The New ""Stageless"" ECS Scheduler: We've fully rebuilt our scheduler to be more flexible. We no longer need ""stages"" to handle ""exclusive system scheduling"". Any system of any type can be ordered relative to any other system (even if they have exclusive access to the ECS World. Many scheduling APIs are now cleaner and easier to use. And we've taken the chance to improve related APIs like States as well.
Depth and Normal Prepass: This will give rendering feature developers access to the depth buffer and normals during the main render phase, which enables a variety of render features and optimizations.
Screen Space Ambient Occlusion: This is a popular, relatively cheap illumination technique that can make scenes look much more natural. It builds on the Depth Prepass work.
Asset Preprocessing: We're reworking our asset system to allow for pre processing assets into more efficient forms, which can make deployed games faster to load, prettier, and faster. This is a full asset system rework that improves a good portion of our asset APIs.
Windows as Entities: Windows are now ECS Entities instead of Resources, which makes them more natural to construct and query, more extensible, and opens the doors to including them in Bevy Scenes.
UI Style Split: Breaks the monolithic UI style type out into smaller pieces, embracing a less centralized and more extensible pattern.

See you in about a month!



"
https://news.ycombinator.com/rss,Blaze: A high-performance C++ math library,https://bitbucket.org/blaze-lib/blaze/src/master/,Comments,"


Bitbucket













































"
https://news.ycombinator.com/rss,Scissors NOT Gate [video],https://www.youtube.com/watch?v=uVS7YGSKmJM,Comments,Scissors NOT gate - YouTubeAboutPressCopyrightContact usCreatorsAdvertiseDevelopersTermsPrivacyPolicy & SafetyHow YouTube worksTest new features© 2023 Google LLC
https://news.ycombinator.com/rss,Intel Core i9-13900T CPU benchmarks show faster than 12900K 125W performance,https://wccftech.com/intel-core-i9-13900t-cpu-benchmarks-show-faster-than-12900k-125w-performance-at-35w/,Comments,"

HardwareReport
Intel Core i9-13900T CPU Benchmarks Show Faster Than 12900K 125W Performance at 35W

Hassan Mujtaba •
Jan 14, 2023 02:44 PM EST

•
Copy Shortlink
























Intel recently introduced brand new 13th Gen T-series chips which feature the Core i9-13900T that operates at a 35W TDP. The new chip has been benchmarked within Geekbench 5 and showcases impressive performance given its limited power budget.
Intel's 13th Gen Core i9-13900T 35W CPU Beats The 125W Core i9-12900K In Geekbench 5 Benchmark
Starting with the specifications, the Intel Core i9-13900T is a variation of the Core i9-13900 series that comes with a limited TDP design. While the standard chips boast 125W TDP in the unlocked and 65W TDP on the Non-K SKUs, the T-series chip is limited to a 35W TDP.  The Unlocked CPU is rated at up to 253W, the Non-K is rated at up to 219W while the T-series chip is rated at up to 106 Watts which is less than half the power budget of its higher-end siblings.
Related StoryHassan MujtabaIntel Core i9-13900KS, World’s First 6 GHz CPU, Now Available For $699 USThe Intel Core i9-13900T retains the same core configuration with 24 cores that are made up of 8 P-Cores and 16 E-Cores with 32 threads, a base clock of 1.10 GHz, a boost of up to 5.30 GHz & 68 MB of cache (L2+L3). The CPU also comes at a slightly lower price point of $549.00 US. Now the CPU is tested within the Geekbench 5 benchmark using an ASUS TUF Gaming B660M-PLUS WIFI board and coupled with 64 GB of DDR5 memory.

The CPU scored 2178 points in the single-core and 17339 points in the multi-core tests. We used the Intel Core i9-12900K for comparison which scores 1901 points in single-core and 17272 points in multi-core tests. This puts the Intel Core i9-13900T up to 15% faster in single-core and slightly faster in multi-threaded tests which is very impressive considering the Core i9-12900K also has a higher 125W base TDP (3.58x higher) and a peak TDP rating of 241W (2.27x higher).

Intel Core i9-13900KS Single-Thread CPU Benchmark (Geekbench 5)

Single-Core


050010001500200025003000




050010001500200025003000





Core i9-13900KS

2.3k


Core i9-13900K

2.2k


Ryzen 9 7900X

2.2k


Ryzen 9 7950X

2.2k


Ryzen 7 7700X

2.2k


Core i9-13900T

2.2k


Ryzen 5 7600X

2.2k


Ryzen 9 7900

2.1k


Core i9-13900

2.1k


Ryzen 7 7700

2.1k


Core i9-12900KS

2.1k


Core i9-13900HX

2k


Ryzen 5 7600

2k


Core i7-13700K

2k


Core i5-13600K

1.9k


Core i9-12900K

1.9k


Core i7-12700K

1.9k


M2 Max

1.9k


M1 Max

1.8k


Core i5-12600K

1.7k


Ryzen 9 5950X

1.7k


Ryzen 7 5800X

1.7k


Ryzen 9 5900X

1.7k


Ryzen 5 5600X

1.6k







Intel Core i9-13900KS Multi-Thread CPU Benchmark (Geekbench 5)

Multi-Core


050001000015000200002500030000




050001000015000200002500030000





Core i9-13900KS

26.8k


Core i9-13900K

24.3k


Ryzen 9 7950X

24.4k


Core i9-13900HX

20.9k


Core i9-13900

20.1k


Core i7-13700K

19.8k


Ryzen 9 7900X

19.3k


Core i9-12900KS

19k


Ryzen 9 7900

18.6k


Core i9-13900T

17.3k


Core i9-12900K

17.3k


Ryzen 9 5950X

16.5k


Core i5-13600K

16.1k


M2 Max

14.6k


Core i7-12700K

14.1k


Ryzen 7 7700X

14.1k


Ryzen 9 5900X

14k


Ryzen 7 7700

12.7k


M1 Max

12.3k


Core i5-12600K

11.6k


Ryzen 5 7600X

11.4k


Ryzen 5 7600

11.3k


Ryzen 7 5800X

10.3k


Ryzen 5 5600X

8.2k






This goes off to show the immense efficiency that Intel's 10nm ESF process node and the new hybrid architecture packs and we will also get to see some similar results with the mobility lineup, especially the 13th Gen HX parts which are going to ship in enthusiast-grade gaming laptops in the coming months. AMD also introduced its brand new 65W Ryzen 7000 Non-X CPUs which have been showcasing some impressive efficiency feats on their own with the Zen 4 core architecture.
News Source: Benchleaks
				
				Share this story
				 Facebook
 Twitter






Deal of the Day











Further Reading




 AMD Ryzen 9 7950X3D CPU Shown To Beat Intel Core i9-13900K In Games With Up To 24% Lead


 Intel Core i9-13980HX CPU Powered MSI Raider GE78HX Laptop Matches High-End Desktop CPUs In Performance


 Intel Core i9-13980HX Flagship Raptor Lake-HX CPU Spotted In ASUS’s Next-Gen ROG STRIX Laptop


 It’s Over 9000! Intel Core i9-13900KS Becomes The First CPU To Achieve 9 GHz Frequency World Record













Comments




Please enable JavaScript to view the comments.










Trending Stories


NASA Captures Star Eaten By Black Hole 300 Million Light Years Away



				109 Active Readers



SpaceX’s Rockets Split Up In Mid Air For Rare & Stunning Views At 5,000 Km/h+



				71 Active Readers



PlayStation 5 Vertical Orientation Issue Clarified by Technician; Issue Happens on “Unopened” Consoles



				40 Active Readers



ARK Survival Evolved Unreal Engine 5 Remaster Teased By Studio Wildcard; UE5 Fan Imagining Released



				22 Active Readers



Intel Core i9-13900T CPU Benchmarks Show Faster Than 12900K 125W Performance at 35W



				16 Active Readers








Popular Discussions


AMD Radeon RX 7900 XTX Failure Rates Reportedly At 11%, RMA’s Piling Up But Users Not Receiving Cards



				3100 Comments



AMD Radeon RX 6000 GPUs Mysteriously Start Dying, German Repair Shop Receives 48 Cards With Cracked Chips



				3019 Comments



Intel Lunar Lake To Feature A Brand New CPU Architecture Built From The Ground-Up, Perf/Watt Focused at Mobile



				2757 Comments



AMD To Give The Love of 3D V-Cache This Valentines With Its Ryzen 7000 X3D CPUs Launch



				2021 Comments



Intel Arc A770 Performs Above AMD & NVIDIA In DirectStorage 1.1 Performance Benchmark



				1802 Comments








	 







"
https://news.ycombinator.com/rss,Show HN: Sketch – AI code-writing assistant that understands data content,https://github.com/approximatelabs/sketch,Comments,"








approximatelabs

/

sketch

Public




 

Notifications



 

Fork
    13




 


          Star
 393
  









        AI code-writing assistant that understands data content
      





393
          stars
 



13
          forks
 



 


          Star

  





 

Notifications












Code







Issues
0






Pull requests
0






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







approximatelabs/sketch









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





6
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




bluecoconut

update readme wording




        …
      




        9d567ec
      

Jan 16, 2023





update readme wording


9d567ec



Git stats







133

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github/workflows



remove python 3.7, add tests, remove uneeded code



Dec 15, 2022









sketch



some bug fix and copy addition



Jan 16, 2023









tests



remove python 3.7, add tests, remove uneeded code



Dec 15, 2022









.gitignore



update with edit and work on text2sql



Oct 13, 2022









README.md



update readme wording



Jan 16, 2023









dev-requirements.txt



starting to rebuild sketch



Dec 15, 2022









pyproject.toml



rename to pandas extension, add missing requirement, use base64 encoding



Jan 11, 2023









setup.py



starting to rebuild sketch



Dec 15, 2022




    View code
 


















sketch
Demo
How to use
.sketch.ask
.sketch.howto
.sketch.apply
Sketch currently uses prompts.approx.dev to help run with minimal setup
How it works





README.md




sketch
Sketch is an AI code-writing assistant for pandas users that understands the context of your data, greatly improving the relevance of suggestions. Sketch is usable in seconds and doesn't require adding a plugin to your IDE.
pip install sketch
Demo
Here we follow a ""standard"" (hypothetical) data-analysis workflow, showing a Natural Language interace that successfully navigates many tasks in the data stack landscape.

Data Catalogging:

General tagging (eg. PII identification)
Metadata generation (names and descriptions)


Data Engineering:

Data cleaning and masking (compliance)
Derived feature creation and extraction


Data Analysis:

Data questions
Data visualization








sketch-demo.mp4





Try it out in colab: 
How to use
It's as simple as importing sketch, and then using the .sketch extension on any pandas dataframe.
import sketch
Now, any pandas dataframe you have will have an extension registered to it. Access this new extension with your dataframes name .sketch
.sketch.ask
Ask is a basic question-answer system on sketch, this will return an answer in text that is based off of the summary statistics and description of the data.
Use ask to get an understanding of the data, get better column names, ask hypotheticals (how would I go about doing X with this data), and more.
df.sketch.ask(""Which columns are integer type?"")
.sketch.howto
Howto is the basic ""code-writing"" prompt in sketch. This will return a code-block you should be able to copy paste and use as a starting point (or possibly ending!) for any question you have to ask of the data. Ask this how to clean the data, normalize, create new features, plot, and even build models!
df.sketch.howto(""Plot the sales versus time"")
.sketch.apply
apply is a more advanced prompt that is more useful for data generation. Use it to parse fields, generate new features, and more. This is built directly on lambdaprompt. In order to use this, you will need to set up a free account with OpenAI, and set an environment variable with your API key. OPENAI_API_KEY=YOUR_API_KEY
df['review_keywords'] = df.sketch.apply(""Keywords for the review [{{ review_text }}] of product [{{ product_name }}] (comma separated):"")
df['capitol'] = pd.DataFrame({'State': ['Colorado', 'Kansas', 'California', 'New York']}).sketch.apply(""What is the capitol of [{{ State }}]?"")
Sketch currently uses prompts.approx.dev to help run with minimal setup
In the future, we plan to update the prompts at this endpoint with our own custom foundation model, built to answer questions more accurately than GPT-3 can with its minimal data context.
You can also directly call OpenAI directly (and not use our endpoint) by using your own API key. To do this, set 2 environment variables.
(1) SKETCH_USE_REMOTE_LAMBDAPROMPT=False
(2) OPENAI_API_KEY=YOUR_API_KEY
How it works
Sketch uses efficient approximation algorithms (data sketches) to quickly summarize your data, and feed that information into language models. Right now it does this by summarizing the columns and writing these summary statistics as additional context to be used by the code-writing prompt. In the future we hope to feed these sketches directly into custom made ""data + language"" foundation models to get more accurate results.









About

      AI code-writing assistant that understands data content
    
Topics



  python


  data-science


  data


  ai


  tabular-data


  pandas


  df


  sketches


  dataframe


  copilot


  codex


  ds


  datasketches


  gpt3


  lambdaprompt


  datasketch



Resources





      Readme
 


Stars





393
    stars

Watchers





4
    watching

Forks





13
    forks







    Releases





6
tags







    Packages 0


        No packages published 







        Used by 22
 




























            + 14
          







    Contributors 2








bluecoconut
Justin Waugh

 






jmbiven
Mike Biven

 





Languages










Python
100.0%











"
https://news.ycombinator.com/rss,Boris Yeltsin's visit to a suburban Houston supermarket in 1989,http://beelineblogger.blogspot.com/2016/01/how-supermarket-visit-brought-down.html,Comments,"


















BeeLine: How A Supermarket Visit Brought Down The Soviet Union








































































BeeLine




The Shortest Route To What You Need To Know




















Scott Beeken





BeeLine



View my complete profile








































































Tuesday, January 5, 2016








How A Supermarket Visit Brought Down The Soviet Union





Many point to the fact that the Soviet Union collapse occurred as the Soviets were baited into trying to compete with the defense build-up instituted by Ronald Reagan.

The Soviets just did not have the financial resources to match the United States in defense spending while also tending to the needs of its citizenry. The Soviets spent money on guns rather than butter. Something had to give and the Soviet people were the ones that suffered.

However, a little known visit to a suburban Houston supermarket in 1989 by Boris Yeltsin appears to have been the catalyst that ended up bringing down the Soviet Union.

Yeltsin visited the Johnson Space Center in Houston in September, 1989 to tour mission control and to view a model of the planned International Space Station.

After visiting the Space Center, Yeltsin made an unplanned stop at a local Randall's grocery store that was close by before heading to the airport.

That visit changed the course of history.

At the time, Yeltsin was a newly elected member of the Soviet Parliament and the Supreme Soviet and had been a key ally of the General Secretary of the Communist Party Mikhail Gorbachev, who was initiating reforms but the pace of which was too slow for Yeltsin.

Houston Chronicle reporter Stephanie Asin was with Yeltsin on the visit to the grocery store that day.


Yeltsin, then 58, “roamed the aisles of Randall’s nodding his head in amazement,” wrote Asin. He told his fellow Russians in his entourage that if their people, who often must wait in line for most goods, saw the conditions of U.S. supermarkets, “there would be a revolution.”

“Even the Politburo doesn’t have this choice. Not even Mr. Gorbachev,” he said.

The fact that stores like these were on nearly every street corner in America amazed him. They even offered free cheese samples. According to Asin, Yeltsin didn’t leave empty-handed, as he was given a small bag of goodies to enjoy on his trip.

This is a picture of Yeltsin touring the grocery store.



Credit: Houston Chronicle


This Houston Chronicle story from 2014 fills in the rest of the story.


About a year after the Russian leader left office, a Yeltsin biographer later wrote that on the plane ride to Yeltsin’s next destination, Miami, he was despondent. He couldn’t stop thinking about the plentiful food at the grocery store and what his countrymen had to subsist on in Russia.

In Yeltsin’s own autobiography, he wrote about the experience at Randall’s, which shattered his view of communism, according to pundits. Two years later, he left the Communist Party and began making reforms to turn the economic tide in Russia. 

“When I saw those shelves crammed with hundreds, thousands of cans, cartons and goods of every possible sort, for the first time I felt quite frankly sick with despair for the Soviet people,” Yeltsin wrote. “That such a potentially super-rich country as ours has been brought to a state of such poverty! It is terrible to think of it.”

To give you some perspective on what was available in the Soviet Union at that time, here is a picture of a Russian store from that era.




Credit: Gennady Galperin/Reuters


An aide to Yeltsin later reported that in that visit to the grocery store in Houston “the last vestige of Bolshevism collapsed” inside his boss.

Two years later Yeltsin was elected to the newly created office of President of the Russian Federation after the collapse of the Soviet Union with Gorbachev.

Yeltsin immediately began dismantling the socialist economic system and introducing capitalism to the Russians. In the process he attempted to convert the world's largest command economy into a free-market one. 

The results of that transition were rocky in large part to cronyism in the break-up of many of the large state-owned businesses. In the process, many Russian oligarchs were created and Yeltsin eventually resigned his office in 1999 haunted by charges of corruption and incompetence.

His successor?

Vladimir Putin.

The falling price of oil has put a similar squeeze on Putin and the Russians today. Putin has been popular with the Russian people based on his macho style and nationalistic bombast. However, potential trouble lurks for Putin because of the Russian economy.

The Russian consumer is being squeezed with annual inflation of almost 20% and the average Russian spends about 50% of their income on food.

By comparison, the average American spends only 8% of income on groceries.

Will groceries once again determine the future of Russia?






Posted by



BeeLine




at

8:21 PM
















Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest














42 comments:




andreSeptember 14, 2017 at 4:52 AMobat viagraviagra asliReplyDeleteRepliesUnknownOctober 29, 2021 at 10:14 AMGreat Article Artificial Intelligence Projects Project Center in Chennai JavaScript Training in Chennai JavaScript Training in Chennai Project Centers in Chennai DeleteRepliesReplyReplychegekhanMarch 28, 2018 at 8:11 AMUseful Information, your blog is sharing unique information....Thanks for sharing!!! buy bakery products online south-cbuy branded food online in panganiReplyDeleteRepliesReplyUnknownSeptember 27, 2018 at 7:24 AMThank you for your post. This is excellent information. It is amazing and wonderful to visit your site.buy bakery products online south-c ReplyDeleteRepliesReplyYK AgencyDecember 25, 2018 at 11:30 AMSupermarket in Dubai Great article. Cool.ReplyDeleteRepliesReplyLuck CityFebruary 11, 2019 at 3:50 AMWithin this webpage, you'll see the page, you need to understand this data. https://digitalglobal.comReplyDeleteRepliesReplyjames brownNovember 22, 2019 at 6:45 AMAwesome blog. I enjoyed reading your articles. This is truly a read for me. I have bookmarked it and I am looking forward to reading new articles. Keep up the good work!Kroger customer surveyReplyDeleteRepliesReplyKroger experienceDecember 10, 2019 at 8:05 AMPlease share more like that.Kroger experienceReplyDeleteRepliesReplyAnonymousDecember 30, 2019 at 7:47 PMGreat article! Yeltsin revealed as a realist! I never knew this ...ReplyDeleteRepliesReplyDavid Grant Stewart, Sr., EgyptologistJanuary 1, 2020 at 3:17 PMWho paid you to write this drivel? You are either incredibly gullible or on the take from the USSR propaganda machine. You look and say the Soviet civilian economy is bad. There is no civilian economy in the USSR. The country does not have a war machine. The country is a war machine.ReplyDeleteRepliesReplyNeha UppalJanuary 9, 2020 at 3:33 AMThanks for sharing such beautiful information with us. I hope you will share some more information about best grocery shopping app. Please keep sharing.ReplyDeleteRepliesReplyjames brownFebruary 13, 2020 at 1:15 PMHey There. I found your blog using This is a very well written article. I’ll be sure to bookmark it and come back to read more of your useful info. Thanks for the post. I’ll definitely return.https://krogerexperiencee.com/ReplyDeleteRepliesReplyjames brownFebruary 18, 2020 at 7:24 AMGreat post I would like to thank you for the efforts you have made in writing this interesting and knowledgeable article.https://tellthebelll.usReplyDeleteRepliesReplysurvey monkey usaFebruary 19, 2020 at 7:45 AMGreat things you’ve always with us. Just keep writing this kind of posts.The time which was wasted in traveling for tuition now it can be used for studies.Thankssurvey monkey usaReplyDeleteRepliesReplydanielwilsonnFebruary 19, 2020 at 3:43 PM Thank you again for all the knowledge u distribute,Good post. I was very interested in the article, it's quite inspiring I should admit. I like visiting you site since I always come across interesting articles like this one.Great Job, I greatly appreciate that.Do Keep sharing! Regards,https://krogerfeeedback.us/ReplyDeleteRepliesReplydahliaApril 23, 2020 at 3:21 AM Thanks for sharing this information. I really like your post very much. You have really shared an informative and interesting post with people  TellTheBell ReplyDeleteRepliesReplypatronsurveysApril 24, 2020 at 8:23 AMAfter Kroger and Walmart, I prefer to go to Tesco supermarket. Do you know what? there is quality in products with an extraordinary service tescoviews com offers a platform to complete the Tesco customer satisfaction survey to win £1000 Gift Card & 25 Club Points. Survey site is giving a lot of store surveys at one place to complete.ReplyDeleteRepliesReplyjames brownJuly 14, 2020 at 3:24 AMThis comment has been removed by the author.ReplyDeleteRepliesReplyjames brownJuly 14, 2020 at 3:25 AMIts a great pleasure reading your post.Its full of information I am looking for and I love to post a comment that ""The content of your post is awesome"" Great work.https://krogerexperiencee.com/greatpeople-me-kroger-employee-login-portal/ReplyDeleteRepliesReplyNFL FanSeptember 20, 2020 at 4:46 PMThe official source for NFL news, video highlights, fantasy football, game-day coverage, schedules, stats, scores and more. Ravens FootballReplyDeleteRepliesReplytellthebellSeptember 22, 2020 at 11:42 AM Excellent website you have  so much cool information!..tellthebellReplyDeleteRepliesReplydgcustomerfirstJune 25, 2021 at 6:49 AMTo take an interest in the Dollar general super market survey, it is important to arrange a few things at one of its Branches initially. Visit the authority survey site of Dollar General survey at Dgcustomerfirst.Com and Win A $100 gift voucher. Then, at that point, you need to save the receipt of the store. Then, at that point, go to the authority survey site of dollar general. The Dg survey is accessible in both English or Spanish.ReplyDeleteRepliesReplymybkexperienceAugust 3, 2021 at 2:30 AMDollar general survey is an online platform that collects customers' most recent shopping experiences and overall customer satisfaction. Participate in the survey and be the lucky person to get enlisted in dg customer first winners. ReplyDeleteRepliesReplyarnavharperAugust 30, 2021 at 12:33 PMRemote for Fire TV is designed specifically to control Fire TV, Fire TV Cube and Fire TV Stick. Just connect mobile device and a TV or media player to the Firestick Remote.ReplyDeleteRepliesReplySEOOctober 12, 2021 at 3:37 AMdelta international recruitment agency in pakistanReplyDeleteRepliesReplyHealthandBeautyTipsNovember 6, 2021 at 5:24 PMEmployees can Perform Kroger E-Schedule Login at the Feed Kroger Login Portal once their Schedule Credentials are verified. If you are unable to sign in to Kroger Login then you need to contact the branch manager.Feed KrogerReplyDeleteRepliesReplyUnknownJanuary 27, 2022 at 5:48 AMThe NASA dark Brant IX sounding rocket conveyed the payload to an apogee of 177 miles prior to plunging by parachute and arriving at White Sands. Dgcustomerfirst.com Survey ReplyDeleteRepliesReplydgcustomerfirst.comMay 19, 2022 at 3:35 AMThe dgcustomerfirst.com criticism review permits customers to enter the Dollar General Sweepstakes of Cash $100 in the wake of finishing the overview.dgcustomerfirstwin.shop Survey  ReplyDeleteRepliesReplywww.DGCustomerFirst.comJune 6, 2022 at 10:50 AMDollar general survey is an online platform that collects customers' most recent shopping experiences and overall customer satisfaction. https://idgcustomerfirst.org/ReplyDeleteRepliesReplydgcustomerfirsts.shopJune 10, 2022 at 6:07 AMAlso, you have a superb opportunity to partake in the client criticism overview. DGCustomerFirst 2022 or Dollar General Survey is a study led by Dollar General's authorities for all United States inhabitants.dgcustomerfirsts executed a connected with the WWW review to take an arrangement about dgcustomerfirsts Helpline notwithstanding your support level thereafter visiting service  Click here dgcustomerfirsts ReplyDeleteRepliesReplySteveJune 16, 2022 at 9:58 AMGreat Article, it was very informative. That was such thought-provoking content. I enjoyed reading your content. Every week, I look forward to your column. In my opinion, this one is one of the best articles you have written so far.How to Change Instagram PasswordChange Windows 10 PasswordSubwaylistensHome Depot SurveyDQFanFeedback.com ReplyDeleteRepliesReplyUmairJune 30, 2022 at 6:44 AMirescopk.comReplyDeleteRepliesReplyDgcustomerfirstscom.shopJuly 9, 2022 at 6:51 AMFormerly referred to as J L Turner, Dollar General has numerous subsidiaries viz Dollar General Market, Dollar General Financial, Dollar General Global Sourcing, and lots extra.dgcustomerfirstscom executed a connected with the WWW review to take an arrangement about dgcustomerfirstscom Helpline notwithstanding your support level thereafter visiting service  Click here dgcustomerfirstscom ReplyDeleteRepliesReplyAnonymousJuly 19, 2022 at 1:35 PMLiveTheOrangeLife – Official Portal www.LiveTheOrangeLife.comWalmartOne Login - Walmartone.com Login Guidemyaccountaccess.comonevanillaJCP Associate KioskReplyDeleteRepliesReplySmith AdomJuly 30, 2022 at 12:03 PMTalkToWendys executed a connected with the WWW review to take an arrangement about TalkToWendys Helpline notwithstanding your support level thereafter visiting service  Click here TalkToWendys  It is mandatory to make a purchase at Wendy’s once before being a participant in this survey.ReplyDeleteRepliesReplyInformTarget.comAugust 10, 2022 at 3:38 AMRules are guidelines, and they are set to be accompanied. If you want to participate within the survey effectively, you need to adhere to the rules and policies set apart via the informtarget.Com remarks survey.informtargets executed a connected with the WWW review to take an arrangement about informtargets Helpline notwithstanding your support level thereafter visiting service  Click here informtargets.shop ReplyDeleteRepliesReplyTellBaskinRobbinsAugust 19, 2022 at 5:39 AMThere are some basic rules and requirements of this Baskin Robbins Customer Satisfaction Survey which I even have furnished in this newsletter. tellbaskinrobbins executed a connected with the WWW review to take an arrangement about tellbaskinrobbins Helpline notwithstanding your support level thereafter visiting service  Click here tellbaskinrobbins ReplyDeleteRepliesReplyTalktofoodlionAugust 20, 2022 at 2:37 AMTalktofoodlion The company's full name is general Dollar, and it is offering a $100 incentive to customers who take the time to participate in this little survey. visit here Talktofoodlion ReplyDeleteRepliesReplyFaiz IsrailiAugust 21, 2022 at 5:02 AMThe procedure is requesting information from individuals using a questionnaire, which may be completed offline or online. New technologies, however, are frequently disseminated via digital channels like social media, email, QR codes, or URLs. dgcustomerfirstReplyDeleteRepliesReplyTellBaskinRobbinsSeptember 8, 2022 at 7:11 AMTellBaskinRobbins After responding to the feedback questions, participants are expected to rate their experience shopping at Baskin Robbins.The feedback survey is sponsored by Baskin Robbins in order to better understand its service quality TellBaskinRobbinsReplyDeleteRepliesReplyNikithaOctober 8, 2022 at 1:31 AMmybkexperience customer satisfaction survey which is an online platform to get timely feedback from their customers about the food and services. This can improve their services according to customer’s needs and at the same time, rewards their customer for their time and loyalty towards the restaurant. So you can answer the questions and eat delicious food for free at the same time. Now you might be wondering about how to participate in the survey or what are the requirements and much more.ReplyDeleteRepliesReplySEOOctober 14, 2022 at 5:59 AMtop recruitment agencies in pakistan for saudi arabiaReplyDeleteRepliesReplyAdd commentLoad more...























Newer Post


Older Post

Home




Subscribe to:
Post Comments (Atom)















BeeLine Email Subscription

Get new posts by email:  Subscribe




Follow @BeeLineBlog




Followers











Blog Archive








        ► 
      



2023

(5)





        ► 
      



January

(5)









        ► 
      



2022

(139)





        ► 
      



December

(11)







        ► 
      



November

(10)







        ► 
      



October

(12)







        ► 
      



September

(12)







        ► 
      



August

(13)







        ► 
      



July

(11)







        ► 
      



June

(12)







        ► 
      



May

(12)







        ► 
      



April

(12)







        ► 
      



March

(12)







        ► 
      



February

(10)







        ► 
      



January

(12)









        ► 
      



2021

(131)





        ► 
      



December

(15)







        ► 
      



November

(12)







        ► 
      



October

(9)







        ► 
      



September

(13)







        ► 
      



August

(14)







        ► 
      



July

(11)







        ► 
      



June

(10)







        ► 
      



May

(6)







        ► 
      



April

(10)







        ► 
      



March

(11)







        ► 
      



February

(7)







        ► 
      



January

(13)









        ► 
      



2020

(154)





        ► 
      



December

(11)







        ► 
      



November

(12)







        ► 
      



October

(14)







        ► 
      



September

(11)







        ► 
      



August

(12)







        ► 
      



July

(13)







        ► 
      



June

(14)







        ► 
      



May

(12)







        ► 
      



April

(16)







        ► 
      



March

(16)







        ► 
      



February

(10)







        ► 
      



January

(13)









        ► 
      



2019

(145)





        ► 
      



December

(14)







        ► 
      



November

(11)







        ► 
      



October

(9)







        ► 
      



September

(12)







        ► 
      



August

(13)







        ► 
      



July

(12)







        ► 
      



June

(12)







        ► 
      



May

(14)







        ► 
      



April

(13)







        ► 
      



March

(12)







        ► 
      



February

(10)







        ► 
      



January

(13)









        ► 
      



2018

(139)





        ► 
      



December

(11)







        ► 
      



November

(10)







        ► 
      



October

(10)







        ► 
      



September

(10)







        ► 
      



August

(12)







        ► 
      



July

(13)







        ► 
      



June

(12)







        ► 
      



May

(12)







        ► 
      



April

(13)







        ► 
      



March

(12)







        ► 
      



February

(9)







        ► 
      



January

(15)









        ► 
      



2017

(132)





        ► 
      



December

(9)







        ► 
      



November

(10)







        ► 
      



October

(15)







        ► 
      



September

(9)







        ► 
      



August

(13)







        ► 
      



July

(12)







        ► 
      



June

(9)







        ► 
      



May

(13)







        ► 
      



April

(12)







        ► 
      



March

(10)







        ► 
      



February

(10)







        ► 
      



January

(10)









        ▼ 
      



2016

(119)





        ► 
      



December

(11)







        ► 
      



November

(15)







        ► 
      



October

(15)







        ► 
      



September

(10)







        ► 
      



August

(2)







        ► 
      



July

(9)







        ► 
      



June

(11)







        ► 
      



May

(6)







        ► 
      



April

(9)







        ► 
      



March

(11)







        ► 
      



February

(12)







        ▼ 
      



January

(8)

In the Middle of 5th Avenue
Risky Business
That's for the Birds
An Inconvenient Truth +10
Citizen Cruz
Context on Guns
How A Supermarket Visit Brought Down The Soviet Union
A Humble Servant?










        ► 
      



2015

(71)





        ► 
      



December

(9)







        ► 
      



November

(5)







        ► 
      



October

(2)







        ► 
      



September

(2)







        ► 
      



August

(9)







        ► 
      



July

(7)







        ► 
      



June

(6)







        ► 
      



May

(3)







        ► 
      



April

(10)







        ► 
      



March

(6)







        ► 
      



February

(4)







        ► 
      



January

(8)









        ► 
      



2014

(88)





        ► 
      



December

(5)







        ► 
      



November

(7)







        ► 
      



October

(10)







        ► 
      



September

(9)







        ► 
      



August

(5)







        ► 
      



July

(8)







        ► 
      



June

(7)







        ► 
      



May

(9)







        ► 
      



April

(7)







        ► 
      



March

(6)







        ► 
      



February

(5)







        ► 
      



January

(10)









        ► 
      



2013

(115)





        ► 
      



December

(8)







        ► 
      



November

(5)







        ► 
      



October

(13)







        ► 
      



September

(9)







        ► 
      



August

(7)







        ► 
      



July

(10)







        ► 
      



June

(10)







        ► 
      



May

(9)







        ► 
      



April

(13)







        ► 
      



March

(10)







        ► 
      



February

(7)







        ► 
      



January

(14)









        ► 
      



2012

(139)





        ► 
      



December

(6)







        ► 
      



November

(16)







        ► 
      



October

(16)







        ► 
      



September

(9)







        ► 
      



August

(14)







        ► 
      



July

(13)







        ► 
      



June

(8)







        ► 
      



May

(11)







        ► 
      



April

(9)







        ► 
      



March

(9)







        ► 
      



February

(11)







        ► 
      



January

(17)









        ► 
      



2011

(188)





        ► 
      



December

(12)







        ► 
      



November

(10)







        ► 
      



October

(10)







        ► 
      



September

(10)







        ► 
      



August

(17)







        ► 
      



July

(18)







        ► 
      



June

(11)







        ► 
      



May

(3)







        ► 
      



April

(16)







        ► 
      



March

(19)







        ► 
      



February

(25)







        ► 
      



January

(37)









About Me





BeeLine


Scott Beeken has practiced as an attorney, CPA and has been an officer with two Fortune 500 companies overseeing diverse functions such as Taxation, Employee Benefits, Human Resources, Real Estate Facilities, Risk Management, Corporate Communications, Marketing and Advertising. In addition to writing BeeLine, he is a Keynote Speaker, Author  and Strategic Consultant.

View my complete profile




























Total Pageviews

























Theme images by luoman. Powered by Blogger.
























"
https://news.ycombinator.com/rss,Show HN: Cross-Platform GitHub Action,https://github.com/marketplace/actions/cross-platform-action,Comments,"





Marketplace
Actions
Cross Platform Action






play-circle





GitHub Action
Cross Platform Action




v0.9.0
Latest version







    Use latest version
 









play-circle






Cross Platform Action
Provides cross platform runner


Installation
Copy and paste the following snippet into your .yml file.












- name: Cross Platform Action
  uses: cross-platform-actions/action@v0.9.0



          Learn more about this action in cross-platform-actions/action












Choose a version







v0.9.0

                Cross Platform Action 0.9.0
              

 




v0.8.0

                Cross Platform Action 0.8.0
              

 




v0.7.0

                Cross Platform Action 0.7.0
              

 




v0.6.2

                Cross Platform Action 0.6.2
              

 




v0.6.1

                Cross Platform Action 0.6.1
              

 




v0.6.0

                Cross Platform Action 0.6.0
              

 




v0.5.0

                Cross Platform Action 0.5.0
              

 




v0.4.0

                Cross Platform Action 0.4.0
              

 




v0.3.1

                Cross Platform Action 0.3.1
              

 




v0.3.0

                Cross Platform Action 0.3.0
              

 








Cross-Platform GitHub Action
This project provides a GitHub action for running GitHub Action workflows on
multiple platforms. This includes platforms that GitHub Actions doesn't
currently natively support.
Features
Some of the features that are supported include:

Multiple operating system with one single action
Multiple versions of each operating system
Allows to use default shell or Bash shell
Low boot overhead
Fast execution

Usage
Here's a sample workflow file which will setup a matrix resulting in four jobs.
One which will run on FreeBSD 13.1, one which runs OpenBSD 7.2, one which runs
NetBSD 9.2 and one which runs OpenBSD 7.2 on ARM64.
name: CI

on: [push]

jobs:
  test:
    runs-on: ${{ matrix.os.host }}
    strategy:
      matrix:
        os:
          - name: freebsd
            architecture: x86-64
            version: '13.1'
            host: macos-12

          - name: openbsd
            architecture: x86-64
            version: '7.2'
            host: macos-12

          - name: openbsd
            architecture: arm64
            version: '7.2'
            host: ubuntu-latest

          - name: netbsd
            architecture: x86-64
            version: '9.2'
            host: ubuntu-latest

    steps:
      - uses: actions/checkout@v2

      - name: Test on ${{ matrix.os.name }}
        uses: cross-platform-actions/action@v0.9.0
        env:
          MY_ENV1: MY_ENV1
          MY_ENV2: MY_ENV2
        with:
          environment_variables: MY_ENV1 MY_ENV2
          operating_system: ${{ matrix.os.name }}
          architecture: ${{ matrix.os.architecture }}
          version: ${{ matrix.os.version }}
          shell: bash
          run: |
            uname -a
            echo $SHELL
            pwd
            ls -lah
            whoami
            env | sort
Different platforms need to run on different runners, see the
Runners section below.
Inputs
This section lists the available inputs for the action.



Input
Required
Default Value
Description




run
✓
✗
Runs command-line programs using the operating system's shell. This will be executed inside the virtual machine.


operating_system
✓
✗
The type of operating system to run the job on. See Supported Platforms.


version
✓
✗
The version of the operating system to use. See Supported Platforms.


shell
✗
default
The shell to use to execute the commands. Defaults to the default shell for the given operating system. Allowed values are: default, sh and bash


environment_variables
✗
""""
A list of environment variables to forward to the virtual machine. The list should be separated with spaces.



All inputs are expected to be strings. It's important that especially the
version is explicitly specified as a string, using single or double quotes.
Otherwise YAML might interpet the value as a numeric value instead of a string.
This might lead to some unexpected behavior. If the version is specified as
version: 13.0, YAML will interpet 13.0 as a floating point number, drop the
fraction part (because 13 and 13.0 are the same) and the GitHub action will
only see 13 instead of 13.0. The solution is to explicitly state that a
string is required by using quotes: version: '13.0'.
Supported Platforms
This sections lists the currently supported platforms by operating system. Each
operating system will list which versions are supported.
OpenBSD (openbsd)



Version
x86-64
arm64




7.2
✓
✓


7.1
✓
✓


6.9
✓
✓


6.8
✓
✗



FreeBSD (freebsd)



Version
x86-64




13.1
✓


13.0
✓


12.4
✓


12.2
✓



NetBSD (netbsd)



Version
x86-64




9.2
✓



Runners
This section list the different combinations of platforms and on which runners
they can run.



Runner
OpenBSD
FreeBSD
NetBSD




Linux
✓
✓
✓


macos-10.15, macos-11, macos-12
✓
✓
✗



Under the Hood
GitHub Actions currently only support the following platforms: macOS, Linux and
Windows. To be able to run other platforms, this GitHub action runs the
commands inside a virtual machine (VM). If the host platform is macOS the
hypervisor can take advantage of nested virtualization.
The FreeBSD and OpenBSD VMs run on the xhyve hypervisor (on a macOS
host), while the other platforms run on the QEMU hypervisor (on a Linux
host). xhyve is built on top of Apple's Hypervisor
framework. The Hypervisor framework allows to implement hypervisors with
support for hardware acceleration without the need for kernel extensions. xhyve
is a lightweight hypervisor that boots the guest operating systems quickly and
requires no dependencies outside of what's provided by the system. QEMU is a
more general purpose hypervisor that runs on most host platforms and supports
most guest systems. It's a bit slower than xhyve because it's general purpose
and it cannot use nested virtualization on the Linux hosts provided by GitHub.
The VM images running inside the hypervisor are built using Packer.
It's a tool for automatically creating VM images, installing the guest
operating system and doing any final provisioning.
The GitHub action uses SSH to communicate and execute commands inside the VM.
It uses rsync to share files between the guest VM and the host. xhyve
does not have any native support for sharing files. To authenticate the SSH
connection a unique key pair is used. This pair is generated each time the
action is run. The public key is added to the VM image and the private key is
stored on the host. Since xhyve does not support file sharing, a secondar hard
drive, which is backed by a file, is created. The public key is stored on this
hard drive, which is then mounted by the VM. At boot time, the secondary hard
drive will be identified and the public key will be copied to the appropriate
location.
To reduce the time it takes for the GitHub action to start executing the
commands specified by the user, it aims to boot the guest operating systems as
fast as possible. This is achieved in a couple of ways:


By downloading resources, like the hypervisor and a few other
tools, instead of installing them through a package manager


No compression is used for the resources that are downloaded. The size is
small enough anyway and it's faster to download the uncompressed data than
it is to download compressed data and then uncompress it.


It leverages async/await to perform tasks asynchronously. Like
downloading the VM image and other resources at the same time


It performs as much as possible of the setup ahead of time when the VM image
is provisioned


Local Development
Prerequisites

NodeJS
npm
git

Instructions


Install the above prerequisites


Clone the repository by running:
git clone https://github.com/cross-platform-actions/action



Navigate to the newly cloned repository: cd action


Install the dependencies by running: npm install


Run any of the below npm commands


npm Commands
The following npm commands are available:

build - Build the GitHub action
format - Reformat the code
lint - Lint the code
package - Package the GitHub action for distribution and end to end testing
test - Run unit tests
all - Will run all of the above commands

Running End to End Tests
The end to end tests can be run locally by running it through Act. By
default, resources and VM images will be downloaded from github.com. By running
a local HTTP server it's possible to point the GitHub action to local resources.
Prerequisites

Docker
Act

Instructions


Install the above prerequisites


Copy test/workflows/ci.yml.example to
test/workflows/ci.yml


Make any changes you like to test/workflows/ci.yml, this is file ignored by
Git


Build the GitHub action by running: npm run build


Package the GitHub action by running: npm run package


Run the GitHub action by running: act --privileged -W test/workflows


Providing Resources Locally
The GitHub action includes a development dependency on a HTTP server. The
test/http directory contains a skeleton of a directory structure
which matches the URLs that the GitHub action uses to download resources. All
files within the test/http are ignore by Git.


Add resources as necessary to the test/http directory


In one shell, run the following command to start the HTTP server:
./node_modules/http-server/bin/http-server test/http -a 127.0.0.1

The -a flag configures the HTTP server to only listen for incoming
connections from localhost, no external computers will be able to connect.


In another shell, run the GitHub action by running:
act --privileged -W test/workflows --env CPA_RESOURCE_URL=<url>

Where <url> is the URL inside Docker that points to localhost of the host
machine, for macOS, this is http://host.docker.internal:8080. By default,
the HTTP server is listening on port 8080.






Stars

 


          Star
 47
  





Contributors



 

 


Categories


  Continuous integration


  Testing




Links



cross-platform-actions/action
    



Open issues
        1




Pull requests
      1




Report abuse
 

Cross Platform Action is not certified by GitHub. It is provided by a third-party and is governed by separate terms of service, privacy policy, and support documentation.
    




"
https://news.ycombinator.com/rss,Microsoft returns to the Altair,https://hackaday.com/2023/01/15/microsoft-returns-to-the-altair/,Comments,"


Microsoft Returns To The Altair


                18 Comments            

by:
Al Williams



January 15, 2023















Title:


Copy

Short Link:


Copy






The Altair 8800 arguably launched Microsoft. Now [Dave Glover] from Microsoft offers an emulated and potentially cloud-based Altair emulation with CP/M and Microsoft Basic. You can see a video of the project below. One thing that makes it a bit odd compared to other Altair clones we’ve seen is that the emulator runs in a Docker environment and is fully cloud-enabled. You can interact with it via a PCB front panel, or a terminal running in a web browser.
The core emulator is MIT-licensed and seems like it would run nearly everywhere. We were a little surprised there wasn’t an instance in the Azure cloud that you could spin up to test drive. Surely a few hundred Altairs running at once wouldn’t even make a dent in a modern CPU.

There are plenty of Altair emulators and even replicas with authentic CPUs out there. But we have to admit the Wiki documentation on this one is uncommonly well done. Even if you don’t want to use this emulator, you might find the collection of data about the Altair useful.
Don’t know how to use a computer front panel? Learn on the Altair or a PDP/8, even if you don’t have a real one. For simulated hardware, the project that turns an Arduino Due into an Altair works well. If you just want to play Zork, you can do that in your browser, for sure.





 



















Posted in RetrocomputingTagged altair 8800 


Post navigation

← A Number Maze For Younger HackersA Flex Sensor For A Glove Controller Using An LDR → 






            18 thoughts on “Microsoft Returns To The Altair”        





paulvdh says: 



							January 15, 2023 at 1:24 am						




@05:42 “I don’t have the source code for CP/M, I’m not even sure it exists anymore.
https://hackaday.com/2014/10/06/cpm-source-code-released/
This video is apparently only made as entertainment, just some guys blabbing and not caring about facts, That may be all right for some, it makes me wonder what else is off and  I lost interest.


Report comment 
Reply 





MG says: 



							January 15, 2023 at 1:51 am						




Even just off the top of my head, the fact that it’s MIT-licensed is nothing special; the entire core of MAME and a significant number of machine drivers are all MIT/3-clause BSD, which includes the Intel 8080A core that one would need to spin up an Altair 8800 emulator as well.


Report comment 
Reply 





stappers says: 



							January 15, 2023 at 3:59 am						




Thanks for the click bait warning.


Report comment 
Reply 





me says: 



							January 16, 2023 at 2:33 am						




fortunately i read here, so i re-read witho more attention the initial post, so I avoid to lose some minutes of my life looking at another-not-wanted-at-all-cliclbite video. Thanks yo you, dude!


Report comment 
Reply 







Joshua says: 



							January 15, 2023 at 1:58 pm						




What’s also kind of forgotten:
CP/M was thoroughly understood.
It was cloned and derived a dozen times, if not more.
People who worked with assembly language could literally “read” how CP/M worked and flowed through the system.
My father, among many others, wrote his own floppy controller routines for CP/M.
It’s kind of sad to see how incompetent the matter is being handled these days. Such a blunder. *sigh*
Back in the 70s and 80s, it was just natural to know about the internals of the Z-80 and/or CP/M. 
There were hundreds of books being written about all the deepest mechanisms of them.


Report comment 
Reply 





bruceeifler@gmail.com says: 



							January 15, 2023 at 6:15 pm						




“How to program the z80.  Roger Zaks


Report comment 
Reply 





Mr Name Required says: 



							January 15, 2023 at 11:14 pm						




Rodnay Zaks, with the ‘a’. A well-thumbed book for me.


Report comment 
Reply 











daveboltman says: 



							January 15, 2023 at 4:50 am						




Has Bill Gates really got over himself now?
https://en.wikipedia.org/wiki/An_Open_Letter_to_Hobbyists


Report comment 
Reply 





Ken says: 



							January 15, 2023 at 7:37 am						




You must not have been involved in the hobby “back in the day” – the copying of commercial products like Microsoft BASIC was rampant, almost celebrated in the community.
This was before Microsoft had even started licensing ROM BASIC to the major home/personal computer manufacturers, so stolen paper tapes were a serious hit to MS revenues.


Report comment 
Reply 





stappers says: 



							January 15, 2023 at 7:53 am						




And the problem of lack of wallet-voting still exists.  Might be because of the same word for libre and gratuit.
Where I do agree with https://justforfunnoreally.dev/  I also do agree with young Bill Gates that people should do some kind of payment for what they want.


Report comment 
Reply 







Joshua says: 



							January 15, 2023 at 1:48 pm						




Out of context. 
I’m no expert of American copyright law, but as far as I understand:
Material released before 1977 was considered Public Domain, unless the copyright is/was explicitly expressed.
I assume this covers material/works of engineers, students, universities etc. which didn’t mention the copyright in their papers. 
Go double check yourself, if you wish. I’m lazy right now. 😁


Report comment 
Reply 





The Commenter Formerly Known As Ren says: 



							January 15, 2023 at 4:29 pm						




No, he hasn’t, he is now a James Bond level super villain.


Report comment 
Reply 







Sok Puppette says: 



							January 15, 2023 at 6:39 am						




If it cannot play tunes on an AM radio, it is not a satisfying Altair emulator


Report comment 
Reply 





Ken says: 



							January 15, 2023 at 8:13 am						




Neat project, I too am impressed by the level of documentation. While I’m not too interested in running weather/climate change analysis programs on an emulated 50 year-old system, I realize it was important to inject some cloud activity to jazz up the project.
Interesting to note that you need to install Linux on a windows machine to run the code.
Neat how the sense hat reduces the need for ‘blinkin lights’ to its most basic elements, like the computer prop in the bat cave or any 1960s sci-fi movie/tv show…


Report comment 
Reply 





paulvdh says: 



							January 15, 2023 at 9:25 am						




At 5:42 he says he does not know whether the CP/M source code still exists…
He clearly has not checked Hackaday lately…
https://hackaday.com/2014/10/06/cpm-source-code-released/


Report comment 
Reply 





Joshua says: 



							January 15, 2023 at 1:51 pm						




Not just CP/M, also MP/M, the power user version of CP/M. The real thing, so to say. :)


Report comment 
Reply 







SayWhat? says: 



							January 15, 2023 at 12:16 pm						




I remember hacking “protected” MS Basic programs back in the day. All it took was finding the one byte in memory  that set the code to be not listable LOL.


Report comment 
Reply 





demon256 says: 



							January 15, 2023 at 3:45 pm						




Heh, my first computer ran CP/M.


Report comment 
Reply 



Leave a Reply					Cancel reply










Please be kind and respectful to help make the comments section excellent. (Comment Policy)This site uses Akismet to reduce spam. Learn how your comment data is processed.







Search

Search for:



Never miss a hack
Follow on facebook
Follow on twitter
Follow on youtube
Follow on rss
Contact us
Subscribe










If you missed it







AI-Controlled Twitch V-Tuber Has More Followers Than You


 39 Comments				








Ask Hackaday: What’s Your Worst Repair Win?


 173 Comments				








The Surprisingly Simple Way To Steal Cryptocurrency


 65 Comments				








Excuse Me, Your Tie Is Unzipped


 101 Comments				








All About USB-C: Resistors And Emarkers


 16 Comments				



More from this category





Our Columns







2022 FPV Contest: Congratulations To The Winners!


 2 Comments				








Machining With Electricity Hack Chat


 6 Comments				








Hackaday Links: January 15, 2023


 16 Comments				








Too Many Pixels


 35 Comments				








Hackaday Podcast 201: Faking A Transmission, Making Nuclear Fuel, And A Slidepot With A Twist


 2 Comments				



More from this category

"
https://news.ycombinator.com/rss,The Pretty Good House,https://www.prettygoodhouse.org,Comments,"






Pretty Good House























































the Pretty Good House



the Pretty Good House


Read More


PGH 2.0


Certificates


links


the BS*+Beer Show











the Pretty Good House



the Pretty Good House/
Read More/
PGH 2.0/
Certificates/
links/
the BS*+Beer Show/













the Pretty Good House
A Guide to Creating Better Homes



The Pretty Good House Approach to Building
How to Design and Build a Pretty (darn) Good House or Renovation







the Pretty Good House



the Pretty Good House/
Read More/
PGH 2.0/
Certificates/
links/
the BS*+Beer Show/












Pretty Good House provides a framework and guidelines to focus on the core issues that should be front and center when designing and building a high quality home or renovation.
 
About the name…. In southern Maine, there’s a small group of building professionals that get together monthly to discuss building science.  It sounds boring, I know, but truly it isn’t.  Food is brought in, beer is consumed, a blackboard is present and a topic is brought up for discussion.  (topics  like, high performance windows, venting cathedral ceilings, moisture migration in wall assemblies, slab on grade details, etc.) Since we are all peers, and colleagues (with egos, attitude, and a good sense of humor) the conversation is usually lively, informal and frankly all over the place.  There’s usually a moderator to keep everyone reined in and on topic.  Every once in a while I have the pleasure of being that moderator, but that is only when builder, Dan Kolbert can’t attend.  Dan while on a rant once, expressed frustration with the status quo, and even more frustration with the various rating systems that had him jumping through many hoops to prove that his work was in fact “green” and energy efficient.  As many can attest, these systems can be very costly in time and money, and in the case of many green projects, they aren’t doing anything to advance the project.  Rather they are simply certifying them.  So Dan simply stated, “I just want to build a pretty good house.”- Chris Briley
 



  












We’ve been talking about it for some time, but now the Pretty Good House book is real!It is available directly through the Taunton store, local bookshops, and on Amazon. The link below allows you to order the book.Updates on the book, cover, and release will be all over our social media channels. We do not plan to make an email list with updates at this time.Click HERE to order your copy!Meet the authors this fall:Midwest Building Science Symposium Kansas City - Sept 14th & 15th - Mike & EmilyCommon Ground FairUnity, ME Sept 23-25th - Dan & MikeSoutheast Building Science SymposiumChattanooga, TN - Oct 12th - 13th - Emily The Entre Architect Community Annual MeetingAustin, TX - Nov 1-3rd - Mike & Emily South Bend City Housing SymposiumSouth Bend, IN - Dates TBD (Late Nov / Early Dec) Dan & Chris
 
Guidelines for Designing and Building a Pretty Good HouseBuilding a house or doing a major renovation is a daunting task.  This diagram shows how the PGH pros think about things!  Scroll down a little bit further for explanation of the main points. 















Featured

























Economics






Key to the PGH approach is balancing expenditures and gains. Where other programs use specific energy-use targets or other criteria, and the building code establishes a baseline (“the worst house you can legally build”), a PGH goes above code until it stops making financial sense. read more....
Read More →


























The Team Approach






Choosing the right team members can make a big difference on project going smoothly.read more….
Read More →


























Climate - Where are you?






Where you live plays an important part in the design of a PGH.read more…
Read More →


























Design






Design is much more than floor plans and making pretty things. The whole process of construction is designed as well.read more…
Read More →


























The Building Shell - Envelope basics






Every house needs to keep rain and cold (or hot) air out, and conditioned air inside. In a Pretty Good House it’s easiest and best to identify dedicated control layersread more….
Read More →


























Envelope Details






Basic physics. This is where the rubber meets the road. A PGH builder or architect is also something of a science geek.read more….
Read More →


























Windows and Doors






Even the best windows and doors on the market perform much worse than even an average wall, but they can also allow solar energy into the house, and of course light and views are important. Selection of specific brands and models of windows and doors comes into the conversation very early in the process.read more…
Read More →


























Materials






Local is good. While we can get materials from nearly anywhere on earth, buying locally supports the local economy, typically keeps the carbon footprint smaller, and makes a house a product of its environment instead of a cookie-cutter house that could be anywhere. read more…
Read More →


























Mechanical Systems






Mechanical systems include the equipment needed to heat, cool and control humidity in a house, and also the plumbing system. The primary reason for mechanical systems is occupant comfort. A PGH allows mechanicals to be smaller, less expensive and generally simpler.read more…
Read More →


























Electrical and Lighting






Lighting can be divided into two categories: natural and artificial. Daylighting, using windows and other glazing, reduces the need for artificial lighting during the day. Wiring in a PGH must include a well thought out strategy for sealing all penetrations in the building shell.read more…
Read More →


























Verification






There are many ways to check your results. Third party verification systems such as Energy Star or Passive house may influence bank financing but are not the focus of a Pretty Good House.read more….
Read More →


























Owner / Occupant






Occupants can be creative in the ways they break the rules so it’s important for them to be educated on how to operate their Pretty Good home.read more…
Read More →












If you feel that you have designed and built a Pretty Good House, please feel free to download, fill out and display a Pretty Good Certificate, signed by the Pretty Good House authors—available in two flavors, plain and fancy! Just click on the image and you can download a PDF. 

























Relevant organizations and references the BS* + Beer Show a weekly (for now) Zoom based meeting of building science nerds from around the country - links to local chapters coming soonTo watch previous episodes of The BS* and Beer Show - Find the YouTube Channel HEREGreen Building Advisor - This is where many of us initially heard the term “Pretty Good House”  The original article is HEREBuilding Science Corporation provides training, research and can provide a deep dive into building scienceFine Homebuilding The original magazine that we all know and love has a ton of information on high performance building including some great podcastsJournal of Light Construction has really stepped up to the plate with lots of informationPro Trade Craft - Best practices and great videos. This is a great place for builders to learn.E3 Podcast - Emily Mottram interviews various people with a focus on Energy and EfficiencyGreen Architects’ Lounge - A podcast covering many topics relevant to Green Building, high performance building and Pretty Good House infoBuilding Green - industry watchdog and long time provider of product vetting and technical expertise to the profession.NESEA the Northeast Sustainable Energy Association is a resource for builders and homeowners - they put on symposiums in Boston and NYC, builder training, home tours etc.Maine Indoor Air Quality Council - information, links and training.  The quality of the air inside a Pretty Good House is paramount!
 
Mike Maines and Robert Swinburne presented the Pretty Good House at the 2020 Fine Homebuilding Summit












    



















  





    Contact site administation
  


Terms and ConditionsThe Pretty Good House website copyright 2019 by the Pretty Good House Crew



 


 






 




"
https://news.ycombinator.com/rss,Hubble finds black hole twisting captured star into donut shape,https://phys.org/news/2023-01-hubble-hungry-black-hole-captured.html,Comments,"400 Bad request
Your browser sent an invalid request.
We highly recommend setting a meaningful User-Agent header.

"
https://news.ycombinator.com/rss,Show HN: Otterkit – COBOL compiler for .NET,https://github.com/otterkit/otterkit,Comments,"








otterkit

/

otterkit

Public




 

Notifications



 

Fork
    1




 


          Star
 48
  









        Otterkit COBOL Compiler
      





otterkit.com


License





     Apache-2.0 license
    






48
          stars
 



1
          fork
 



 


          Star

  





 

Notifications












Code







Issues
1






Pull requests
0






Discussions







Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Discussions
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







otterkit/otterkit









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








4
branches





0
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




KTSnowy

[Update]: Added SourceId, LevelStack and FileIndex comments




        …
      




        2365003
      

Jan 16, 2023





[Update]: Added SourceId, LevelStack and FileIndex comments


2365003



Git stats







175

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github/ISSUE_TEMPLATE



[Update]: Update issue templates and security policy notice



Nov 19, 2022









Assets



[Update]: Added dotnet tool and project templates Nuget packages



Dec 24, 2022









OtterkitTemplatePack



[Update]: Update Otterkit.Templates version number



Dec 24, 2022









libotterkit @ a0256a7



[Update/Fix]: Apply Roslyn Analyzers fixes and change suggestions



Jan 4, 2023









src



[Update]: Added SourceId, LevelStack and FileIndex comments



Jan 16, 2023









.gitattributes



Create .gitattributes



Oct 13, 2022









.gitignore



[Update]: Added dotnet tool and project templates Nuget packages



Dec 24, 2022









.gitmodules



[Update]: Added libotterkit as a git submodule



Nov 2, 2022









LICENSE



[Update]: Added year and name to the LICENSE file



Dec 19, 2022









README.md



[Update]: Add project icon to the README header



Jan 11, 2023









SECURITY.md



[Update]: Change ""runtime"" to ""compiler"" in the security policy



Nov 19, 2022









THIRD-PARTY-LICENSES



[Update]: Added libmpdec license notice



Oct 23, 2022




    View code
 















 Otterkit COBOL Compiler
About
Installation
Quick Install
Build from Source
Standard Acknowledgement





README.md




 Otterkit COBOL Compiler
Otterkit is a free and open source compiler for the COBOL Programming Language on the .NET platform.
Warning: The project is currently in pre-release, so not all of the standard has been implemented.
About
COBOL was created in 1959 by the CODASYL Committee (With Rear Admiral Grace Hopper as a technical consultant to the committee), its design follows Grace Hopper's belief that programs should be written in a language that is close to English. It prioritizes readability, reliability, and long-term maintenance. The language has been implemented throughout the decades on many platforms with many dialects, and the Otterkit COBOL compiler is a free and open source implementation of the ISO COBOL 2022 Standard on the .NET platform.
Installation
Quick Install
Otterkit is available to install on the Nuget package manager (.NET 7 is required). To install, type into the command line:
dotnet tool install --global Otterkit --version 1.0.15-alpha

Build from Source
First, clone the git repo from https://github.com/otterkit/otterkit.git to get the source code. To access the libotterkit submodule inside, use the --recurse-submodules --remote-submodules flag on the clone command. To run, navigate into the src folder (for the compiler, not libotterkit) and then type dotnet run into the command line.
Standard Acknowledgement
Any organization interested in reproducing the COBOL standard and specifications in whole or in part,
using ideas from this document as the basis for an instruction manual or for any other purpose, is free
to do so. However, all such organizations are requested to reproduce the following acknowledgment
paragraphs in their entirety as part of the preface to any such publication (any organization using a
short passage from this document, such as in a book review, is requested to mention ""COBOL"" in
acknowledgment of the source, but need not quote the acknowledgment):
COBOL is an industry language and is not the property of any company or group of companies, or of any
organization or group of organizations.
No warranty, expressed or implied, is made by any contributor or by the CODASYL COBOL Committee
as to the accuracy and functioning of the programming system and language. Moreover, no
responsibility is assumed by any contributor, or by the committee, in connection therewith.
The authors and copyright holders of the copyrighted materials used herein:

FLOW-MATIC® (trademark of Sperry Rand Corporation), Programming for the 'UNIVAC® I and
II, Data Automation Systems copyrighted 1958,1959, by Sperry Rand Corporation;
IBM Commercial Translator Form No F 28-8013, copyrighted 1959 by IBM;
FACT, DSI 27A5260-2760, copyrighted 1960 by Minneapolis-Honeywell

Have specifically authorized the use of this material in whole or in part, in the COBOL specifications.
Such authorization extends to the reproduction and use of COBOL specifications in programming
manuals or similar publications.









About

      Otterkit COBOL Compiler
    





otterkit.com


Topics



  compiler


  dotnet


  cobol



Resources





      Readme
 
License





     Apache-2.0 license
    

Security policy





      Security policy
    



Stars





48
    stars

Watchers





2
    watching

Forks





1
    fork







    Releases

No releases published






    Contributors 3








KTSnowy
Gabriel Gonçalves

 






TriAttack238
Sean Vo

 






gabrielesilinic
Gabriele Silingardi

 





Languages










C#
100.0%











"
https://news.ycombinator.com/rss,Elliptic Curves: The Great Mystery,https://www.cantorsparadise.com/elliptic-curves-the-great-mystery-61599a93c61d,Comments,"Published inCantor’s ParadiseKasper MüllerFollowJan 14·11 min read·Member-onlySaveElliptic Curves: The Great MysteryA surprisingly beautiful blend of algebra, geometry, and number theoryImage from Wikimedia CommonsThese curves defined by very simple equations are shrouded in mystery and elegance. In fact, the equations describing them are so simple that even high-school students would be able to understand them.However, a ton of simple questions about them remain unsolved despite tenacious efforts by some of the greatest mathematicians in the world. But that’s not all. As you will soon see, this theory connects various important fields of mathematics because it turns out that elliptic curves are more than just plane curves!Let’s grab a cup of coffee and start from the beginning.Introduction and MotivationIn mathematics, we often solve problems by stating them in a different setting than they originally occurred in. An example of this is that some geometric problems can be turned into algebraic problems and vice versa.A classical problem going back thousands of years is whether a positive integer n is the area of some right triangle with rational side lengths, that is such that the lengths of all three sides are expressible as fractions of whole numbers. In this case, n is called a congruent number). For instance, 6 is a congruent number because it is the area of the right triangle with side lengths 3, 4 and 5.In 1640, Fermat famously proved that 1 is not a congruent number. He did so using his famous method of proof by infinite descent.As the amazing mathematician, Keith Conrad notes about the result:This leads to a weird proof that √2 is irrational. If √2 were rational then √2, √2 and 2 would be the sides of a rational right triangle with area 1. This is a contradiction of 1 not being a congruent number!Since Fermat’s proof, the hunt for proving or disproving that numbers are congruent has been ongoing.Amazingly, one can show by elementary methods that for each triple of rational numbers (a, b, c) such that a² + b² = c² and 1/2 ab = n, we can find two rational numbers x and y such that y² = x³ - n²x and y ≠ 0 and conversely for each rational pair (x, y) such that y² = x³ - n²x and y ≠ 0, we can find three rational numbers a, b, c such that a² + b² = c² and 1/2 ab = n.That is, right triangles with area n correspond exactly to rational solutions to the equation y² = x³ - n²x with y ≠ 0 and vice versa. A mathematician would say that there is a bijection between the two sets.Therefore, a rational number n > 0 is congruent if and only if the equation y² = x³ - n²x has a rational solution (x, y) with y ≠ 0. For example, since 1 is not congruent, the only rational solutions to y² = x² - x have y = 0.For the interested reader, the exact correspondence is the following.If we try this correspondence on the triangle with side lengths 3, 4, and 5, and with area 6, then the corresponding solution is (x, y) = (12, 36).To me, this is absolutely amazing. One starts with a problem in number theory and geometry and through algebra, transforms it into a problem about rational points on plane curves!The equation y² = x³ - n²x is an example of an elliptic curve.Elliptic CurvesIn general, if f(x) denotes a third-degree polynomial with a non-zero discriminant (i.e. all the roots are distinct), then y² = f(x) describes an elliptic curve except for one important addition to this object, namely what is called a “point at infinity”.Basically, a point at infinity is a point where parallel lines can meet. It is out of the scope of this article to go into projective geometry to proper define it but this is a wonderful and exciting subject that I strongly encourage you to look up.Now, by a minor algebraic miracle, it turns out that we can make a suitable (rational) change of coordinates, and get a new curve on the form y² = x³ + ax + b such that rational points on the two curves are in one-to-one correspondence. The second transformed curve however is typically easier to work with.Because of this, we sometimes assume that an elliptic curve is on this form and so from now on we will assume that too, that is, when we say “elliptic curve”, we mean a curve on the form y² = x³ + ax + b together with a point 𝒪 at infinity.Throughout this article, unless otherwise stated, we’ll assume that the coefficients a and b are rational numbers.Elliptic curves take on two typical shapes which are graphed below.Image from Wikimedia CommonsHowever, if we consider x and y as complex variables, the curves will look entirely different. In fact, they will then take the form of a complex torus or doughnut!So why do we study elliptic curves and what can we do with them?First of all, many number theoretic problems can be translated into problems about Diophantine equations, secondly, it turns out that elliptic curves are related to discrete geometric objects called lattices and deeply connected to some very important objects called modular forms which are certain extremely symmetric complex functions with a lot of number theoretic information in them.Actually, the connection between elliptic curves and modular forms turned out to be the key to proving Fermat’s Last Theorem which Andrew Wiles achieved in the 90s through several years of intensive work on this connection.The story about this quest and the proof of the Theorem is in my opinion one of the most beautiful pursuits in all of science - unfortunately, as pointed out by my friend Kenneth Nielsen, the margin in this Medium post is too narrow to contain it!So I guess I’ll have to write another article.Elliptic curves are also used in cryptography to encrypt messages and online transactions.The most important feature of them, however, is the mind-blowing fact that they are more than just curves and more than just geometry. In fact, they have an algebraic structure on them called an Abelian group structure with respect to a cool geometric operation - a kind of geometric addition rule for how to add points on the curve together.If you don’t know what an Abelian group is, you can think about it as a set of objects with an operation defined on them such that they have the same kind of structure as the integers with respect to addition (except they can be finite).More specifically, for a group with the operation *, it needs to be stable with respect to the operation (i.e. if a is in the group and b is in the group, then a * b is in the group), there is an identity element e (0 for the integers) such that a * e = a for all elements a in the group, and for each element a, there is an inverse element c, such that when you operate them together you get back the identity element (a * c = e). Furthermore, the group operation has to be associative i.e. a * (b * c) = (a * b) * c. That is, it doesn’t matter which elements you add together first. If the commutative law holds ( i.e. a * b = b * a) then the group is called an Abelian group.Examples of Abelian groups are:The integers ℤ with respect to the operation +.The action of rotating a square clockwise by 90 degreesVector spaces with vectors as elements and vector addition as the operationThe fancy terminology for a curve with an Abelian group structure is Abelian variety.What is so amazing about elliptic curves is that we can define an operation (let’s denote it ⊕) between rational points on them (that is, both the x and y coordinates are rational numbers) such that the set of those points on the curve becomes an Abelian group with respect to the operation ⊕ and with identity element 𝒪 (the point at infinity).Let’s define the operation.If you take two rational points on the curve (for example P and Q) and consider a line through them, then the line intersects the curve at another rational point (possibly the point at infinity). Let’s call this point -R.Now, because the curve is symmetric about the x-axis, we get another rational point R when we reflect -R about it. There is a drawing of this below.Addition rule for elliptic curves - image from Wikimedia CommonsThis reflected point (R in the above image) is the addition of the two aforementioned points (P and Q above). We can write P ⊕ Q = R.One can prove (and this is actually not easy) that this operation is associative, which is really surprising, at least to me. Also, the point at infinity acts as a (unique) identity for this operation and each point has an inverse point (which is just the point you get by reflecting about the x-axis). It is also Abelian (i.e. P ⊕ Q = Q ⊕ P).The MysteryIt turns out that two different elliptic curves can have vastly different groups associated with them. An important invariant that in some sense is the most defining feature is what is known as the rank of the curve (or group).A curve can have a finite or an infinite number of rational points on them. This can be hard to handle, so what we are interested in, is how many points we need in order to generate all the others by the aforementioned addition rule. These generators are called basis points.The rank is a dimensionality measure like the dimension of a vector space and indicates how many independent basis points (on the curve) have infinite order (i.e. we can keep adding it without getting our starting point back). If the curve only contains a finite amount of rational points on it, then the rank is zero. There is still a group but it is finite.Calculating the rank of an elliptic curve is notoriously difficult but we have a nice result due to Mordell which tells us that the rank is always finite. That is, we only need a finite amount of basis points in order to generate all the rational points on the curve.One of the most important and interesting problems in number theory is called the Birch and Swinnerton-Dyer Conjecture and it is all about the rank of elliptic curves. In fact, it is so difficult and important that it is one of the Millenium Problems.You actually get a million dollars if you solve it!Finding rational points on elliptic curves with rational coefficients is hard. One way to approach this is by reducing the curve modulo p where p is a prime number. This means that instead of considering the rational solution set of the equation y² = x³ + ax + b, we consider the rational solution set of the congruence y² ≡ x³ + ax + b (mod p) where for this to make sense we might have to clear denominators by multiplying by an integer on both sides.So we are considering two numbers with the same remainder when divided by p to be equal in this new space. The great thing about this is that now there are only a finite number of things to check. Let’s denote the number of rational solutions to such a reduced curve modulo p, by Np.In the early 1960s, Peter Swinnerton-Dyer used the EDSAC-2 computer (not exactly a Macbook!) at the University of Cambridge Computer Laboratory to calculate the number of points modulo p on elliptic curves with known rank. He worked together with the mathematician Bryan John Birch in understanding elliptic curves and after the computer had crunched a bunch of products of the formfor growing x, they got the following output taken from data associated with the curve E: y² = x³ − 5x (as an example). I should note that the x-axis is really log log x and the y-axis is log y.The curve in question is y² = x³ − 5x. This is a curve of rank 1 and one of the curves originally looked at by Birch and Swinnerton-Dyer.Now, I am a mathematician and not a statistician but even I can see a clear trend and it does seem that the regression line has slope 1 on this plot.The curve E has rank 1 and when they tried different curves of varying ranks, they found the same pattern every time. The slope of the fitted regression line, it seemed, was always equal to the rank of the curve in question.More precisely, they stated the bold conjecture thatHere C is some constant.This computer crunching adventure combined with a great deal of far-sightedness led them to make a general conjecture about the behavior of a curve’s Hasse–Weil L-function L(E, s) at s = 1.This L-function is defined as follows. Letand let the discriminant of the curve be denoted Δ. Then we can define the L-function associated with E as the following Euler productWe view this as a function of the complex variable s.Their conjecture now has the form:Conjecture (Birch and Swinnerton-Dyer): Let E be any elliptic curve over ℚ. The rank of the abelian group E(ℚ) of rational points of the curve E is equal to the order of the zero of L(E, s) at s = 1.The reason why it was quite far-sighted is that, at the time, they didn’t even know if a so-called analytic continuation existed for all such L-functions. The problem was that L(E, s) as defined above only converges when Re(s) > 3/2.That they can all be evaluated at s = 1 by analytic continuation was first proved in 2001 again by using the close connection to modular forms that Andrew Wiles proved.Sometimes the conjecture is stated using the Taylor expansion of the L-function, but it is saying the same thing in a different way. The field of rational numbers can be replaced by a more general field but I didn’t want to go into more abstraction than necessary.The subject of Elliptic Curves is a beautiful dance between number theory, abstract algebra and geometry. There is a lot more to say about them than what I have sketched here, but I hope that you got a feeling or a glimpse of something that one could call astounding.We reached the end of this article…If you have any questions, comments or concerns, then please reach out.If you like to read articles like this one on Medium, you can get a membership for full access: simply click here.Thanks for reading.MathematicsMathScienceTechnologyHistory----5More from Cantor’s ParadiseFollowMedium’s #1 Math PublicationRead more from Cantor’s ParadiseRecommended from MediumKasper MüllerinCantor’s ParadiseSums of Odd Numbers Have Interesting PropertiesJesus NajerainCantor’s ParadiseMandelbulb: Three Dimensional FractalsCristian Miguel LunaGeometric DistributionKasper MüllerinCantor’s ParadiseThe Insect That Discovered the Prime NumbersEliran TurgemaninMath SimplifiedMultiply Numbers By Drawing LinesKasper MüllerinCantor’s ParadiseThe Beauty of MathematicsNishesh GogiainAnalytics VidhyaTHE STORY OF LOGISTIC REGRESSION CONTINUES…Eliran TurgemaninCantor’s ParadiseCity Planning Using Graph TheoryAboutHelpTermsPrivacyGet the Medium app"
https://news.ycombinator.com/rss,"The November 2020 landslide, tsunami, and outburst flood at Elliot Creek",https://nautil.us/the-ecological-catastrophe-youve-never-heard-of-257291/,Comments,"
403 Forbidden

403 Forbidden
nginx


"
https://news.ycombinator.com/rss,"Show HN: Vento, a screen recorder that lets you rewind and record over mistakes",https://vento.so,Comments,"ventoNew RecordingLog inStress-free Screen Recording Constantly restarting your screen recordings? With Vento, just pause, rewind, and carry on instead - keeping calm helps too :)Come back on a desktop computer to try us out!Install Chrome ExtensionStart Recording“To record great videos, one must first rewind.” - Gandhi, probably.Video Not Availablevento© 2023 Vento. All rights reserved.Say hello! We don’t bite. Well, maybe one of us does.hello@vento.so"
https://news.ycombinator.com/rss,The Whole Code Catalog (2019),https://futureofcoding.org/catalog/,Comments,"




The Whole Code Catalog








The

Whole Code

Catalog



Edition 1
September 9, 2019
Steve Krouse
Dark

Over the past year and a half, I've been collaborating with Dark (a new programming language startup) to research programming languages and tools. Today we are sharing these reviews publicly.
The Whole Earth Catalog was before my time, but I grew up immersed in many of the conversations it began. This series borrows its name, and continues in its tradition of ""access to tools"". Stewart Brand’s original inspiration for the Catalog was helping his ""friends who were starting their own civilization hither and yon in the sticks."" This series is likewise designed to help visionary friends, but where his friends were living off-the-grid, my friends are creating the programming environments of the future. This series is democratizing access to tools one-step removed. These reviews are for the makers of tools, those that are pushing our computational interfaces forward. The goal of the Whole Code Catalog is to provide inspiration for the tools you will one day create.
Other similar efforts to catalog futuristic computational interfaces include:

          
Ivan Reese's Visual Programming Codex
Chaim Gingold's Gadget Survey
Katherine Ye's collection of links on notation
Eric Hosick's Visual Programming Languages - Snapsnots
Jonathan Edward's Gallery of Programmer Interfaces
Daniel Garcia's Mind Bicycles


If you enjoy this catalog, you may also enjoy the Future of Coding Slack Community and Future of Coding podcast.




Thanks to the Dark team for their support and partnership. Thanks to Ivan Reese, Jonathan Edwards, Aidan Cuniffee, Nicolae Rusan, Geoffrey Litt, and Matt Marcus for their feedback.






"
https://news.ycombinator.com/rss,Intel Core i9-13900T CPU benchmarks show faster than 12900K 125W performance,https://wccftech.com/intel-core-i9-13900t-cpu-benchmarks-show-faster-than-12900k-125w-performance-at-35w/,Comments,"

HardwareReport
Intel Core i9-13900T CPU Benchmarks Show Faster Than 12900K 125W Performance at 35W

Hassan Mujtaba •
Jan 14, 2023 02:44 PM EST

•
Copy Shortlink
























Intel recently introduced brand new 13th Gen T-series chips which feature the Core i9-13900T that operates at a 35W TDP. The new chip has been benchmarked within Geekbench 5 and showcases impressive performance given its limited power budget.
Intel's 13th Gen Core i9-13900T 35W CPU Beats The 125W Core i9-12900K In Geekbench 5 Benchmark
Starting with the specifications, the Intel Core i9-13900T is a variation of the Core i9-13900 series that comes with a limited TDP design. While the standard chips boast 125W TDP in the unlocked and 65W TDP on the Non-K SKUs, the T-series chip is limited to a 35W TDP.  The Unlocked CPU is rated at up to 253W, the Non-K is rated at up to 219W while the T-series chip is rated at up to 106 Watts which is less than half the power budget of its higher-end siblings.
Related StoryHassan MujtabaIntel Core i9-13900KS, World’s First 6 GHz CPU, Now Available For $699 USThe Intel Core i9-13900T retains the same core configuration with 24 cores that are made up of 8 P-Cores and 16 E-Cores with 32 threads, a base clock of 1.10 GHz, a boost of up to 5.30 GHz & 68 MB of cache (L2+L3). The CPU also comes at a slightly lower price point of $549.00 US. Now the CPU is tested within the Geekbench 5 benchmark using an ASUS TUF Gaming B660M-PLUS WIFI board and coupled with 64 GB of DDR5 memory.

The CPU scored 2178 points in the single-core and 17339 points in the multi-core tests. We used the Intel Core i9-12900K for comparison which scores 1901 points in single-core and 17272 points in multi-core tests. This puts the Intel Core i9-13900T up to 15% faster in single-core and slightly faster in multi-threaded tests which is very impressive considering the Core i9-12900K also has a higher 125W base TDP (3.58x higher) and a peak TDP rating of 241W (2.27x higher).

Intel Core i9-13900KS Single-Thread CPU Benchmark (Geekbench 5)

Single-Core


050010001500200025003000




050010001500200025003000





Core i9-13900KS

2.3k


Core i9-13900K

2.2k


Ryzen 9 7900X

2.2k


Ryzen 9 7950X

2.2k


Ryzen 7 7700X

2.2k


Core i9-13900T

2.2k


Ryzen 5 7600X

2.2k


Ryzen 9 7900

2.1k


Core i9-13900

2.1k


Ryzen 7 7700

2.1k


Core i9-12900KS

2.1k


Core i9-13900HX

2k


Ryzen 5 7600

2k


Core i7-13700K

2k


Core i5-13600K

1.9k


Core i9-12900K

1.9k


Core i7-12700K

1.9k


M2 Max

1.9k


M1 Max

1.8k


Core i5-12600K

1.7k


Ryzen 9 5950X

1.7k


Ryzen 7 5800X

1.7k


Ryzen 9 5900X

1.7k


Ryzen 5 5600X

1.6k







Intel Core i9-13900KS Multi-Thread CPU Benchmark (Geekbench 5)

Multi-Core


050001000015000200002500030000




050001000015000200002500030000





Core i9-13900KS

26.8k


Core i9-13900K

24.3k


Ryzen 9 7950X

24.4k


Core i9-13900HX

20.9k


Core i9-13900

20.1k


Core i7-13700K

19.8k


Ryzen 9 7900X

19.3k


Core i9-12900KS

19k


Ryzen 9 7900

18.6k


Core i9-13900T

17.3k


Core i9-12900K

17.3k


Ryzen 9 5950X

16.5k


Core i5-13600K

16.1k


M2 Max

14.6k


Core i7-12700K

14.1k


Ryzen 7 7700X

14.1k


Ryzen 9 5900X

14k


Ryzen 7 7700

12.7k


M1 Max

12.3k


Core i5-12600K

11.6k


Ryzen 5 7600X

11.4k


Ryzen 5 7600

11.3k


Ryzen 7 5800X

10.3k


Ryzen 5 5600X

8.2k






This goes off to show the immense efficiency that Intel's 10nm ESF process node and the new hybrid architecture packs and we will also get to see some similar results with the mobility lineup, especially the 13th Gen HX parts which are going to ship in enthusiast-grade gaming laptops in the coming months. AMD also introduced its brand new 65W Ryzen 7000 Non-X CPUs which have been showcasing some impressive efficiency feats on their own with the Zen 4 core architecture.
News Source: Benchleaks
				
				Share this story
				 Facebook
 Twitter






Deal of the Day











Further Reading




 AMD Ryzen 9 7950X3D CPU Shown To Beat Intel Core i9-13900K In Games With Up To 24% Lead


 Intel Core i9-13980HX CPU Powered MSI Raider GE78HX Laptop Matches High-End Desktop CPUs In Performance


 Intel Core i9-13980HX Flagship Raptor Lake-HX CPU Spotted In ASUS’s Next-Gen ROG STRIX Laptop


 It’s Over 9000! Intel Core i9-13900KS Becomes The First CPU To Achieve 9 GHz Frequency World Record













Comments




Please enable JavaScript to view the comments.










Trending Stories


SpaceX’s Rockets Split Up In Mid Air For Rare & Stunning Views At 5,000 Km/h+



				55 Active Readers



NASA Captures Star Eaten By Black Hole 300 Million Light Years Away



				49 Active Readers



Intel Core i9-13900T CPU Benchmarks Show Faster Than 12900K 125W Performance at 35W



				31 Active Readers



PlayStation 5 Vertical Orientation Issue Clarified by Technician; Issue Happens on “Unopened” Consoles



				21 Active Readers



ARK Survival Evolved Unreal Engine 5 Remaster Teased By Studio Wildcard; UE5 Fan Imagining Released



				19 Active Readers








Popular Discussions


AMD Radeon RX 7900 XTX Failure Rates Reportedly At 11%, RMA’s Piling Up But Users Not Receiving Cards



				3100 Comments



Intel Lunar Lake To Feature A Brand New CPU Architecture Built From The Ground-Up, Perf/Watt Focused at Mobile



				2757 Comments



AMD To Give The Love of 3D V-Cache This Valentines With Its Ryzen 7000 X3D CPUs Launch



				2021 Comments



Intel Arc A770 Performs Above AMD & NVIDIA In DirectStorage 1.1 Performance Benchmark



				1803 Comments



AMD Radeon RX 6000 GPUs Mysteriously Start Dying, German Repair Shop Receives 48 Cards With Cracked Chips



				1701 Comments








	 







"
https://news.ycombinator.com/rss,"Louis Le Prince, the missing inventor of an early motion-picture camera",https://en.wikipedia.org/wiki/Louis_Le_Prince,Comments,"



Louis Le Prince - Wikipedia










































Louis Le Prince

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search
French inventor and Father Of Cinematography
For the composer, see Louis Le Prince (composer).


Louis Le PrinceLe Prince c. 1885BornLouis Aimé Augustin Le Prince(1841-08-28)28 August 1841Metz, FranceDisappeared16 September 1890Dijon, FranceStatusDeclared dead on
                       16 September 1897 (aged 56)Occupation(s)Artist, art teacher, inventorSpouseElizabeth Le Prince-Whitley
​ ​(m. 1869)​
Louis Aimé Augustin Le Prince (28 August 1841 – disappeared 16 September 1890, declared dead 16 September 1897) was a French artist and the inventor of an early motion-picture camera, possibly the first person to shoot a moving picture sequence using a single lens camera and a strip of (paper) film.[1][2] He has been credited as the ""Father of Cinematography"",[3] but his work did not influence the commercial development of cinema—owing at least in part to the great secrecy surrounding it.[4][5]
A Frenchman who also worked in the United Kingdom and the United States, Le Prince's motion-picture experiments culminated in 1888 in Leeds, England.[6] In October of that year, he filmed moving-picture sequences of family members in Roundhay Garden and his son playing the accordion, using his single-lens camera and Eastman's paper negative film.[7] At some point in the following eighteen months he also made a film of Leeds Bridge. This work may have been slightly in advance of the inventions of contemporaneous moving-picture pioneers, such as the British inventors William Friese-Greene and Wordsworth Donisthorpe, and was years in advance of that of Auguste and Louis Lumière and William Kennedy Dickson (who did the moving image work for Thomas Edison).
Le Prince was never able to perform a planned public demonstration of his camera in the US because he mysteriously vanished; he was last known to be boarding a train on 16 September 1890.[1] Multiple conspiracy theories have emerged about the reason for his disappearance, including: a murder set up by Edison, secret homosexuality, disappearance in order to start a new life, suicide because of heavy debts and failing experiments, and a murder by his brother over their mother's will. No conclusive evidence exists for any of these theories. In 2004, a police archive in Paris was found to contain a photograph of a drowned man bearing a strong resemblance to Le Prince who was discovered in the Seine just after the time of his disappearance,[7] but it has been claimed that the body was too short to be Le Prince.[8]
In early 1890, Edison workers had begun experimenting with using a strip of celluloid film to capture moving images. The first public results of these experiments were shown in May 1891.[9] However, Le Prince's widow and son Adolphe were keen to advance Louis's cause as the inventor of cinematography. In 1898, Adolphe appeared as a witness for the defence in a court case brought by Edison against the American Mutoscope Company. This suit claimed that Edison was the first and sole inventor of cinematography, and thus entitled to royalties for the use of the process. Adolphe was involved in the case but was not allowed to present his father's two cameras as evidence, although films shot with cameras built according to his father's patent were presented. Eventually the court ruled in favour of Edison. A year later that ruling was overturned,[9] but Edison then reissued his patents and succeeded in controlling the US film industry for many years.[9]

Contents

1 Early life and education
2 Career
3 Disappearance
4 Patents and cameras
5 Later recognition

5.1 Le Prince Cine Camera-Projector types


6 Legacy

6.1 Remaining material and production
6.2 Man Walking Around a Corner (16-Lens Camera)
6.3 Roundhay Garden Scene (Single-Lens Camera MkII)
6.4 Traffic Crossing Leeds Bridge (Single-Lens Camera MkII)
6.5 Accordion Player (Single-Lens Camera MkII)


7 See also
8 References
9 Sources
10 External links


Early life and education[edit]
Le Prince was born on 28 August 1841 in Metz.[10][11] His family referred to him as ""Augustin"" and English-speaking friends would later call him ""Gus"".[12] Le Prince's father was a major of artillery in the French Army[13] and an officer of the Légion d'honneur. When growing up, he reportedly spent time in the studio of his father's friend, the pioneer of photography Louis Daguerre,[13] from whom Le Prince may have received some lessons on photography and chemistry before he was 10 years old. His education went on to include the study of painting in Paris and post-graduate chemistry at Leipzig University,[13] which provided him with the academic knowledge he was to utilise in the future.

Career[edit]
 Le Prince in the 1880s
In conclusion, I would say that Mr. Le Prince was in many ways a very extraordinary man, apart from his inventive genius, which was undoubtedly great. He stood 6ft. 3in. or 4in. (190cm) in his stockings, well built in proportion, and he was most gentle and considerate and, though an inventor, of an extremely placid disposition which nothing appeared to ruffle.— Declaration of Frederic Mason (wood-worker and assistant of Le Prince, April 21, 1931, American consulate of Bradford, England)
Le Prince moved to Leeds, England in 1866, after being invited to join John Whitley,[1] a friend from college, in Whitley Partners of Hunslet, a firm of brass founders making valves and components.[14][15] In 1869 he married Elizabeth Whitley, John's sister[1] and a talented artist. When in Paris during their honeymoon, Le Prince repeatedly visited a magic show, fascinated by an illusion with moving transparent figures, presumably a dancing skeleton projection at the Théâtre Robert-Houdin with multiple reflections of mirrors focused on one point or a variation of Pepper's Ghost.[16]
Le Prince and his wife started a school of applied art, the Leeds Technical School of Art,[17] and became well renowned for their work in fixing coloured photographs on to metal and pottery, leading to them being commissioned for portraits of Queen Victoria and the long-serving Prime Minister William Gladstone produced in this way; these were included alongside other mementos of the time in a time capsule—manufactured by Whitley Partners of Hunslet—which was placed in the foundations of Cleopatra's Needle on the embankment of the River Thames.[citation needed]
In 1881, Le Prince went to the United States[13] as an agent for Lincrusta Walton, staying in the country along with his family once his contract had ended.[citation needed] He became the manager for a small group of French artists who produced large panoramas, usually of famous battles, that were exhibited in New York City, Washington, D.C. and Chicago.[13][14]
During this time he began experiments relating to the production of 'moving' photographs, designing a camera that utilised sixteen lenses,[14] which was the first invention he patented. Although the camera was capable of 'capturing' motion, it wasn't a complete success because each lens photographed the subject from a slightly different viewpoint and thus the image would have jumped about, if he had been able to project it (which is unknown).

 Plaque on Leeds Bridge
After his return to Leeds in May 1887,[14] Le Prince built a single-lens camera in mid-late 1888. An experimental model was developed in a workshop at 160 Woodhouse Lane, Leeds and used to shoot his motion-picture films. It was first used on 14 October 1888 to shoot what would become known as Roundhay Garden Scene and a sequence of his son Adolphe playing the accordion. Le Prince later used it to film road traffic and pedestrians crossing Leeds Bridge. The film was shot from Hicks the Ironmongers, now the British Waterways building on the south east side of the bridge,[1] now marked with a commemorative Blue plaque.

Disappearance[edit]
In September 1890, Le Prince was preparing for a trip to the United States, supposedly to publicly premiere his work and join his wife and children. Before this journey, he decided to return to France to visit his brother in Dijon. Then, on 16 September, he took a train to Paris but, having taken a later train than planned, his friends missed him in Paris. He was never seen again by his family or friends.[1] The last person to see Le Prince at the Dijon station was his brother.[18]  The French police, Scotland Yard and the family undertook exhaustive searches, but never found him.
Le Prince was officially declared dead in 1897.[19]
A number of wild and mostly unsubstantiated theories were proposed, including:

Patent Wars assassination, ""Equity 6928""
Christopher Rawlence pursues the assassination theory, along with other theories, and discusses the Le Prince family's suspicions of Edison over patents (the Equity 6928) in his 1990 book and documentary The Missing Reel. Rawlence claims that at the time that he vanished, Le Prince was about to patent his 1889 projector in the UK and then leave Europe for his scheduled New York official exhibition. His widow assumed foul play though no concrete evidence has ever emerged and Rawlence prefers the suicide theory. In 1898, Le Prince's elder son Adolphe, who had assisted his father in many of his experiments, was called as a witness for the American Mutoscope Company in their litigation with Edison [Equity 6928]. By citing Le Prince's achievements, Mutoscope hoped to annul Edison's subsequent claims to have invented the moving-picture camera. Le Prince's widow Lizzie and Adolphe hoped that this would gain recognition for Le Prince's achievement, but when the case went against Mutoscope their hopes were dashed. Two years later Adolphe Le Prince was found dead on Fire Island near New York.[20]
Disappearance ordered by the family
In 1966, Jacques Deslandes proposed a theory in Histoire comparée du cinéma (The Comparative History of Cinema), claiming that Le Prince voluntarily disappeared due to financial reasons and ""familial conveniences"". Journalist Léo Sauvage quotes a note shown to him by Pierre Gras, director of the Dijon municipal library, in 1977, that claimed Le Prince died in Chicago in 1898, having moved there at the family's request because he was homosexual; but he rejects that assertion.[21] There is no evidence to suggest that Le Prince was gay.[22]
Fratricide, murder for money
In 1967, Jean Mitry proposed, in Histoire du cinéma, that Le Prince was killed. Mitry notes that if Le Prince truly wanted to disappear, he could have done so at any time prior to that. Thus, he most likely never boarded the train in Dijon. He also wonders why, if his brother, who was confirmed as the last person to have seen Le Prince alive, knew Le Prince was suicidal, he didn't try to stop Le Prince, and why he didn't report Le Prince's mental state to the police before it was too late.[23]
Suicide by drowning
A photograph of a drowned man pulled from the Seine in 1890, strongly resembling Le Prince, was discovered in 2003 during research in the Paris police archives.[13][24] This somehow led to the conclusion that he must have failed to get his moving picture to work, had heavy debts, and thus chose to take his own life.[18] It has been claimed that the found body was too short to be Le Prince.[8]
Patents and cameras[edit]
On 10 January 1888 Le Prince was granted an American patent on a 16-lens device that he claimed could serve as both motion picture camera (which he termed ""the receiver or photo-camera"") and a projector (which he called "" the deliverer or stereopticon"").[25] That same day he took out a near-identical provisional patent for the same devices in Great Britain, proposing ""a system of preferably 3, 4, 8, 9, 16 or more lenses"". Shortly before the final version was submitted he added a sentence which described a single-lens system, but this was neither fully explained nor illustrated, unlike the several pages of description of the multi-lens system,[26] meaning the single-lens camera was not legally covered by patent.

 60mm spools used for developing film shot in single-lens camera. Each section would carry 4 frames of negative (1930 Science Museum, London)
This addendum was submitted on 10 October 1888[27] and, on 14 October, Le Prince used his single-lens camera to film Roundhay Garden Scene. During the period 1889-1890 he worked with the mechanic James Longley on various ""deliverers"" (projectors) with one, two, three and sixteen lenses.  The images were to be separated, printed and mounted individually, sometimes on a flexible band, moved by metal eyelets. The single lens projector used individual pictures mounted in wooden frames.[27] His assistant, James Longley, claimed the three-lens version was the most successful.[27] Those close to Le Prince have testified to him projecting his first films in his workshop as tests, but they were never presented to anyone outside his immediate circle of family and associates and the nature of the projector is unknown.
In 1889 he took French-American dual citizenship in order to establish himself with his family in New York City and to follow up his research. However, he was never able to perform his planned public exhibition at Morris–Jumel Mansion in Manhattan, in September 1890, due to his disappearance.

Later recognition[edit]
This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: ""Louis Le Prince"" – news · newspapers · books · scholar · JSTOR (October 2017) (Learn how and when to remove this template message)
Even though Le Prince's achievement is remarkable, with only William Friese-Greene and Wordsworth Donisthorpe achieving anything comparable in the period 1888-1890, his work was largely forgotten until the 1920s, as he disappeared before the first public demonstration of the result of his work, having never shown his invention to any photographic society or scientific institution or the general public.
For the April 1894 commercial exploitation of his personal kinetoscope parlor, Thomas Edison is credited in the US as the inventor of cinema, while in France, the Lumière Brothers are hailed as inventors of the Cinématographe device and for the first commercial exhibition of motion-picture films, in Paris in 1895.
However, in Leeds, Le Prince is celebrated as a local hero. On 12 December 1930, the Lord Mayor of Leeds unveiled a bronze memorial tablet at 160 Woodhouse Lane, Le Prince's former workshop. In 2003, the University's Centre for Cinema, Photography and Television was named in his honour. Le Prince's workshop in Woodhouse Lane was until recently the site of the BBC in Leeds, and is now part of the Leeds Beckett University Broadcasting Place complex, where a blue plaque commemorates his work. (coordinates: 53°48′20.58″N 1°32′56.74″W﻿ / ﻿53.8057167°N 1.5490944°W﻿ / 53.8057167; -1.5490944). Reconstructions of his film strips are shown in the cinema of the Armley Mills Industrial Museum, Leeds.
In France, an appreciation society was created as L'Association des Amis de Le Prince (Association of Le Prince's Friends), which still exists in Lyon.
In 1990, Christopher Rawlence wrote The Missing Reel, The Untold Story of the Lost inventor of Moving Pictures and produced the TV programme The Missing Reel (1989) for Channel Four, a dramatised feature on the life of Le Prince.
In 1992, the Japanese filmmaker Mamoru Oshii (Ghost in the Shell) directed Talking Head, an avant-garde feature film paying tribute to the cinematography history's tragic ending figures such as George Eastman, Georges Méliès and Louis Le Prince who is credited as ""the true inventor of eiga"", 映画, Japanese for ""motion picture film"".
In 2013, a feature documentary, The First Film was produced, with new research material and documentation on the life of Le Prince and his patents. Produced and directed by Leeds-born David Nicholas Wilkinson with research by Irfan Shah, it was filmed in England, France and the United States by Guerilla Films.[28] The First Film features several film historians to tell the story, including Michael Harvey, Irfan Shah, Stephen Herbert, Mark Rance, Daniel Martin, Jacques Pfend, Adrian Wootton, Tony North, Mick McCann, Tony Earnshaw, Carol S Ward, Liz Rymer, and twice Oscar-nominated cinematographer Tony Pierce-Roberts. Le Prince's great-great-granddaughter Laurie Snyder also makes an appearance. It had its world première in June 2015 at the Edinburgh Film Festival and opened in UK cinemas on 3 July 2015. The film also played in festivals in the US, Canada, Russia, Ireland and Belgium. On 8 September 2016 it played at the Morris-Jumel Mansion in New York, where 126 years earlier Le Prince planned to show his films.

Le Prince Cine Camera-Projector types[edit]


Model
Specs
Design
Manufacture
Patents


16-lens camera and projector
Patent: ""Method of and apparatus for producing animated pictures. of natural scenery and life"" (USA) and in all later foreign patents.Designation: LePrince 16-lens camera/projector Framerate: 16 frames per second (according to patent)Medium: Glass plates and Eastman paper film
1886, New York
Made in Paris, 1887
US Patent No.376,247/217,809IssuedWashington2 November 1886Accepted10 January 1888
FR Patent No.188,089IssuedParis11 January 1888AcceptedJune 1890
(and BR patent 423 - see below)



Single-lens camera
Patent: Mentioned but not described or illustrated in ""Improvements in the Method of and Apparatus for Producing Animated Photographic Pictures""Designation: Le Prince single-lens ""receiver"" (camera) Mk2Framerate: 5-7 frames per secondLenses: Viewfinder (upper) & Photograph (lower)Film: sensitised paper film & gelatin stripping film (60mm)Focus: lever (backward/forward)
Leeds
1888


*Frederic Mason(body/ wooden parts)*James William Longley (design and working parts)Made in Leeds, 1888
BR Patent no 423
IssuedLondon10 January 1888Accepted16 November 1888 [Mentioned but not described]





Single-Lens
Projector


Single-lens ""deliverer"" (projector). Each frame was printed on glass and mounted in a mahogany frame. These were moved before the lens in a continuous spiral. The heat of the lamp and the movement of the frames often caused the glass to break. Top framerate: 7fps.

Leeds
1889


Made in Leeds, 1889

Never patented


3-Lens Projector
3-lens ""deliverer"" (projector), used frames mounted individually in three flexible strips of Willesden paper with brass eyelets to move them. Projection presumably alternated 1-2-3 between the three strips/lenses and each strip moved when the light was cut off.
Leeds
1889/ 1890


Made in Leeds 1889 or 1890

Never patented

Legacy[edit]
Remaining material and production[edit]
 Back view of Le Prince's single-lens Cine Camera-Projector MkII opened (Science Museum, London, 1930).
Le Prince developed a single-lens camera in his workshop at 160 Woodhouse Lane, Leeds, which was used to shoot his motion-picture films. Remaining surviving production consists of two scenes in the garden at Oakwood Grange (his wife's family home, in Roundhay) and another of Leeds Bridge.
Forty years later, Le Prince's daughter, Marie, gave the remaining apparatus to the  Science Museum, London (later transferred to the National Museum of Photography, Film and Television (NMPFT), Bradford, which opened in 1983 and is now the National Science and Media Museum). In May 1931, photographic plates were produced by workers of the Science Museum from paper print copies provided by Marie Le Prince.[2] In 1999, these were re-animated to produce digital versions. Roundhay Garden was alleged by the Le Prince family to have been shot at 12 frame/s and Leeds Bridge at 20 frame/s, although this is not borne out by the NMPFT versions (see below) or motion analysis, with both films being estimated at a consistent 7 frames a second.[29]
All available versions of these sequences are derived from materials held by the National Science and Media Museum.

Man Walking Around a Corner (16-Lens Camera)[edit]
Main article: Man Walking Around A Corner




Sequence of 12 complete frames + 4 partial frames, from National Science Museum, London circa 1931. (Courtesy NMPFT, Bradford) NMPFT. Filmed in Paris before 18.08.1887.




The only existing images from Le Prince's 16-lens camera are a sequence of 16 frames of a man walking around a corner. This appears to have been shot onto a single glass plate (which has since broken), rather than the twin strips of Eastman paper film envisaged in his patent. Jacques Pfend, a French cinema-historian and Le Prince specialist, confirms that these images were shot in Paris, at the corner of Rue Bochart-de-Saron (where Le Prince was living) and Avenue Trudaine. Le Prince sent 8 images of his mechanic running (which may be from this sequence) to his wife in New York City in a letter dated 18 August 1887,[30] which suggests it represented a significant camera test. Exposure is very irregular from lens to lens with some of the images almost completely bleached out, which Le Prince later on fixed.

Roundhay Garden Scene (Single-Lens Camera MkII)[edit]
Main article: Roundhay Garden Scene




Roundhay, 1888 original 20 frames by National Science Museum, London 1931 (Courtesy of NMPFT, Bradford).






Animation of Roundhay frames with image stabilised NMPFT, Bradford 1999.




The 1931 National Science Museum copy of what remains of a sequence shot in Roundhay Garden features 20 frames. The frames appear to have been printed in reverse from the negative, but this is corrected in the video. The film's damaged edge results in distortion and deformation on the right side of the stabilised digital movie. The scene was shot in Le Prince's father-in-law's garden at Oakwood Grange, Roundhay on 14 October 1888. The NMPFT animation lasts two seconds at 24fps (frames per second), meaning the original footage is playing at 10fps. In this version, the action is speeded up - the original footage was probably shot at 7fps.

Traffic Crossing Leeds Bridge (Single-Lens Camera MkII)[edit]
 Video clip, 2 seconds
Louis Le Prince filmed traffic crossing Leeds Bridge from Hicks the Ironmongers[1] at the following coordinates: 53°47′37.70″N 1°32′29.18″W﻿ / ﻿53.7938056°N 1.5414389°W﻿ / 53.7938056; -1.5414389.[31]





6-frame sequence (118-120 & 122–124) of Leeds Bridge (National Science Museum, London 1923)






20-frame sequence of Leeds Bridge (National Science Museum)(Courtesy NMPFT, Bradford)




The earliest copy belongs to the 1923 NMPFT inventory (frames 118–120 and 122–124), though this longer sequence comes from the 1931 inventory (frames 110–129). According to Adolphe Le Prince who assisted his father when this film was shot in late October 1888, it was taken at 20fps. However, the digitally stabilised sequence produced by the NMPFT lasts two seconds, meaning the footage is playing here at 10fps. As with the Roundhay Garden sequence, its appearance is sped up, suggesting the original footage was probably shot at 7fps. This would fit with what we know of the projection experiments, where James Longley reported a top speed of 7fps.[32]

Accordion Player (Single-Lens Camera MkII)[edit]
 2 frames per second amateur remastering of all 19 frames; 10 frames per second version









Copy of original 19 frames (numbered 41–59) by National Science Museum, London 1931 (Courtesy of NMPFT, Bradford).




The last remaining film of Le Prince's single-lens camera is a sequence of frames of Adolphe Le Prince playing a diatonic button accordion. It was recorded on the steps of the house of Joseph Whitley, Louis's father-in-law.[2] The recording date may be the same as Roundhay Garden as the camera is in a similar position and Adolphe is dressed the same. The NMPFT has not remastered this film. An amateur animation of the first 17 frames is here on YouTube. The running speed appears to be 5-6fps

See also[edit]
List of people who disappeared
Roundhay Garden Scene
References[edit]


^ a b c d e f g ""BBC Education – Local Heroes Le Prince Biography"". Archived from the original on 28 November 1999. Retrieved 27 May 2008.{{cite web}}:  CS1 maint: bot: original URL status unknown (link), BBC, archived on 28 November 1999

^ a b c Howells, Richard (Summer 2006). ""Louis Le Prince: the body of evidence"". Screen. Oxford, UK: Oxford Journals. 47 (2): 179–200. doi:10.1093/screen/hjl015.

^ The ""Father"" Of Kinematography: Leeds Memorial Pioneer Work In England. The Manchester Guardian (1901–1959), Manchester, England 13 December 1930: 19.

^ Fischer, Paul (April 2022). The Man who Invented Motion Pictures: A True Tale of Obsession, Murder, and the Movies. Simon & Schuster. ISBN 9781982114824.

^ Greenblatt, Leah (14 April 2022). ""He Created the First Known Movie. Then He Vanished. In his new book, ""The Man Who Invented Motion Pictures,"" Paul Fischer investigates the life — and mysterious disappearance — of Louis Le Prince"". The New York Times. Retrieved 17 April 2022.

^ ""Louis Le Prince, who shot the world's first film in Leeds"". BBC. 24 August 2016.

^ a b ""Pioneers of Early Cinema: 1, Louis Aimé Augustin Le Prince (1841–1890?)"" (PDF). www.nationalmediamuseum.org.uk. p. 2. Retrieved 25 November 2012. he developed a single-lens camera which he used to make moving picture sequences at the Whitley family home in Roundhay and of Leeds Bridge in October 1888. ... it has been claimed that a photograph of a drowned man in the Paris police archives is that of Le Prince.

^ a b ""The tragedy of Louis Le Prince"". www.acmi.net.au. Retrieved 20 June 2022.

^ a b c Spehr, Paul (2008). The Man Who Made Movies: W.K.L. Dickson. United Kingdom: John Libbey Publishing Ltd.

^ ""Archives Municipales de Metz - Visualiseur"". Retrieved 9 May 2020.

^ Aulas, Jean-Jacques; Pfend, Jacques (1 December 2000). ""Louis Aimé Augustin Leprince, inventeur et artiste, précurseur du cinéma"". 1895. Mille Huit Cent Quatre-vingt-quinze (in French) (32): Footnote 4. doi:10.4000/1895.110. ISSN 0769-0959. The birth certificate mentions ""born August on the 28th, 1841 at 5am. The common mistake of making him born in 1842 comes from an article of Ernest Kilburn Scott, mistake made since then in numerous articles, including the one by Simon Popple

^ Aulas, Jean-Jacques; Pfend, Jacques (1 December 2000). ""Louis Aimé Augustin Leprince, inventeur et artiste, précurseur du cinéma"". 1895. Mille Huit Cent Quatre-vingt-quinze (in French) (32): 9–74. doi:10.4000/1895.110. ISSN 0769-0959.

^ a b c d e f Herbert, Stephen. ""Louis Aimé Augustin Le Prince"". Who's Who of Victorian Cinema. Archived from the original on 21 July 2006. Retrieved 26 August 2006.

^ a b c d Adventures in CyberSound: Le Prince, Louis Aimé Augustin, Dr Russell Naughton (using source: Michael Harvey, NMPFT Pioneers of Early Cinema: 1. Louis Aimé Augustin Le Prince)

^ ""Pioneers of Early Cinema: Louis Aimé Augustin Le Prince (1841-1890?)"" (PDF). National Media Museum. June 2011.

^ ""Louis Le Prince – New Thinking: Part 1"". The Optilogue. 21 November 2022. Retrieved 23 November 2022.

^ Thomas Deane Tucker (2020). Peripatetic Frame: Images of Walking in Film. Edinburgh University Press. p. 18.

^ a b ""The Shadow Traps"". www.stitcher.com. Retrieved 4 November 2019.

^ Hannavy, John, ed. (2000). Encyclopedia of nineteenth-century photography. Vol. 1. CRC Press. p. 837. ISBN 978-0-415-97235-2.

^ Burns, Paul. ""The History of the Discovery of Cinematography"". – ""After his disappearance, the Le Prince family led by his wife and son went to court against Edison in what became known as Equity 6928. The famous Patent Wars ensued and by 1908 Thomas Edison was regarded as sole inventor of motion pictures, in the US at least. However, in 1902, two years after Le Prince's son Adolphe had testified in the suit, he was found shot dead on Fire Island, New York.""

^ Léo Sauvage, ""Un épisode mystérieux de l'histoire du cinéma : La disparition de Le Prince"", Historia, n° 430 bis, sept. 1982, p. 45-51: ""une telle affirmation (...) est totalement dépourvue de vraisemblance"".

^ Dembowski (1995): ""Pierre Gras, conservateur en chef de la Bibliothèque publique de Dijon, en 1977, montra à Léo Sauvage une note (il la cite dans son ouvrage), prise lors de la visite d'un historien connu (il a tu son nom) qui avait déclaré : – Le Prince est mort à Chicago en 1898, disparition volontaire exigée par la famille. Homosexualité. Disons clairement qu'il n'y a pas l'ombre d'une preuve à l'appui d'une telle assertion.""

^ Dembowski (1995): ""S'il en était ainsi, pourquoi n'a-t-il rien fait pour l'empêcher de réaliser son funeste projet, pourquoi n'a-t-il pas averti la police à temps?""

^ ""The mystery of Leeds's long-lost movie pioneer"". The Telegraph. 23 June 2015. Retrieved 9 May 2020 – via www.telegraph.co.uk.

^ ""Method of and apparatus for producing animated pictures of natural scenery and life"". 10 January 1888. Retrieved 29 December 2017.

^ ""Patents Completed"". British Journal of Photography. 35: 793.

^ a b c Aulas & Pfend, Jean-Jacques & Jacques (1 December 2000). ""Louis Aimé Augustin Leprince, inventeur et artiste, précurseur du cinéma"". 1895. Revue de l'association française de recherche sur l'histoire du cinéma. 32.

^ ""The First Film"". Guerilla Group. Retrieved 16 June 2018.

^ ""Cinematography"". National Museum of Photography, Film and Television. Archived from the original on 11 July 2006. Retrieved 16 April 2009.

^ Letter dated 18 August 1887 in Louis Le Prince Collection at Leeds University Library

^ ""Google Earth Community: First Moving Pictures"". Retrieved 9 May 2020.

^ Letter from James Longley to Louis le Prince 8 August 1889. ""The best result that I got was 426 per minute"" - From Le Prince Collection in Leeds University Library.


Sources[edit]

Insight Collections and Research Centre
Guinness Book of Movie Facts and Feats
Who's Who of Victorian Cinema
The Career of Louis Aimée Augustin Le Prince by E. Kilburn Scott (July 1931)
""La naissance du cinéma : cent sept ans et un crime..."" by Irénée Dembowski (in Kino 1989, translated from Polish to French in Cahiers de l'AFIS, numéro 182, nov.–déc. by Michel Rouzé, quoted by Alliage numéro 22 1995)
The Missing Reel, by Christopher Rawlence (Athenum Publishers, New York, 1990)
""Le Prince's Early Film Cameras"", by Simon Popple (in Photographica World, September 1993)
""Le Prince and the Lumières"", by Rod Varley (in Making of the Modern World, Science Museum, UK, 1992)
""Career of Louis Aimée Augustin Le Prince"", by E. Kilburn Scott, (in Journal of the Society of Motion Picture Engineers, US, July 1931)
Burns, Paul The History of the Discovery of Cinematography An Illustrated Chronology
""The Pioneer Work of Le Prince in Kinematography"", by E. Kilburn Scott (in The Photographic Journal #63, August 1923, pp. 373–378)
""Louis Aimée Augustin Le Prince"" by Merritt Crawford (in Cinema, 1 December 1930, pp. 28–31)
L'affaire Lumière. Du mythe à l'histoire, enquête sur les origines du cinéma by Léo Sauvage, 1985 ISBN 2-86244-045-0
Ingenious Le Prince 16-lens camera
""Louis Le Prince: the body of evidence"" by Richard Howells (in Screen vol.47 #2, Oxford University Press, 2006)
""Le Prince, inventeur et artiste, précurseur du cinema"" by Jean-Jacques Aulas and Jacques Pfend (in Revue d'Histoire du Cinéma N°32, December 2000, p. 9) ISSN 0769-0959
New research centre honours father of film
Essential Films, chapter 2, Culture Wars by Ion Martea
Roundhay Garden Scene (1888), Culture Wars by Ion Martea
Traffic Crossing Leeds Bridge (1888), Culture Wars by Ion Martea
The Indispensable Murder Book, edited by Joseph Henry Jackson (New York: The Book Society, 1951), pp. 437–464, ""The Red and White Girdle"" by Christopher Morley.  This deals with the murder of Gouffe, and shows the intense study of that trunk murder in 1889–90.

The facts concerning the life and death of LOUIS AIME AUGUSTIN LEPRINCE, pioneer of the moving pîcture and his family, by Jacques Pfend (Sarreguemines/57200/France) 2014.ISBN 9782954244198.
Why Leeds was the birthplace of film
External links[edit]



Wikimedia Commons has media related to Louis Le Prince.


Louis Le Prince at IMDb
Traffic Crossing Leeds Bridge at IMDb
L'EMPREINTE DE LOUIS AIME AUGUSTIN LEPRINCE DANS L'HISTOIRE DU CINEMA.(Université Paris Ouest, par Marie Crémaschi. sep. 2013.
Jean-Jacques Aulas et Jacques Pfend, Louis Aimé Augustin Leprince, inventeur et artiste, précurseur du cinéma
Adventures In Cybersound – extended biography by Dr Russell Naughton, RMIT University, Melbourne. Retrieved 2008-09-26
Roundhay Garden Scene on YouTube
Leeds Bridge on YouTube
Accordion Player by Louis Le Prince on YouTube a rough video from the first 17 frames
Louis Le Prince Centre for Cinema, Photography, and Television. University of Leeds. Retrieved 2008-09-26
The Legend of Louis Le Prince
Leodis – a photographic archive of Leeds. Leeds Library & Information Service. Allows search for key terms such as Louis Le Prince or Leeds Bridge or Bridge End or Hick Brothers or Auto Express (workshop site), etc.
Science Museum, London
National Science and Media Museum, Bradford
Armley Mills- Leeds Industrial Museum
Le Prince single-lens camera 1888, Science & Society Picture Library
Chronomedia year 1888 (Terramedia)
Burns, Paul The History of the Discovery of Cinematography 1885–1889 An Illustrated Chronological History
Local films for local people (BBC Bradford & West Yorkshire)
www.louisleprince.net Authority control General
ISNI
1
VIAF
1
WorldCat
National libraries
Spain
France (data)
Germany
United States
Japan
Netherlands
Other
FAST
National Archives (US)
Social Networks and Archival Context
SUDOC (France)
1





Retrieved from ""https://en.wikipedia.org/w/index.php?title=Louis_Le_Prince&oldid=1134021110""
Categories: 1841 births1890s missing person cases19th-century French peopleCinema pioneersDiscovery and invention controversiesFrench cinematographersFrench cinema pioneersFrench expatriates in the United KingdomFrench expatriates in the United StatesFrench film directorsMissing person cases in FrancePeople from MetzSilent film directorsLeeds Blue PlaquesLouis Le PrinceLeipzig University alumniHidden categories: CS1 maint: bot: original URL status unknownCS1 French-language sources (fr)Articles containing French-language textCS1: Julian–Gregorian uncertaintyArticles with short descriptionShort description is different from WikidataUse dmy dates from March 2021Articles with hCardsAll articles with unsourced statementsArticles with unsourced statements from May 2008Articles needing additional references from October 2017All articles needing additional referencesCommons category link is on WikidataIMDb ID different from WikidataArticles with ISNI identifiersArticles with VIAF identifiersArticles with WorldCat identifiersArticles with BNE identifiersArticles with BNF identifiersArticles with GND identifiersArticles with LCCN identifiersArticles with NDL identifiersArticles with NTA identifiersArticles with FAST identifiersArticles with NARA identifiersArticles with SNAC-ID identifiersArticles with SUDOC identifiersArticles containing video clipsYear of death unknown



Navigation menu



Personal tools


Not logged inTalkContributionsCreate accountLog in





Namespaces


ArticleTalk





English









Views


ReadEditView history





More

























Navigation


Main pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate




Contribute


HelpLearn to editCommunity portalRecent changesUpload file




Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageWikidata item




Print/export


Download as PDFPrintable version




In other projects


Wikimedia Commons




Languages


العربيةCatalàČeštinaDeutschEestiΕλληνικάEspañolفارسیFrançaisBahasa IndonesiaItalianoMalagasyمصرىNederlands日本語Norsk bokmålPolskiPortuguêsRomânăРусскийSimple EnglishSrpskohrvatski / српскохрватскиSuomiSvenskaУкраїнська中文
Edit links






 This page was last edited on 16 January 2023, at 16:51 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License 3.0;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Mobile view
Developers
Statistics
Cookie statement










"
https://news.ycombinator.com/rss,Scaling Bevy Development,https://bevyengine.org/news/scaling-bevy-development/,Comments,"



    Scaling Bevy Development
  
Posted on January 14, 2023 by Carter Anderson
    
    
      (
      
      
@cart


@cart_cart



cartdev

      
      )
    




The Bevy community has grown a lot over the past couple of years. We've had over 3,642 pull requests, 599 contributors, 357,917 downloads, and 21,200 github stars. Together we've built the most popular, most used Rust game engine on the market and the second most popular game engine on GitHub. Up until now, in the interest of maintaining a consistent vision and quality bar, I've kept our decision making process as small and centralized as possible. And I scaled out our leadership roles as slowly as possible. I believe this was the right call for Bevy's early days, but we are overdue for changes that bias toward more trust and development agility.
The Current State Of Things
#

I have been slowly delegating responsibility over time, and each time I placed trust in someone it yielded massive benefits to both the project and my personal well being. We now have 3 more fantastic Maintainers: Alice (@alice-i-cecile), François (@mockersf), and Rob (@superdump). And their scope has grown over time.
But even today, I must personally approve every ""controversial"" change to Bevy, where ""controversial"" is basically anything that meaningfully expands our features, user experience, or scope. With the volume of controversial changes we're seeing now, and the number of things I would still like to personally design and build, this is untenable. I have long since reached the limits of my bandwidth and the community has felt those limits for long enough.
When Bevy was younger and the community was newer, this level of conservativeness made sense. I knew what I wanted to build, how the pieces fit together, and what my capabilities were. What I didn't know as well was what everyone else wanted to build, what their capabilities were, and how that all fit into my personal vision.
I can now happily report that the situation has changed. We have proven technical experts in a variety of areas. They regularly design and build huge pieces of Bevy in a way that aligns with our collective vision for the future. Sometimes we have different ideas about what the future should look like, but we almost always reach consensus eventually. They have done outstanding technical work, built trust, and made Bevy amazing. They deserve a seat at the table.
With that preamble out of the way, I am excited to announce two major changes to the Bevy Organization.
A New Role: Subject Matter Expert
#

Subject Matter Experts (SME for short) are Bevy developers that have consistently demonstrated technical expertise and synchronized vision within a given ""subject area"" (ex: Rendering, ECS, Reflection, Animation). They must have contributed and reviewed significant pieces of Bevy within their area. These people have the ability to vote on controversial pull requests in their subject area (both code changes and RFCs). SMEs are also great people to reach out to if you have questions about a given subject area in Bevy.
If two SMEs within a given subject area approve a ""controversial"" PR, a maintainer can now merge it. We are intentionally keeping the number of SMEs within a subject area small to make establishing consensus and enforcing a consistent vision easier. For now, 2 SMEs is the bare minimum to allow voting to occur, 3 is the ""sweet spot"", and 4 will be allowed under some circumstances, but is the ""upper limit"".
As Project Lead, I can still merge controversial PRs. Consider 2 SME approvals as equivalent to a Project Lead approval. As a last line of defense for cohesion, the Project Lead maintains final say on changes. While I will by default defer to the SMEs, if they approve a PR that I firmly believe is the wrong direction for Bevy, I will still block or revert the change. Preserving consistent vision and quality is critically important to me. But I intend to bias toward trust and consensus as much as possible.
It is the job of SMEs to strive for consensus amongst themselves, the wider Bevy community, and Project Leads. They can merge controversial changes without me, but they still must do their best to anticipate my reaction to those changes (and discuss the changes with me ahead of time when that feels relevant). Likewise, I will try to establish consensus with SMEs and the wider community before making changes myself.
We are rolling out SMEs slowly so we can tweak the process over time and avoid changing too much too quickly. We've largely started with subject areas that have the most activity and the clearest subject matter experts. Join me in welcoming our initial set of SMEs!

Rendering: @superdump (Rob Swain), @robtfm (Rob Macdonald)
ECS: @BoxyUwU (Boxy), @james7132 (James Liu), @maniwani (Joy)
Reflection: @MrGVSV (Gino Valente), @jakobhellermann (Jakob Hellermann)
Animation: @james7132 (James Liu), @mockersf (François Mockers)

Hopefully there aren't too many surprises here. These people have been building fantastic things in their areas for a long time now.
We will be rolling out more subject areas (and the SMEs inside them) as SMEs prove themselves within the Bevy project and express interest. New areas are largely defined by the experts doing work inside them. We expect areas like UI, Editor, and Audio to be populated in reasonably short order.
We have also left spots open in each of the subject areas above. If you believe you meet our SME criteria for any current or proposed subject area and have interest in the role, don't hesitate to reach out to myself or any of the other maintainers. We will consider new candidates regularly. Just because you weren't included in this first batch doesn't mean we don't think you would be a good fit!
A New Maintainer
#

We also want to improve our velocity for merging uncontroversial pull requests. And there will be new maintainership load associated with facilitating the SME process. The current maintainers and I have unanimously agreed that it is time to bring on one more maintainer.
Join me in welcoming James Liu (@james7132) as our latest maintainer! James has proven themselves to be a technical expert across many of Bevy's systems (especially ECS, animation, parallelization / task scheduling, and optimization). You may have noticed that they also have the SME role for ECS and Animation. They have contributed huge volumes of code changes, provided solid reviews, are easy to work with, and have a vision for Bevy's future that aligns with ours.
A New Bevy Organization Document
#

We have a new Bevy Organization Document, which describes how the Bevy Organization will work going forward. It outlines the functionality of each role, as well as the expectations we have for them. The previously existing roles (Project Lead, Maintainer) still work the same way, but their definition and scope have been made much clearer.
The biggest changes to the organization are the new SME role and an initial description of ""role rotation"":
Roles like Project Lead, Maintainer, and SME are intentionally kept in limited supply to ensure a cohesive project vision. However these roles can be taxing, sometimes other aspects of life need to take priority, and qualified motivated people deserve a chance to lead. To resolve these issues, we plan on building in ""role rotation"". What this looks like hasn't yet been determined (as this issue hasn't come up yet and we are still in the process of scaling out our team), but we will try to appropriately balance the needs and desires of both current and future leaders, while also ensuring consistent vision and continuity for Bevy.
The Bevy People Page
#

Bevy is a community-driven project. It makes sense for the people behind Bevy and the roles they fill to be easily discoverable.
To make that happen, François (@mockersf) and I built a new Bevy People page. ""Bevy people"" can opt-in to listing their name and/or pseudonym, their social information (GitHub, Discord, Mastodon, Twitter, personal website, itch.io, etc), a sponsorship link, and a personal bio describing who they are and what they work on.
It also displays the current Bevy Organization roles these people occupy. You could call it an ""org chart"" if you wanted to, but my anti-bureaucracy reflexes prevent me from doing so. The Bevy community will always be as flat and ""people first"" as possible.
It is open to anyone (both Bevy Organization members and the wider Bevy community). If you would like to see yourself on this page, create a pull request here.
This is what it looks like!

Looking Forward
#

The next Bevy release (Bevy 0.10) is roughly a month away and I'm very excited for the changes we have in the pipeline. I can't guarantee all of these will make it in, but they're all shaping up nicely:

The New ""Stageless"" ECS Scheduler: We've fully rebuilt our scheduler to be more flexible. We no longer need ""stages"" to handle ""exclusive system scheduling"". Any system of any type can be ordered relative to any other system (even if they have exclusive access to the ECS World. Many scheduling APIs are now cleaner and easier to use. And we've taken the chance to improve related APIs like States as well.
Depth and Normal Prepass: This will give rendering feature developers access to the depth buffer and normals during the main render phase, which enables a variety of render features and optimizations.
Screen Space Ambient Occlusion: This is a popular, relatively cheap illumination technique that can make scenes look much more natural. It builds on the Depth Prepass work.
Asset Preprocessing: We're reworking our asset system to allow for pre processing assets into more efficient forms, which can make deployed games faster to load, prettier, and faster. This is a full asset system rework that improves a good portion of our asset APIs.
Windows as Entities: Windows are now ECS Entities instead of Resources, which makes them more natural to construct and query, more extensible, and opens the doors to including them in Bevy Scenes.
UI Style Split: Breaks the monolithic UI style type out into smaller pieces, embracing a less centralized and more extensible pattern.

See you in about a month!



"
https://news.ycombinator.com/rss,Boris Yeltsin's visit to a suburban Houston supermarket in 1989 (2016),http://beelineblogger.blogspot.com/2016/01/how-supermarket-visit-brought-down.html,Comments,"


















BeeLine: How A Supermarket Visit Brought Down The Soviet Union








































































BeeLine




The Shortest Route To What You Need To Know




















Scott Beeken





BeeLine



View my complete profile








































































Tuesday, January 5, 2016








How A Supermarket Visit Brought Down The Soviet Union





Many point to the fact that the Soviet Union collapse occurred as the Soviets were baited into trying to compete with the defense build-up instituted by Ronald Reagan.

The Soviets just did not have the financial resources to match the United States in defense spending while also tending to the needs of its citizenry. The Soviets spent money on guns rather than butter. Something had to give and the Soviet people were the ones that suffered.

However, a little known visit to a suburban Houston supermarket in 1989 by Boris Yeltsin appears to have been the catalyst that ended up bringing down the Soviet Union.

Yeltsin visited the Johnson Space Center in Houston in September, 1989 to tour mission control and to view a model of the planned International Space Station.

After visiting the Space Center, Yeltsin made an unplanned stop at a local Randall's grocery store that was close by before heading to the airport.

That visit changed the course of history.

At the time, Yeltsin was a newly elected member of the Soviet Parliament and the Supreme Soviet and had been a key ally of the General Secretary of the Communist Party Mikhail Gorbachev, who was initiating reforms but the pace of which was too slow for Yeltsin.

Houston Chronicle reporter Stephanie Asin was with Yeltsin on the visit to the grocery store that day.


Yeltsin, then 58, “roamed the aisles of Randall’s nodding his head in amazement,” wrote Asin. He told his fellow Russians in his entourage that if their people, who often must wait in line for most goods, saw the conditions of U.S. supermarkets, “there would be a revolution.”

“Even the Politburo doesn’t have this choice. Not even Mr. Gorbachev,” he said.

The fact that stores like these were on nearly every street corner in America amazed him. They even offered free cheese samples. According to Asin, Yeltsin didn’t leave empty-handed, as he was given a small bag of goodies to enjoy on his trip.

This is a picture of Yeltsin touring the grocery store.



Credit: Houston Chronicle


This Houston Chronicle story from 2014 fills in the rest of the story.


About a year after the Russian leader left office, a Yeltsin biographer later wrote that on the plane ride to Yeltsin’s next destination, Miami, he was despondent. He couldn’t stop thinking about the plentiful food at the grocery store and what his countrymen had to subsist on in Russia.

In Yeltsin’s own autobiography, he wrote about the experience at Randall’s, which shattered his view of communism, according to pundits. Two years later, he left the Communist Party and began making reforms to turn the economic tide in Russia. 

“When I saw those shelves crammed with hundreds, thousands of cans, cartons and goods of every possible sort, for the first time I felt quite frankly sick with despair for the Soviet people,” Yeltsin wrote. “That such a potentially super-rich country as ours has been brought to a state of such poverty! It is terrible to think of it.”

To give you some perspective on what was available in the Soviet Union at that time, here is a picture of a Russian store from that era.




Credit: Gennady Galperin/Reuters


An aide to Yeltsin later reported that in that visit to the grocery store in Houston “the last vestige of Bolshevism collapsed” inside his boss.

Two years later Yeltsin was elected to the newly created office of President of the Russian Federation after the collapse of the Soviet Union with Gorbachev.

Yeltsin immediately began dismantling the socialist economic system and introducing capitalism to the Russians. In the process he attempted to convert the world's largest command economy into a free-market one. 

The results of that transition were rocky in large part to cronyism in the break-up of many of the large state-owned businesses. In the process, many Russian oligarchs were created and Yeltsin eventually resigned his office in 1999 haunted by charges of corruption and incompetence.

His successor?

Vladimir Putin.

The falling price of oil has put a similar squeeze on Putin and the Russians today. Putin has been popular with the Russian people based on his macho style and nationalistic bombast. However, potential trouble lurks for Putin because of the Russian economy.

The Russian consumer is being squeezed with annual inflation of almost 20% and the average Russian spends about 50% of their income on food.

By comparison, the average American spends only 8% of income on groceries.

Will groceries once again determine the future of Russia?






Posted by



BeeLine




at

8:21 PM
















Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest














42 comments:




andreSeptember 14, 2017 at 4:52 AMobat viagraviagra asliReplyDeleteRepliesUnknownOctober 29, 2021 at 10:14 AMGreat Article Artificial Intelligence Projects Project Center in Chennai JavaScript Training in Chennai JavaScript Training in Chennai Project Centers in Chennai DeleteRepliesReplyReplychegekhanMarch 28, 2018 at 8:11 AMUseful Information, your blog is sharing unique information....Thanks for sharing!!! buy bakery products online south-cbuy branded food online in panganiReplyDeleteRepliesReplyUnknownSeptember 27, 2018 at 7:24 AMThank you for your post. This is excellent information. It is amazing and wonderful to visit your site.buy bakery products online south-c ReplyDeleteRepliesReplyYK AgencyDecember 25, 2018 at 11:30 AMSupermarket in Dubai Great article. Cool.ReplyDeleteRepliesReplyLuck CityFebruary 11, 2019 at 3:50 AMWithin this webpage, you'll see the page, you need to understand this data. https://digitalglobal.comReplyDeleteRepliesReplyjames brownNovember 22, 2019 at 6:45 AMAwesome blog. I enjoyed reading your articles. This is truly a read for me. I have bookmarked it and I am looking forward to reading new articles. Keep up the good work!Kroger customer surveyReplyDeleteRepliesReplyKroger experienceDecember 10, 2019 at 8:05 AMPlease share more like that.Kroger experienceReplyDeleteRepliesReplyAnonymousDecember 30, 2019 at 7:47 PMGreat article! Yeltsin revealed as a realist! I never knew this ...ReplyDeleteRepliesReplyDavid Grant Stewart, Sr., EgyptologistJanuary 1, 2020 at 3:17 PMWho paid you to write this drivel? You are either incredibly gullible or on the take from the USSR propaganda machine. You look and say the Soviet civilian economy is bad. There is no civilian economy in the USSR. The country does not have a war machine. The country is a war machine.ReplyDeleteRepliesReplyNeha UppalJanuary 9, 2020 at 3:33 AMThanks for sharing such beautiful information with us. I hope you will share some more information about best grocery shopping app. Please keep sharing.ReplyDeleteRepliesReplyjames brownFebruary 13, 2020 at 1:15 PMHey There. I found your blog using This is a very well written article. I’ll be sure to bookmark it and come back to read more of your useful info. Thanks for the post. I’ll definitely return.https://krogerexperiencee.com/ReplyDeleteRepliesReplyjames brownFebruary 18, 2020 at 7:24 AMGreat post I would like to thank you for the efforts you have made in writing this interesting and knowledgeable article.https://tellthebelll.usReplyDeleteRepliesReplysurvey monkey usaFebruary 19, 2020 at 7:45 AMGreat things you’ve always with us. Just keep writing this kind of posts.The time which was wasted in traveling for tuition now it can be used for studies.Thankssurvey monkey usaReplyDeleteRepliesReplydanielwilsonnFebruary 19, 2020 at 3:43 PM Thank you again for all the knowledge u distribute,Good post. I was very interested in the article, it's quite inspiring I should admit. I like visiting you site since I always come across interesting articles like this one.Great Job, I greatly appreciate that.Do Keep sharing! Regards,https://krogerfeeedback.us/ReplyDeleteRepliesReplydahliaApril 23, 2020 at 3:21 AM Thanks for sharing this information. I really like your post very much. You have really shared an informative and interesting post with people  TellTheBell ReplyDeleteRepliesReplypatronsurveysApril 24, 2020 at 8:23 AMAfter Kroger and Walmart, I prefer to go to Tesco supermarket. Do you know what? there is quality in products with an extraordinary service tescoviews com offers a platform to complete the Tesco customer satisfaction survey to win £1000 Gift Card & 25 Club Points. Survey site is giving a lot of store surveys at one place to complete.ReplyDeleteRepliesReplyjames brownJuly 14, 2020 at 3:24 AMThis comment has been removed by the author.ReplyDeleteRepliesReplyjames brownJuly 14, 2020 at 3:25 AMIts a great pleasure reading your post.Its full of information I am looking for and I love to post a comment that ""The content of your post is awesome"" Great work.https://krogerexperiencee.com/greatpeople-me-kroger-employee-login-portal/ReplyDeleteRepliesReplyNFL FanSeptember 20, 2020 at 4:46 PMThe official source for NFL news, video highlights, fantasy football, game-day coverage, schedules, stats, scores and more. Ravens FootballReplyDeleteRepliesReplytellthebellSeptember 22, 2020 at 11:42 AM Excellent website you have  so much cool information!..tellthebellReplyDeleteRepliesReplydgcustomerfirstJune 25, 2021 at 6:49 AMTo take an interest in the Dollar general super market survey, it is important to arrange a few things at one of its Branches initially. Visit the authority survey site of Dollar General survey at Dgcustomerfirst.Com and Win A $100 gift voucher. Then, at that point, you need to save the receipt of the store. Then, at that point, go to the authority survey site of dollar general. The Dg survey is accessible in both English or Spanish.ReplyDeleteRepliesReplymybkexperienceAugust 3, 2021 at 2:30 AMDollar general survey is an online platform that collects customers' most recent shopping experiences and overall customer satisfaction. Participate in the survey and be the lucky person to get enlisted in dg customer first winners. ReplyDeleteRepliesReplyarnavharperAugust 30, 2021 at 12:33 PMRemote for Fire TV is designed specifically to control Fire TV, Fire TV Cube and Fire TV Stick. Just connect mobile device and a TV or media player to the Firestick Remote.ReplyDeleteRepliesReplySEOOctober 12, 2021 at 3:37 AMdelta international recruitment agency in pakistanReplyDeleteRepliesReplyHealthandBeautyTipsNovember 6, 2021 at 5:24 PMEmployees can Perform Kroger E-Schedule Login at the Feed Kroger Login Portal once their Schedule Credentials are verified. If you are unable to sign in to Kroger Login then you need to contact the branch manager.Feed KrogerReplyDeleteRepliesReplyUnknownJanuary 27, 2022 at 5:48 AMThe NASA dark Brant IX sounding rocket conveyed the payload to an apogee of 177 miles prior to plunging by parachute and arriving at White Sands. Dgcustomerfirst.com Survey ReplyDeleteRepliesReplydgcustomerfirst.comMay 19, 2022 at 3:35 AMThe dgcustomerfirst.com criticism review permits customers to enter the Dollar General Sweepstakes of Cash $100 in the wake of finishing the overview.dgcustomerfirstwin.shop Survey  ReplyDeleteRepliesReplywww.DGCustomerFirst.comJune 6, 2022 at 10:50 AMDollar general survey is an online platform that collects customers' most recent shopping experiences and overall customer satisfaction. https://idgcustomerfirst.org/ReplyDeleteRepliesReplydgcustomerfirsts.shopJune 10, 2022 at 6:07 AMAlso, you have a superb opportunity to partake in the client criticism overview. DGCustomerFirst 2022 or Dollar General Survey is a study led by Dollar General's authorities for all United States inhabitants.dgcustomerfirsts executed a connected with the WWW review to take an arrangement about dgcustomerfirsts Helpline notwithstanding your support level thereafter visiting service  Click here dgcustomerfirsts ReplyDeleteRepliesReplySteveJune 16, 2022 at 9:58 AMGreat Article, it was very informative. That was such thought-provoking content. I enjoyed reading your content. Every week, I look forward to your column. In my opinion, this one is one of the best articles you have written so far.How to Change Instagram PasswordChange Windows 10 PasswordSubwaylistensHome Depot SurveyDQFanFeedback.com ReplyDeleteRepliesReplyUmairJune 30, 2022 at 6:44 AMirescopk.comReplyDeleteRepliesReplyDgcustomerfirstscom.shopJuly 9, 2022 at 6:51 AMFormerly referred to as J L Turner, Dollar General has numerous subsidiaries viz Dollar General Market, Dollar General Financial, Dollar General Global Sourcing, and lots extra.dgcustomerfirstscom executed a connected with the WWW review to take an arrangement about dgcustomerfirstscom Helpline notwithstanding your support level thereafter visiting service  Click here dgcustomerfirstscom ReplyDeleteRepliesReplyAnonymousJuly 19, 2022 at 1:35 PMLiveTheOrangeLife – Official Portal www.LiveTheOrangeLife.comWalmartOne Login - Walmartone.com Login Guidemyaccountaccess.comonevanillaJCP Associate KioskReplyDeleteRepliesReplySmith AdomJuly 30, 2022 at 12:03 PMTalkToWendys executed a connected with the WWW review to take an arrangement about TalkToWendys Helpline notwithstanding your support level thereafter visiting service  Click here TalkToWendys  It is mandatory to make a purchase at Wendy’s once before being a participant in this survey.ReplyDeleteRepliesReplyInformTarget.comAugust 10, 2022 at 3:38 AMRules are guidelines, and they are set to be accompanied. If you want to participate within the survey effectively, you need to adhere to the rules and policies set apart via the informtarget.Com remarks survey.informtargets executed a connected with the WWW review to take an arrangement about informtargets Helpline notwithstanding your support level thereafter visiting service  Click here informtargets.shop ReplyDeleteRepliesReplyTellBaskinRobbinsAugust 19, 2022 at 5:39 AMThere are some basic rules and requirements of this Baskin Robbins Customer Satisfaction Survey which I even have furnished in this newsletter. tellbaskinrobbins executed a connected with the WWW review to take an arrangement about tellbaskinrobbins Helpline notwithstanding your support level thereafter visiting service  Click here tellbaskinrobbins ReplyDeleteRepliesReplyTalktofoodlionAugust 20, 2022 at 2:37 AMTalktofoodlion The company's full name is general Dollar, and it is offering a $100 incentive to customers who take the time to participate in this little survey. visit here Talktofoodlion ReplyDeleteRepliesReplyFaiz IsrailiAugust 21, 2022 at 5:02 AMThe procedure is requesting information from individuals using a questionnaire, which may be completed offline or online. New technologies, however, are frequently disseminated via digital channels like social media, email, QR codes, or URLs. dgcustomerfirstReplyDeleteRepliesReplyTellBaskinRobbinsSeptember 8, 2022 at 7:11 AMTellBaskinRobbins After responding to the feedback questions, participants are expected to rate their experience shopping at Baskin Robbins.The feedback survey is sponsored by Baskin Robbins in order to better understand its service quality TellBaskinRobbinsReplyDeleteRepliesReplyNikithaOctober 8, 2022 at 1:31 AMmybkexperience customer satisfaction survey which is an online platform to get timely feedback from their customers about the food and services. This can improve their services according to customer’s needs and at the same time, rewards their customer for their time and loyalty towards the restaurant. So you can answer the questions and eat delicious food for free at the same time. Now you might be wondering about how to participate in the survey or what are the requirements and much more.ReplyDeleteRepliesReplySEOOctober 14, 2022 at 5:59 AMtop recruitment agencies in pakistan for saudi arabiaReplyDeleteRepliesReplyAdd commentLoad more...























Newer Post


Older Post

Home




Subscribe to:
Post Comments (Atom)















BeeLine Email Subscription

Get new posts by email:  Subscribe




Follow @BeeLineBlog




Followers











Blog Archive








        ► 
      



2023

(5)





        ► 
      



January

(5)









        ► 
      



2022

(139)





        ► 
      



December

(11)







        ► 
      



November

(10)







        ► 
      



October

(12)







        ► 
      



September

(12)







        ► 
      



August

(13)







        ► 
      



July

(11)







        ► 
      



June

(12)







        ► 
      



May

(12)







        ► 
      



April

(12)







        ► 
      



March

(12)







        ► 
      



February

(10)







        ► 
      



January

(12)









        ► 
      



2021

(131)





        ► 
      



December

(15)







        ► 
      



November

(12)







        ► 
      



October

(9)







        ► 
      



September

(13)







        ► 
      



August

(14)







        ► 
      



July

(11)







        ► 
      



June

(10)







        ► 
      



May

(6)







        ► 
      



April

(10)







        ► 
      



March

(11)







        ► 
      



February

(7)







        ► 
      



January

(13)









        ► 
      



2020

(154)





        ► 
      



December

(11)







        ► 
      



November

(12)







        ► 
      



October

(14)







        ► 
      



September

(11)







        ► 
      



August

(12)







        ► 
      



July

(13)







        ► 
      



June

(14)







        ► 
      



May

(12)







        ► 
      



April

(16)







        ► 
      



March

(16)







        ► 
      



February

(10)







        ► 
      



January

(13)









        ► 
      



2019

(145)





        ► 
      



December

(14)







        ► 
      



November

(11)







        ► 
      



October

(9)







        ► 
      



September

(12)







        ► 
      



August

(13)







        ► 
      



July

(12)







        ► 
      



June

(12)







        ► 
      



May

(14)







        ► 
      



April

(13)







        ► 
      



March

(12)







        ► 
      



February

(10)







        ► 
      



January

(13)









        ► 
      



2018

(139)





        ► 
      



December

(11)







        ► 
      



November

(10)







        ► 
      



October

(10)







        ► 
      



September

(10)







        ► 
      



August

(12)







        ► 
      



July

(13)







        ► 
      



June

(12)







        ► 
      



May

(12)







        ► 
      



April

(13)







        ► 
      



March

(12)







        ► 
      



February

(9)







        ► 
      



January

(15)









        ► 
      



2017

(132)





        ► 
      



December

(9)







        ► 
      



November

(10)







        ► 
      



October

(15)







        ► 
      



September

(9)







        ► 
      



August

(13)







        ► 
      



July

(12)







        ► 
      



June

(9)







        ► 
      



May

(13)







        ► 
      



April

(12)







        ► 
      



March

(10)







        ► 
      



February

(10)







        ► 
      



January

(10)









        ▼ 
      



2016

(119)





        ► 
      



December

(11)







        ► 
      



November

(15)







        ► 
      



October

(15)







        ► 
      



September

(10)







        ► 
      



August

(2)







        ► 
      



July

(9)







        ► 
      



June

(11)







        ► 
      



May

(6)







        ► 
      



April

(9)







        ► 
      



March

(11)







        ► 
      



February

(12)







        ▼ 
      



January

(8)

In the Middle of 5th Avenue
Risky Business
That's for the Birds
An Inconvenient Truth +10
Citizen Cruz
Context on Guns
How A Supermarket Visit Brought Down The Soviet Union
A Humble Servant?










        ► 
      



2015

(71)





        ► 
      



December

(9)







        ► 
      



November

(5)







        ► 
      



October

(2)







        ► 
      



September

(2)







        ► 
      



August

(9)







        ► 
      



July

(7)







        ► 
      



June

(6)







        ► 
      



May

(3)







        ► 
      



April

(10)







        ► 
      



March

(6)







        ► 
      



February

(4)







        ► 
      



January

(8)









        ► 
      



2014

(88)





        ► 
      



December

(5)







        ► 
      



November

(7)







        ► 
      



October

(10)







        ► 
      



September

(9)







        ► 
      



August

(5)







        ► 
      



July

(8)







        ► 
      



June

(7)







        ► 
      



May

(9)







        ► 
      



April

(7)







        ► 
      



March

(6)







        ► 
      



February

(5)







        ► 
      



January

(10)









        ► 
      



2013

(115)





        ► 
      



December

(8)







        ► 
      



November

(5)







        ► 
      



October

(13)







        ► 
      



September

(9)







        ► 
      



August

(7)







        ► 
      



July

(10)







        ► 
      



June

(10)







        ► 
      



May

(9)







        ► 
      



April

(13)







        ► 
      



March

(10)







        ► 
      



February

(7)







        ► 
      



January

(14)









        ► 
      



2012

(139)





        ► 
      



December

(6)







        ► 
      



November

(16)







        ► 
      



October

(16)







        ► 
      



September

(9)







        ► 
      



August

(14)







        ► 
      



July

(13)







        ► 
      



June

(8)







        ► 
      



May

(11)







        ► 
      



April

(9)







        ► 
      



March

(9)







        ► 
      



February

(11)







        ► 
      



January

(17)









        ► 
      



2011

(188)





        ► 
      



December

(12)







        ► 
      



November

(10)







        ► 
      



October

(10)







        ► 
      



September

(10)







        ► 
      



August

(17)







        ► 
      



July

(18)







        ► 
      



June

(11)







        ► 
      



May

(3)







        ► 
      



April

(16)







        ► 
      



March

(19)







        ► 
      



February

(25)







        ► 
      



January

(37)









About Me





BeeLine


Scott Beeken has practiced as an attorney, CPA and has been an officer with two Fortune 500 companies overseeing diverse functions such as Taxation, Employee Benefits, Human Resources, Real Estate Facilities, Risk Management, Corporate Communications, Marketing and Advertising. In addition to writing BeeLine, he is a Keynote Speaker, Author  and Strategic Consultant.

View my complete profile




























Total Pageviews

























Theme images by luoman. Powered by Blogger.
























"
https://news.ycombinator.com/rss,Show HN: Cross-Platform GitHub Action,https://github.com/marketplace/actions/cross-platform-action,Comments,"





Marketplace
Actions
Cross Platform Action






play-circle





GitHub Action
Cross Platform Action




v0.9.0
Latest version







    Use latest version
 









play-circle






Cross Platform Action
Provides cross platform runner


Installation
Copy and paste the following snippet into your .yml file.












- name: Cross Platform Action
  uses: cross-platform-actions/action@v0.9.0



          Learn more about this action in cross-platform-actions/action












Choose a version







v0.9.0

                Cross Platform Action 0.9.0
              

 




v0.8.0

                Cross Platform Action 0.8.0
              

 




v0.7.0

                Cross Platform Action 0.7.0
              

 




v0.6.2

                Cross Platform Action 0.6.2
              

 




v0.6.1

                Cross Platform Action 0.6.1
              

 




v0.6.0

                Cross Platform Action 0.6.0
              

 




v0.5.0

                Cross Platform Action 0.5.0
              

 




v0.4.0

                Cross Platform Action 0.4.0
              

 




v0.3.1

                Cross Platform Action 0.3.1
              

 




v0.3.0

                Cross Platform Action 0.3.0
              

 








Cross-Platform GitHub Action
This project provides a GitHub action for running GitHub Action workflows on
multiple platforms. This includes platforms that GitHub Actions doesn't
currently natively support.
Features
Some of the features that are supported include:

Multiple operating system with one single action
Multiple versions of each operating system
Allows to use default shell or Bash shell
Low boot overhead
Fast execution

Usage
Here's a sample workflow file which will setup a matrix resulting in four jobs.
One which will run on FreeBSD 13.1, one which runs OpenBSD 7.2, one which runs
NetBSD 9.2 and one which runs OpenBSD 7.2 on ARM64.
name: CI

on: [push]

jobs:
  test:
    runs-on: ${{ matrix.os.host }}
    strategy:
      matrix:
        os:
          - name: freebsd
            architecture: x86-64
            version: '13.1'
            host: macos-12

          - name: openbsd
            architecture: x86-64
            version: '7.2'
            host: macos-12

          - name: openbsd
            architecture: arm64
            version: '7.2'
            host: ubuntu-latest

          - name: netbsd
            architecture: x86-64
            version: '9.2'
            host: ubuntu-latest

    steps:
      - uses: actions/checkout@v2

      - name: Test on ${{ matrix.os.name }}
        uses: cross-platform-actions/action@v0.9.0
        env:
          MY_ENV1: MY_ENV1
          MY_ENV2: MY_ENV2
        with:
          environment_variables: MY_ENV1 MY_ENV2
          operating_system: ${{ matrix.os.name }}
          architecture: ${{ matrix.os.architecture }}
          version: ${{ matrix.os.version }}
          shell: bash
          run: |
            uname -a
            echo $SHELL
            pwd
            ls -lah
            whoami
            env | sort
Different platforms need to run on different runners, see the
Runners section below.
Inputs
This section lists the available inputs for the action.



Input
Required
Default Value
Description




run
✓
✗
Runs command-line programs using the operating system's shell. This will be executed inside the virtual machine.


operating_system
✓
✗
The type of operating system to run the job on. See Supported Platforms.


version
✓
✗
The version of the operating system to use. See Supported Platforms.


shell
✗
default
The shell to use to execute the commands. Defaults to the default shell for the given operating system. Allowed values are: default, sh and bash


environment_variables
✗
""""
A list of environment variables to forward to the virtual machine. The list should be separated with spaces.



All inputs are expected to be strings. It's important that especially the
version is explicitly specified as a string, using single or double quotes.
Otherwise YAML might interpet the value as a numeric value instead of a string.
This might lead to some unexpected behavior. If the version is specified as
version: 13.0, YAML will interpet 13.0 as a floating point number, drop the
fraction part (because 13 and 13.0 are the same) and the GitHub action will
only see 13 instead of 13.0. The solution is to explicitly state that a
string is required by using quotes: version: '13.0'.
Supported Platforms
This sections lists the currently supported platforms by operating system. Each
operating system will list which versions are supported.
OpenBSD (openbsd)



Version
x86-64
arm64




7.2
✓
✓


7.1
✓
✓


6.9
✓
✓


6.8
✓
✗



FreeBSD (freebsd)



Version
x86-64




13.1
✓


13.0
✓


12.4
✓


12.2
✓



NetBSD (netbsd)



Version
x86-64




9.2
✓



Runners
This section list the different combinations of platforms and on which runners
they can run.



Runner
OpenBSD
FreeBSD
NetBSD




Linux
✓
✓
✓


macos-10.15, macos-11, macos-12
✓
✓
✗



Under the Hood
GitHub Actions currently only support the following platforms: macOS, Linux and
Windows. To be able to run other platforms, this GitHub action runs the
commands inside a virtual machine (VM). If the host platform is macOS the
hypervisor can take advantage of nested virtualization.
The FreeBSD and OpenBSD VMs run on the xhyve hypervisor (on a macOS
host), while the other platforms run on the QEMU hypervisor (on a Linux
host). xhyve is built on top of Apple's Hypervisor
framework. The Hypervisor framework allows to implement hypervisors with
support for hardware acceleration without the need for kernel extensions. xhyve
is a lightweight hypervisor that boots the guest operating systems quickly and
requires no dependencies outside of what's provided by the system. QEMU is a
more general purpose hypervisor that runs on most host platforms and supports
most guest systems. It's a bit slower than xhyve because it's general purpose
and it cannot use nested virtualization on the Linux hosts provided by GitHub.
The VM images running inside the hypervisor are built using Packer.
It's a tool for automatically creating VM images, installing the guest
operating system and doing any final provisioning.
The GitHub action uses SSH to communicate and execute commands inside the VM.
It uses rsync to share files between the guest VM and the host. xhyve
does not have any native support for sharing files. To authenticate the SSH
connection a unique key pair is used. This pair is generated each time the
action is run. The public key is added to the VM image and the private key is
stored on the host. Since xhyve does not support file sharing, a secondar hard
drive, which is backed by a file, is created. The public key is stored on this
hard drive, which is then mounted by the VM. At boot time, the secondary hard
drive will be identified and the public key will be copied to the appropriate
location.
To reduce the time it takes for the GitHub action to start executing the
commands specified by the user, it aims to boot the guest operating systems as
fast as possible. This is achieved in a couple of ways:


By downloading resources, like the hypervisor and a few other
tools, instead of installing them through a package manager


No compression is used for the resources that are downloaded. The size is
small enough anyway and it's faster to download the uncompressed data than
it is to download compressed data and then uncompress it.


It leverages async/await to perform tasks asynchronously. Like
downloading the VM image and other resources at the same time


It performs as much as possible of the setup ahead of time when the VM image
is provisioned


Local Development
Prerequisites

NodeJS
npm
git

Instructions


Install the above prerequisites


Clone the repository by running:
git clone https://github.com/cross-platform-actions/action



Navigate to the newly cloned repository: cd action


Install the dependencies by running: npm install


Run any of the below npm commands


npm Commands
The following npm commands are available:

build - Build the GitHub action
format - Reformat the code
lint - Lint the code
package - Package the GitHub action for distribution and end to end testing
test - Run unit tests
all - Will run all of the above commands

Running End to End Tests
The end to end tests can be run locally by running it through Act. By
default, resources and VM images will be downloaded from github.com. By running
a local HTTP server it's possible to point the GitHub action to local resources.
Prerequisites

Docker
Act

Instructions


Install the above prerequisites


Copy test/workflows/ci.yml.example to
test/workflows/ci.yml


Make any changes you like to test/workflows/ci.yml, this is file ignored by
Git


Build the GitHub action by running: npm run build


Package the GitHub action by running: npm run package


Run the GitHub action by running: act --privileged -W test/workflows


Providing Resources Locally
The GitHub action includes a development dependency on a HTTP server. The
test/http directory contains a skeleton of a directory structure
which matches the URLs that the GitHub action uses to download resources. All
files within the test/http are ignore by Git.


Add resources as necessary to the test/http directory


In one shell, run the following command to start the HTTP server:
./node_modules/http-server/bin/http-server test/http -a 127.0.0.1

The -a flag configures the HTTP server to only listen for incoming
connections from localhost, no external computers will be able to connect.


In another shell, run the GitHub action by running:
act --privileged -W test/workflows --env CPA_RESOURCE_URL=<url>

Where <url> is the URL inside Docker that points to localhost of the host
machine, for macOS, this is http://host.docker.internal:8080. By default,
the HTTP server is listening on port 8080.






Stars

 


          Star
 53
  





Contributors



 

 


Categories


  Continuous integration


  Testing




Links



cross-platform-actions/action
    



Open issues
        1




Pull requests
      1




Report abuse
 

Cross Platform Action is not certified by GitHub. It is provided by a third-party and is governed by separate terms of service, privacy policy, and support documentation.
    




"
https://news.ycombinator.com/rss,Microsoft returns to the Altair,https://hackaday.com/2023/01/15/microsoft-returns-to-the-altair/,Comments,"


Microsoft Returns To The Altair


                18 Comments            

by:
Al Williams



January 15, 2023















Title:


Copy

Short Link:


Copy






The Altair 8800 arguably launched Microsoft. Now [Dave Glover] from Microsoft offers an emulated and potentially cloud-based Altair emulation with CP/M and Microsoft Basic. You can see a video of the project below. One thing that makes it a bit odd compared to other Altair clones we’ve seen is that the emulator runs in a Docker environment and is fully cloud-enabled. You can interact with it via a PCB front panel, or a terminal running in a web browser.
The core emulator is MIT-licensed and seems like it would run nearly everywhere. We were a little surprised there wasn’t an instance in the Azure cloud that you could spin up to test drive. Surely a few hundred Altairs running at once wouldn’t even make a dent in a modern CPU.

There are plenty of Altair emulators and even replicas with authentic CPUs out there. But we have to admit the Wiki documentation on this one is uncommonly well done. Even if you don’t want to use this emulator, you might find the collection of data about the Altair useful.
Don’t know how to use a computer front panel? Learn on the Altair or a PDP/8, even if you don’t have a real one. For simulated hardware, the project that turns an Arduino Due into an Altair works well. If you just want to play Zork, you can do that in your browser, for sure.





 



















Posted in RetrocomputingTagged altair 8800 


Post navigation

← A Number Maze For Younger HackersA Flex Sensor For A Glove Controller Using An LDR → 






            18 thoughts on “Microsoft Returns To The Altair”        





paulvdh says: 



							January 15, 2023 at 1:24 am						




@05:42 “I don’t have the source code for CP/M, I’m not even sure it exists anymore.
https://hackaday.com/2014/10/06/cpm-source-code-released/
This video is apparently only made as entertainment, just some guys blabbing and not caring about facts, That may be all right for some, it makes me wonder what else is off and  I lost interest.


Report comment 
Reply 





MG says: 



							January 15, 2023 at 1:51 am						




Even just off the top of my head, the fact that it’s MIT-licensed is nothing special; the entire core of MAME and a significant number of machine drivers are all MIT/3-clause BSD, which includes the Intel 8080A core that one would need to spin up an Altair 8800 emulator as well.


Report comment 
Reply 





stappers says: 



							January 15, 2023 at 3:59 am						




Thanks for the click bait warning.


Report comment 
Reply 





me says: 



							January 16, 2023 at 2:33 am						




fortunately i read here, so i re-read witho more attention the initial post, so I avoid to lose some minutes of my life looking at another-not-wanted-at-all-cliclbite video. Thanks yo you, dude!


Report comment 
Reply 







Joshua says: 



							January 15, 2023 at 1:58 pm						




What’s also kind of forgotten:
CP/M was thoroughly understood.
It was cloned and derived a dozen times, if not more.
People who worked with assembly language could literally “read” how CP/M worked and flowed through the system.
My father, among many others, wrote his own floppy controller routines for CP/M.
It’s kind of sad to see how incompetent the matter is being handled these days. Such a blunder. *sigh*
Back in the 70s and 80s, it was just natural to know about the internals of the Z-80 and/or CP/M. 
There were hundreds of books being written about all the deepest mechanisms of them.


Report comment 
Reply 





bruceeifler@gmail.com says: 



							January 15, 2023 at 6:15 pm						




“How to program the z80.  Roger Zaks


Report comment 
Reply 





Mr Name Required says: 



							January 15, 2023 at 11:14 pm						




Rodnay Zaks, with the ‘a’. A well-thumbed book for me.


Report comment 
Reply 











daveboltman says: 



							January 15, 2023 at 4:50 am						




Has Bill Gates really got over himself now?
https://en.wikipedia.org/wiki/An_Open_Letter_to_Hobbyists


Report comment 
Reply 





Ken says: 



							January 15, 2023 at 7:37 am						




You must not have been involved in the hobby “back in the day” – the copying of commercial products like Microsoft BASIC was rampant, almost celebrated in the community.
This was before Microsoft had even started licensing ROM BASIC to the major home/personal computer manufacturers, so stolen paper tapes were a serious hit to MS revenues.


Report comment 
Reply 





stappers says: 



							January 15, 2023 at 7:53 am						




And the problem of lack of wallet-voting still exists.  Might be because of the same word for libre and gratuit.
Where I do agree with https://justforfunnoreally.dev/  I also do agree with young Bill Gates that people should do some kind of payment for what they want.


Report comment 
Reply 







Joshua says: 



							January 15, 2023 at 1:48 pm						




Out of context. 
I’m no expert of American copyright law, but as far as I understand:
Material released before 1977 was considered Public Domain, unless the copyright is/was explicitly expressed.
I assume this covers material/works of engineers, students, universities etc. which didn’t mention the copyright in their papers. 
Go double check yourself, if you wish. I’m lazy right now. 😁


Report comment 
Reply 





The Commenter Formerly Known As Ren says: 



							January 15, 2023 at 4:29 pm						




No, he hasn’t, he is now a James Bond level super villain.


Report comment 
Reply 







Sok Puppette says: 



							January 15, 2023 at 6:39 am						




If it cannot play tunes on an AM radio, it is not a satisfying Altair emulator


Report comment 
Reply 





Ken says: 



							January 15, 2023 at 8:13 am						




Neat project, I too am impressed by the level of documentation. While I’m not too interested in running weather/climate change analysis programs on an emulated 50 year-old system, I realize it was important to inject some cloud activity to jazz up the project.
Interesting to note that you need to install Linux on a windows machine to run the code.
Neat how the sense hat reduces the need for ‘blinkin lights’ to its most basic elements, like the computer prop in the bat cave or any 1960s sci-fi movie/tv show…


Report comment 
Reply 





paulvdh says: 



							January 15, 2023 at 9:25 am						




At 5:42 he says he does not know whether the CP/M source code still exists…
He clearly has not checked Hackaday lately…
https://hackaday.com/2014/10/06/cpm-source-code-released/


Report comment 
Reply 





Joshua says: 



							January 15, 2023 at 1:51 pm						




Not just CP/M, also MP/M, the power user version of CP/M. The real thing, so to say. :)


Report comment 
Reply 







SayWhat? says: 



							January 15, 2023 at 12:16 pm						




I remember hacking “protected” MS Basic programs back in the day. All it took was finding the one byte in memory  that set the code to be not listable LOL.


Report comment 
Reply 





demon256 says: 



							January 15, 2023 at 3:45 pm						




Heh, my first computer ran CP/M.


Report comment 
Reply 



Leave a Reply					Cancel reply










Please be kind and respectful to help make the comments section excellent. (Comment Policy)This site uses Akismet to reduce spam. Learn how your comment data is processed.







Search

Search for:



Never miss a hack
Follow on facebook
Follow on twitter
Follow on youtube
Follow on rss
Contact us
Subscribe










If you missed it







AI-Controlled Twitch V-Tuber Has More Followers Than You


 39 Comments				








Ask Hackaday: What’s Your Worst Repair Win?


 173 Comments				








The Surprisingly Simple Way To Steal Cryptocurrency


 65 Comments				








Excuse Me, Your Tie Is Unzipped


 101 Comments				








All About USB-C: Resistors And Emarkers


 16 Comments				



More from this category





Our Columns







2022 FPV Contest: Congratulations To The Winners!


 2 Comments				








Machining With Electricity Hack Chat


 6 Comments				








Hackaday Links: January 15, 2023


 16 Comments				








Too Many Pixels


 37 Comments				








Hackaday Podcast 201: Faking A Transmission, Making Nuclear Fuel, And A Slidepot With A Twist


 2 Comments				



More from this category

"
https://news.ycombinator.com/rss,Show HN: Otterkit – COBOL compiler for .NET,https://github.com/otterkit/otterkit,Comments,"








otterkit

/

otterkit

Public




 

Notifications



 

Fork
    1




 


          Star
 64
  









        Otterkit COBOL Compiler
      





otterkit.com


License





     Apache-2.0 license
    






64
          stars
 



1
          fork
 



 


          Star

  





 

Notifications












Code







Issues
1






Pull requests
0






Discussions







Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Discussions
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







otterkit/otterkit









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








4
branches





0
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




KTSnowy

[Update]: Added SourceId, LevelStack and FileIndex comments




        …
      




        2365003
      

Jan 16, 2023





[Update]: Added SourceId, LevelStack and FileIndex comments


2365003



Git stats







175

                      commits
                    







Files

Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github/ISSUE_TEMPLATE


 


 









Assets


 


 









OtterkitTemplatePack


 


 









libotterkit @ a0256a7


 


 









src


 


 









.gitattributes


 


 









.gitignore


 


 









.gitmodules


 


 









LICENSE


 


 









README.md


 


 









SECURITY.md


 


 









THIRD-PARTY-LICENSES


 


 




    View code
 
















 Otterkit COBOL Compiler
About
Installation
Quick Install
Build from Source
Standard Acknowledgement





README.md




 Otterkit COBOL Compiler
Otterkit is a free and open source compiler for the COBOL Programming Language on the .NET platform.
Warning: The project is currently in pre-release, so not all of the standard has been implemented.
About
COBOL was created in 1959 by the CODASYL Committee (With Rear Admiral Grace Hopper as a technical consultant to the committee), its design follows Grace Hopper's belief that programs should be written in a language that is close to English. It prioritizes readability, reliability, and long-term maintenance. The language has been implemented throughout the decades on many platforms with many dialects, and the Otterkit COBOL compiler is a free and open source implementation of the ISO COBOL 2022 Standard on the .NET platform.
Installation
Quick Install
Otterkit is available to install on the Nuget package manager (.NET 7 is required). To install, type into the command line:
dotnet tool install --global Otterkit --version 1.0.15-alpha

Build from Source
First, clone the git repo from https://github.com/otterkit/otterkit.git to get the source code. To access the libotterkit submodule inside, use the --recurse-submodules --remote-submodules flag on the clone command. To run, navigate into the src folder (for the compiler, not libotterkit) and then type dotnet run into the command line.
Standard Acknowledgement
Any organization interested in reproducing the COBOL standard and specifications in whole or in part,
using ideas from this document as the basis for an instruction manual or for any other purpose, is free
to do so. However, all such organizations are requested to reproduce the following acknowledgment
paragraphs in their entirety as part of the preface to any such publication (any organization using a
short passage from this document, such as in a book review, is requested to mention ""COBOL"" in
acknowledgment of the source, but need not quote the acknowledgment):
COBOL is an industry language and is not the property of any company or group of companies, or of any
organization or group of organizations.
No warranty, expressed or implied, is made by any contributor or by the CODASYL COBOL Committee
as to the accuracy and functioning of the programming system and language. Moreover, no
responsibility is assumed by any contributor, or by the committee, in connection therewith.
The authors and copyright holders of the copyrighted materials used herein:

FLOW-MATIC® (trademark of Sperry Rand Corporation), Programming for the 'UNIVAC® I and
II, Data Automation Systems copyrighted 1958,1959, by Sperry Rand Corporation;
IBM Commercial Translator Form No F 28-8013, copyrighted 1959 by IBM;
FACT, DSI 27A5260-2760, copyrighted 1960 by Minneapolis-Honeywell

Have specifically authorized the use of this material in whole or in part, in the COBOL specifications.
Such authorization extends to the reproduction and use of COBOL specifications in programming
manuals or similar publications.









About

      Otterkit COBOL Compiler
    





otterkit.com


Topics



  compiler


  dotnet


  cobol



Resources





      Readme
 
License





     Apache-2.0 license
    

Security policy





      Security policy
    



Stars





64
    stars

Watchers





3
    watching

Forks





1
    fork







    Releases

No releases published






    Contributors 3








KTSnowy
Gabriel Gonçalves

 






TriAttack238
Sean Vo

 






gabrielesilinic
Gabriele Silingardi

 





Languages










C#
100.0%











"
https://news.ycombinator.com/rss,Elliptic Curves: The Great Mystery,https://www.cantorsparadise.com/elliptic-curves-the-great-mystery-61599a93c61d,Comments,"Published inCantor’s ParadiseKasper MüllerFollowJan 14·11 min read·Member-onlySaveElliptic Curves: The Great MysteryA surprisingly beautiful blend of algebra, geometry, and number theoryImage from Wikimedia CommonsThese curves defined by very simple equations are shrouded in mystery and elegance. In fact, the equations describing them are so simple that even high-school students would be able to understand them.However, a ton of simple questions about them remain unsolved despite tenacious efforts by some of the greatest mathematicians in the world. But that’s not all. As you will soon see, this theory connects various important fields of mathematics because it turns out that elliptic curves are more than just plane curves!Let’s grab a cup of coffee and start from the beginning.Introduction and MotivationIn mathematics, we often solve problems by stating them in a different setting than they originally occurred in. An example of this is that some geometric problems can be turned into algebraic problems and vice versa.A classical problem going back thousands of years is whether a positive integer n is the area of some right triangle with rational side lengths, that is such that the lengths of all three sides are expressible as fractions of whole numbers. In this case, n is called a congruent number). For instance, 6 is a congruent number because it is the area of the right triangle with side lengths 3, 4 and 5.In 1640, Fermat famously proved that 1 is not a congruent number. He did so using his famous method of proof by infinite descent.As the amazing mathematician, Keith Conrad notes about the result:This leads to a weird proof that √2 is irrational. If √2 were rational then √2, √2 and 2 would be the sides of a rational right triangle with area 1. This is a contradiction of 1 not being a congruent number!Since Fermat’s proof, the hunt for proving or disproving that numbers are congruent has been ongoing.Amazingly, one can show by elementary methods that for each triple of rational numbers (a, b, c) such that a² + b² = c² and 1/2 ab = n, we can find two rational numbers x and y such that y² = x³ - n²x and y ≠ 0 and conversely for each rational pair (x, y) such that y² = x³ - n²x and y ≠ 0, we can find three rational numbers a, b, c such that a² + b² = c² and 1/2 ab = n.That is, right triangles with area n correspond exactly to rational solutions to the equation y² = x³ - n²x with y ≠ 0 and vice versa. A mathematician would say that there is a bijection between the two sets.Therefore, a rational number n > 0 is congruent if and only if the equation y² = x³ - n²x has a rational solution (x, y) with y ≠ 0. For example, since 1 is not congruent, the only rational solutions to y² = x² - x have y = 0.For the interested reader, the exact correspondence is the following.If we try this correspondence on the triangle with side lengths 3, 4, and 5, and with area 6, then the corresponding solution is (x, y) = (12, 36).To me, this is absolutely amazing. One starts with a problem in number theory and geometry and through algebra, transforms it into a problem about rational points on plane curves!The equation y² = x³ - n²x is an example of an elliptic curve.Elliptic CurvesIn general, if f(x) denotes a third-degree polynomial with a non-zero discriminant (i.e. all the roots are distinct), then y² = f(x) describes an elliptic curve except for one important addition to this object, namely what is called a “point at infinity”.Basically, a point at infinity is a point where parallel lines can meet. It is out of the scope of this article to go into projective geometry to proper define it but this is a wonderful and exciting subject that I strongly encourage you to look up.Now, by a minor algebraic miracle, it turns out that we can make a suitable (rational) change of coordinates, and get a new curve on the form y² = x³ + ax + b such that rational points on the two curves are in one-to-one correspondence. The second transformed curve however is typically easier to work with.Because of this, we sometimes assume that an elliptic curve is on this form and so from now on we will assume that too, that is, when we say “elliptic curve”, we mean a curve on the form y² = x³ + ax + b together with a point 𝒪 at infinity.Throughout this article, unless otherwise stated, we’ll assume that the coefficients a and b are rational numbers.Elliptic curves take on two typical shapes which are graphed below.Image from Wikimedia CommonsHowever, if we consider x and y as complex variables, the curves will look entirely different. In fact, they will then take the form of a complex torus or doughnut!So why do we study elliptic curves and what can we do with them?First of all, many number theoretic problems can be translated into problems about Diophantine equations, secondly, it turns out that elliptic curves are related to discrete geometric objects called lattices and deeply connected to some very important objects called modular forms which are certain extremely symmetric complex functions with a lot of number theoretic information in them.Actually, the connection between elliptic curves and modular forms turned out to be the key to proving Fermat’s Last Theorem which Andrew Wiles achieved in the 90s through several years of intensive work on this connection.The story about this quest and the proof of the Theorem is in my opinion one of the most beautiful pursuits in all of science - unfortunately, as pointed out by my friend Kenneth Nielsen, the margin in this Medium post is too narrow to contain it!So I guess I’ll have to write another article.Elliptic curves are also used in cryptography to encrypt messages and online transactions.The most important feature of them, however, is the mind-blowing fact that they are more than just curves and more than just geometry. In fact, they have an algebraic structure on them called an Abelian group structure with respect to a cool geometric operation - a kind of geometric addition rule for how to add points on the curve together.If you don’t know what an Abelian group is, you can think about it as a set of objects with an operation defined on them such that they have the same kind of structure as the integers with respect to addition (except they can be finite).More specifically, for a group with the operation *, it needs to be stable with respect to the operation (i.e. if a is in the group and b is in the group, then a * b is in the group), there is an identity element e (0 for the integers) such that a * e = a for all elements a in the group, and for each element a, there is an inverse element c, such that when you operate them together you get back the identity element (a * c = e). Furthermore, the group operation has to be associative i.e. a * (b * c) = (a * b) * c. That is, it doesn’t matter which elements you add together first. If the commutative law holds ( i.e. a * b = b * a) then the group is called an Abelian group.Examples of Abelian groups are:The integers ℤ with respect to the operation +.The action of rotating a square clockwise by 90 degreesVector spaces with vectors as elements and vector addition as the operationThe fancy terminology for a curve with an Abelian group structure is Abelian variety.What is so amazing about elliptic curves is that we can define an operation (let’s denote it ⊕) between rational points on them (that is, both the x and y coordinates are rational numbers) such that the set of those points on the curve becomes an Abelian group with respect to the operation ⊕ and with identity element 𝒪 (the point at infinity).Let’s define the operation.If you take two rational points on the curve (for example P and Q) and consider a line through them, then the line intersects the curve at another rational point (possibly the point at infinity). Let’s call this point -R.Now, because the curve is symmetric about the x-axis, we get another rational point R when we reflect -R about it. There is a drawing of this below.Addition rule for elliptic curves - image from Wikimedia CommonsThis reflected point (R in the above image) is the addition of the two aforementioned points (P and Q above). We can write P ⊕ Q = R.One can prove (and this is actually not easy) that this operation is associative, which is really surprising, at least to me. Also, the point at infinity acts as a (unique) identity for this operation and each point has an inverse point (which is just the point you get by reflecting about the x-axis). It is also Abelian (i.e. P ⊕ Q = Q ⊕ P).The MysteryIt turns out that two different elliptic curves can have vastly different groups associated with them. An important invariant that in some sense is the most defining feature is what is known as the rank of the curve (or group).A curve can have a finite or an infinite number of rational points on them. This can be hard to handle, so what we are interested in, is how many points we need in order to generate all the others by the aforementioned addition rule. These generators are called basis points.The rank is a dimensionality measure like the dimension of a vector space and indicates how many independent basis points (on the curve) have infinite order (i.e. we can keep adding it without getting our starting point back). If the curve only contains a finite amount of rational points on it, then the rank is zero. There is still a group but it is finite.Calculating the rank of an elliptic curve is notoriously difficult but we have a nice result due to Mordell which tells us that the rank is always finite. That is, we only need a finite amount of basis points in order to generate all the rational points on the curve.One of the most important and interesting problems in number theory is called the Birch and Swinnerton-Dyer Conjecture and it is all about the rank of elliptic curves. In fact, it is so difficult and important that it is one of the Millenium Problems.You actually get a million dollars if you solve it!Finding rational points on elliptic curves with rational coefficients is hard. One way to approach this is by reducing the curve modulo p where p is a prime number. This means that instead of considering the rational solution set of the equation y² = x³ + ax + b, we consider the rational solution set of the congruence y² ≡ x³ + ax + b (mod p) where for this to make sense we might have to clear denominators by multiplying by an integer on both sides.So we are considering two numbers with the same remainder when divided by p to be equal in this new space. The great thing about this is that now there are only a finite number of things to check. Let’s denote the number of rational solutions to such a reduced curve modulo p, by Np.In the early 1960s, Peter Swinnerton-Dyer used the EDSAC-2 computer (not exactly a Macbook!) at the University of Cambridge Computer Laboratory to calculate the number of points modulo p on elliptic curves with known rank. He worked together with the mathematician Bryan John Birch in understanding elliptic curves and after the computer had crunched a bunch of products of the formfor growing x, they got the following output taken from data associated with the curve E: y² = x³ − 5x (as an example). I should note that the x-axis is really log log x and the y-axis is log y.The curve in question is y² = x³ − 5x. This is a curve of rank 1 and one of the curves originally looked at by Birch and Swinnerton-Dyer.Now, I am a mathematician and not a statistician but even I can see a clear trend and it does seem that the regression line has slope 1 on this plot.The curve E has rank 1 and when they tried different curves of varying ranks, they found the same pattern every time. The slope of the fitted regression line, it seemed, was always equal to the rank of the curve in question.More precisely, they stated the bold conjecture thatHere C is some constant.This computer crunching adventure combined with a great deal of far-sightedness led them to make a general conjecture about the behavior of a curve’s Hasse–Weil L-function L(E, s) at s = 1.This L-function is defined as follows. Letand let the discriminant of the curve be denoted Δ. Then we can define the L-function associated with E as the following Euler productWe view this as a function of the complex variable s.Their conjecture now has the form:Conjecture (Birch and Swinnerton-Dyer): Let E be any elliptic curve over ℚ. The rank of the abelian group E(ℚ) of rational points of the curve E is equal to the order of the zero of L(E, s) at s = 1.The reason why it was quite far-sighted is that, at the time, they didn’t even know if a so-called analytic continuation existed for all such L-functions. The problem was that L(E, s) as defined above only converges when Re(s) > 3/2.That they can all be evaluated at s = 1 by analytic continuation was first proved in 2001 again by using the close connection to modular forms that Andrew Wiles proved.Sometimes the conjecture is stated using the Taylor expansion of the L-function, but it is saying the same thing in a different way. The field of rational numbers can be replaced by a more general field but I didn’t want to go into more abstraction than necessary.The subject of Elliptic Curves is a beautiful dance between number theory, abstract algebra and geometry. There is a lot more to say about them than what I have sketched here, but I hope that you got a feeling or a glimpse of something that one could call astounding.We reached the end of this article…If you have any questions, comments or concerns, then please reach out.If you like to read articles like this one on Medium, you can get a membership for full access: simply click here.Thanks for reading.MathematicsMathScienceTechnologyHistory----6More from Cantor’s ParadiseFollowMedium’s #1 Math PublicationRead more from Cantor’s ParadiseRecommended from MediumKasper MüllerinCantor’s ParadiseSums of Odd Numbers Have Interesting PropertiesJesus NajerainCantor’s ParadiseMandelbulb: Three Dimensional FractalsCristian Miguel LunaGeometric DistributionKasper MüllerinCantor’s ParadiseThe Insect That Discovered the Prime NumbersEliran TurgemaninMath SimplifiedMultiply Numbers By Drawing LinesKasper MüllerinCantor’s ParadiseThe Beauty of MathematicsNishesh GogiainAnalytics VidhyaTHE STORY OF LOGISTIC REGRESSION CONTINUES…Eliran TurgemaninCantor’s ParadiseCity Planning Using Graph TheoryAboutHelpTermsPrivacyGet the Medium app"
https://news.ycombinator.com/rss,One man’s two-decade quest to let the ‘Irish Giant’ rest in peace,https://www.theguardian.com/culture/2023/jan/14/he-did-not-want-this-one-mans-two-decade-quest-to-let-the-irish-giant-rest-in-peace,Comments,"Museums‘He did not want this’: one man’s two-decade quest to let the ‘Irish Giant’ rest in peaceResearchers spurred by injustice explain why 18th century Irish man famed for his exceptional height deserves burial he wanted The skeleton of Charles Byrne (centre) on display at the Hunterian Museum in London before a six-year revamp which will see his remains removed to storage. Photograph: David Gee/AlamyThe skeleton of Charles Byrne (centre) on display at the Hunterian Museum in London before a six-year revamp which will see his remains removed to storage. Photograph: David Gee/AlamyAlexandra ToppingSat 14 Jan 2023 03.00 ESTLast modified on Mon 16 Jan 2023 09.41 ESTThomas Muinzer recalls the day when, as a bored student in Belfast learning about property law, a few sentences about the 18th century “Irish Giant” Charles Byrne caught his eye.“I saw a footnote about a celebrity Irish giant from present day Northern Ireland whose remains were stolen on the way to his funeral – querying whether or not that was the theft of property, because it was a dead body,” he says.Muinzer learned that for more than 200 years Byrne’s skeleton – despite his fervent and explicit wishes – had been on display at the Hunterian Museum at Lincoln’s Inn Fields, central London, and became “convinced that something very unfortunate had been done to his remains and his posthumous memory”.It was a the start of a two decades long obsession, as Muinzer, now co-Director of Aberdeen University’s Centre for Energy Law, teamed up with leading medical ethicist Prof Len Doyal, and other campaigners including the late Dame Hilary Mantel, to request Byrne’s remains to be removed from public display.This week, they finally achieved that goal. The Royal College of Surgeons, which runs the museum, announced that when it reopens after a six-year revamp this spring, Byrne’s remains will no longer be displayed but will be kept in storage and be available for “bona fide medical research”.A 1784 posthumous sketch of Charles Byrne, known as the Irish Giant, depicted with Edinburgh notables. Photograph: AlamyAt least 2.3 metres (7ft 7in) in stature, Byrne made a living exhibiting himself in the years before his death, aged 22. But historical records reveal he was horrified by the idea that after his death his body would be put on display, says Doyal, emeritus professor of medical ethics at Queen Mary University in London, who co-authored a paper about Byrne with Muinzer in 2011.“Everything that has happened, to this point, went against Byrne’s explicit wishes,” he says. “There was no question that Byrne did not want this to happen. And it did.”Born in County Derry in 1761, Byrne, who had acromegaly (overgrowth of the bones) and gigantism, set off for London in his late teens. “He was not just, as it were, a fairground celebrity,” says Doyal, who added that Byrne was seen as a gentleman giant. “He mixed with quite well known and wealthy people.”When he died in 1783 a newspaper of the time noted that “a whole tribe of surgeons put in a claim for the poor departed Irishman surrounding his house just as harpooners would an enormous whale”. Before Byrne could be buried, Hunter reportedly bribed one of his friends to secretly swap the corpse for dead weight and bring the body to him. Four years later Hunter put Byrne’s skeleton on display.Byrne’s popularity in fashionable London society is remembered in this cartoon The Surprising Irish Giant of St James’s Street from 1785. Photograph: AlamyThe way Byrne’s remains were obtained was “absolutely wrong”, says Dawn Kemp, Director of Museums at the Royal College of Surgeons, but the argument about what should now happen was not black and white.Kemp says the college’s decision to retain the skeleton should not be considered “definitive”, but says since 1799 its trustees had been legally bound to preserve the collection of John Hunter – the pioneering Scottish surgeon and anatomist who the museum is named after – in its entirety. She also argues that, in 2023, it cannot be predicted how the remains could be useful to medical research in the future.Stephen Fry calls for return of Parthenon marbles to AthensRead moreLike the debate over the Parthenon marbles in the British Museum, the museum has reluctantly found itself at the frontline of a cultural battle which has little taste for measured debate. Kemp believes some people “think that we’re being curatorial cowards and that this is the dumbing down of museums”, while others had called the decision to retain Byrne’s remains and not carry out his last wishes of being buried at sea “evil”, she says.“I don’t want to be part of this move on social media to polarise the debate, because I think it’s nuanced, [and] it’s really important,” she says. “The wrong has been done to Byrne in 1783, we’re not going to right it by making a quick decision now.”Doyal and Muinzer argue that DNA from the skeleton has already been obtained, and they suspect the museum would allow medical students to see the skeleton in private. Kemp insists this is not the case and says a new programme of talks called Hunterian Provocations will explore issues around the display of human remains and the acquisition of specimens during British colonial expansion.The late Queen on a visit to Hunterian Museum in London in 1962 views the skeleton of Charles Bryne, acquired by a collector by subterfuge, researchers say. Photograph: PA Media“There’s no need for the Hunterian to keep this body,” says Doyal. “Byrne’s original wish was to be buried at sea. That’s what he wanted, that’s what he should get.”Muinzer recalls that when he first saw Byrne’s remains in 2011 it was alongside a quotation from the contemporary diarist Sylas Neville calling Byrne an ill-bred, disagreeable beast, while Hunter’s involvement had been “air-brushed” out of the story. Now, perhaps, there is a chance for the Irish Giant to be seen in a new light. “His is a remarkable story that captures the imagination,” Muinzer says. “And there may be more to come.” This article was amended on 16 January 2023. A speaker referred to “a celebrity Irish giant from present day Northern Ireland”, but in the writing process, “present day” was omitted from the quote; it has been restored.TopicsMuseumsHistory of scienceMedical researchParthenon marblesIrelandNorthern IrelandHeritagefeaturesReuse this contentMost viewedMost viewed"
https://news.ycombinator.com/rss,"Show HN: Vento, a screen recorder that lets you rewind and record over mistakes",https://vento.so,Comments,"ventoNew RecordingLog inStress-free Screen Recording Constantly restarting your screen recordings? With Vento, just pause, rewind, and carry on instead - keeping calm helps too :)Install Chrome ExtensionStart Recording“To record great videos, one must first rewind.” - Gandhi, probably.Video Not Availablevento© 2023 Vento. All rights reserved.Say hello! We don’t bite. Well, maybe one of us does.hello@vento.so"
https://news.ycombinator.com/rss,Project Mage is an effort to build a power-user environment in Common Lisp,https://project-mage.org,Comments,"




Project Mage













Project Mage
Home
About
Campaign
Code
Contact
RSS









nil


Project Mage: The Elevator Pitch →



Hi, and welcome! Project Mage is an effort to build a power-user environment and a set of applications in Common Lisp. For a comprehensive discussion, see The Power of Structure. But to get a brief idea of what this project is about, you just might want to check out Project Mage: The Elevator Pitch first. Otherwise, the essays listed below may be read in any order.








Project Mage: The Elevator Pitch (17 January 2023) The Power of Structure (12 January 2023) Emacs is Not Enough (12 January 2023) On Flexibility and Software Temples (12 January 2023) Overcoming the Print-Statement (12 January 2023) Data-Supplied Web as a Model of Free Web (12 January 2023) Isn't It Obvious That C Programmers Wrote Git? (12 January 2023) Epilogue (12 January 2023) [Appendix] Why Common Lisp for This Project (12 January 2023) [Appendix] All Else is Not Enough (12 January 2023)







...proudly created, delivered and presented to you by: some-mthfka. Mthfka!



"
https://news.ycombinator.com/rss,Memory Safety Approaches Speed Up and Slow Down Development Velocity,https://verdagon.dev/blog/when-to-use-memory-safe-part-2,Comments,"







How Memory Safety Approaches Speed Up and Slow Down Development Velocity





































Languages ∩ Architecture


RSS
Github




            Sponsor us on GitHub!
          
r/Vale
Twitter
Discord








How Memory Safety Approaches Speed Up and Slow Down Development Velocity
Part 2 of the Memory Safety Expedition

Jan 16, 2023
 — 
Evan Ovadia
 — 
Sponsor us on GitHub!




Every March, developers worldwide each try to make a roguelike game in less than 168 hours as part of the 7DRL Challenge. Roguelike games are hard, involving procedural level generation, pathfinding, line-of-sight algorithms, and more dijkstra maps than you can imagine.









Most people don't survive the week. 0




Years of 7DRL challenges has taught me that development velocity is the most important thing to optimize for. It doesn't matter how perfect your code is if it doesn't make it into the hands of the players in time.




A few weeks ago I wrote the first part of our little expedition, which explored the various memory safety approaches. Today, let's talk about how they might help or harm development velocity!




You might learn something new, like:



Modern non-memory-safe languages can use recent techniques to zero in on memory errors so quickly, that they can be fast to develop in.


Reference counting has a hidden superpower that can help track down logic problems.


Borrow checking can sometimes help and sometimes hurt development velocity, even after the initial learning curve.





 Why it's important



Developer velocity is important. Not just for delivering a complete game in 7 days, but in a lot of every day software engineering situations too:



A startup needs to get a working product into the hands of its users before the investment money dries up.


One needs developer velocity to be able to respond to unforeseeable events, such as market opportunities or new regulatory requirements. 1


A company needs to reduce costs to stay in business, including reducing development costs.





Some napkin math to illustrate that last one:



Google used 15.5 terawatt hours of electricity in 2020, most which went to data centers. We'll conservatively assume rather expensive electricity ($0.199/kwh for CA). That comes out to $3.085 billion. 


Google has 27,169 software engineers. Some are higher, but let's conservatively use the entry level average yearly compensation which is currently $178,751. That comes out to $4.856 billion. 2



As you can see, software development can be much more expensive than power usage, so it makes sense to primarily optimize for development velocity. 3




 The Language Factor

The choice of language, and its memory safety approaches, is a big factor in development velocity.




There are generally four approaches to memory safety:



Manual memory management (MMM), like in C, Ada, Zig, Odin, etc.


Borrow checking, like in Rust, Cone, Cyclone, etc.


Garbage collection (GC), like in Java, Go, Python, Javascript, etc. 4


Reference counting (RC), like in Swift, Nim, Lobster, etc.





There's also a fifth approach, generational references with regions which we'll talk about elsewhere; this series is comparing the more traditional approaches.




But by the end of this, you'll have a much better idea of which is best for a particular situation, whether it be a game development challenge, web server, or anything else.




 What Actually Is Developer Velocity?



Development velocity is a nebulous concept, but I would say it's how fast we can expand, modify, and maintain our codebase to deliver value to the user and not cause too much collateral damage.




The ""deliver value to the user"" is very important here. It means doing something that will help the user in a specific way. Without that clear goal, we can get caught up in making our features absolutely perfect according to some arbitrary artistic criteria. As anyone who has launched a product can tell you, it's better to have two solid, tested, flexible features instead of one absolutely perfect one.




The ""too much"" is also important. There is often a tradeoff between moving fast and keeping things absolutely correct. A big part of software engineering is weighing the value of new features against the risk and relative severity of any bugs that might slip into production for a while.




With that in mind, let's see how the various approaches do!




 Manual Memory Management (MMM) can be slow



People that come to C from higher-level languages often think that they should code like Java but manually inserting malloc and free calls. Let's call this style ""naive C"".




One can become pretty skilled in mentally tracking where these calls should go, but the approach tends to fall apart after a while: it isn't resilient to changes and refactoring. It's common to accidentally change the code and break some previous assumptions, resulting in a memory bug.




Memory bugs are notoriously difficult to solve. Naive C doesn't have great development velocity.




 It can also be fast

...and that's why experienced C developers don't really use this ""naive C"" style of coding.




Instead, they:



Use safer practices which drastically reduce memory problems, such as Architected MMM where we use arrays and arenas instead of malloc and free, always copying in/out of tagged unions instead of pointing into them, and so on.


Use static analysis to enforce safer practices, such as SPARK.


Use things like Address Sanitizer and Valgrind to detect buffer overflows, dangling pointers, memory leaks, and double-frees.


Use special allocators that don't reuse memory, to better detect problems.


If using C++, using RAII to automatically prevent double-free and memory leaks at compile-time.





With these, developer velocity can be stellar even with an MMM language. Modern MMM languages even include these kinds of mechanisms, such as Zig's Release-Safe mode.




There is a slight cost to these memory approaches. Address Sanitizer increases run-time by 73%. This makes it almost as slow as garbage collection. 5




However, that doesn't matter if we can just enable it in debug mode to get the error detection improvements for developer velocity and turn them off in release mode, just like assertions. As we talked about in Part 1, we wouldn't want this for a public-facing server, but it's a great tradeoff for many games and settings like webapps and mobile apps where any security issues can be sandboxed away. 6




Google Earth proved that this strategy can work well. In an average Google Earth quarter, only 3-5% of bug reports were traceable to memory safety problems, and Address Sanitizer made them trivial to reproduce in development mode.




Another developer velocity benefit from MMM languages is that they aim to be as simple as possible to keep compile times low. Languages like C, Zig, and Odin tend to compile much faster than more complex languages like Scala and Rust. This greatly helps developer velocity. 7 On top of that, Zig's simplicity helps its ability to hot-reload code changes which could make it one of the fastest native languages to develop in.





How Memory Safety Approaches Speed Up and Slow Down Development Velocity


 Why it's important


 The Language Factor


 What Actually Is Developer Velocity?


 Manual Memory Management (MMM) can be slow


 It can also be fast


 Future MMM Improvements


 Borrow Checking




 Drawback: Prototyping and Iterating


 Benefit: Concurrency


 Drawback: Unsupported Patterns


 Benefit: Top-Down, Flattened Architectures


 Drawback: Leaky Abstractions and API Stability




 Borrow Checking and Beyond


 Garbage Collection


 GC'd code can be more correct


 GC can be slow in specific domains


 Garbage Collection's Future


 Reference Counting


 Conclusions




Side Notes
(interesting tangential thoughts)




Notes [–]
Notes [+]
0
1
2
3
4
5
6
7



0


By this I mean, most people don't have a finished game by the end. But they still celebrate with the rest of us, after an honorable struggle!




1


We also need developer velocity to develop fast enough to compensate for product managers that wildly underestimate how long it will take to make something.




2


Just to keep it simple, let's not include benefits or bonuses, which make it even higher.




3


This of course varies by company and team; a startup will have much higher development costs, and a company like Oracle would probably have more server costs.




4


By ""garbage collection"" I'm specifically referring to tracing garbage collection.




5


 From TheNewStack:



Java: 1.85x run time


Go: 2.83x run time


Haskell: 3.55x run time







6


This works particularly well for apps that only talk to a trusted first-party server, which includes most apps. It doesn't work as well for programs where clients indirectly send each other data, such as multiplayer first person shooter games.




7


I can speak from experience; every time I have a project that takes more than five seconds to compile, I tend to get distracted by Reddit or something shiny.











 Future MMM Improvements

There are a few approaches on the horizon that improve upon MMM even more.




To address more of the memory unsafety in MMM languages, CHERI detects any memory safety problems at run-time using capabilities.




From Microsoft Security Response Center:


We’ve assessed the theoretical impact of CHERI on all the memory safety vulnerabilities we received in 2019, and concluded that in its current state, and combined with other mitigations, it would have deterministically mitigated at least two thirds of all those issues.




In other words, CHERI can reduce memory-unsafety related slowdowns by two thirds, which is pretty incredible. AMD CPUs are even starting to have hardware support for it, bringing its run-time overhead down to 6.8%.




That remaining one third is solved by Vale which takes this approach even further in the form of its generational references. By building it into the language itself, it can know exactly when it does or doesnt need to perform a check, and by adding native region support it can theoretically skip the vast majority of them, bringing its run-time overhead even closer to zero, within the noise of any C or C++ program. With that kind of speed, we can leave the protections enabled even in release mode.










 Borrow Checking

Borrow checking is another mechanism that can be added to a low-level language to protect against memory problems, similar to SPARK. Rust is the main language with borrow checkers, though there are some newer languages that are designing borrow checkers with better development velocity.




Its main advantages are that:



It detects memory safety bugs, like the aforementioned 3-5% of Google Earth bugs, at compile time. That means that instead of getting 80 bug reports, they might have only 77 if they used Rust.


The borrow checker saves time when using concurrency.


It tends to default us into architectures that, in certain situations, help us develop faster.





We know that Rust often has better developer velocity than naive C, but that's not surprising. Naive C is an easy comparison, since it doesn't have the strong static type system or generics that almost every modern GC'd and MMM 8 language has. So how does borrow checking fare against more modern languages and practices?




Unfortunately, borrow checking has some considerable developer velocity drawbacks which make it much slower than languages with garbage collection, and sometimes even slower than MMM. 9




I'll link to some quotes and examples (green to emphasize that they're just anecdotes) to complete the picture. Note that these aren't data, just everyday people's experiences. 10 Keep in mind that these are mostly comparing borrow checking to garbage collection.




From Matt Welsh:


Rust has made the decision that safety is more important than developer productivity. This is the right tradeoff to make in many situations — like building code in an OS kernel, or for memory-constrained embedded systems — but I don’t think it’s the right tradeoff in all cases, especially not in startups where velocity is crucial.




In the words of another software architect:


It gives me such a great feeling of safety and control over your process, but at the end of the day it can be very tedious. The real killer is when I'm trying to prototype. If I don't already know what my interfaces are going to look like, Rust really slows me down. Compile times don't help here either.




We often write it off as just a learning curve problem, but it's apparently true even for more experienced rustaceans.




These weaknesses are can be subtle, and often don't appear in small projects when it's still easy to change a program's architecture. They start to reveal themselves more in larger projects, as a project starts scaling across multiple developers. As one user says, ""Six weeks into the whole thing and people spend more time un-breaking builds than writing code.""




Note that this doesn't mean borrow checking is a bad thing. It just means that to get its benefits, you'll have to pay some development velocity costs.




We don't often hear about the underlying reasons, 11 but knowledge of language design and software architecture helps us identify some of the root causes:



The borrow checker adds extra constraints that are usually unnecessary in other paradigms 12 which causes artificial complexity and extra refactoring, which slows developer velocity.


Forced coupling, which means that refactors and changes in one area will unnecessarily cause refactors and changes in neighboring code.


It doesn't support a lot of the patterns which enable better decoupling and looser architectures.


The borrow checker makes mutability inherently leaky, causing it to leak through abstractions.





These are rather tricky concepts to grasp, so let's explore these a little more.







Notes [–]
Notes [+]
8
9
10
11
12



8


Zig has comptime and Odin has parametric polymorphism.




9


Keep in mind that a lot of Rust's developer velocity (especially compared to C) doesn't come from borrow checking, but from having a much stronger static type system and generics, which GC languages and most newer MMM languages also have nowadays.




10


I would love to see an actual experiment measuring the actual developer velocity between different languages. I'm not sure how they would factor out the learning curve costs, but it's probably possible.




11


And there's a surprising lack of articles covering the tradeoffs of Rust online. I'm not sure why this is the case.




12


 Some folks think that the borrow checker's rules are inherent to programming and should be followed in any paradigm. This is false. Aliasability-xor-mutability is just a rough approximation of the real rule, dereference-xor-destroyed. Shared mutability doesn't always cause memory unsafety; dereferencing destroyed data is the real danger.












 Drawback: Prototyping and Iterating



Some say that Rust is terrible language for prototyping, it makes draft coding very difficult, and that other languages are much faster to change.




This is largely because it imposes extra constraints compared to other paradigms. Universally, the more constraints and rules you add to a problem, the more rigid the solution space is. When you want to make a change, you can't just do the simplest change, you need to find a change that also satisfies the extra constraints of the borrow checker.




One user says, ""Rust’s complexity regularly slows things down when you’re working on system design/architecture, and regularly makes things faster when you’re implementing pieces within a solid design (but if it’s not solid, it may just grind you to a total halt).""




Borrow checking tends to make refactoring more extensive because of the borrow checker's forced coupling: as a codebase's components become more interconnected and coupled, then changes in one will require changes in another. In borrow checking, every function is much more coupled to its callers and callees by mutability constraints.




From Using Rust at a startup: A cautionary tale: 13


What really bites is when you need to change the type signature of a load-bearing interface and find yourself spending hours changing every place where the type is used only to see if your initial stab at something is feasible. And then redoing all of that work when you realize you need to change it again.




Another user says, ""In general I find refactoring to be a far more excruciating process in Rust than in other languages."" 14




It's also a little worse because borrow checker also requires you to satisfy its constraints up-front for every single iteration of your code, long before it actually matters. In the words of one user, it ""forces me to put the cart before the horse"". Most code goes through multiple iterations before it's complete, 15 so we're paying this cost more often than other paradigms.




Once you're done prototyping, it can be difficult to change your program, like moving through molasses. Refactoring can be a massive pain, especially if you change the way you store some data.




And as one user says, ""In theory ... it's possible to avoid the escape hatches if you are careful to structure access to your data juuuuust right. That works out fine if you're the only person working on an infrequently-changing program with a small model that you understand very well. If everyone needs to hook into the first two or three function call levels of the main loop, and if your model is anything but small, that doesn't work out anymore.""




Of course, these users' experiences aren't universal, it might depend greatly on the domain. Stateless programs like command line tools or low-abstraction domains like embedded programming will have less friction with the borrow checker than domains with a lot of interconnected state like apps, stateful programs, or complex turn-based games. This is probably why some people find the borrow checker to be slower to work with, while some people think it's just fine.







Notes [–]
Notes [+]
13
14
15



13


Some interesting discussion on this quote.




14


The user continues on to say ""It is also far more satisfying."" There's nothing quite like the feeling of being done with a refactor, especially when it improves one's code.




15


Some people are brilliant and can get everything completely right on the first try, but alas, I am not one of them, and I haven't met one either.











 Benefit: Concurrency

The borrow checker may cause slower development velocity when prototyping and iteration, but it also helps in some ways if our program is using concurrency.




The borrow checker helps protect against data races. A data race is when: 16



Two or more threads [or async tasks] concurrently accessing a location of memory.


One or more of them is a write.


One or more of them is unsynchronized.





The reading CPU will get a partial, inconsistent view of the data because the writing CPU hasn't finished writing to it yet. These bugs are very difficult to detect, because they depend on the scheduling of the threads, which is effectively random.




These bugs can take days to track down, slowing developer velocity for programs that use concurrency.




Most languages, including C, Java, and Swift, offer no protections against data races. Go offers partial protection against data races by encouraging and defaulting to message passing, but one can still suffer the occasional data race.




Rust uses the borrow checker to protects us from data races at compile time, by isolating one thread's memory from another thread's memory, and offering standard library tools that let us safely share data between threads, such as Mutex<T>.




Not all programs use (or should use) concurrency, but if your program does, the borrow checker may improve your developer velocity in these areas of your program.







Notes [–]
Notes [+]
16



16


From The Rustonomicon.











 Drawback: Unsupported Patterns



The borrow checker is a very effective static analysis mechanism, but it still tends to be incompatible with a lot of simple, useful, and safe patterns:



Graphs; it generally only thrives with a strict tree hierarchy of data.


It has a lot of trouble with observers and any callback-based code.


Back-references; you cannot get from a child to a parent.


Dependency references, such as a HomeViewController's reference to a NetworkRequester.


Intrusive data structures.


Many forms of RAII 17 plus higher RAII.


Delegates, like in iOS GUI widgets.





All of these are generally impossible within the rules of borrow checker. There are workarounds for some, but they have their own complexities and limitations.




When the easiest solution is one of these, then one has to spend extra time to find workarounds or an entire different approach that satisfies the borrow checker. This is artificial complexity, and can slow down development velocity. For example, I once saw someone who brought in an entire Rust framework (Yew) as a dependency because they couldn't make an observer work within the borrow checker.




Luckily, one can avoid some of these problems by leaning more heavily on workarounds like Rc or RefCell, though many in the Rust community say that these are a last resort, avoided, and should be refactored out whenever possible, but these workarounds can help avoid forced coupling and help improve developer velocity if used well.







Notes [–]
Notes [+]
17



17


RAII is about automatically affecting the world outside our object. To do that, the borrow checker often requires us to take a &mut parameter or return a value, but we can't change drop's signature. To see this in action, try to make a handle that automatically removes something from a central collection.











 Benefit: Top-Down, Flattened Architectures

The borrow checker often influences us into a top-down architecture, which can help us maintain assumptions and invariants in our programs.




In short, a top-down architecture is where you organize your program's functions into a tree (or a directed acyclic graph), such that a parent can call a child, but a child cannot call a parent.




This is a very subtle but powerful effect for your program. Explaining it would take an entire three articles on its own, but check out this video by Brian Will where he talks about the benefits of this kind of ""procedural"" style. 18




GC'd languages like Pony, and functional languages like Haskell and Clojure also have this benefit. In Rust it's a natural side-effect of (or rather, requirement for) its memory safety approach.




The borrow checker also influences us toward a ""flatter"" organization, where all of our program's long-term state is held in central collections, reminiscent of a relational database. This leads naturally into certain architectures like ECS. This architecture works pretty well for a lot of programs.




Note that it can also be a bad fit for some programs. More complex turn-based games, such as roguelikes, are better with other architectures. ECS isn't as flexible or extensible in those situations.




 Drawback: Leaky Abstractions and API Stability

There are occasional features in programming languages that are inherently leaky, in that they tend to leak through abstractions.




An example that illustrates the concept, unrelated to memory safety, is async/await.


If foo() calls an async function bar(), then we need to make foo itself async as well, and a lot of foo's callers as well. It's usually doable... unless we need to change a function signature that overrides a method on an trait. When a feature forces us to change a trait method's signature, it is an inherently leaky feature.


Leakiness can make refactors much more widespread, and can even lead to an architectural deadlock when we run into an trait method we can't change, such as one in a public API.




In a similar way, the borrow checker is also inherently leaky. For example, when our method expects a &mut reference to some data, it imposes a global constraint that nobody else has a shared reference to that data. 19 This often manifests as a &mut in our callers and all of their callers. This ""caller-infectious requirement"" makes borrow checking inherently leaky, and can be a particularly thorny problem when it runs into classes that have fixed interfaces, where there is no way to pass extra data through them.




I'll repeat Matt Welsh's quote, since it applies here as well:


What really bites is when you need to change the type signature of a load-bearing interface and find yourself spending hours changing every place where the type is used only to see if your initial stab at something is feasible. And then redoing all of that work when you realize you need to change it again.




When this occurs, one user says you can do ""one of three things: 1) Refactor into an unrecognizable mess. 2) Add a lot of RwLock or RefCell. 3) Abandon the project. The issue is that to maintain the same interface, it gets to be so hacky that maintence becomes borderline impossible.""




Leaky features directly conflict with the main benefits of decoupling and abstraction, which help isolate changes in one area from affecting another. Maintaining decoupling is a very important principle for development velocity, no matter what paradigm you're using.




Some people say that ""abstractions are bad anyway"". I'll take this opportunity to point out some of the most successful and beneficial abstractions in programming history, such as file descriptors, the UDP protocol, and the Linux operating system. 20




To sum up, the constraints we add for memory safety are sometimes in conflict with the necessary constraints of API stability and beneficial abstraction, and there's sometimes a limit to how many constraints you can add to a problem before it becomes impossible to satisfy them all. 21




Don't get me wrong, one can get sufficient developer velocity in Rust, especially compared to naive C or C++. These drawbacks are real, but they aren't crippling. Plenty of projects have been completed in a reasonable time with Rust... it's just often slower than other approaches.







Notes [–]
Notes [+]
18
19
20
21



18


It's a great video, explaining a good architecture, though I would disagree with his conclusion that object-oriented coding is therefore bad. One can easily use a top-down object-oriented architecture. In an iOS app, simply never do any mutation if calling a delegate method, only when called from above. Many React apps are architected this way, and so is Google Earth.




19


This is also true of the converse; a & reference puts a constraint on all other code that they don't have any &mut references.




20


 More good programming abstractions:



If statements


Docker


JVM bytecode



And some from real life:



Amazon.com


Driving


Spoken language!







21


A quote from Harry Potter and the Methods of Rationality. Shout-out to my fellow Ravenclaws!











 Borrow Checking and Beyond

A lot of languages are working on borrow checking blends that are better for development velocity.




Some languages are using it under the hood:



Lobster is using borrowing and other static analysis techniques under the hood to eliminate a lot of reference counting overhead.


Kind is using a blend of borrowing and cloning to fuel a purely functional language.





Vale is building something similar to borrow checking but at the regions level, to largely eliminate memory safety overhead without introducing aliasing restrictions. Its opt-in nature means the user can use it where it makes sense and doesn't hinder development velocity.




Verona and Forty2 are experimenting with mixing regions and garbage collection.




Some languages are also putting borrow checkers on top of simpler and more flexible foundations:



Austral is building one on top of linear types.


Val is working on one built on top of Mutable Value Semantics.





Cone is particularly interesting because it builds a borrow checker on top of any user-specified memory management strategy.




 Garbage Collection



Garbage collection is probably the best approach for developer velocity. It completely decouples your goals from the constraints of memory management. You are free to solve your problem without tracking extra requirements, such as C++'s single ownership or Rust's borrow checking.




This is a good thing, because most of your code doesn't need to care about memory management. In most programs, profiling shows only a small portion of code that's performance sensitive, requiring more precise control of memory. 22




I particularly liked this quote from the Garbage Collection Handbook:


Above all, memory management is a software engineering issue. Well-designed programs are built from components [...] that are highly cohesive and loosely coupled. [...] modules should not have to know the rules of the memory management game played by other modules. [...] GC uncouples the problem of memory management from interfaces.




In garbage collection, we don't have to satisfy the move-to-move constraint, or borrow-to-borrow constraint. We dont have to worry about matching pointers versus values. There's just one kind of reference, rather than Rust's 5 or C++'s 7. 23




GC also doesn't cause any refactoring to satisfy single ownership (like C++'s unique_ptr) or mutability requirements (like in Rust), because those concepts never existed to begin with. 24




 GC'd code can be more correct



There are certain errors that arise in borrow checked and MMM languages, which don't happen in GC languages.




Often, instead of holding a reference to an object like in a GC'd language, the borrow checker will force us to hold a key into a central collection, such as an ID into a hash map. If we try to ""dereference"" 25 the ID of an object that no longer exists, we often get a run-time error (a None or Err usually) that we have to handle or propagate.




Pony is a great example of how a garbage collected language can reach further towards correctness than other languages. Pony is one of the only languages that literally cannot cause a run-time error. Like Erlang, it is incredibly resilient. 26




Garbage collection can also be better with privacy, since objects are never accidentally reused or mixed up with each other. For example, the borrow checker can turn memory safety problems into privacy problems if one's not careful.




 GC can be slow in specific domains

If one has unusually constrained latency requirements, such as in high frequency trading or a real-time first-person shooter game, it can take quite some time to refactor and tune the program to not have unwelcome latency spikes.




For example, to avoid the Java garbage collector firing in a specific scope, one has to completely avoid the new keyword in that scope, and avoid calling any functions that might sneakily use new. Coding without new in Java is a particularly arcane challenge.




It's a little easier in C#, where we can use the struct keyword to make a class without any heap allocation. High-performance Unity games sometimes use this style, but it's still rather difficult.




 Garbage Collection's Future

Garbage collection is more than fast enough for most situations. And even for those situations where it's latency spikes are too burdensome, there are solutions on the horizon.




Cone and Verona will allow us to explicitly separate GC regions from each other, such that we can create and destroy a temporary short-lived region before its first collection even needs to happen. By using regions wisely, one can probably avoid the vast majority of collections. 27




Cone aims to take that even further by blending in a borrow checker, plus allowing more allocation strategies such as arenas, reference counting, or even custom ones from the users themselves.




With these advances, we might be able to get GC's development velocity advantages without the usual performance drawbacks.




 Reference Counting



Reference counting, like in Swift, generally has the benefits that garbage collection does.




It does have one drawback: any cycle of references pointing at each other could cause a memory leak, wasting the available memory. This can be largely mitigated with good tooling that detects these cycles in development.




However, reference counting has three nice benefits: weak references, deterministic destruction, and the ability to make constraint references.




A weak reference is a mechanism for determining if the pointed-at object is still alive. This can be useful for a program's logic. For example, a Rocket might check if the target Spaceship has already been destroyed, to know whether it should safely fall into the planet's ocean.




Reference-counted objects are destroyed deterministically, which helps us have finer control over our program's performance.




One can also specify where they expect a reference-counted object to be destroyed, simply by asserting that they have the last reference (in other words, asserting the reference count is 1 at the end of the scope). I call this a constraint reference 28 and it can help us detect our program's logic bugs in a way that no other paradigm can.




 Conclusions

MMM, borrow checking, GC, and RC each have their strengths and weaknesses. However, in the dimension of development velocity, my general conclusions would be:



Garbage collection is often the best, since it decouples memory concerns from the actual problem at hand.


Reference counting is almost as good as garbage collection, and possibly even better if one uses its specific unique abilities.


MMM can be pretty good, as long as one uses proper architectures and tools to detect memory problems.


Borrow checking is almost as good as the others, if we augment it with Rust's other aspects like RefCell.





But that's too general! Here's some rough guidelines to help with more specific situations:



When making a game, default to something GC or RC, like Swift or C#. They offer great development velocity, plus value types (struct) for the more performance-sensitive areas. If one need even more performance, then:



If working on a single player game, an MMM language with proper architecture and memory safety mitigations is a stellar option.


If working on a multiplayer game that has particular security risks, then Rust is a good option.



When making a web server, default to a memory-safe language with RC or GC and good concurrency support like Pony or Go.



If working on something extremely latency-sensitive, Rust could be better. MMM isn't really advisable for something exposed to the network.



When making a mobile app, stick to Swift, Kotlin, or Typescript. GC is particularly good here because its pauses can happen in between user actions.


If working on something that needs high reliability and correctness, look into something like Pony. It's run-time is guaranteed to never cause a run-time error that's not explicitly created by the user.





There are also a lot of other languages improving, blending, and even creating new memory safety paradigms.




Of course, one should consider all this in context. Developer velocity is one of the most important factors in language choice, but not the only factor:



An open-source project might want a language that encourages simplicity more, for a healthier community of contributors.


A real-time first-person shooter game would still want an approach with more predictable performance, even if it takes longer to code.


A safety critical application would still want an approach that encourages more correctness, even at the cost of developer velocity.





I hope this post has given you a broader perspective on how various memory safety approaches affect development velocity!




If you're interested in this kind of thing, then check out Part 1 which talks about memory safety in unsafe languages, and keep an eye out for the next parts on our RSS feed, twitter, discord server, or subreddit!




If you found this interesting or entertaining, please consider sponsoring me:







     Sponsor me on GitHub!
  



With your help, I can write this kind of nonsense more often!




Cheers,


- Evan Ovadia









Notes [–]
Notes [+]
22
23
24
25
26
27
28



22


This is also why C# offers value types (struct) for more precise control over memory.




23


In C++, there's Ship, Ship&, Ship*, const Ship*, const Ship&, unique_ptr<Ship>, shared_ptr<Ship>. In Rust, there's Ship, &Ship, &mut Ship, Box<Ship>, Rc<Ship>. Cell<T> and RefCell<T> might also count, bringing Rust to 7 too, perhaps.




24


Though I wouldn't turn down some sort of single ownership / RAII being added to a GC'd language. It would prevent us ever forgetting to call .dispose() on a class's child.




25


By this I mean use the index or ID to look up an object in the central hash map.




26


Note that not all garbage collected languages strive for correctness... most of them have some flavor of null or nil.




27


I suspected Pony could do something similar, but after a quick discussion with one of the developers, it turns out not to be the case.




28


Vale used to be based on these constraint references, check out this article from long ago for more.















        Copyright © 2022 Evan Ovadia  
      




"
https://news.ycombinator.com/rss,Louis Le Prince,https://en.wikipedia.org/wiki/Louis_Le_Prince,Comments,"



Louis Le Prince - Wikipedia










































Louis Le Prince

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search
French inventor and Father Of Cinematography
For the composer, see Louis Le Prince (composer).


Louis Le PrinceLe Prince c. 1885BornLouis Aimé Augustin Le Prince(1841-08-28)28 August 1841Metz, FranceDisappeared16 September 1890Dijon, FranceStatusDeclared dead on
                       16 September 1897 (aged 56)Occupation(s)Artist, art teacher, inventorSpouseElizabeth Le Prince-Whitley
​ ​(m. 1869)​
Louis Aimé Augustin Le Prince (28 August 1841 – disappeared 16 September 1890, declared dead 16 September 1897) was a French artist and the inventor of an early motion-picture camera, possibly the first person to shoot a moving picture sequence using a single lens camera and a strip of (paper) film.[1][2] He has been credited as the ""Father of Cinematography"",[3] but his work did not influence the commercial development of cinema—owing at least in part to the great secrecy surrounding it.[4][5]
A Frenchman who also worked in the United Kingdom and the United States, Le Prince's motion-picture experiments culminated in 1888 in Leeds, England.[6] In October of that year, he filmed moving-picture sequences of family members in Roundhay Garden and his son playing the accordion, using his single-lens camera and Eastman's paper negative film.[7] At some point in the following eighteen months he also made a film of Leeds Bridge. This work may have been slightly in advance of the inventions of contemporaneous moving-picture pioneers, such as the British inventors William Friese-Greene and Wordsworth Donisthorpe, and was years in advance of that of Auguste and Louis Lumière and William Kennedy Dickson (who did the moving image work for Thomas Edison).
Le Prince was never able to perform a planned public demonstration of his camera in the US because he mysteriously vanished; he was last known to be boarding a train on 16 September 1890.[1] Multiple conspiracy theories have emerged about the reason for his disappearance, including: a murder set up by Edison, secret homosexuality, disappearance in order to start a new life, suicide because of heavy debts and failing experiments, and a murder by his brother over their mother's will. No conclusive evidence exists for any of these theories. In 2004, a police archive in Paris was found to contain a photograph of a drowned man bearing a strong resemblance to Le Prince who was discovered in the Seine just after the time of his disappearance,[7] but it has been claimed that the body was too short to be Le Prince.[8]
In early 1890, Edison workers had begun experimenting with using a strip of celluloid film to capture moving images. The first public results of these experiments were shown in May 1891.[9] However, Le Prince's widow and son Adolphe were keen to advance Louis's cause as the inventor of cinematography. In 1898, Adolphe appeared as a witness for the defence in a court case brought by Edison against the American Mutoscope Company. This suit claimed that Edison was the first and sole inventor of cinematography, and thus entitled to royalties for the use of the process. Adolphe was involved in the case but was not allowed to present his father's two cameras as evidence, although films shot with cameras built according to his father's patent were presented. Eventually the court ruled in favour of Edison. A year later that ruling was overturned,[9] but Edison then reissued his patents and succeeded in controlling the US film industry for many years.[9]

Contents

1 Early life and education
2 Career
3 Disappearance
4 Patents and cameras
5 Later recognition

5.1 Le Prince Cine Camera-Projector types


6 Legacy

6.1 Remaining material and production
6.2 Man Walking Around a Corner (16-Lens Camera)
6.3 Roundhay Garden Scene (Single-Lens Camera MkII)
6.4 Traffic Crossing Leeds Bridge (Single-Lens Camera MkII)
6.5 Accordion Player (Single-Lens Camera MkII)


7 See also
8 References
9 Sources
10 External links


Early life and education[edit]
Le Prince was born on 28 August 1841 in Metz.[10][11] His family referred to him as ""Augustin"" and English-speaking friends would later call him ""Gus"".[12] Le Prince's father was a major of artillery in the French Army[13] and an officer of the Légion d'honneur. When growing up, he reportedly spent time in the studio of his father's friend, the pioneer of photography Louis Daguerre,[13] from whom Le Prince may have received some lessons on photography and chemistry before he was 10 years old. His education went on to include the study of painting in Paris and post-graduate chemistry at Leipzig University,[13] which provided him with the academic knowledge he was to utilise in the future.

Career[edit]
 Le Prince in the 1880s
In conclusion, I would say that Mr. Le Prince was in many ways a very extraordinary man, apart from his inventive genius, which was undoubtedly great. He stood 6ft. 3in. or 4in. (190cm) in his stockings, well built in proportion, and he was most gentle and considerate and, though an inventor, of an extremely placid disposition which nothing appeared to ruffle.— Declaration of Frederic Mason (wood-worker and assistant of Le Prince, April 21, 1931, American consulate of Bradford, England)
Le Prince moved to Leeds, England in 1866, after being invited to join John Whitley,[1] a friend from college, in Whitley Partners of Hunslet, a firm of brass founders making valves and components.[14][15] In 1869 he married Elizabeth Whitley, John's sister[1] and a talented artist. When in Paris during their honeymoon, Le Prince repeatedly visited a magic show, fascinated by an illusion with moving transparent figures, presumably a dancing skeleton projection at the Théâtre Robert-Houdin with multiple reflections of mirrors focused on one point or a variation of Pepper's Ghost.[16]
Le Prince and his wife started a school of applied art, the Leeds Technical School of Art,[17] and became well renowned for their work in fixing coloured photographs on to metal and pottery, leading to them being commissioned for portraits of Queen Victoria and the long-serving Prime Minister William Gladstone produced in this way; these were included alongside other mementos of the time in a time capsule—manufactured by Whitley Partners of Hunslet—which was placed in the foundations of Cleopatra's Needle on the embankment of the River Thames.[citation needed]
In 1881, Le Prince went to the United States[13] as an agent for Lincrusta Walton, staying in the country along with his family once his contract had ended.[citation needed] He became the manager for a small group of French artists who produced large panoramas, usually of famous battles, that were exhibited in New York City, Washington, D.C. and Chicago.[13][14]
During this time he began experiments relating to the production of 'moving' photographs, designing a camera that utilised sixteen lenses,[14] which was the first invention he patented. Although the camera was capable of 'capturing' motion, it wasn't a complete success because each lens photographed the subject from a slightly different viewpoint and thus the image would have jumped about, if he had been able to project it (which is unknown).

 Plaque on Leeds Bridge
After his return to Leeds in May 1887,[14] Le Prince built a single-lens camera in mid-late 1888. An experimental model was developed in a workshop at 160 Woodhouse Lane, Leeds and used to shoot his motion-picture films. It was first used on 14 October 1888 to shoot what would become known as Roundhay Garden Scene and a sequence of his son Adolphe playing the accordion. Le Prince later used it to film road traffic and pedestrians crossing Leeds Bridge. The film was shot from Hicks the Ironmongers, now the British Waterways building on the south east side of the bridge,[1] now marked with a commemorative Blue plaque.

Disappearance[edit]
In September 1890, Le Prince was preparing for a trip to the United States, supposedly to publicly premiere his work and join his wife and children. Before this journey, he decided to return to France to visit his brother in Dijon. Then, on 16 September, he took a train to Paris but, having taken a later train than planned, his friends missed him in Paris. He was never seen again by his family or friends.[1] The last person to see Le Prince at the Dijon station was his brother.[18]  The French police, Scotland Yard and the family undertook exhaustive searches, but never found him.
Le Prince was officially declared dead in 1897.[19]
A number of wild and mostly unsubstantiated theories were proposed, including:

Patent Wars assassination, ""Equity 6928""
Christopher Rawlence pursues the assassination theory, along with other theories, and discusses the Le Prince family's suspicions of Edison over patents (the Equity 6928) in his 1990 book and documentary The Missing Reel. Rawlence claims that at the time that he vanished, Le Prince was about to patent his 1889 projector in the UK and then leave Europe for his scheduled New York official exhibition. His widow assumed foul play though no concrete evidence has ever emerged and Rawlence prefers the suicide theory. In 1898, Le Prince's elder son Adolphe, who had assisted his father in many of his experiments, was called as a witness for the American Mutoscope Company in their litigation with Edison [Equity 6928]. By citing Le Prince's achievements, Mutoscope hoped to annul Edison's subsequent claims to have invented the moving-picture camera. Le Prince's widow Lizzie and Adolphe hoped that this would gain recognition for Le Prince's achievement, but when the case went against Mutoscope their hopes were dashed. Two years later Adolphe Le Prince was found dead on Fire Island near New York.[20]
Disappearance ordered by the family
In 1966, Jacques Deslandes proposed a theory in Histoire comparée du cinéma (The Comparative History of Cinema), claiming that Le Prince voluntarily disappeared due to financial reasons and ""familial conveniences"". Journalist Léo Sauvage quotes a note shown to him by Pierre Gras, director of the Dijon municipal library, in 1977, that claimed Le Prince died in Chicago in 1898, having moved there at the family's request because he was homosexual; but he rejects that assertion.[21] There is no evidence to suggest that Le Prince was gay.[22]
Fratricide, murder for money
In 1967, Jean Mitry proposed, in Histoire du cinéma, that Le Prince was killed. Mitry notes that if Le Prince truly wanted to disappear, he could have done so at any time prior to that. Thus, he most likely never boarded the train in Dijon. He also wonders why, if his brother, who was confirmed as the last person to have seen Le Prince alive, knew Le Prince was suicidal, he didn't try to stop Le Prince, and why he didn't report Le Prince's mental state to the police before it was too late.[23]
Suicide by drowning
A photograph of a drowned man pulled from the Seine in 1890, strongly resembling Le Prince, was discovered in 2003 during research in the Paris police archives.[13][24] This somehow led to the conclusion that he must have failed to get his moving picture to work, had heavy debts, and thus chose to take his own life.[18] It has been claimed that the found body was too short to be Le Prince.[8]
Patents and cameras[edit]
On 10 January 1888 Le Prince was granted an American patent on a 16-lens device that he claimed could serve as both motion picture camera (which he termed ""the receiver or photo-camera"") and a projector (which he called "" the deliverer or stereopticon"").[25] That same day he took out a near-identical provisional patent for the same devices in Great Britain, proposing ""a system of preferably 3, 4, 8, 9, 16 or more lenses"". Shortly before the final version was submitted he added a sentence which described a single-lens system, but this was neither fully explained nor illustrated, unlike the several pages of description of the multi-lens system,[26] meaning the single-lens camera was not legally covered by patent.

 60mm spools used for developing film shot in single-lens camera. Each section would carry 4 frames of negative (1930 Science Museum, London)
This addendum was submitted on 10 October 1888[27] and, on 14 October, Le Prince used his single-lens camera to film Roundhay Garden Scene. During the period 1889-1890 he worked with the mechanic James Longley on various ""deliverers"" (projectors) with one, two, three and sixteen lenses.  The images were to be separated, printed and mounted individually, sometimes on a flexible band, moved by metal eyelets. The single lens projector used individual pictures mounted in wooden frames.[27] His assistant, James Longley, claimed the three-lens version was the most successful.[27] Those close to Le Prince have testified to him projecting his first films in his workshop as tests, but they were never presented to anyone outside his immediate circle of family and associates and the nature of the projector is unknown.
In 1889 he took French-American dual citizenship in order to establish himself with his family in New York City and to follow up his research. However, he was never able to perform his planned public exhibition at Morris–Jumel Mansion in Manhattan, in September 1890, due to his disappearance.

Later recognition[edit]
This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: ""Louis Le Prince"" – news · newspapers · books · scholar · JSTOR (October 2017) (Learn how and when to remove this template message)
Even though Le Prince's achievement is remarkable, with only William Friese-Greene and Wordsworth Donisthorpe achieving anything comparable in the period 1888-1890, his work was largely forgotten until the 1920s, as he disappeared before the first public demonstration of the result of his work, having never shown his invention to any photographic society or scientific institution or the general public.
For the April 1894 commercial exploitation of his personal kinetoscope parlor, Thomas Edison is credited in the US as the inventor of cinema, while in France, the Lumière Brothers are hailed as inventors of the Cinématographe device and for the first commercial exhibition of motion-picture films, in Paris in 1895.
However, in Leeds, Le Prince is celebrated as a local hero. On 12 December 1930, the Lord Mayor of Leeds unveiled a bronze memorial tablet at 160 Woodhouse Lane, Le Prince's former workshop. In 2003, the University's Centre for Cinema, Photography and Television was named in his honour. Le Prince's workshop in Woodhouse Lane was until recently the site of the BBC in Leeds, and is now part of the Leeds Beckett University Broadcasting Place complex, where a blue plaque commemorates his work. (coordinates: 53°48′20.58″N 1°32′56.74″W﻿ / ﻿53.8057167°N 1.5490944°W﻿ / 53.8057167; -1.5490944). Reconstructions of his film strips are shown in the cinema of the Armley Mills Industrial Museum, Leeds.
In France, an appreciation society was created as L'Association des Amis de Le Prince (Association of Le Prince's Friends), which still exists in Lyon.
In 1990, Christopher Rawlence wrote The Missing Reel, The Untold Story of the Lost inventor of Moving Pictures and produced the TV programme The Missing Reel (1989) for Channel Four, a dramatised feature on the life of Le Prince.
In 1992, the Japanese filmmaker Mamoru Oshii (Ghost in the Shell) directed Talking Head, an avant-garde feature film paying tribute to the cinematography history's tragic ending figures such as George Eastman, Georges Méliès and Louis Le Prince who is credited as ""the true inventor of eiga"", 映画, Japanese for ""motion picture film"".
In 2013, a feature documentary, The First Film was produced, with new research material and documentation on the life of Le Prince and his patents. Produced and directed by Leeds-born David Nicholas Wilkinson with research by Irfan Shah, it was filmed in England, France and the United States by Guerilla Films.[28] The First Film features several film historians to tell the story, including Michael Harvey, Irfan Shah, Stephen Herbert, Mark Rance, Daniel Martin, Jacques Pfend, Adrian Wootton, Tony North, Mick McCann, Tony Earnshaw, Carol S Ward, Liz Rymer, and twice Oscar-nominated cinematographer Tony Pierce-Roberts. Le Prince's great-great-granddaughter Laurie Snyder also makes an appearance. It had its world première in June 2015 at the Edinburgh Film Festival and opened in UK cinemas on 3 July 2015. The film also played in festivals in the US, Canada, Russia, Ireland and Belgium. On 8 September 2016 it played at the Morris-Jumel Mansion in New York, where 126 years earlier Le Prince planned to show his films.

Le Prince Cine Camera-Projector types[edit]


Model
Specs
Design
Manufacture
Patents


16-lens camera and projector
Patent: ""Method of and apparatus for producing animated pictures. of natural scenery and life"" (USA) and in all later foreign patents.Designation: LePrince 16-lens camera/projector Framerate: 16 frames per second (according to patent)Medium: Glass plates and Eastman paper film
1886, New York
Made in Paris, 1887
US Patent No.376,247/217,809IssuedWashington2 November 1886Accepted10 January 1888
FR Patent No.188,089IssuedParis11 January 1888AcceptedJune 1890
(and BR patent 423 - see below)



Single-lens camera
Patent: Mentioned but not described or illustrated in ""Improvements in the Method of and Apparatus for Producing Animated Photographic Pictures""Designation: Le Prince single-lens ""receiver"" (camera) Mk2Framerate: 5-7 frames per secondLenses: Viewfinder (upper) & Photograph (lower)Film: sensitised paper film & gelatin stripping film (60mm)Focus: lever (backward/forward)
Leeds
1888


*Frederic Mason(body/ wooden parts)*James William Longley (design and working parts)Made in Leeds, 1888
BR Patent no 423
IssuedLondon10 January 1888Accepted16 November 1888 [Mentioned but not described]





Single-Lens
Projector


Single-lens ""deliverer"" (projector). Each frame was printed on glass and mounted in a mahogany frame. These were moved before the lens in a continuous spiral. The heat of the lamp and the movement of the frames often caused the glass to break. Top framerate: 7fps.

Leeds
1889


Made in Leeds, 1889

Never patented


3-Lens Projector
3-lens ""deliverer"" (projector), used frames mounted individually in three flexible strips of Willesden paper with brass eyelets to move them. Projection presumably alternated 1-2-3 between the three strips/lenses and each strip moved when the light was cut off.
Leeds
1889/ 1890


Made in Leeds 1889 or 1890

Never patented

Legacy[edit]
Remaining material and production[edit]
 Back view of Le Prince's single-lens Cine Camera-Projector MkII opened (Science Museum, London, 1930).
Le Prince developed a single-lens camera in his workshop at 160 Woodhouse Lane, Leeds, which was used to shoot his motion-picture films. Remaining surviving production consists of two scenes in the garden at Oakwood Grange (his wife's family home, in Roundhay) and another of Leeds Bridge.
Forty years later, Le Prince's daughter, Marie, gave the remaining apparatus to the  Science Museum, London (later transferred to the National Museum of Photography, Film and Television (NMPFT), Bradford, which opened in 1983 and is now the National Science and Media Museum). In May 1931, photographic plates were produced by workers of the Science Museum from paper print copies provided by Marie Le Prince.[2] In 1999, these were re-animated to produce digital versions. Roundhay Garden was alleged by the Le Prince family to have been shot at 12 frame/s and Leeds Bridge at 20 frame/s, although this is not borne out by the NMPFT versions (see below) or motion analysis, with both films being estimated at a consistent 7 frames a second.[29]
All available versions of these sequences are derived from materials held by the National Science and Media Museum.

Man Walking Around a Corner (16-Lens Camera)[edit]
Main article: Man Walking Around A Corner




Sequence of 12 complete frames + 4 partial frames, from National Science Museum, London circa 1931. (Courtesy NMPFT, Bradford) NMPFT. Filmed in Paris before 18.08.1887.




The only existing images from Le Prince's 16-lens camera are a sequence of 16 frames of a man walking around a corner. This appears to have been shot onto a single glass plate (which has since broken), rather than the twin strips of Eastman paper film envisaged in his patent. Jacques Pfend, a French cinema-historian and Le Prince specialist, confirms that these images were shot in Paris, at the corner of Rue Bochart-de-Saron (where Le Prince was living) and Avenue Trudaine. Le Prince sent 8 images of his mechanic running (which may be from this sequence) to his wife in New York City in a letter dated 18 August 1887,[30] which suggests it represented a significant camera test. Exposure is very irregular from lens to lens with some of the images almost completely bleached out, which Le Prince later on fixed.

Roundhay Garden Scene (Single-Lens Camera MkII)[edit]
Main article: Roundhay Garden Scene




Roundhay, 1888 original 20 frames by National Science Museum, London 1931 (Courtesy of NMPFT, Bradford).






Animation of Roundhay frames with image stabilised NMPFT, Bradford 1999.




The 1931 National Science Museum copy of what remains of a sequence shot in Roundhay Garden features 20 frames. The frames appear to have been printed in reverse from the negative, but this is corrected in the video. The film's damaged edge results in distortion and deformation on the right side of the stabilised digital movie. The scene was shot in Le Prince's father-in-law's garden at Oakwood Grange, Roundhay on 14 October 1888. The NMPFT animation lasts two seconds at 24fps (frames per second), meaning the original footage is playing at 10fps. In this version, the action is speeded up - the original footage was probably shot at 7fps.

Traffic Crossing Leeds Bridge (Single-Lens Camera MkII)[edit]
 Video clip, 2 seconds
Louis Le Prince filmed traffic crossing Leeds Bridge from Hicks the Ironmongers[1] at the following coordinates: 53°47′37.70″N 1°32′29.18″W﻿ / ﻿53.7938056°N 1.5414389°W﻿ / 53.7938056; -1.5414389.[31]





6-frame sequence (118-120 & 122–124) of Leeds Bridge (National Science Museum, London 1923)






20-frame sequence of Leeds Bridge (National Science Museum)(Courtesy NMPFT, Bradford)




The earliest copy belongs to the 1923 NMPFT inventory (frames 118–120 and 122–124), though this longer sequence comes from the 1931 inventory (frames 110–129). According to Adolphe Le Prince who assisted his father when this film was shot in late October 1888, it was taken at 20fps. However, the digitally stabilised sequence produced by the NMPFT lasts two seconds, meaning the footage is playing here at 10fps. As with the Roundhay Garden sequence, its appearance is sped up, suggesting the original footage was probably shot at 7fps. This would fit with what we know of the projection experiments, where James Longley reported a top speed of 7fps.[32]

Accordion Player (Single-Lens Camera MkII)[edit]
 2 frames per second amateur remastering of all 19 frames; 10 frames per second version









Copy of original 19 frames (numbered 41–59) by National Science Museum, London 1931 (Courtesy of NMPFT, Bradford).




The last remaining film of Le Prince's single-lens camera is a sequence of frames of Adolphe Le Prince playing a diatonic button accordion. It was recorded on the steps of the house of Joseph Whitley, Louis's father-in-law.[2] The recording date may be the same as Roundhay Garden as the camera is in a similar position and Adolphe is dressed the same. The NMPFT has not remastered this film. An amateur animation of the first 17 frames is here on YouTube. The running speed appears to be 5-6fps

See also[edit]
List of people who disappeared
Roundhay Garden Scene
References[edit]


^ a b c d e f g ""BBC Education – Local Heroes Le Prince Biography"". Archived from the original on 28 November 1999. Retrieved 27 May 2008.{{cite web}}:  CS1 maint: bot: original URL status unknown (link), BBC, archived on 28 November 1999

^ a b c Howells, Richard (Summer 2006). ""Louis Le Prince: the body of evidence"". Screen. Oxford, UK: Oxford Journals. 47 (2): 179–200. doi:10.1093/screen/hjl015.

^ The ""Father"" Of Kinematography: Leeds Memorial Pioneer Work In England. The Manchester Guardian (1901–1959), Manchester, England 13 December 1930: 19.

^ Fischer, Paul (April 2022). The Man who Invented Motion Pictures: A True Tale of Obsession, Murder, and the Movies. Simon & Schuster. ISBN 9781982114824.

^ Greenblatt, Leah (14 April 2022). ""He Created the First Known Movie. Then He Vanished. In his new book, ""The Man Who Invented Motion Pictures,"" Paul Fischer investigates the life — and mysterious disappearance — of Louis Le Prince"". The New York Times. Retrieved 17 April 2022.

^ ""Louis Le Prince, who shot the world's first film in Leeds"". BBC. 24 August 2016.

^ a b ""Pioneers of Early Cinema: 1, Louis Aimé Augustin Le Prince (1841–1890?)"" (PDF). www.nationalmediamuseum.org.uk. p. 2. Retrieved 25 November 2012. he developed a single-lens camera which he used to make moving picture sequences at the Whitley family home in Roundhay and of Leeds Bridge in October 1888. ... it has been claimed that a photograph of a drowned man in the Paris police archives is that of Le Prince.

^ a b ""The tragedy of Louis Le Prince"". www.acmi.net.au. Retrieved 20 June 2022.

^ a b c Spehr, Paul (2008). The Man Who Made Movies: W.K.L. Dickson. United Kingdom: John Libbey Publishing Ltd.

^ ""Archives Municipales de Metz - Visualiseur"". Retrieved 9 May 2020.

^ Aulas, Jean-Jacques; Pfend, Jacques (1 December 2000). ""Louis Aimé Augustin Leprince, inventeur et artiste, précurseur du cinéma"". 1895. Mille Huit Cent Quatre-vingt-quinze (in French) (32): Footnote 4. doi:10.4000/1895.110. ISSN 0769-0959. The birth certificate mentions ""born August on the 28th, 1841 at 5am. The common mistake of making him born in 1842 comes from an article of Ernest Kilburn Scott, mistake made since then in numerous articles, including the one by Simon Popple

^ Aulas, Jean-Jacques; Pfend, Jacques (1 December 2000). ""Louis Aimé Augustin Leprince, inventeur et artiste, précurseur du cinéma"". 1895. Mille Huit Cent Quatre-vingt-quinze (in French) (32): 9–74. doi:10.4000/1895.110. ISSN 0769-0959.

^ a b c d e f Herbert, Stephen. ""Louis Aimé Augustin Le Prince"". Who's Who of Victorian Cinema. Archived from the original on 21 July 2006. Retrieved 26 August 2006.

^ a b c d Adventures in CyberSound: Le Prince, Louis Aimé Augustin, Dr Russell Naughton (using source: Michael Harvey, NMPFT Pioneers of Early Cinema: 1. Louis Aimé Augustin Le Prince)

^ ""Pioneers of Early Cinema: Louis Aimé Augustin Le Prince (1841-1890?)"" (PDF). National Media Museum. June 2011.

^ ""Louis Le Prince – New Thinking: Part 1"". The Optilogue. 21 November 2022. Retrieved 23 November 2022.

^ Thomas Deane Tucker (2020). Peripatetic Frame: Images of Walking in Film. Edinburgh University Press. p. 18.

^ a b ""The Shadow Traps"". www.stitcher.com. Retrieved 4 November 2019.

^ Hannavy, John, ed. (2000). Encyclopedia of nineteenth-century photography. Vol. 1. CRC Press. p. 837. ISBN 978-0-415-97235-2.

^ Burns, Paul. ""The History of the Discovery of Cinematography"". – ""After his disappearance, the Le Prince family led by his wife and son went to court against Edison in what became known as Equity 6928. The famous Patent Wars ensued and by 1908 Thomas Edison was regarded as sole inventor of motion pictures, in the US at least. However, in 1902, two years after Le Prince's son Adolphe had testified in the suit, he was found shot dead on Fire Island, New York.""

^ Léo Sauvage, ""Un épisode mystérieux de l'histoire du cinéma : La disparition de Le Prince"", Historia, n° 430 bis, sept. 1982, p. 45-51: ""une telle affirmation (...) est totalement dépourvue de vraisemblance"".

^ Dembowski (1995): ""Pierre Gras, conservateur en chef de la Bibliothèque publique de Dijon, en 1977, montra à Léo Sauvage une note (il la cite dans son ouvrage), prise lors de la visite d'un historien connu (il a tu son nom) qui avait déclaré : – Le Prince est mort à Chicago en 1898, disparition volontaire exigée par la famille. Homosexualité. Disons clairement qu'il n'y a pas l'ombre d'une preuve à l'appui d'une telle assertion.""

^ Dembowski (1995): ""S'il en était ainsi, pourquoi n'a-t-il rien fait pour l'empêcher de réaliser son funeste projet, pourquoi n'a-t-il pas averti la police à temps?""

^ ""The mystery of Leeds's long-lost movie pioneer"". The Telegraph. 23 June 2015. Retrieved 9 May 2020 – via www.telegraph.co.uk.

^ ""Method of and apparatus for producing animated pictures of natural scenery and life"". 10 January 1888. Retrieved 29 December 2017.

^ ""Patents Completed"". British Journal of Photography. 35: 793.

^ a b c Aulas & Pfend, Jean-Jacques & Jacques (1 December 2000). ""Louis Aimé Augustin Leprince, inventeur et artiste, précurseur du cinéma"". 1895. Revue de l'association française de recherche sur l'histoire du cinéma. 32.

^ ""The First Film"". Guerilla Group. Retrieved 16 June 2018.

^ ""Cinematography"". National Museum of Photography, Film and Television. Archived from the original on 11 July 2006. Retrieved 16 April 2009.

^ Letter dated 18 August 1887 in Louis Le Prince Collection at Leeds University Library

^ ""Google Earth Community: First Moving Pictures"". Retrieved 9 May 2020.

^ Letter from James Longley to Louis le Prince 8 August 1889. ""The best result that I got was 426 per minute"" - From Le Prince Collection in Leeds University Library.


Sources[edit]

Insight Collections and Research Centre
Guinness Book of Movie Facts and Feats
Who's Who of Victorian Cinema
The Career of Louis Aimée Augustin Le Prince by E. Kilburn Scott (July 1931)
""La naissance du cinéma : cent sept ans et un crime..."" by Irénée Dembowski (in Kino 1989, translated from Polish to French in Cahiers de l'AFIS, numéro 182, nov.–déc. by Michel Rouzé, quoted by Alliage numéro 22 1995)
The Missing Reel, by Christopher Rawlence (Athenum Publishers, New York, 1990)
""Le Prince's Early Film Cameras"", by Simon Popple (in Photographica World, September 1993)
""Le Prince and the Lumières"", by Rod Varley (in Making of the Modern World, Science Museum, UK, 1992)
""Career of Louis Aimée Augustin Le Prince"", by E. Kilburn Scott, (in Journal of the Society of Motion Picture Engineers, US, July 1931)
Burns, Paul The History of the Discovery of Cinematography An Illustrated Chronology
""The Pioneer Work of Le Prince in Kinematography"", by E. Kilburn Scott (in The Photographic Journal #63, August 1923, pp. 373–378)
""Louis Aimée Augustin Le Prince"" by Merritt Crawford (in Cinema, 1 December 1930, pp. 28–31)
L'affaire Lumière. Du mythe à l'histoire, enquête sur les origines du cinéma by Léo Sauvage, 1985 ISBN 2-86244-045-0
Ingenious Le Prince 16-lens camera
""Louis Le Prince: the body of evidence"" by Richard Howells (in Screen vol.47 #2, Oxford University Press, 2006)
""Le Prince, inventeur et artiste, précurseur du cinema"" by Jean-Jacques Aulas and Jacques Pfend (in Revue d'Histoire du Cinéma N°32, December 2000, p. 9) ISSN 0769-0959
New research centre honours father of film
Essential Films, chapter 2, Culture Wars by Ion Martea
Roundhay Garden Scene (1888), Culture Wars by Ion Martea
Traffic Crossing Leeds Bridge (1888), Culture Wars by Ion Martea
The Indispensable Murder Book, edited by Joseph Henry Jackson (New York: The Book Society, 1951), pp. 437–464, ""The Red and White Girdle"" by Christopher Morley.  This deals with the murder of Gouffe, and shows the intense study of that trunk murder in 1889–90.

The facts concerning the life and death of LOUIS AIME AUGUSTIN LEPRINCE, pioneer of the moving pîcture and his family, by Jacques Pfend (Sarreguemines/57200/France) 2014.ISBN 9782954244198.
Why Leeds was the birthplace of film
External links[edit]



Wikimedia Commons has media related to Louis Le Prince.


Louis Le Prince at IMDb
Traffic Crossing Leeds Bridge at IMDb
L'EMPREINTE DE LOUIS AIME AUGUSTIN LEPRINCE DANS L'HISTOIRE DU CINEMA.(Université Paris Ouest, par Marie Crémaschi. sep. 2013.
Jean-Jacques Aulas et Jacques Pfend, Louis Aimé Augustin Leprince, inventeur et artiste, précurseur du cinéma
Adventures In Cybersound – extended biography by Dr Russell Naughton, RMIT University, Melbourne. Retrieved 2008-09-26
Roundhay Garden Scene on YouTube
Leeds Bridge on YouTube
Accordion Player by Louis Le Prince on YouTube a rough video from the first 17 frames
Louis Le Prince Centre for Cinema, Photography, and Television. University of Leeds. Retrieved 2008-09-26
The Legend of Louis Le Prince
Leodis – a photographic archive of Leeds. Leeds Library & Information Service. Allows search for key terms such as Louis Le Prince or Leeds Bridge or Bridge End or Hick Brothers or Auto Express (workshop site), etc.
Science Museum, London
National Science and Media Museum, Bradford
Armley Mills- Leeds Industrial Museum
Le Prince single-lens camera 1888, Science & Society Picture Library
Chronomedia year 1888 (Terramedia)
Burns, Paul The History of the Discovery of Cinematography 1885–1889 An Illustrated Chronological History
Local films for local people (BBC Bradford & West Yorkshire)
www.louisleprince.net Authority control General
ISNI
1
VIAF
1
WorldCat
National libraries
Spain
France (data)
Germany
United States
Japan
Netherlands
Other
FAST
National Archives (US)
Social Networks and Archival Context
SUDOC (France)
1





Retrieved from ""https://en.wikipedia.org/w/index.php?title=Louis_Le_Prince&oldid=1134021110""
Categories: 1841 births1890s missing person cases19th-century French peopleCinema pioneersDiscovery and invention controversiesFrench cinematographersFrench cinema pioneersFrench expatriates in the United KingdomFrench expatriates in the United StatesFrench film directorsMissing person cases in FrancePeople from MetzSilent film directorsLeeds Blue PlaquesLouis Le PrinceLeipzig University alumniHidden categories: CS1 maint: bot: original URL status unknownCS1 French-language sources (fr)Articles containing French-language textCS1: Julian–Gregorian uncertaintyArticles with short descriptionShort description is different from WikidataUse dmy dates from March 2021Articles with hCardsAll articles with unsourced statementsArticles with unsourced statements from May 2008Articles needing additional references from October 2017All articles needing additional referencesCommons category link is on WikidataIMDb ID different from WikidataArticles with ISNI identifiersArticles with VIAF identifiersArticles with WorldCat identifiersArticles with BNE identifiersArticles with BNF identifiersArticles with GND identifiersArticles with LCCN identifiersArticles with NDL identifiersArticles with NTA identifiersArticles with FAST identifiersArticles with NARA identifiersArticles with SNAC-ID identifiersArticles with SUDOC identifiersArticles containing video clipsYear of death unknown



Navigation menu



Personal tools


Not logged inTalkContributionsCreate accountLog in





Namespaces


ArticleTalk





English









Views


ReadEditView history





More

























Navigation


Main pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate




Contribute


HelpLearn to editCommunity portalRecent changesUpload file




Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageWikidata item




Print/export


Download as PDFPrintable version




In other projects


Wikimedia Commons




Languages


العربيةCatalàČeštinaDeutschEestiΕλληνικάEspañolفارسیFrançaisBahasa IndonesiaItalianoMalagasyمصرىNederlands日本語Norsk bokmålPolskiPortuguêsRomânăРусскийSimple EnglishSrpskohrvatski / српскохрватскиSuomiSvenskaУкраїнська中文
Edit links






 This page was last edited on 16 January 2023, at 16:51 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License 3.0;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Mobile view
Developers
Statistics
Cookie statement










"
https://news.ycombinator.com/rss,Building a 100% New Commodore 64 [video],https://www.youtube.com/watch?v=Qhdc-jgwqVQ,Comments,THIS CENTURY’S FIRST 100% NEW COMMODORE 64! - YouTubeAboutPressCopyrightContact usCreatorsAdvertiseDevelopersTermsPrivacyPolicy & SafetyHow YouTube worksTest new features© 2023 Google LLC
https://news.ycombinator.com/rss,Using jlink to cross-compile minimal JREs,https://jakewharton.com/using-jlink-to-cross-compile-minimal-jres/,Comments,"





Using jlink to cross-compile minimal JREs - Jake Wharton
























Jake Wharton


Using jlink to cross-compile minimal JREs
16 January 2023
jlink is a JDK tool to create bespoke, minimal JREs for your applications.
Let’s try it with a “Hello, world!” program:
class Main {
  public static void main(String... args) {
    System.out.println(""Hello, world!"");
  }
}

My laptop is an M1 Mac and I have downloaded the Azul Zulu JDK 19 build for it.
With the JDK I can both compile Java and then run the resulting program.
$ mkdir out
$ zulu19.30.11-ca-jdk19.0.1-macosx_aarch64/bin/javac -d out in/Main.java
$ zulu19.30.11-ca-jdk19.0.1-macosx_aarch64/bin/java -cp out Main
Hello, world!

Azul Zulu also provides a JRE that I can use to run compiled programs.
$ zulu19.30.11-ca-jre19.0.1-macosx_aarch64/bin/java -cp out Main
Hello, world!

Note the slight change in folder name (“jdk” → “jre”).
If we were shipping this to end-users it would be an easy win for binary size.
$ du -hs zulu*
329M    zulu19.30.11-ca-jdk19.0.1-macosx_aarch64
136M    zulu19.30.11-ca-jre19.0.1-macosx_aarch64

But 136MiB just for “Hello, world”? Don’t tell Reddit or Hacker News!
Thankfully, jlink is here to help us build a minimal JRE with only what we need.
Given our program, a sibling tool, jdeps, lists the Java modules which are required.
$ zulu19.30.11-ca-jdk19.0.1-macosx_aarch64/bin/jdeps \
      --print-module-deps \
      out/Main.class
java.base

Our program is so simple that it only needs the “base” module.
Now with jlink we can produce a minimal JRE.
$ zulu19.30.11-ca-jdk19.0.1-macosx_aarch64/bin/jlink \
      --compress 2 \
      --strip-debug \
      --no-header-files \
      --no-man-pages \
      --output zulu-hello-jre \
      --add-modules java.base

$ du -hs zulu*
 28M    zulu-hello-jre
329M    zulu19.30.11-ca-jdk19.0.1-macosx_aarch64
136M    zulu19.30.11-ca-jre19.0.1-macosx_aarch64

28MiB won’t win any language wars, but it’s a massive 80% savings over the full JRE.
$ zulu-hello-jre/bin/java -cp out Main
Hello, world!

We can ship it to our client and call it a day, right?
$ tar -czf hello.tgz zulu-hello-jre out

$ scp hello.tgz jw@server:
hello.tgz            100%   14MB   2.0MB/s   00:07

$ ssh jw@server ""tar xzf hello.tgz && zulu-hello-jre/bin/java -cp out Main""
bash: zulu-hello-jre/bin/java: cannot execute binary file: Exec format error

Nope!
While the Java bytecode we compiled is platform independent, the JRE is specific to each platform and my server runs Linux x64.
Thankfully, jlink can operate on JDKs for different platforms.
Let’s download the Linux x64 JDK and point jlink at its Java modules using --module-path.
$ zulu19.30.11-ca-jdk19.0.1-macosx_aarch64/bin/jlink \
      --compress 2 \
      --strip-debug \
      --no-header-files \
      --no-man-pages \
      --output zulu-hello-jre-linux-x64 \
      --module-path zulu19.30.11-ca-jdk19.0.1-linux_x64/jmods
      --add-modules java.base

$ du -hs zulu*
 28M    zulu-hello-jre
 36M    zulu-hello-jre-linux-x64
338M    zulu19.30.11-ca-jdk19.0.1-linux_x64
329M    zulu19.30.11-ca-jdk19.0.1-macosx_aarch64
136M    zulu19.30.11-ca-jre19.0.1-macosx_aarch64

The Linux x64 JRE is a little larger than the one for my ARM Mac, but it’s still small compared to the full-size JRE.
Does it work on the client?
$ tar -czf hello-linux.tgz zulu-hello-jre-linux-x64 out

$ scp hello-linux.tgz jw@server:
hello.tgz            100%   16MB   2.1MB/s   00:08

$ ssh jw@server ""tar xzf hello-linux.tgz && zulu-hello-jre-linux-x64/bin/java -cp out Main""
Hello, world!

It works! Now we can grab JDKs for any architecture for any platform and use our host jlink to effectively cross-compile minimal JREs for each target.
This is a great solution for multi-architecture Docker containers, desktop clients like JetBrains Compose UI, shipping to devices where you can’t fit a full JDK, and more.
Be sure to explore all the options on jdeps and jlink for ways to keep your runtimes small.
— Jake Wharton




"
https://news.ycombinator.com/rss,Programmer salaries in the age of LLMs,https://milkyeggs.com/?p=303,Comments,"



Milky Eggs  » Blog Archive   » Programmer salaries in the age of LLMs










































 













Milky Eggs






Home
About
All Posts
Links




Programmer salaries in the age of LLMs

What happens to the distribution of programmer salaries in the age of LLMs? I argue they will separate bimodally, much like what happened to lawyers’ salaries in the 1990s due to the rise of the Internet.
Dan Luu has previously written about lawyers’ salaries. I will paste some graphs from his article. First, lawyers in 1991:


Next, lawyers in 2000:


What happened? Well, if you’re an elite lawyer in the modern age, you probably command a large team of lower-tier lawyers, cheap paralegals, and so on, who are very good at using the Internet to look up case law and draft your opinions and so on. In contrast, if you’re an elite lawyer in the year 1990, it’s much more valuable for you specifically to have an encyclopedic knowledge of all the relevant case law, and it’s much harder for you to delegate modular pieces of work off to less competent people.
Indeed Cowen makes a similar point about lawyers in Who gains and loses from the new AI? where he writes:

The returns to factual knowledge are falling, continuing a trend that started with databases, search engines and Wikipedia. It is no longer so profitable to be a lawyer who knows a large amount of accumulated case law. Instead, the skills of synthesis and persuasion are more critical for success.

I claim that the trend which AI/ML continues for lawyers is one that it starts for programmers. Just like how a partner at Cravath likely sketches an outline of how they want to approach a particular case and swarms of largely replaceable lawyers fill in the details, we are perhaps converging to a future where a FAANG L7 can just sketch out architectural details and the programmer equivalent of paralegals will simply query the latest LLM and clean up the output. Note that querying LLMs and making the outputted code conform to specifications is probably a lot easier than writing the code yourself ー and other LLMs can also help you fix up the code and integrate the different modules together!
More generally, the farther such technologies advance, the more existing technical professions will undergo a sort of “paralegalization” and bifurcation of the talent distribution.
Somewhat-above-average programmers (including myself) had a decent run of it, but it may be over for us before too long.

January 16th, 2023  | Posted in Technology


3 Responses to “Programmer salaries in the age of LLMs”


Monday assorted links - Marginal REVOLUTION Says:
						
January 16th, 2023 at 7:58 pm 
[…] Programmer salaries in an age of LLMs.  And cybercriminals starting to use […]


 Milkman Says:
						
January 17th, 2023 at 4:27 am 
This model for programmers isn’t new, it was already in place 20 years ago at least as early as the dot com bubble. Even then there was an small group of “architectss” and countless nameless “coders”, often in least/best cost countries. This trend is likely to continue.
Another example of this trend is the medical professions with physician assistants, nurse practitioners, and a slew of other para-medical personnel coming to the fore.


Programmer salaries in the age of LLMs by Michelangelo11 - HackTech Says:
						
January 17th, 2023 at 7:44 am 
[…] Programmer salaries in the age of LLMs […]


Leave a Reply


Name (required)

Mail (will not be published) (required)

Website





Δ












			 Subscribe to Milky Eggs via RSS here.
		




"
https://news.ycombinator.com/rss,Intel Core i9-13900T CPU benchmarks show faster than 12900K 125W performance,https://wccftech.com/intel-core-i9-13900t-cpu-benchmarks-show-faster-than-12900k-125w-performance-at-35w/,Comments,"

HardwareReport
Intel Core i9-13900T CPU Benchmarks Show Faster Than 12900K 125W Performance at 35W

Hassan Mujtaba •
Jan 14, 2023 02:44 PM EST

•
Copy Shortlink
























Intel recently introduced brand new 13th Gen T-series chips which feature the Core i9-13900T that operates at a 35W TDP. The new chip has been benchmarked within Geekbench 5 and showcases impressive performance given its limited power budget.
Intel's 13th Gen Core i9-13900T 35W CPU Beats The 125W Core i9-12900K In Geekbench 5 Benchmark
Starting with the specifications, the Intel Core i9-13900T is a variation of the Core i9-13900 series that comes with a limited TDP design. While the standard chips boast 125W TDP in the unlocked and 65W TDP on the Non-K SKUs, the T-series chip is limited to a 35W TDP.  The Unlocked CPU is rated at up to 253W, the Non-K is rated at up to 219W while the T-series chip is rated at up to 106 Watts which is less than half the power budget of its higher-end siblings.
Related StoryHassan MujtabaIntel Core i9-13900KS, World’s First 6 GHz CPU, Now Available For $699 USThe Intel Core i9-13900T retains the same core configuration with 24 cores that are made up of 8 P-Cores and 16 E-Cores with 32 threads, a base clock of 1.10 GHz, a boost of up to 5.30 GHz & 68 MB of cache (L2+L3). The CPU also comes at a slightly lower price point of $549.00 US. Now the CPU is tested within the Geekbench 5 benchmark using an ASUS TUF Gaming B660M-PLUS WIFI board and coupled with 64 GB of DDR5 memory.

The CPU scored 2178 points in the single-core and 17339 points in the multi-core tests. We used the Intel Core i9-12900K for comparison which scores 1901 points in single-core and 17272 points in multi-core tests. This puts the Intel Core i9-13900T up to 15% faster in single-core and slightly faster in multi-threaded tests which is very impressive considering the Core i9-12900K also has a higher 125W base TDP (3.58x higher) and a peak TDP rating of 241W (2.27x higher).

Intel Core i9-13900KS Single-Thread CPU Benchmark (Geekbench 5)

Single-Core


050010001500200025003000




050010001500200025003000





Core i9-13900KS

2.3k


Core i9-13900K

2.2k


Ryzen 9 7900X

2.2k


Ryzen 9 7950X

2.2k


Ryzen 7 7700X

2.2k


Core i9-13900T

2.2k


Ryzen 5 7600X

2.2k


Ryzen 9 7900

2.1k


Core i9-13900

2.1k


Ryzen 7 7700

2.1k


Core i9-12900KS

2.1k


Core i9-13900HX

2k


Ryzen 5 7600

2k


Core i7-13700K

2k


Core i5-13600K

1.9k


Core i9-12900K

1.9k


Core i7-12700K

1.9k


M2 Max

1.9k


M1 Max

1.8k


Core i5-12600K

1.7k


Ryzen 9 5950X

1.7k


Ryzen 7 5800X

1.7k


Ryzen 9 5900X

1.7k


Ryzen 5 5600X

1.6k







Intel Core i9-13900KS Multi-Thread CPU Benchmark (Geekbench 5)

Multi-Core


050001000015000200002500030000




050001000015000200002500030000





Core i9-13900KS

26.8k


Core i9-13900K

24.3k


Ryzen 9 7950X

24.4k


Core i9-13900HX

20.9k


Core i9-13900

20.1k


Core i7-13700K

19.8k


Ryzen 9 7900X

19.3k


Core i9-12900KS

19k


Ryzen 9 7900

18.6k


Core i9-13900T

17.3k


Core i9-12900K

17.3k


Ryzen 9 5950X

16.5k


Core i5-13600K

16.1k


M2 Max

14.6k


Core i7-12700K

14.1k


Ryzen 7 7700X

14.1k


Ryzen 9 5900X

14k


Ryzen 7 7700

12.7k


M1 Max

12.3k


Core i5-12600K

11.6k


Ryzen 5 7600X

11.4k


Ryzen 5 7600

11.3k


Ryzen 7 5800X

10.3k


Ryzen 5 5600X

8.2k






This goes off to show the immense efficiency that Intel's 10nm ESF process node and the new hybrid architecture packs and we will also get to see some similar results with the mobility lineup, especially the 13th Gen HX parts which are going to ship in enthusiast-grade gaming laptops in the coming months. AMD also introduced its brand new 65W Ryzen 7000 Non-X CPUs which have been showcasing some impressive efficiency feats on their own with the Zen 4 core architecture.
News Source: Benchleaks
				
				Share this story
				 Facebook
 Twitter






Deal of the Day











Further Reading




 AMD Ryzen 9 7950X3D CPU Shown To Beat Intel Core i9-13900K In Games With Up To 24% Lead


 Intel Core i9-13980HX CPU Powered MSI Raider GE78HX Laptop Matches High-End Desktop CPUs In Performance


 Intel Core i9-13980HX Flagship Raptor Lake-HX CPU Spotted In ASUS’s Next-Gen ROG STRIX Laptop


 It’s Over 9000! Intel Core i9-13900KS Becomes The First CPU To Achieve 9 GHz Frequency World Record













Comments




Please enable JavaScript to view the comments.










Trending Stories


NASA Captures Star Eaten By Black Hole 300 Million Light Years Away



				43 Active Readers



SpaceX’s Rockets Split Up In Mid Air For Rare & Stunning Views At 5,000 Km/h+



				38 Active Readers



Intel Core i9-13900T CPU Benchmarks Show Faster Than 12900K 125W Performance at 35W



				24 Active Readers



JEDEC Plans To Ditch SO-DIMM & Embrace CAMM As The Next Memory Standard For Laptops



				24 Active Readers



Intel’s Core i9-13980HX CPU Makes Laptops Faster Than AMD’s HEDT Threadripper 32-Core Chips



				23 Active Readers








Popular Discussions


AMD Radeon RX 7900 XTX Failure Rates Reportedly At 11%, RMA’s Piling Up But Users Not Receiving Cards



				3100 Comments



Intel Lunar Lake To Feature A Brand New CPU Architecture Built From The Ground-Up, Perf/Watt Focused at Mobile



				2757 Comments



AMD To Give The Love of 3D V-Cache This Valentines With Its Ryzen 7000 X3D CPUs Launch



				2021 Comments



Intel Arc A770 Performs Above AMD & NVIDIA In DirectStorage 1.1 Performance Benchmark



				1803 Comments



AMD Radeon RX 6000 GPUs Mysteriously Start Dying, German Repair Shop Receives 48 Cards With Cracked Chips



				1701 Comments








	 







"
https://news.ycombinator.com/rss,Will Floating Point 8 Solve AI/ML Overhead?,https://semiengineering.com/will-floating-point-8-solve-ai-ml-overhead/,Comments,"












Will Floating Point 8 Solve AI/ML Overhead?

























































































 
 
 












 










 


Search for:



 Subscribe

中文 English 


















Home
Systems & Design
Low Power - High Performance
Manufacturing, Packaging & Materials
Test, Measurement & Analytics
Auto, Security & Pervasive Computing




Special Reports

Business & Startups
Jobs
Knowledge Center
Technical Papers 

Home';
				AI/ML/DLArchitecturesAutomotiveCommunication/Data MovementDesign & VerificationLithographyManufacturingMaterialsMemoryOptoelectronics / PhotonicsPackagingPower & PerformanceQuantumSecurityTest & AnalyticsTransistorsZ-End Applications


Events & Webinars 

Events
Webinars



Videos & Research

Videos
Industry Research



Newsletters





MENU 

Home
Special Reports
Systems & Design
Low Power-High Performance
Manufacturing, Packaging & Materials
Test, Measurement & Analytics
Auto, Security & Pervasive Computing
Knowledge Center
Videos
Startup Corner
Business & Startups
Jobs
Technical Papers 
Events
Webinars
Industry Research
Special Reports







































Home >
                                                                    Low Power-High Performance >
                                                                Will Floating Point 8 Solve AI/ML Overhead?                                                    

















Low Power-High Performance

Will Floating Point 8 Solve AI/ML Overhead?









Less precision equals lower power, but standards are required to make this work.





								January 12th, 2023 - 

								By: Karen Heyman








While the media buzzes about the Turing Test-busting results of ChatGPT, engineers are focused on the hardware challenges of running large language models and other deep learning networks. High on the ML punch list is how to run models more efficiently using less power, especially in critical applications like self-driving vehicles where latency becomes a matter of life or death.
AI already has led to a rethinking of computer architectures, in which the conventional von Neumann structure is replaced by near-compute and at-memory floorplans. But novel layouts aren’t enough to achieve the power reductions and speed increases required for deep learning networks. The industry also is updating the standards for floating-point (FP) arithmetic.
“There is a great deal of research and study on new data types in AI, as it is an area of rapid innovation,” said David Bell, product marketing director, Tensilica IP at Cadence. “Eight-bit floating-point (FP8) data types are being explored as a means to minimize hardware — both compute resources and memory — while preserving accuracy for network models as their complexities grow.”
As part of that effort, researchers at Arm, Intel, and Nvidia published a white paper proposing “FP8 Formats for Deep Learning.” [1]
“Bit precision has been a very active topic of debate in machine learning for several years,” said Steve Roddy, chief marketing officer at Quadric. “Six or eight years ago when models began to explode in size (parameter count), the sheer volume of shuffling weight data into and out of training compute (either CPU or GPU) became the performance limiting bottleneck in large training runs. Faced with a choice of ever more expensive memory interfaces, such as HBM, or cutting bit precision in training, a number of companies experimented successfully with lower-precision floats. Now that networks have continued to grow exponentially in size, the exploration of FP8 is the next logical step in reducing training bandwidth demands.”
How we got here
Floating-point arithmetic is a kind of scientific notation, which condenses the number of digits needed to represent a number. This trick is pulled off by an arithmetic expression first codified by IEEE working group 754 in 1986, when floating-point operations generally were performed on a co-processor.
IEEE 754 describes how the radix point (more commonly known in English as the “decimal” point) doesn’t have a fixed position, but rather “floats” where needed in the expression. It allows numbers with extremely long streams of digits (whether originally to the left or right of a fixed point) to fit into the limited bit-space of computers. It works in either base 10 or base 2, and it’s essential for computing, given that binary numbers extend to many more digits than decimal numbers (100 = 1100100).
 

Fig. 1: 12.345 as a base-10 floating-point number. Source: Wikipedia
 
While this is both an elegant solution and the bane of computer science students worldwide, its terms are key to understanding how precision is achieved in AI. The statement has three parts:

A sign bit, which determines whether the number is positive (0) or negative (1);
An exponent, which determines the position of the radix point, and
A mantissa, or significand, which represents the most significant digits of the number.


Fig. 2: IEEE 754 floating-point scheme. Source: WikiHow
As shown in figure 2, while the exponent gains 3 bits in a 64-bit representation, the mantissa jumps from 32 bits to 52 bits. Its length is key to precision.
IEEE 754, which defines FP32 bit and FP64, was designed for scientific computing, in which precision was the ultimate consideration. Currently, IEEE working group P3109 is developing a new standard for machine learning, aligned with the current (2019) version of 754. P3109 aims to create a floating-point 8 standard.
Precision tradeoffs
Machine learning often needs less precision than a 32-bit scheme. The white paper proposes two different flavors of FP8: E4M3 (4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa).
“Neural networks are a bit strange in that they are actually remarkably tolerant to relatively low precision,” said Richard Grisenthwaite, executive vice president and chief architect at Arm. “In our paper, we showed you don’t need 32 bits of mantissa for precision. You can use only two or three bits, and four or five bits of exponent will give you sufficient dynamic range. You really don’t need the massive precision that was defined in 754, which was designed for finite element analysis and other highly precise arithmetic tasks.”
Consider a real-world example: A weather forecast needs the extreme ranges of 754, but a self-driving car doesn’t need the fine-grained recognition of image search. The salient point is not whether it’s a boy or girl in the middle of the road. It’s just that the vehicle must immediately stop, with no time to waste on calculating additional details. So it’s fine to use a floating point with a smaller exponent and much smaller mantissa, especially for edge devices, which need to optimize energy usage.
“Energy is a fundamental quantity and no one’s going to make it go away as an issue,” said Martin Snelgrove, CTO of Untether AI. “And it’s also not a narrow one. Worrying about energy means you can’t afford to be sloppy in your software or your arithmetic. If doing a 32-bit floating point makes everything easier, but massively more power consuming, you just can’t do it. Throwing an extra 1,000 layers at something makes it slightly more accurate, but the value for power isn’t there. There’s an overall discipline about energy — the physics says you’re going to pay attention to this, whether you like it or not.”
In fact, to save energy and performance overhead, many deep learning networks had already shifted to an IEEE-approved 16-bit floating point and other formats, including mantissa-less integers. [2]
“Because compute energy and storage is at a premium in devices, nearly all high-performance device/edge deployments of ML always have been in INT8,” Quadric’s Roddy said. “Nearly all NPUs and accelerators are INT-8 optimized. An FP32 multiply-accumulate calculation takes nearly 10X the energy of an INT8 MAC, so the rationale is obvious.”
Why FP8 is necessary
The problem starts with the basic design of a deep learning network. In the early days of AI, there were simple, one-layer models that only operated in a feedforward manner. In 1986, David Rumelart, Geoffrey Hinton, and Ronald Williams published a breakthrough paper on back-propagation [3] that kicked off the modern era of AI. As their abstract describes, “The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units, which are not part of the input or output, come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units.”
In other words, they created a system in which better results could be achieved by adding more and more layers into a model, which would be improved by incorporating “learned” adjustments. Decades later, their ideas so vastly improved machine translation and transcription that college professors remain unsure whether undergraduates’ essays have been written by bots.
But additional layers require additional processing power. “Larger networks with more and more layers were found to be progressively more successful at neural networks tasks, but in certain applications this success came with an ultimately unmanageable increase in memory footprint, power consumption, and compute resources. It became imperative to reduce the size of the data elements (activations, weights, gradients) from 32 bits, and so the industry started using 16-bit formats, such as Bfloat16 and IEEE FP16,” according to the paper jointly written by Arm/Intel/Nvidia.
“The tradeoff fundamentally is with an 8-bit floating-point number compared to a 32-bit one,” said Grisenthwaite. “I can have four times the number of weights and activations in the same amount of memory, and I can get far more computational throughput as well. All of that means I can get much higher performance. I can make the models more involved. I can have more weights and activations at each of the layers. And that’s proved to be more useful than each of the individual points being hyper-accurate.”
Behind these issues are the two basic functions in machine learning, training and inference. Training is the first step in which, for example, the AI learns to classify features in an image by reviewing a dataset. With inference, the AI is given novel images outside of the training set and asked to classify them. If all goes as it should, the AI should distinguish that tails and wings are not human features, and at finer levels, that airplanes do not have feathers and a tube with a tail and wings is not a bird.
“If you’re doing training or inference, the math is identical,” said Ron Lowman, strategic marketing manager for IoT at Synopsys. “The difference is you do training over a known data set thousands of times, maybe even millions of times, to train what the results will be. Once that’s done, then you take an unknown picture and it will tell you what it should be. From a math perspective, a hardware perspective, that’s the big difference. So when you do training, you want to do that in parallel, rather than doing it in a single hardware implementation, because the time it takes to do training is very costly. It could take weeks or months, or even years in some cases, and that just costs too much.”
In industry, training and inference have become separate specialties, each with its own dedicated teams.
“Most companies that are deploying AI have a team of data scientists that create neural network architectures and train the networks using their datasets,” said Bob Beachler, vice president of product at Untether AI. “Most of the autonomous vehicle companies have their own data sets, and they use that as a differentiating factor. They train using their data sets on these novel network architectures that they come up with, which they feel gives them better accuracy. Then that gets taken to a different team, which does the actual implementation in the car. That is the inference portion of it.”
Training requires a wide dynamic range for the continual adjustment of coefficients that is the hallmark of backpropagation. The inference phase is computing on the inputs, rather than learning, so it needs much less dynamic range. “Once you’ve trained the network, you’re not tweaking the coefficients, and the dynamic range required is dramatically reduced,” explained Beachler.
For inference, continuing operations in FP32 or FP16 is just unnecessary overhead, so there’s a quantization step to shift the network down to FP8 or Integer 8 (Int8), which has become something of a de facto standard for inference, driven largely by TensorFlow.
“The idea of quantization is you’re taking all the floating point 32 bits of your model and you’re essentially cramming it into an eight-bit format,” said Gordon Cooper, product manager for Synopsys’ Vision and AI Processor IP. “We’ve done accuracy tests and for almost every neural network-based object detection. We can go from 32-bit floating point to Integer 8 with less than 1% accuracy loss.”
For quality/assurance, there’s often post-quantization retraining to see how converting the floating-point value has affected the network, which could iterate through several passes.
This is why training and inference can be performed using different hardware. “For example, a common pattern we’ve seen is accelerators using NVIDIA GPUs, which then end up running the inference on general purpose CPUs,” said Grisenthwaite.
The other approach is chips purpose-built for inference.
“We’re an inference accelerator. We don’t do training at all,” says Untether AI’s Beachler. “We place the entire neural network on our chip, every layer and every node, feed data at high bandwidth into our chip, resulting in each and every layer of the network computed inside our chip. It’s massively parallelized multiprocessing. Our chip has 511 processors, each of them with single instruction multiple data (SIMD) processing. The processing elements are essentially multiply/accumulate functions, directly attached to memory. We call this the Energy Centric AI computing architecture. This Energy Centric AI Computing architecture results in a very short distance for the coefficients of a matrix vector to travel, and the activations come in through each processing element in a row-based approach. So the activation comes in, we load the coefficients, do the matrix mathematics, do the multiply/accumulate, store the value, move the activation to the next row, and move on. Short distances of data movement equates to low power consumption.”
In broad outline, AI development started with CPUs, often with FP co-processors, then moved to GPUs, and now is splitting into a two-step process of GPUs (although some still use CPUs) for training and CPUs or dedicated chips for inference.
The creators of general-purpose CPU architectures and dedicated inference solutions may disagree on which approach will dominate. But they all agree that the key to a successful handoff between training and inference is a floating-point standard that minimizes the performance overhead and risk of errors during quantization and transferring operations between chips. Several companies, including NVIDIA, Intel, and Untether, have brought out FP8-based chips.
“It’s an interesting paper,” said Cooper. “8-bit floating point, or FP8, is more important on the training side. But the benefits they’re talking about with FP8 on the inference side is that you possibly can skip the quantization. And you get to match the format of what you’ve done between training and inference.”
Nevertheless, as always, there are still many challenges still to consider.
“The cost is one of model conversion — FP32 trained model converted to INT8. And that conversion cost is significant and labor intensive,” said Roddy. “But if FP8 becomes real, and if the popular training tools begin to develop ML models with FP8 as the native format, it could be a huge boon to embedded inference deployments. Eight-bit weights take the same storage space, whether they are INT8 or FP8. The energy cost of moving 8 bits (DDR to NPU, etc.) is the same, regardless of format. And a Float8 multiply-accumulate is not significantly more power consumptive than an INT8 MAC. FP8 would rapidly be adopted across the silicon landscape.  But the key is not whether processor licensors would rapidly adopt FP8. It’s whether the mathematicians building training tools can and will make the switch.”
Conclusion
As the quest for lower power continues, there’s debate about whether there might even be a FP4 standard, in which only 4 bits carry a sign, an exponent, and mantissa. People who follow a strict neuromorphic interpretation have even discussed binary neural networks, in which the input functions like an axon spike, just 0 or 1.
“Our sparsity level is going to go up,” said Untether’s Snelgrove. “There are hundreds of papers a day on new neural net techniques. Any one of them could completely revolutionize the field. If you talk to me in a year, all of these words could mean different things.”
At least at the moment, it’s hard to imagine that lower FPs or integer schemes could contain enough information for practical purposes. Right now, various flavors of FP8 are undergoing the slow grind towards standardization. For example, Graphcore, AMD, and Qualcomm have also brought a detailed FP8 proposal to the IEEE. [4]
“The advent of 8-bit floating point offers tremendous performance and efficiency benefits for AI compute,” said Simon Knowles, CTO and co-founder of Graphcore. “It is also an opportunity for the industry to settle on a single, open standard, rather than ushering in a confusing mix of competing formats.”
Indeed, everyone is optimistic there will be a standard — eventually. “We’re involved in IEEE P3109, as are many, many companies in this industry,” said Arm’s Grisenthwaite. “The committee has looked at all sorts of different formats. There are some really interesting ones out there. Some of them will stand the test of time, and some of them will fall by the wayside. We all want to make sure we’ve got complete compatibility and don’t just say, ‘Well, we’ve got six different competing formats and it’s all a mess, but we’ll call it a standard.”
References 

Micikevicius, P., et al. FP8 Formats for Deep Learning. Last revised Sep 29 2022 arXiv:2209.05433v2. https://doi.org/10.48550/arXiv.2209.05433
Sapunov, G. FP64, FP32, FP16, BFLOAT16, TF32, and other members of the ZOO. Medium. May 16, 2020. https://moocaholic.medium.com/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407
Rumelhart, D., Hinton, G. & Williams, R. Learning representations by back-propagating errors. Nature 323, 533–536 (1986). https://doi.org/10.1038/323533a0
Noune, B. 8-bit Numerical Formats for Deep Neural Networks. Submitted June 6 2022 arXiv:2206.02915 https://doi.org/10.48550/arXiv.2206.02915

Additional Reading:
How to convert a number from decimal to IEEE 754 Floating Point Representation.
Number Representation and Computer Arithmetic
https://web.ece.ucsb.edu/~parhami/pubs_folder/parh02-arith-encycl-infosys.pdf
Computer Representation of Numbers and Computer Arithmetic
https://people.cs.vt.edu/~asandu/Courses/CS3414/comp_arithm.pdf











 Tags: 8-bit floating point AI AI models AI/ML/DL AMD ARM base 10 base 2 BFLOAT16 Cadence Cadence Design Systems ChatGPT deep learning E4M3 E5M2 edge inference floating point FP16 FP32 FP64 FP8 Graphcore IEEE 754 inference INT8 Integer 8 Intel machine learning mantissa ML training neural network Nvidia P3109 Quadric Quadric.io Qualcomm Synopsys training Untether AI





Karen Heyman   (all posts)


							Karen Heyman is a technology editor at Semiconductor Engineering.
						





Leave a Reply Cancel replyComment * Name*(Note: This name will be displayed publicly)
Email*(This will not be displayed publicly) 
 

Δ 




 Knowledge Centers Blogs 
Spiking Neural Network (SNN)

Published on June 23, 2021



Recurrent Neural Network (RNN)

Published on June 25, 2019



Edge Computing

Published on April 4, 2019



Neural Networks

Published on July 25, 2017



Architectures

Published on 



Machine Learning (ML)

Published on May 10, 2017



Convolutional Neural Network (CNN)

Published on 



Artificial Intelligence (AI)

Published on 




Technical Papers
Manycore-FPGA Architecture Employing Novel Duet Adapters To Integrate eFPGAs in a Scalable, Non-Intrusive, Cache-Coherent Manner (Princeton) January 16, 2023 by Technical Paper LinkHardware Trojan Detection Case Study Based on 4 Different ICs Manufactured in Progressively Smaller CMOS Process Technologies January 11, 2023 by Technical Paper LinkQuantum Computing Architecture Enabling  Communication Between Superconducting Quantum Processors (MIT) January 11, 2023 by Technical Paper LinkArbitrary Precision DNN Accelerator Controlled by a RISC-V CPU (Ecole Polytechnique Montreal, IBM, Mila, CMC) January 10, 2023 by Technical Paper LinkTechnique For Printing Electronic Circuits Onto Curved & Corrugated Surfaces Using Metal Nanowires (NC State) January 10, 2023 by Technical Paper Link 

  Trending Articles

RISC-V Pushes Into The Mainstream

Open-source processor cores are beginning to show up in heterogeneous SoCs and packages.


by Marie C. Baca and Ed Sperling



Will Floating Point 8 Solve AI/ML Overhead?

Less precision equals lower power, but standards are required to make this work.


by Karen Heyman



How Secure Are RISC-V Chips?

Open source by itself doesn’t guarantee security. It still comes down to the fundamentals of design.


by Jeff Goldman



RISC-V decoupled Vector Processing Unit (VPU) For HPC



by Technical Paper Link



Startup Funding: December 2022

Wafer manufacturing and GPUs draw investment; 106 companies raise $2.8B.


by Jesse Allen






Knowledge Centers Entities, people and technologies explored
Learn More



Related Articles

Foundational Changes In Chip Architectures

New memory approaches and challenges in scaling CMOS point to radical changes — and potentially huge improvements — in semiconductor designs. 


by Brian Bailey



Will Floating Point 8 Solve AI/ML Overhead?

Less precision equals lower power, but standards are required to make this work.


by Karen Heyman



How Memory Design Optimizes System Performance

Changes are steady in the memory hierarchy, but how and where that memory is accessed is having a big impact.


by John Koon



Startup Funding: October 2022

113 startups raise $3.5B; batteries, AI, and new architectures top the list.


by Jesse Allen



Startup Funding: November 2022

127 startups raise $2.6B; data center connectivity, quantum computing, and batteries draw big funding.


by Jesse Allen



IC Stresses Affect Reliability At Advanced Nodes

Thermal mismatch in heterogeneous designs, different use cases, can impact everything from accelerated aging to warpage and system failures.


by Ann Mutschler



3D-IC Reliability Degrades With Increasing Temperature

Electromigration and other aging factors become more complicated along the z axis.


by Ann Mutschler



On-Chip Power Distribution Modeling Becomes Essential Below 7nm

Why and when it’s needed, and what tools and technologies are required.


by Ann Mutschler











Sponsors






























Advertise with us





Advertise with us





Advertise with us





Newsletter Signup



Popular Tags2.5D
5G
7nm
advanced packaging
AI
ANSYS
Apple
Applied Materials
ARM
Atrenta
automotive
business
Cadence
EDA
eSilicon
EUV
finFETs
GlobalFoundries
Google
IBM
imec
Intel
IoT
IP
Lam Research
machine learning
memory
Mentor
Mentor Graphics
MIT
Moore's Law
Nvidia
NXP
Qualcomm
Rambus
Samsung
security
SEMI
Siemens
Siemens EDA
software
Sonics
Synopsys
TSMC
verification
Recent CommentsWZIS on Arbitrary Precision DNN Accelerator Controlled by a RISC-V CPU (Ecole Polytechnique Montreal, IBM, Mila, CMC)Rama Chaganti on Growing System Complexity Drives More IP ReuseTL on How Secure Are RISC-V Chips?Frank on The Good And Bad Of Bi-Directional ChargingSandeep Dixit on The Good And Bad Of Bi-Directional ChargingHertz on How Secure Are RISC-V Chips?Andrew on How Software Utilizes CoresAsaf Jivilik on Cybord: Electronic Component TraceabilitySantosh Kurinec on Where All The Semiconductor Investments Are Goingdick freebird on Designing And Securing Chips For Outer SpaceAkshay on Designing And Securing Chips For Outer SpaceRaj on Is UCIe Really Universal?Andrew TAM on How Software Utilizes CoresRiko R on Designing For Multiple DieDan Ganousis on RISC-V Pushes Into The MainstreamIvan Batinic on IC Stresses Affect Reliability At Advanced NodesGiovanni Lostumbo on A Power-First ApproachMohammed Zakir Hussain on Embracing the Challenges Of Cybersecurity In Automotive ApplicationsLaura Peters on Week In Review: Manufacturing, TestAiv on Week In Review: Manufacturing, TestRoss Youngblood on High Voltage Testing Races AheadMark Olivas on Cybord: Electronic Component TraceabilityKarl Stevens on The Drive Toward Virtual PrototypesRon Lavallee on The Politics Of StandardsDenis McCarthy on Hot Trends In Semiconductor Thermal ManagementTom Smith on Are We Too Hard On Artificial Intelligence For Autonomous Driving?Maya F on Where All The Semiconductor Investments Are GoingSaikatm on Balancing Power And Heat In Advanced Chip DesignsDoug L. on Holistic 3D-IC Interposer Analysis In Product DesignsAndy Deng on Post-Quantum And Pre-Quantum Security Issues GrowJohn Dunn on Post-Quantum And Pre-Quantum Security Issues Growmadmax2069 on Chip Design Shifts As Fundamental Laws Run Out Of SteamMatthew Slyman on Chip Design Shifts As Fundamental Laws Run Out Of SteamDouglas MacIntyre on Chip Design Shifts As Fundamental Laws Run Out Of SteamJose on Universal Verification Methodology Running Out Of SteamZhengji Lu on Moving From AMBA ACE to CHI For CoherencyJohn Bennice on A Power-First Approach[email protected] on Chip Design Shifts As Fundamental Laws Run Out Of SteamMatthew on Chip Design Shifts As Fundamental Laws Run Out Of SteamKarthik Krishnamoorthy on AI-Powered VerificationCPlusPlus4Ever on Chip Design Shifts As Fundamental Laws Run Out Of SteamDouglas on Chip Design Shifts As Fundamental Laws Run Out Of SteamBowie Poag on Chip Design Shifts As Fundamental Laws Run Out Of SteamEugene on Startup Funding: October 2022Wesley Sung on Fan-Out And Packaging ChallengesHong Xiao on Chip Design Shifts As Fundamental Laws Run Out Of SteamRobert Anderson on Chip Design Shifts As Fundamental Laws Run Out Of SteamMike Frank on A Power-First ApproachWilliam Ruby on A Power-First ApproachPeter C Salmon on A Power-First ApproachDr. Dev Gupta on Which Foundry Is In The Lead? It Depends.Steve Hoover on A Power-First ApproachDylanP on Which Foundry Is In The Lead? It Depends.Asaf Jivilik on Cybord: Electronic Component TraceabilityChris @ crossPORt on Foundational Changes In Chip ArchitecturesMark Olivas on Cybord: Electronic Component TraceabilityAri ben David on Constraints On The Electricity GridJeff Zika on Auto Safety Tech Adds New IC Design ChallengesJung Yoon on Foundational Changes In Chip ArchitecturesSchrodinger's Cat's Advocate on Foundational Changes In Chip ArchitecturesRigTig on Foundational Changes In Chip ArchitecturesSteve on Foundational Changes In Chip ArchitecturesPrashant Purwar on Why Mask Blanks Are CriticalMostafa Abdelgawwad on Radar For Automotive: How Far Can A Radar See?yieldWerx on Managing Wafer RetestJohn Horner on A Brief History of TestLakshm J on ESD Requirements Are ChangingDr. Dev Gupta on Improving Redistribution Layers for Fan-out Packages And SiPsAkarsh on Better PMIC Design Using Multi-Physics SimulationTodd Bermensolo on Reducing Schedule Slips With Automated Post-Route Verification Of SerDes High Speed Serial LinksLaur Rizzatti on Why Geofencing Will Enable L5Raj Raghuram on The Complex Art Of Handling S-ParametersStevo on CHIPS Act: U.S. Releases New Implementation StrategySantosh Kurinec on Quantum Research Bits: Sept. 12Lewis Sternberg on ML And UVM Share Same FlawsRoger Stierman on L5 Adoption Hinges on 5G/6GMarcel on MicroLEDs Move Toward CommercializationRagu Athreya on Is There A Limit To The Number of Layers In 3D-NAND?Brian Bailey on AI Power Consumption ExplodingDavid S on AI Power Consumption ExplodingMike Cormack on Cryogenic CMOS Becomes CoolLance Harvie on New Uses For AI In ChipsDoc R on Electronics And Its Role In Climate ChangeMagdy Abadir on Is Standardization Required For Security?guest on How Overlay Keeps Pace With EUV PatterningSantosh Kurinec on Week In Review, Manufacturing, Testsravani on Timing Library LVF Validation For Production Design FlowsDr. F on A Sputnik Moment For ChipsGary Dagastine on A Sputnik Moment For ChipsMike Sottak on A Sputnik Moment For ChipsRobert Pearson on A Sputnik Moment For ChipsRaye E. Ward on A Sputnik Moment For ChipsMichael Williams on A Look Inside RF DesignSURESHBABU CHILUGODU on Week In Review: Manufacturing, TestJC Bouzigues, Menta on Customizing ProcessorsSteve Swendrowski on IC Package Illustrations, From 2D To 3DEMV on Hybrid Bonding Moves Into The Fast LaneDr. Appo van der Wiel on Variation Making Trouble In Advanced Packageswang yu on Verification Of Functional SafetyFrederick Chen on High-NA EUV May Be Closer Than It AppearsFact Cheq on The Week In Review: DesignShiwen Huang on E-beam’s Role Grows For Detecting IC DefectsAdele Hars on Wafer Shortage Improvement In Sight For 300mm, But Not 200mmDavid A. Humphreys on IMS2022 Booth Tour: EDA And Measurement Science ConvergeMerritt on Can Analog Make A Comeback?subra ganesan on Meeting Processor Performance And Safety Requirements For New ADAS & Autonomous Vehicle SystemsGeorge on Building A More Secure SoCAmit Garg on A New Breed Of EDA RequiredKarl Stevens on A Minimal RISC-VKarl Stevens on EDA Gaps At The Leading EdgeMicah Forstein MS. on Risks Rise As Robotic Surgery Goes MainstreamDr. Punam Raskar on Who Does Processor Validation?Dr. Dev Gupta on Variation Making Trouble In Advanced PackagesCox on DRAM Thermal Issues Reach Crisis PointDavid Leary on DRAM Thermal Issues Reach Crisis PointGeeeeeee on DRAM Thermal Issues Reach Crisis PointPedro Ferro Laks on SOT-MRAM To Challenge SRAMObviously silly on DRAM Thermal Issues Reach Crisis PointSimon on DRAM Thermal Issues Reach Crisis PointGareth on Energy Harvesting Starting To Gain Traction

 





Research Bits: Jan. 17 Jesse AllenChip Industry’s Technical Pape... Linda Christensen 










  










About

About us
Contact us
Advertising on SemiEng
Newsletter SignUp



Navigation



Homepage
Special Reports
Systems & Design
Low Power-High Perf
Manufacturing, Packaging & Materials
Test, Measurement & Analytics
Auto, Security & Pervasive Computing




Videos
Jobs
Technical Papers
Events
Webinars
Knowledge Centers

Industry Research
Business & Startups
Newsletters





Connect With Us

Facebook
Twitter  @semiEngineering
LinkedIn
YouTube




Copyright ©2013-2023 SMG   |  Terms of Service  |  Privacy Policy





This site uses cookies. By continuing to use our website, you consent to our Cookies PolicyACCEPT Manage consent




Close






Privacy Overview 
This website uses cookies to improve your experience while you navigate through the website. The cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. We do not sell any personal information.

By continuing to use our website, you consent to our Privacy Policy. If you access other websites using the links provided, please be aware they may have their own privacy policies, and we do not accept any responsibility or liability for these policies or for any personal data which may be collected through these sites. Please check these policies before you submit any personal information to these sites.

 





								Necessary							


Necessary

Always Enabled




									Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.								






								Non-necessary							


Non-necessary





									Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website.								












SAVE & ACCEPT










 

 
 

 
























 








"
https://news.ycombinator.com/rss,We Need to Know LR and Recursive Descent Parsing Techniques,https://tratt.net/laurie/blog/2023/why_we_need_to_know_lr_and_recursive_descent_parsing_techniques.html,Comments,"






    Laurence Tratt: Why We Need to Know LR and Recursive Descent Parsing Techniques














Home
        > Blog



≡

  Home
> Blog






email  
Mastodon  
Twitter  


Mastodon  
Twitter








Why We Need to Know LR and Recursive Descent Parsing Techniques

       January 17 2023




Blog archive
 
Last 10 blog posts
Why We Need to Know LR and Recursive Descent Parsing Techniques
Compiled and Interpreted Languages: Two Ways of Saying Tomato
Software Security Research Position
How Might Generative AI Change Programming?
pizauth: differentiating transient from permanent errors
November Links
More Evidence for Problems in VM Warmup
What is a Research Summer School?
October Links
pizauth: another alpha release
 






A couple of people have asked me for my thoughts on part an article from Tiark Rompf
called (roughly) ""Just
Write the Parser"" which advocates the use of recursive descent parsing over
more ""formal"" approaches to parsing, at least in the context of teaching
compilers. I'm fairly sure I read this article a
couple of years ago, but it may have been updated, or simply have been
discovered anew. Either way, since I admire both Tiark and his work
a great deal, it was useful for me to engage with his ideas and
compare them to my own.

In this post, I'm going to summarise Tiark's argument as I see it, and try
to explain where I agree and disagree with that summary. I will inevitably
rehash many of the arguments from my 2020 ""Which Parsing
Approach?"" post, but I'm going to try to focus on some factors which I now
realise were at best buried, and at worst inadequately explained, in that
previous post.


Summarising the argument
In essence I believe Tiark is making two points:


Making parsing a major component of teaching compilers is misguided.
Context-free grammars, and their associated parsing techniques, don't align
well with real-world compilers, and thus we should deemphasise CFGs (Context-Free
Grammars) and their associated parsing algorithms.


I agree with the first point. Learning has to involve prioritisation: we can't
learn the underlying theory of everything. Parsing seems to me to be
one of those topics where a fairly superficial understanding of the underlying
theory is sufficient for most people to use it effectively. Personally I would
rather spend the time that a deep understanding of the underlying theory of parsing
requires elsewhere: there are many other parts of compilers that better reward
a deeper understanding [1].

However, I disagree with the second point — or, at least, I disagree
with part of it. It is true that many real-world languages' syntaxes are not compatible
with LR parsing (and the like) and must be parsed using
recursive descent parsing.
Without wishing to put words in his mouth, I suspect that Tiark sees this as a virtue of
recursive descent parsing.

In contrast, I have come, over time, to see
languages that require recursive descent parsing as reflecting
a flaw in our approach to language design [2].
I believe it has led us to design languages
that are not just hard to parse, that don't just tend to have buggy parsers,
but which are harder than necessary to understand.


Differentiating LR and recursive descent parsing
Let's start with some basic definitions. A grammar is a
specification of the valid sentences in a language. Note that grammars need not
be executable — indeed, they may be specified formally or informally,
explicitly or implicitly. A parser is an implementation of a grammar, which
checks whether an input is valid with respect to that grammar or not. Grammars
are ambiguous (e.g. a grammar such as E: E ""+"" E | E ""-""
E is ambiguous because 2+3*4 can be parsed as equivalent to
either (2+3)*4 or 2+(3*4) with it) or unambiguous.

For example, all of us share a common understanding of the grammar for basic
arithmetic expressions. We can then implement an LR or a recursive descent
parser which takes arithmetic expressions as inputs and checks whether they
conform to the arithmetic grammar.

In practise, grammars and parsers are often closer in nature than their
abstract definitions suggest. In one direction, we can automatically derive
parsers from some grammars (e.g. those written to conform to the LR subset of
CFGs). In the other direction, a parser implicitly defines a grammar [3]. There is
a subtle corollary to this: if I manually create a parser from the
specification of a grammar, I may introduce bugs, which means that the parser's
implicit grammar may not quite match the explicit grammar I thought I was
implementing!

There are various ways of implementing parsers, but for me there are
generally only two which we really need to consider: LR parsing and recursive
descent parsing.

LR parsing defines a subset of CFGs: every grammar in the LR subset is
guaranteed to be unambiguous [4]. From an LR
grammar we can automatically derive an LR parser (e.g. using a tool like YACC
or lrpar). A challenge with LR
parsing is that some grammars are difficult to express unambiguously (e.g.
arithmetic expressions): some are impossible to express unambiguously (e.g.
many real-world programming languages).

In practise, a recursive descent parser can be thought of as meaning ""a
hand-written written parser"": unless you go out of your way to do otherwise,
nearly all hand-written parsers are recursive descent parsers. As this suggests,
recursive descent parsers are easy to write. Since they're also just normal
programs, we can use them to parse any language we can conceive of.

Presented like this there there doesn't seem to be much of a choice
between the two. Recursive descent parsing is easier and more flexible,
so let's just use that!

However, recursive descent parsing has an inherent consequence: it produces
parsers which resolve ambiguities in the underlying grammar without telling you
that they have been resolved. For example, a recursive descent parser for
arithmetic expressions will parse 2+3*4 as equivalent to either
(2+3)*4 or 2+(3*4), but not both. However, since a
recursive descent parser is just a normal program, you might not even realise
that you have written your parser in such a way that it has chosen to parse
just one of the possibilities — hopefully you chose to parse it as
equivalent to 2+(3*4)!

In contrast, a CFG can't be LR if it has such an ambiguity. If you give a
grammar such as E: E ""+"" E | E ""-"" E to YACC it will tell you it
is ambiguous (using the infamous, and in some ways unfortunate, terminology of ""shift/shift"",
""reduce/reduce"", or ""accept/reduce""). Although few people think of it as such,
LR is effectively a type checker for grammar ambiguity: if you don't get any
errors, your grammar is guaranteed to be unambiguous. One of the beauties of LR
parsing is that the same grammar that has been ""type checked"" to be unambiguous
can then be automatically turned into a parser: there is no way that you can
accidentally introduce ambiguity, or other bugs, into the resulting parser
[5].

Fundamentally, recursive descent parsers have no equivalent ""type checker"",
and we know that we can't build such an equivalent for the general case.
Recursive descent parsers thus parse the languages they parse, but we don't, in
general, know how to recover the grammar that such parsers are implicitly
parsing. That is not the tautology it sounds: my experience is that bugs in
recursive parsers are common. I have had the dubious pleasure of converting a
number of recursive descent parsers to LR grammars and none of the non-trivial
recursive descent parsers has parsed the language their authors expected.
Sometimes they accept inputs they shouldn't; sometimes they reject inputs they
shouldn't.

This wouldn't be as much of a problem if it was easy to test parsers, but it turns out
to be surprisingly hard to create good parser suites. Most parser test suites
that I've seen focus most of their effort on testing that ""good"" inputs are
accepted. Since the range of ""bad"" inputs is, in general, much more diverse,
test suites rarely cover much of the space of ""bad"" inputs to check that
they're rejected [6].

There is an additional, easily over-looked, factor that comes from
relying on recursive descent parsers: learners cannot,
in any sensible time, understand from the recursive descent parser the
language's syntax. Many languages ignore this problem at first, and hope
that learners can guess the language's syntax from examples. This is inevitably
unsatisfactory, and it is then common over time for languages to try deriving
a CFG from the recursive descent parser, to help learners and tool authors.
However, these CFGs rarely fully match the recursive descent parser, leaving
learners confused, and tool authors frustrated.

There are two other use-cases for recursive descent parsing. The first is
performance. Although there aren't, to the best of my knowledge, modern
performance numbers, it is reasonable to assume that recursive descent parsing
is generally faster than LR parsing. However, ""faster"" is relative: on modern
machines, even a naive LR parser on a huge grammar will happily parse thousands
of lines of code a second which is enough for the vast majority of use cases.
The second is error recovery. LR parsers have traditionally had terrible error
recovery. Immodestly, I now claim that some work I was
involved in allows LR parsers to have error recovery that is better than
all but the best recursive descent parsers.


Summary

It seems to me that if you believe static type checking allows you to write
programs that are more reliable than if you used dynamic type checking then,
logically, you must also believe that LR parsing is more reliable than
recursive descent parsing. Similarly, if you believe that explicit static types
help readers better understand code, then you must also believe that
explicit LR grammars better convey to readers the grammar of a language than
the equivalent recursive descent parser.

There is then a separate debate to be had about whether the costs of LR
parsing are worth the benefits. My personal experience is that, for unambiguous
languages, it takes relatively little practise to design LR grammars. It is
then more than just a bonus that we can automatically generate a parser
from the grammar. Indeed, and to my surprise, I've found it markedly quicker to write an LR grammar
than to write, test, and debug the equivalent recursive descent parser.
My advice for writing LR grammars to those new to it boils down to,
in essence, ""write your grammar in
small bits, continually test it for LR-ness, and if you get a shift/reduce
(etc.) error, undo your last change, and try a variation."" If that sounds
trite, or patronising, please bear in mind that it's exactly the approach I
take myself!

If you agree with this line of thinking, then you might find the following
list of recommendations useful:


New languages (whether they be ""programming languages"" or mere
""configuration languages"" and the like) should come with an LR grammar: you
then have a specification that users can understand, and people can automatically
derive parsers for their favourite use-case directly from that specification.
Specifying new languages in this style is not difficult, and the restrictions
on syntax rarely onerous. If writing a grammar in LR style is hard for you as
the language author, it will almost certainly be hard for users to understand!

Existing languages have often evolved in a manner that makes it
difficult, or impossible, to specify an LR grammar. There's no point in
trying to fight this: just use recursive descent parsing.

The more complex your recursive descent parser, the more effort you need to
put into testing. Without extensive testing, your parser will do things you
didn't expect: it is just normal code, after all, and all of us make mistakes
when coding!

If you're writing a parser for a language which doesn't have an LR grammar,
it can be worth trying to create one. If you're lucky you'll find that your
language has an unambiguous grammar; if you're unlucky, you'll at least
understand where some of the points of ambiguity are [7]. I find that this makes writing a recursive descent parser a less fraught
task, as at least I know where the likely bugs due to ambiguity will lie.

If you're writing a parser for a language which is ambiguous, you
might be able to use the conflict resolution offered by YACC and
friends. However, in general, I suggest avoiding YACC's conflict resolution if
possible, as you have to at least somewhat understand the underlying LR
algorithm to understand how conflicts have been resolved.

If you need the best possible performance or error recovery, recursive
descent parsing is the best choice. If you know you will need this and
you're designing a new language, at least create an LL grammar, since from
that you can derive a recursive descent parser. [Bear in mind that LL grammars
are more annoying to write than LR grammars which is why I suggest
avoiding LL unless you have this very specific use case.]

LALR and SLR are subsets of LR parsing that are irritating to use and, on
any machine from the last 20 years, have no meaningful advantages. Avoid.

Otherwise, just use LR parsing, whether it's YACC or the equivalent for
your favourite language (e.g. I am fond of lrpar for Rust since I wrote it).



Looking back to Tiark's article, my opinion on teaching parsing is two-fold.
First, parsing isn't generally worth much of student's time, so teach them the
minimum possible. Second, the minimum possible is, in my opinion, that which is
needed to write: recursive descent parsing, so that they can parse existing
languages which lack an unambiguous grammar; and LR parsing so that they don't
keep designing hard-to-parse languages in the future.





If you’d like updates on new blog posts: follow me on
Mastodon
or Twitter;
or subscribe to the RSS feed;
or subscribe to email updates:




Failed to subscribe: please load page and try again
Sending...




Footnotes
[1] I didn't have the opportunity to study compilers as an undergraduate. If I had, I
would not have wanted parsing to take up more than 1 week of such a course: I'd
have wanted to know more about the many other parts that constitute a compiler!
[2] Note I'm using the word ""design"" deliberately: it is not, in my opinion, a
flaw to implement parsers with recursive descent, provided there is a
non-recursive descent design of the grammar elsewhere.
[3] More formally, and generally, we can talk about ""the language a parser accepts"".
[4] Though there are some unambiguous grammars that are not in the LR subset. That
need not concern us here.
[5] This isn't true for Yacc which allows action code to change how the parser
operates, allowing it to parse non-LR languages. Perhaps unsurprisingly, I do
not consider this a good idea, and lrpar does not feature such functionality.
[6] Oddly enough, my experience of converting recursive descent parsers to LR
grammars suggests that recursive descent parsers seem about as
often to reject ""good"" inputs as to accept ""bad"" inputs. I don't have a
convincing explanation as to why this might be the case.
[7] Though depending on how many ambiguities your grammar has, you might find that
some ""shadow"" others. In other words, by failing to write one part of a grammar
in the LR subset, you might not be able to write other parts of your grammar in
the LR subset either, causing some ambiguities to remain unnoticed.
☒I didn't have the opportunity to study compilers as an undergraduate. If I had, I
would not have wanted parsing to take up more than 1 week of such a course: I'd
have wanted to know more about the many other parts that constitute a compiler!☒Note I'm using the word ""design"" deliberately: it is not, in my opinion, a
flaw to implement parsers with recursive descent, provided there is a
non-recursive descent design of the grammar elsewhere.☒More formally, and generally, we can talk about ""the language a parser accepts"".☒Though there are some unambiguous grammars that are not in the LR subset. That
need not concern us here.☒This isn't true for Yacc which allows action code to change how the parser
operates, allowing it to parse non-LR languages. Perhaps unsurprisingly, I do
not consider this a good idea, and lrpar does not feature such functionality.☒Oddly enough, my experience of converting recursive descent parsers to LR
grammars suggests that recursive descent parsers seem about as
often to reject ""good"" inputs as to accept ""bad"" inputs. I don't have a
convincing explanation as to why this might be the case.☒Though depending on how many ambiguities your grammar has, you might find that
some ""shadow"" others. In other words, by failing to write one part of a grammar
in the LR subset, you might not be able to write other parts of your grammar in
the LR subset either, causing some ambiguities to remain unnoticed.
Comments





Comment:



Name:



Homepage:
 (optional)


Email:

(used only to verify your comment: it is not displayed)





Notify me of further comments






Failed to submit comment: please load page and try again
Submitting...





Can't load comments











Home
        > Blog



≡

  Home
> Blog






email  
Mastodon  
Twitter  


Mastodon  
Twitter








"
https://news.ycombinator.com/rss,Dijkstra Maps Visualized,http://www.roguebasin.com/index.php/Dijkstra_Maps_Visualized,Comments,"
RogueBasinSorry! This site is experiencing technical difficulties.Try waiting a few minutes and reloading.(Cannot access the database)"
https://news.ycombinator.com/rss,The Scandal of Financial Nihilism,https://read.lukeburgis.com/p/the-scandal-of-financial-nihilism,Comments,"























The Scandal of Financial Nihilism - by Luke Burgis













SubscribeSign inShare this postThe Scandal of Financial Nihilismread.lukeburgis.comCopy linkTwitterFacebookEmailThe Scandal of Financial NihilismThe Wheels Keep on Turning—and the anti-mimetic breaks are breaking. Luke BurgisJan 14913Share this postThe Scandal of Financial Nihilismread.lukeburgis.comCopy linkTwitterFacebookEmailArticle voiceover1×0:00-16:29Audio playback is not supported on your browser. Please upgrade.“It is because you don't know the end and purpose of things that you think the wicked and the criminal have power and happiness.” — Boethius, On the Consolation of PhilosophyNobody has a precise idea how much wealth anyone else has. There is a good chance that you don’t even know your own. We are all engaged in an elaborate game of assumptions, even about our own future. The fact that men are posting gain and loss porn on Reddit is not a sign of transparency, but opacity. The narratives only go skin deep. Beneath the narratives, people are silently crying out for order—a link between cause and effect, or some indication that hard work still matters, or the assurance that they will be rewarded for good decisions and not for reckless ones.When you make and/or lose a lot of money quickly, without the ability to see exactly why and how the windfall or bust happened, it can lead to financial nihilism. When you see—or think you see—other people doing the same, it can lead to financial nihilism. When you work very hard and do All the Right Things (as Dave Ramsey, or some other financial guru, would have you do) and then proceed to lose most of your net worth in the course of a month due to unexpected circumstances, it can lead to financial nihilism. There are a few different ways of defining this particular form of nihilism. Each of them points to a different aspect of it.My friend Demetri Kofinas has referred to financial nihilism as “a philosophy that treats the objects of speculation as though they were intrinsically worthless.” It is a philosophy that fully embraces the view that reality is completely subjective. I should note that the Subjective Theory of Value in economics does not, in any way, imply that reality itself is subjective—but rather that subjects determine the value of an object, and ultimately its price, through their collective decisions. Each of these actors could be acting with a greater or lesser vision of and adherence to reality. But for the financial nihilist, the subjective completely subsumes the objective. The truth is that both exist. For example: beauty, at least classically understood, is objective—it has certain characteristics, like claritas (clarity), consonantia (proportion), integritas (integrity/coherence)—but I perceive it subjectively. Conflating the two leads to problems. Financial nihilism is usually downstream from ontological nihilism. I don’t think people are nihilistic in a domain-specific sense; I believe that nihilism is diffusive and easily bleeds over into all aspects of life, from relationships to one’s health. (For instance: you hear about a friend who takes extremely good care of himself but is nevertheless unexpectedly diagnosed with cancer, and so you think: ‘screw it, what’s the use of this discipline?’). We have entered a truly nihilistic age, which is reflected in our politics as much if not more than our markets. It’s as if it doesn’t actually matter what people say or do anymore. This spirit of political nihilism was summed up by Donald Trump saying “I could stand in the middle of Fifth Avenue and shoot somebody, and I wouldn't lose any voters, OK? It's, like, incredible."" Yes, it is incredible. And that’s why politics is no longer credible. Nihilism, like all of life, ultimately comes down to the fundamental question of belief. People don’t know who they can trust or who they can believe in, but they must believe in something. There is a time-bound aspect to financial nihilism. If in one second you can lose or make a lot of money in a manner that feels completely detached from any amount of “value” that you believe you created  (the idea that all people who have money must have created a ton of “value” is problematic, because most people don’t understand what value means in this specific case—which is subjective value), then you begin to get the sense that the quality of your work has no connection to what people will pay for it, or to financial success. Not even your intelligence can save you. You can analyze a single stock for six months (as one long-only hedge fund manager told me) and buy it with a complete conviction that it’s undervalued, and then watch as it loses 80% in the course of a single year. If the bears are making money based purely on mimesis and momentum, what’s the use of pouring yourself out doing analysis for 40+ hours? Eventually, you come to place here you simply wait, second by second, to see if anything changes. Having lost a firm belief in your own agency, you place bets, you wait, you see what happens. If nothing matters, then what good is there in being good? Abandonment to non-divine providence is the only thing left.If you’re unable to transcend the transactional—to exit the logic of the game (the gamification of the stock market, the NFT swindles, the latest crypto narrative)—you simply enter deeply into it for fear of being left out or left behind. The small amount of nihilism that led to your decision to play breeds more nihilism once you’re deep inside of the game. Now, in the moment, you may feel that you truly believe in the game, but you don’t really. You can’t even be an honest nihilist because you have had to make friends with dishonest wealth.Lastly, a financial nihilist can be described as someone who has no skin in the game. No doubt we could list more, but I’ll leave it at that for now. SubscribeFinancial nihilism comes down to an even more fundamental problem. It is an epistemic problem. Many people simply see nothing worth believing in and so they are, in the words of Neil Postman, amusing themselves to death.Unfortunately, I think it’s even darker than that. Financial nihilism very often stems from a basic (unacknowledged) belief that life is simply not worth living. If we’re bankrupt a few years from now, who cares? We don’t want to be here in the first place. If a person lacks the will to live and engage deeply with the world, what is there to fear about financial loss? The pain of loss may in fact be the only way they have left to feel anything at all. Nihilism lies at the heart of many of our health problems, political problems, and relationship problems. When there is no hope for the future, why do we expect people to care about decisions that affect the future? For lack of vision people perish. Lastly, we lack a rich understanding of the difference between wealth and money. Wealth, as Paul Graham has written, is what we want: food, clothes, a home, the ability to travel and learn about the world, and so forth. Money is simply a system we’ve devised for helping us get the things that we want, or think we want, in a specialized world. Obviously, though, we want much more than food, clothes, houses, cars, and things. We want loving relationships, lives full of beauty, nourishing conversations, and many more immaterial things that we often say we “can’t put a price on.”But of course we can. How much would you pay, right now, to be magically transported to a wonderful meal with a handful of wise people who are interested in you and wanted to engage in conversation about all of the topics that were most important to you? I would pay a handsome price.   I can’t buy that, though. Nobody can. (Some extraordinarily rich people do attempt to buy these kinds of experiences and relationships, but those purchases carry one big caveat: genuine, loving care and attention is not something anyone can buy. Many extremely rich people go their entire lives wondering if anyone is really their friend—someone interested in them for who they are, rather than their bank account. I can’t imagine that kind of existential loneliness on account of money.) What I can do, though, is invest in things that I feel matter—and resist the temptation to invest in things that I don’t think matter merely so that I’m part of the game. I can refuse to play a nihilistic game. That is one freedom that each of has. We get to choose.Financial nihilists of the speculative variety aren’t necessarily greedy people who seek money at all costs. They are people who have ceased to believe that money means anything—so it ends up meaning everything. The financial nihilists actually have something right: money doesn’t matter as much as most people think it does. Their downfall is their inability to believe that a greater reality exists for which money is simply one layer. The economic layer is an important layer, no doubt, but it is not the Layer Zero of life. If money is ordered to nothing else, then it becomes ordered to itself—to its own propagation and self-replication. And that is precisely the definition of a meme. That is why we have a meme market and a meme economy. SubscribeWhen I was a kid, I begged my parents for a Nintendo Entertain System for well over two years before they finally gave in. Well, my dad gave in. All of my friends had one. My mimetic desire was raging. One day, I sat down and made a list of “100 reasons why I should have a Nintendo.” (A miraculous accomplishment, when I think about it). I wish it was still around somewhere because it would be hilarious to read. I would’ve gladly posted it here for your amusement.I don’t remember a single item on the list, but I know that I came up with 100. I’m sure some of them were noble (I had pledged to buy learning games that would help develop my brain—that much I do remember). But many of my “reasons” were likely suspect, to say the least.My dad took one look at the list and decided that he had had enough. It was not that the list had convinced him that I should have a Nintendo. I can’t imagine that it was all that persuasive. It was the fact that I had actually taken the time to make it. He just couldn’t take it anymore. I think he thought that his kid, his only son and only child, had finally lost it.He told me to get in the car and drove me to the nearest Best Buy where he bought me a Nintendo and told me to pick out three games (I would’ve been happy with one). When he got home, he got in a little spat with my mom who couldn’t believe that he had succumbed. I started feeling bad. Had I manipulated him into buying it for me? My dad was a truck driver. He worked his ass off for every penny he ever made. He had no idea what a Nintendo even was. After seeing my mom’s concern, and my own distress, my dad turned to me and said: “Son, it’s only money.” He smiled, sat down, cracked open a beer, and started watching a Detroit Tigers game.I’ve never forgotten that. My dad wasn’t a financial nihilist. He—perhaps more than anybody I’ve ever known—knew what money is. It was something real to him because it led to, or made possible, real things. But he had perspective, and he had order. He could say “it’s only money” without a hint of the cool cynicism of the man who blows his Bitcoin windfall on a new car that he doesn’t need. My dad said “it’s only money” because he secretly knew that he was wealthy.  3Share this postThe Scandal of Financial Nihilismread.lukeburgis.comCopy linkTwitterFacebookEmailPrevious3 CommentsLuke BurgisJan 15AuthorFriends, there were a couple of types in the original piece. I apologize for that. I'm still a one-man shop at this point (but looking for help...!). Thanks to the few of you who pointed them out. I corrected what I was able to find, but always welcome you pointing them out. Thank you, as always, for reading. Expand full commentReplyCollapseAndrew VierraJan 16Really liking the article voiceover enjoying how it works best for me by using my phone/bluetooth and reading along on the laptop for pausing purposes. The article was excellent and highlights for me something Gil Bailie talked about  when he mentioned a virtical relationship and a horizontal relationship in a talk with the Dominican School on Youtube. Or the talk I had with my gf this morning about the 1st and the 10th Commandment. We will always worship something whether its a partner, money, or a car but to have a primary relationship with something greater will be more fruitful if it comes first. The article also reminds me of the book The Noonday Devil by Jean Charles Nault OSB in writing about Acedia the author outlines one who flees their cell spreading scandal in an empty attempt to avoid ones primary duty. Beautiful how the article ends with something I will always try to silently remember to myself that fondness inside of each of us that is wealth. Expand full commentReplyCollapse1 more comments…TopNewCommunityNo postsReady for more?Subscribe© 2023 Luke BurgisPrivacy ∙ Terms ∙ Collection notice Publish on SubstackGet the appSubstack is the home for great writing

















        This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts
    



"
https://news.ycombinator.com/rss,Sirum (YC Nonprofit) Is Hiring Chief of Staff and Product Managers in Atlanta,https://sirum.org/careers/,Comments,"






Careers - SIRUM - Saving Medicine : Saving Lives






























 















Donate Medicine

Manufacturers & Wholesalers
Pharmacies
Nursing Homes & Assisted Living
Individuals


Receive Medicine

Community Partners
Resources for Individuals


About Us

Press
Careers
Support Our Work


Sign Up
Sign In





Join our team.We’re a fast-growing, non-profit, for-impact startup at the intersection of health + technology + social enterprise. We're looking for future SIRUMites ready to roll up their sleeves and help us build a national solution for the drug pricing epidemic. See open rolesWorking at SIRUMWe're a small team that's passionate about our mission of reimagining healthcare access for those in need. We come from organizations like McKinsey, the Clinton Foundation, and Stanford Biology and include everyone from programmers to volunteers. We like to work hard, solve tough problems, and debate who has the best lunch in Palo Alto. BenefitsCompetitive salaryWe work hard to be competitive for a tech-forward social impact organization.Huge social impactYou will see and feel the tangible impact of your work on a daily basis. Awesome teamFun virtual team that's up for adventures from virtual cooking classes and escape rooms to putt-putt and paddleboardingGreat health benefits100% of monthly premiums for medical, dental, and vision insurance for full-time employeesAccess to on-demand primary care, personal health advocates, mental health support, and more401k matchingFull-time employees eligible for a 4% matchTime off to rest2+ weeks of paid company holidays annually4 weeks of accrued PTO annuallyCurrent Open RolesCheck out our current open roles below -- we're constantly growing and post new opportunities all the time.Don't see something that's the right fit? Drop us a line below. We love hearing from people who share our mission. See open roles Fellow & Associate ProgramSIRUM Fellowship & Associate roles are unique 1-2 year opportunities for ambitious recent graduates and early career professionals. Working directly with SIRUM’s co-founders and owning a critical set of special projects reflective of their skills, Fellows and Associates become key players in our fast-growing start up.They take on important roles in:- Strategic Partnerships- Product Management- Strategy & AnalysisAfter their 1-2 years, Fellows and Associates might stay on with SIRUM or go on to attend top graduate programs. Our Alums have gone on to top medical schools, business schools, and more.We look for Fellows & Associates in our Palo Alto and Atlanta offices.See Fellow & Associate roles Need more information?If you didn't find what you're looking for, get in touch and we'd be happy to help. 
 


[email protected](650) 488-7434Visit our FacebookVisit our TwitterVisit our LinkedIn©SIRUM Copyright 2022.Donate MedicineManufacturers & WholesalersPharmaciesNursing Homes & Assisted LivingIndividualsReceive MedicineCommunity PartnersResources for IndividualsAbout UsPressCareersSupport Our Work 







crossmenuchevron-down






linkedin





facebook



pinterest



youtube



rss



twitter



instagram





facebook-blank



rss-blank



linkedin-blank



pinterest



youtube



twitter



instagram











"
https://news.ycombinator.com/rss,How does it know I want CSV? – An HTTP trick,https://csvbase.com/blog/2,Comments,"










How does it know I want csv? ⁠— An HTTP trick




















            csvbase (alpha)







Paste/Upload


Blog


About







csvbase is a website for hosting table data.  For more details see the about page.





How does it know I want csv? ⁠— An HTTP trick



Forgotten parts of RFC2616
2023-01-17
by Cal Paterson

How come when you visit
https://csvbase.com/meripaterson/stock-exchanges
in a browser you get a webpage –:

but when you curl the same url you get a csv file?:

The url is the same - so how come?
The answer is HTTP's built-in ""content negotiation"".
How content negotiation works
When an HTTP client sends any request, it sends ""headers"" with that request.
Here are the headers that Google Chrome sends:
accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9
accept-encoding: gzip, deflate, br
accept-language: en-GB,en-US;q=0.9,en;q=0.8
cache-control: no-cache
pragma: no-cache
sec-ch-ua: ""Google Chrome"";v=""105"", ""Not)A;Brand"";v=""8"", ""Chromium"";v=""105""
sec-ch-ua-mobile: ?0
sec-ch-ua-platform: ""Linux""
sec-fetch-dest: document
sec-fetch-mode: navigate
sec-fetch-site: none
sec-fetch-user: ?1
upgrade-insecure-requests: 1
user-agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36

That's a lot.  Only the first of these is relevant: the accept header:
accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9

The accept header is an unordered list of preferences for what media type
(aka ""Content Type"", or file format) the server should send.
Chrome is saying:


Ideally, give me:
text/html (ie: HTML)
application/xhtml+xml (XHTML)
image/avif
image/webp
or image/apng (a animated version of a PNG)



Otherwise (""q"", or quality, 0.9), give me
application/xml
or application/signed-exchange;v=b3



And if none of these are available (lowest priority; q=0.8):
just send me anything (*/*)



csvbase has an HTML representation of the url being requested, which is
Chrome's joint top preference, so it just replies with that.
What accept header does curl send?  It sends just:
accept: */*

Much shorter.  Curl will take anything.  And csvbase has a default format:
csv, so it replies with that.
""Why though?"", escape hatches and non-negotiables
The main reason why csvbase bothers at all with this is to make it easier to
export tables.  For example, to get a table loaded into
pandas, all you have to do
is paste the url into the first argument for pandas' read_csv - the same url
as for the page:

This works in most tools - curl, pandas,
R and many others.
But not all.  Some, like Apache
Spark, ask for HTML for some
reason.  So csvbase has an escape hatch from content negotiation — adding
a file extension:
https://csvbase.com/meripaterson/stock-exchanges.csv
always returns csv file.
This escape hatch is also useful for other formats.  Media types are
controlled by the Internet Assigned Numbers
Authority.
Not every file format has been given a media type.  For example, parquet (the
current favourite of most data scientists) doesn't have a media type.  Neither
does jsonlines.  At least not yet.
csvbase can output in those formats too, but there's no way to content neogiate
them - at least not until the IANA get around to officially assigning them a
media type.
Until then, use:
https://csvbase.com/meripaterson/stock-exchanges.parquet
You can even do:
import pandas as pd

pd.read_parquet(""https://csvbase.com/meripaterson/stock-exchanges.parquet"")










Source code
Privacy policy
Terms





"
https://news.ycombinator.com/rss,Safe Native Code (2015),http://joeduffyblog.com/2015/12/19/safe-native-code/,Comments,"



 



Joe Duffy - Safe Native Code










Joe Duffy's Blog









        Founder/CEO Pulumi •
        Cloud, languages, and developer tools guy •
        Eat, sleep, code, repeat
    



Popular:
Pulumi
Midori
Software Leadership





















  © Joe Duffy, 2018







                December 19, 2015 
            


Safe Native Code


In my first Midori post, I described how safety was the
foundation of everything we did.  I mentioned that we built an operating system out of safe code, and yet stayed
competitive with operating systems like Windows and Linux written in C and C++.  In many ways, system architecture
played a key role, and I will continue discussing how in future posts.  But, at the foundation, an optimizing compiler
that often eeked out native code performance from otherwise “managed”, type- and memory-safe code, was one of our most
important weapons.  In this post, I’ll describe some key insights and techniques that were essential to our success.
Overview
When people think of C#, Java, and related languages, they usually think of Just-In-Time (JIT) compilation.  Especially back in the mid-2000s when Midori began.  But
Midori was different, using more C++-like Ahead-Of-Time (AOT) compilation from the outset.
AOT compiling managed, garbage collected code
presents some unique challenges compared to C and C++.  As a result, many AOT efforts don’t achieve parity with their
native counterparts.  .NET’s NGEN technology is a good example of this.  In fact, most efforts in .NET have exclusively
targeted startup time; this is clearly a key metric, but when you’re building an operating system and everything on top,
startup time just barely scratches the surface.
Over the course of 8 years, we were able to significantly narrow the gap between our version of C# and classical C/C++
systems, to the point where basic code quality, in both size of speed dimensions, was seldom the deciding factor when
comparing Midori’s performance to existing workloads.  In fact, something counter-intuitive happened.  The ability to
co-design the language, runtime, frameworks, operating system, and the compiler – making tradeoffs in one area to gain
advantages in other areas – gave the compiler far more symbolic information than it ever had before about the program’s
semantics and, so, I dare say, was able to exceed C and C++ performance in a non-trivial number of situations.
Before diving deep, I have to put in a reminder.  The architectural decisions – like Async Everywhere and Zero-Copy IO (coming soon) – had more to do with us
narrowing the gap at a “whole system” level.  Especially the less GC-hungry way we wrote systems code.  But the
foundation of a highly optimizing compiler, that knew about and took advantage of safety, was essential to our results.
I would also be remiss if I didn’t point out that the world has made considerable inroads in this area alongside us.
Go has straddled an elegant line between systems performance and safety.  Rust is just plain awesome.  The .NET Native and, related, Android Runtime projects have brought a nice taste of AOT to C# and Java in a more
limited way, as a “silent” optimization technique to avoid mobile application lag caused by JITting.  Lately, we’ve
been working on bringing AOT to a broader .NET setting with the CoreRT project.
Through that effort I hope we can bring some of the lessons learned below to a real-world setting.  Due to the
delicate balance around breaking changes it remains to be seen how far we can go.  It took us years to get
everything working harmoniously, measured in man-decades, however, so this transfer of knowledge will take time.
First thing’s first.  Let’s quickly recap: What’s the difference between native and managed code, anyway?
What’s the same
I despise the false dichotomy “native and managed,” so I must apologize for using it.  After reading this article, I
hope to have convinced you that it’s a continuum.  C++ is safer these days than ever before, and likewise, C# performant.  It’s amusing how many of these lessons apply
directly to the work my team is doing on Safe C++ these days.
So let’s begin by considering what’s the same.
All the basic dragon book topics apply to managed as
much as they do native code.
In general, compiling code is a balancing act between, on one hand emitting the most efficient instruction sequences
for the target architecture, to execute the program quickly; and on the other hand emitting the smallest encoding of
instructions for the target architecture, to store the program compactly and effectively use the memory system on the
target device.  Countless knobs exist on your favorite compiler to dial between the two based on your scenario.  On
mobile, you probably want smaller code, whereas on a multimedia workstation, you probably want the fastest.
The choice of managed code doesn’t change any of this.  You still want the same flexibility.  And the techniques you’d
use to achieve this in a C or C++ compiler are by and large the same as what you use for a safe language.
You need a great inliner.  You want common subexpression
elimination (CSE), constant propagation and folding, strength reduction,
and an excellent loop optimizer.  These days, you probably want to
use static single assignment form (SSA), and some unique
SSA optimizations like global value numbering (although you need
to be careful about working set and compiler throughput when using SSA everywhere).  You will need specialized machine
dependent optimizers for the target architectures that are important to you, including register allocators.  You’ll eventually want a global analyzer that does interprocedural
optimizations, link-time code-generation to extend those interprocedural optimizations across passes, a vectorizer for modern processors (SSE, NEON, AVX, etc.), and most definitely
profile guided optimizations (PGO) to inform all of the
above based on real-world scenarios.
Although having a safe language can throw some interesting curveballs your way that are unique and interesting – which
I’ll cover below – you’ll need all of the standard optimizing compiler things.
I hate to say it, but doing great at all of these things is “table stakes.”  Back in the mid-2000s, we had to write
everything by hand.  Thankfully, these days you can get an awesome off-the-shelf optimizing compiler like LLVM that has most of these things already battle tested, ready to go, and ready for you to help improve.
What’s different
But, of course, there are differences.  Many.  This article wouldn’t be very interesting otherwise.
The differences are more about what “shapes” you can expect to be different in the code and data structures thrown at
the optimizer.  These shapes come in the form of different instruction sequences, logical operations in the code that
wouldn’t exist in the C++ equivalent (like more bounds checking), data structure layout differences (like extra object
headers or interface tables), and, in most cases, a larger quantity of supporting runtime data structures.
Objects have “more to them” in most managed languages, compared to frugal data types in, say, C.  (Note that C++ data
structures are not nearly as frugal as you might imagine, and are probably closer to C# than your gut tells you.)  In
Java, every object has a vtable pointer in its header.  In C#, most do, although structs do not.  The GC can impose
extra layout restrictions, such as padding and a couple words to do its book-keeping.  Note that none of this is really
specific to managed languages – C and C++ allocators can inject their own words too, and of course, many C++ objects
also carry vtables – however it’s fair to say that most C and C++ implementations tend to be more economical in these
areas.  In most cases, for cultural reasons more than hard technical ones.  Add up a few thousand objects in a heap,
especially when your system is built of many small processes with isolated heaps, like Midori, and it adds up quickly.
In Java, you’ve got a lot more virtual dispatch, because methods are virtual by default.  In C#, thankfully, methods
are non-virtual by default.  (We even made classes sealed by default.)  Too much virtual dispatch can totally screw
inlining which is a critical optimization to have for small functions.  In managed languages you tend to have more
small functions for two reasons: 1) properties, and 2) higher level programmers tend to over-use abstraction.
Although it’s seldom described this formally, there’s an “ABI” (Application Binary Interface) that governs interactions between code and the runtime.
The ABI is where the rubber meets the road.  It’s where things like calling conventions, exception handling, and, most
notably, the GC manifest in machine code.  This is not unique to managed code!  C++ has a “runtime” and therfore an
ABI too.  It’s just that it’s primarily composed of headers, libraries like allocators, and so on, that are more
transparently linked into a program than with classical C# and Java virtual machines, where a runtime is non-negotiable
(and in the JIT case, fairly heavy-handed).  Thinking of it this way has been helpful to me, because the isomorphisms
with C++ suddenly become immediately apparent.
The real biggie is array bounds checks.  A traditional approach is to check that the index is within the bounds of an
array before accessing it, either for loading or storing.  That’s an extra field fetch, compare, and conditional
branch.  Branch prediction these days is quite good, however it’s just
plain physics that if you do more work, you’re going to pay for it.  Interestingly, the work we’re doing with C++’s
array_view<T> incurs all these same costs.
Related to this, there can be null checks where they didn’t exist in C++.  If you perform a method dispatch on a null
object pointer in C++, for example, you end up running the function anyway.  If that function tries to access this,
it’s bound to AV, but in Java and .NET, the compiler is required
(per specification) to explicitly check and throw an exception in these cases, before the call even occurs.  These
little branches can add up too.  We eradicated such checks in favor of C++ semantics in optimized builds.
In Midori, we compiled with overflow checking on by default.  This is different from stock C#, where you must explicitly
pass the /checked flag for this behavior.  In our experience, the number of surprising overflows that were caught,
and unintended, was well worth the inconvenience and cost.  But it did mean that our compiler needed to get really good
at understanding how to eliminate unnecessary ones.
Static variables are very expensive in Java and .NET.  Way more than you’d expect.  They are mutable and so cannot be
stored in the readonly segment of an image where they are shared across processes.  And my goodness, the amount of
lazy-initialization checking that gets injected into the resulting source code is beyond belief.  Switching from
preciseinit to beforefieldinit semantics in .NET helps a little bit, since the checks needn’t happen on every
access to a static member – just accesses to the static variable in question – but it’s still disgusting compared to
a carefully crafted C program with a mixture of constant and intentional global initialization.
The final major area is specific to .NET: structs.  Although structs help to alleviate GC pressure and hence are a
good thing for most programs, they also carry some subtle problems.  The CLI specifies surprising behavior around their
initialization, for example.  Namely if an exception happens during construction, the struct slot must remain zero-
initialized.  The result is that most compilers make defensive copies.  Another example is that the compiler must
make a defensive copy anytime you call a function on a readonly struct.  It’s pretty common for structs to be copied
all over the place which, when you’re counting cycles, hurts, especially since it often means time spent in memcpy.
We had a lot of techniques for addressing this and, funny enough, I’m pretty sure when all was said and done, our
code quality here was better than C++’s, given all of its RAII, copy constructor, destructor, and so on, penalties.
Compilation Architecture
Our architecture involved three major components:

C# Compiler: Performs lexing, parsing, and semantic analysis.  Ultimately
translates from C# textual source code into a CIL-based
intermediate representation (IR).
Bartok: Takes in said IR, does high-level MSIL-based analysis,
transformations, and optimizations, and finally lowers this IR to something a bit closer to a more concrete machine
representation.  For example, generics are gone by the time Bartok is done with the IR.
Phoenix: Takes in this lowered IR, and goes to town on
it.  This is where the bulk of the “pedal to the metal” optimizations happen.  The output is machine code.

The similarities here with Swift’s compiler design, particularly SIL, are evident.  The .NET Native project also
mirrors this architecture somewhat.  Frankly, most AOT compilers for high level languages do.
In most places, the compiler’s internal representation leveraged static single assignment form (SSA).  SSA was preserved until very late in the compilation.
This facilitated and improved the use of many of the classical compiler optimizations mentioned earlier.
The goals of this architecture included:

Facilitate rapid prototyping and experimentation.
Produce high-quality machine code on par with commerical C/C++ compilers.
Support debugging optimized machine code for improved productivity.
Facilitate profile-guided optimizations based on sampling and/or instrumenting code.
Suitable for self-host:
    
The resulting compiled compiler is fast enough.
It is fast enough that the compiler developers enjoy using it.
It is easy to debug problems when the compiler goes astray.



Finally, a brief warning.  We tried lots of stuff.  I can’t remember it all.  Both Bartok and Phoenix existed for years
before I even got involved in them.  Bartok was a hotbed of research on managed languages – ranging from optimizations
to GC to software transactional memory – and Phoenix was meant to replace the shipping Visual C++ compiler.  So,
anyway, there’s no way I can tell the full story.  But I’ll do my best.
Optimizations
Let’s go deep on a few specific areas of classical compiler optimizations, extended to cover safe code.
Bounds check elimination
C# arrays are bounds checked.  So were ours.  Although it is important to eliminate superfluous bounds checks in regular
C# code, it was even more so in our case, because even the lowest layers of the system used bounds checked arrays.  For
example, where in the bowels of the Windows or Linux kernel you’d see an int*, in Midori you’d see an int[].
To see what a bounds check looks like, consider a simple example:
var a = new int[100];
for (int i = 0; i < 100; i++) {
    ... a[i] ...;
}

Here’s is an example of the resulting machine code for the inner loop array access, with a bounds check:
; First, put the array length into EAX:
3B15: 8B 41 08        mov         eax,dword ptr [rcx+8]
; If EDX >= EAX, access is out of bounds; jump to error:
3B18: 3B D0           cmp         edx,eax
3B1A: 73 0C           jae         3B28
; Otherwise, access is OK; compute element's address, and assign:
3B1C: 48 63 C2        movsxd      rax,edx
3B1F: 8B 44 81 10     mov         dword ptr [rcx+rax*4+10h],r8d
; ...
; The error handler; just call a runtime helper that throws:
3B28: E8 03 E5 FF FF  call        2030

If you’re doing this bookkeeping on every loop iteration, you won’t get very tight loop code.  And you’re certainly not
going to have any hope of vectorizing it.  So, we spent a lot of time and energy trying to eliminate such checks.
In the above example, it’s obvious to a human that no bounds checking is necessary.  To a compiler, however, the
analysis isn’t quite so simple.  It needs to prove all sorts of facts about ranges.  It also needs to know that a
isn’t aliased and somehow modified during the loop body.  It’s surprising how hard this problem quickly becomes.
Our system had multiple layers of bounds check eliminations.
First it’s important to note that CIL severely constraints an optimizer by being precise in certain areas.  For example,
accessing an array out of bounds throws an IndexOutOfRangeException, similar to Java’s ArrayOutOfBoundsException.
And the CIL specifies that it shall do so at precisely the exception that threw it.  As we will see later on, our
error model was more relaxed.  It was based fail-fast and permitted code motion that led to inevitable failures
happening “sooner” than they would have otherwise.  Without this, our hands would have been tied for much of what I’m
about to discuss.
At the highest level, in Bartok, the IR is still relatively close to the program input.  So, some simple patterns could
be matched and eliminated.  Before lowering further, the ABCD algorithm – a straightforward value range analysis based on
SSA – then ran to eliminate even more common patterns using a more principled approach than pattern matching.  We were
also able to leverage ABCD in the global analysis phase too, thanks to inter-procedural length and control flow fact
propagation.
Next up, the Phoenix Loop Optimizer got its hands on things.  This layer did all sorts of loop optimizations and, most
relevant to this section, range analysis.  For example:

Loop materialization: this analysis actually creates loops.  It recognizes repeated patterns of code that would be
more ideally represented as loops, and, when profitable, rewrites them as such.  This includes unrolling hand-rolled
loops so that a vectorizer can get its hands on them, even if they might be re-unrolled later on.
Loop cloning, unrolling, and versioning: this analysis creates copies of loops for purposes of specialization.  That
includes loop unrolling, creating architectural-specific versions of a vectorized loop, and so on.
Induction range optimization: this is the phase we are most
concerned with in this section.  It uses induction range analysis to remove unnecessary checks, in addition to doing
classical induction variable optimizations such as widening.  As a byproduct of this phase, bounds checks were
eliminated and coalesced by hoisting them outside of loops.

This sort of principled analysis was more capable than what was shown earlier.  For example, there are ways to write
the earlier loop that can easily “trick” the more basic techniques discussed earlier:
var a = new int[100];

// Trick #1: use the length instead of constant.
for (int i = 0; i < a.length; i++) {
    a[i] = i;
}

// Trick #2: start counting at 1.
for (int i = 1; i <= a.length; i++) {
    a[i-1] = i-1;
}

// Trick #3: count backwards.
for (int i = a.length - 1; i >= 0; i--) {
    a[i] = i;
}

// Trick #4: don't use a for loop at all.
int i = 0;
next:
if (i < a.length) {
    a[i] = i;
    i++;
    goto next;
}

You get the point.  Clearly at some point you can screw the optimizer’s ability to do anything, especially if you
start doing virtual dispatch inside the loop body, where aliasing information is lost.  And obviously, things get more
difficult when the array length isn’t known statically, as in the above example of 100.  All is not lost, however,
if you can prove relationships between the loop bounds and the array.  Much of this analysis requires special knowledge
of the fact that array lengths in C# are immutable.
At the end of the day, doing a good job at optimizing here is the difference between this:
; Initialize induction variable to 0:
3D45: 33 C0           xor         eax,eax
; Put bounds into EDX:
3D58: 8B 51 08        mov         edx,dword ptr [rcx+8]
; Check that EAX is still within bounds; jump if not:
3D5B: 3B C2           cmp         eax,edx
3D5D: 73 13           jae         3D72
; Compute the element address and store into it:
3D5F: 48 63 D0        movsxd      rdx,eax
3D62: 89 44 91 10     mov         dword ptr [rcx+rdx*4+10h],eax
; Increment the loop induction variable:
3D66: FF C0           inc         eax
; If still < 100, then jump back to the loop beginning:
3D68: 83 F8 64        cmp         eax,64h
3D6B: 7C EB           jl          3D58
; ...
; Error routine:
3D72: E8 B9 E2 FF FF  call        2030

And the following, completely optimized, bounds check free, loop:
; Initialize induction variable to 0:
3D95: 33 C0           xor         eax,eax
; Compute the element address and store into it:
3D97: 48 63 D0        movsxd      rdx,eax
3D9A: 89 04 91        mov         dword ptr [rcx+rdx*4],eax
; Increment the loop induction variable:
3D9D: FF C0           inc         eax
; If still < 100, then jump back to the loop beginning:
3D9F: 83 F8 64        cmp         eax,64h
3DA2: 7C F3           jl          3D97

It’s amusing that I’m now suffering deja vu as we go through this same exercise with C++’s new array_view<T> type.
Sometimes I joke with my ex-Midori colleagues that we’re destined to repeat ourselves, slowly and patiently, over the
course of the next 10 years.  I know that sounds arrogant.  But I have this feeling on almost a daily basis.
Overflow checking
As mentioned earlier, in Midori we compiled with checked arithmetic by default (by way of C#’s /checked flag).  This
eliminated classes of errors where developers didn’t anticipate, and therefore code correctly for, overflows.  Of
course, we kept the explicit checked and unchecked scoping constructs, to override the defaults when appropriate,
but this was preferable because a programmer declared her intent.
Anyway, as you might expect, this can reduce code quality too.
For comparison, imagine we’re adding two variables:
int x = ...;
int y = ...;
int z = x + y;

Now imagine x is in ECX and y is in EDX.  Here is a standard unchecked add operation:
03 C2              add         ecx,edx

Or, if you want to get fancy, one that uses the LEA instruction to also store the result in the EAX register using
a single instruction, as many modern compilers might do:
8D 04 11           lea         eax,[rcx+rdx]

Well, here’s the equivalent code with a bounds check inserted into it:
3A65: 8B C1              mov         eax,ecx
3A67: 03 C2              add         eax,edx
3A69: 70 05              jo          3A70
; ...
3A70: E8 B3 E5 FF FF     call        2028

More of those damn conditional jumps (JO) with error handling routines (CALL 2028).
It turns out a lot of the analysis mentioned earlier that goes into proving bounds checks redundant also apply to
proving that overflow checks are redundant.  It’s all about proving facts about ranges.  For example, if you can prove
that some check is dominated by some earlier check, and that
furthermore that earlier check is a superset of the later check, then the later check is unnecessary.  If the opposite
is true – that is, the earlier check is a subset of the later check, then if the subsequent block postdominates the
earlier one, you might move the stronger check to earlier in the program.
Another common pattern is that the same, or similar, arithmetic operation happens multiple times near one another:
int p = r * 32 + 64;
int q = r * 32 + 64 - 16;

It is obvious that, if the p assignment didn’t overflow, then the q one won’t either.
There’s another magical phenomenon that happens in real world code a lot.  It’s common to have bounds checks and
arithmetic checks in the same neighborhood.  Imagine some code that reads a bunch of values from an array:
int data0 = data[dataOffset + (DATA_SIZE * 0)];
int data1 = data[dataOffset + (DATA_SIZE * 1)];
int data2 = data[dataOffset + (DATA_SIZE * 2)];
int data3 = data[dataOffset + (DATA_SIZE * 3)];
.. and so on ...

Well C# arrays cannot have negative bounds.  If a compiler knows that DATA_SIZE is sufficiently small that an
overflowed computation won’t wrap around past 0, then it can eliminate the range check in favor of the bounds check.
There are many other patterns and special cases you can cover.  But the above demonstrates the power of a really good
range optimizer that is integrated with loops optimization.  It can cover a wide array of scenarios, array bounds and
arithmetic operations included.  It takes a lot of work, but it’s worth it in the end.
Inlining
For the most part, inlining is the same as with true native code.  And
just as important.  Often more important, due to C# developers’ tendency to write lots of little methods (like property
accessors).  Because of many of the topics throughout this article, getting small code can be more difficult than in
C++ – more branches, more checks, etc. – and so, in practice, most managed code compilers inline a lot less than
native code compilers, or at least need to be tuned very differently.  This can actually make or break performance.
There are also areas of habitual bloat.  The way lambdas are encoded in MSIL is unintelligable to a naive backend
compiler, unless it reverse engineers that fact.  For example, we had an optimization that took this code:
void A(Action a) {
    a();
}

void B() {
    int x = 42;
    A(() => x++);
    ...
}

and, after inlining, was able to turn B into just:
void B() {
    int x = 43;
    ...
}

That Action argument to A is a lambda and, if you know how the C# compiler encodes lambdas in MSIL, you’ll
appreciate how difficult this trick was.  For example, here is the code for B:
.method private hidebysig instance void
    B() cil managed
{
    // Code size       36 (0x24)
    .maxstack  3
    .locals init (class P/'<>c__DisplayClass1' V_0)
    IL_0000:  newobj     instance void P/'<>c__DisplayClass1'::.ctor()
    IL_0005:  stloc.0
    IL_0006:  nop
    IL_0007:  ldloc.0
    IL_0008:  ldc.i4.s   42
    IL_000a:  stfld      int32 P/'<>c__DisplayClass1'::x
    IL_000f:  ldarg.0
    IL_0010:  ldloc.0
    IL_0011:  ldftn      instance void P/'<>c__DisplayClass1'::'<B>b__0'()
    IL_0017:  newobj     instance void [mscorlib]System.Action::.ctor(object,
                                                                  native int)
    IL_001c:  call       instance void P::A(class [mscorlib]System.Action)
    IL_0021:  nop
    IL_0022:  nop
    IL_0023:  ret
}

To get the magic result required constant propagating the ldftn, recognizing how delegate construction works
(IL_0017), leveraging that information to inline B and eliminate the lambda/delegate altogether, and then, again
mostly through constant propagation, folding the arithmetic into the constant 42 initialization of x.  I always
found it elegant that this “fell out” of a natural composition of multiple optimizations with separate concerns.
As with native code, profile guided optimization made our inlining decisions far more effective.
Structs
CLI structs are almost just like C structs.  Except they’re not.  The CLI imposes some semantics that incur overheads.
These overheads almost always manifest as excessive copying.  Even worse, these copies are usually hidden from your
program.  It’s worth noting, because of copy constructors and destructors, C++ also has some real issues here, often
even worse than what I’m about to describe.
Perhaps the most annoying is that initializing a struct the CLI way requires a defensive copy.  For example, consider
this program, where the initialzer for S throws an exception:
class Program {
    static void Main() {
        S s = new S();
        try {
            s = new S(42);
        }
        catch {
            System.Console.WriteLine(s.value);
        }
    }
}

struct S {
    public int value;
    public S(int value) {
        this.value = value;
        throw new System.Exception(""Boom"");
    }
}

The program behavior here has to be that the value 0 is written to the console.  In practice, that means that the
assignment operation s = new S(42) must first create a new S-typed slot on the stack, construct it, and then and
only then copy the value back over the s variable.  For single-int structs like this one, that’s not a huge deal.
For large structs, that means resorting to memcpy.  In Midori, we knew what methods could throw, and which could not,
thanks to our error model (more later), which meant we could avoid this overhead in nearly all cases.
Another annoying one is the following:
struct S {
    // ...
    public int Value { get { return this.value; } }
}

static readonly S s = new S();

Every single time we read from s.Value:
int x = s.Value;

we are going to get a local copy.  This one’s actually visible in the MSIL.  This is without readonly:
ldsflda    valuetype S Program::s
call       instance int32 S::get_Value()

And this is with it:
ldsfld     valuetype S Program::s
stloc.0
ldloca.s   V_0
call       instance int32 S::get_Value()

Notice that the compiler elected to use ldsfld followed by lodloca.s, rather than loading the address directly,
by way of ldsflda in the first example.  The resulting machine code is even nastier.  I also can’t pass the struct
around by-reference which, as I mention later on, requires copying it and again can be problematic.
We solved this in Midori because our compiler knew about methods that didn’t mutate members.  All statics were immutable
to begin with, so the above s wouldn’t need defensive copies.  Alternatively, or in addition to this, the struct could
have beem declared as immutable, as follows:
immutable struct S {
    // As above ...
}

Or because all static values were immutable anyway.  Alternatively, the properties or methods in question could have
been annotated as readable meaning that they couldn’t trigger mutations and hence didn’t require defensive copies.
I mentioned by-reference passing.  In C++, developers know to pass large structures by-reference, either using * or
&, to avoid excessive copying.  We got in the habit of doing the same.  For example, we had in parameters, as so:
void M(in ReallyBigStruct s) {
    // Read, but don't assign to, s ...
}

I’ll admit we probably took this to an extreme, to the point where our APIs suffered.  If I could do it all over again,
I’d go back and eliminate the fundamental distinction between class and struct in C#.  It turns out, pointers aren’t
that bad after all, and for systems code you really do want to deeply understand the distinction between “near” (value)
and “far” (pointer).  We did implement what amounted to C++ references in C#, which helped, but not enough.  More on
this in my upcoming deep dive on our programming language.
Code size
We pushed hard on code size.  Even more than some C++ compilers I know.
A generic instantiation is just a fancy copy-and-paste of code with some substitutions.  Quite simply, that means an
explosion of code for the compiler to process, compared to what the developer actually wrote.  I’ve covered many of the
performance challenges with generics in the past.  A major problem there is the
transitive closure problem.  .NET’s straightforward-looking List<T> class actually creates 28 types in its transitive
closure!  And that’s not even speaking to all the methods in each type.  Generics are a quick way to explode code size.
I never forgot the day I refactored our LINQ implementation.  Unlike in .NET, which uses extension methods, we made all
LINQ operations instance methods on the base-most class in our collection type hierarchy.  That meant 100-ish nested
classes, one for each LINQ operation, for every single collection instantiated!  Refactoring this was an easy way for
me to save over 100MB of code size across the entire Midori “workstation” operating system image.  Yes, 100MB!
We learned to be more thoughtful about our use of generics.  For example, types nested inside an outer generic are
usually not good ideas.  We also aggressively shared generic instantiations, even more than what the CLR does.  Namely, we shared value type generics, where the
GC pointers were at the same locations.  So, for example, given a struct S:
struct S {
    int Field;
}

we would share the same code representation of List<int> with List<S>.  And, similarly, given:
struct S {
    object A;
    int B;
    object C;
}

struct T {
    object D;
    int E;
    object F;
}

we would share instantiations between List<S> and List<T>.
You might not realize this, but C# emits IL that ensures structs have sequential layout:
.class private sequential ansi sealed beforefieldinit S
    extends [mscorlib]System.ValueType
{
    ...
}

As a result, we couldn’t share List<S> and List<T> with some hypothetical List<U>:
struct U {
    int G;
    object H;
    object I;
}

For this, among other reasons – like giving the compiler more flexibility around packing, cache alignment, and so on
– we made structs auto by default in our language.  Really, sequential only matters if you’re doing unsafe code,
which, in our programming model, wasn’t even legal.
We did not support reflection in Midori.  In principle, we had plans to do it eventually, as a purely opt-in feature.
In practice, we never needed it.  What we found is that code generation was always a more suitable solution.  We shaved
off at least 30% of the best case C# image size by doing this.  Significantly more if you factor in systems where the
full MSIL is retained, as is usually the case, even for NGen and .NET AOT solutions.
In fact, we removed significant pieces of System.Type too.  No Assembly, no BaseType, and yes, even no FullName.
The .NET Framework’s mscorlib.dll contains about 100KB of just type names.  Sure, names are useful, but our eventing
framework leveraged code generation to produce just those you actually needed to be around at runtime.
At some point, we realized 40% of our image sizes were vtables.
We kept pounding on this one relentlessly, and, after all of that, we still had plenty of headroom for improvements.
Each vtable consumes image space to hold pointers to the virtual functions used in dispatch, and of course has a runtime
representation.  Each object with a vtable also has a vtable pointer embedded within it.  So, if you care about size
(both image and runtime), you are going to care about vtables.
In C++, you only get a vtable if your type is polymorphic.  In
languages like C# and Java, on the other hand, you get a vtable even if you don’t want, need, or use it.  In C#, at
least, you can use a struct type to elide them.  I actually love this aspect of Go, where you get a virtual dispatch-
like thing, via interfaces, without needing to pay for vtables on every type; you only pay for what you use, at the
point of coercing something to an interface.
Another vtable problem in C# is that all objects inherit three virtuals from System.Object: Equals, GetHashCode,
and ToString.  Besides the point that these generally don’t do the right thing in the right way anyways – Equals
requires reflection to work on value types, GetHashCode is nondeterministic and stamps the object header (or sync-
block; more on that later), and ToString doesn’t offer formatting and localization controls – they also bloat every
vtable by three slots.  This may not sound like much, but it’s certainly more than C++ which has no such overhead.
The main source of our remaining woes here was the assumption in C#, and frankly most OOP languages like C++ and Java,
that RTTI is always available for downcasts.  This was
particularly painful with generics, for all of the above reasons.  Although we aggressively shared instantiations, we
could never quite fully fold together the type structures for these guys, even though disparate instantiations tended
to be identical, or at least extraordinarily similar.  If I could do it all over agan, I’d banish RTTI.  In 90% of the
cases, type discriminated unions or pattern matching are more appropriate solutions anyway.
Profile guided optimizations (PGO)
I’ve mentioned profile guided optimization (PGO) already.
This was a critical element to “go that last mile” after mostly everything else in this article had been made
competitive.  This gave our browser program boosts in the neighborhood of 30-40% on benchmarks like SunSpider and Octane.
Most of what went into PGO was similar to classical native profilers, with two big differences.
First, we tought PGO about many of the unique optimizations listed throughout this article, such as asynchronous stack
probing, generics instantiations, lambdas, and more.  As with many things, we could have gone on forever here.
Second, we experimented with sample profiling, in addition to the ordinary instrumented profiling.  This is much nicer
from a developer perspective – they don’t need two builds – and also lets you collect counts from real, live running
systems in the data center.  A good example of what’s possible is outlined in this Google-Wide Profiling (GWP) paper.
System Architecture
The basics described above were all important.  But a number of even more impactful areas required deeper architectural
co-design and co-evolution with the language, runtime, framework, and operating system itself.  I’ve written about the
immense benefits of this sort of “whole system” approach before.  It was kind of magical.
GC
Midori was garbage collected through-and-through.  This was a key element of our overall model’s safety and
productivity.  In fact, at one point, we had 11 distinct collectors, each with its own unique characteristics.  (For
instance, see this study.)  We
had some ways to combat the usual problems, like long pause times.  I’ll go through those in a future post, however.
For now, let’s stick to the realm of code quality.
The first top-level decision is: conservative or precise?  A conservative collector is easier to wedge into an
existing system, however it can cause troubles in certain areas.  It often needs to scan more of the heap to get the
same job done.  And it can falsely keep objects alive.  We felt both were unacceptable for a systems programming
environment.  It was an easy, quick decision: we sought precision.
Precision costs you something in the code generators, however.  A precise collector needs to get instructions where to
find its root set.  That root set includes field offsets in data structures in the heap, and also places on the stack
or, even in some cases, registers.  It needs to find these so that it doesn’t miss an object and erroneously collect it
or fail to adjust a pointer during a relocation, both of which would lead to memory safety problems.  There was no magic
trick to making this efficient other than close integration between runtime and code generator, and being thoughtful.
This brings up the topic of cooperative versus preemptive, and the notion of GC safe-points.  A GC operating in
cooperative mode will only collect when threads have reached so-called “safe-points.”  A GC operating in preemptive
mode, on the other hand, is free to stop threads in their tracks, through preemption and thread suspension, so that it
may force a collection.  In general, preemptive requires more bookkeeping, because the roots must be identifiable at
more places, including things that have spilled into registers.  It also makes certain low-level code difficult to
write, of the ilk you’ll probably find in an operating system’s kernel, because objects are subject to movement between
arbitrary instructions.  It’s difficult to reason about.  (See this file, and its associated uses in the CLR codebase, if you
don’t believe me.)  As a result, we used cooperative mode as our default.  We experimented with automatic safe-point
probes inserted by the compiler, for example on loop back-edges, but opted to bank the code quality instead.  It did
mean GC “livelock” was possible, but in practice we seldom ran into this.
We used a generational collector.  This has the advantage of reducing pause times because less of the heap needs to be
inspected upon a given collection.  It does come with one disadvantage from the code generator’s perspective, which is
the need to insert write barriers into the code.  If an older generation object ever points back at a younger generation
object, then the collector – which would have normally preferred to limit its scope to younger generations – must know
to look at the older ones too.  Otherwise, it might miss something.
Write barriers show up as extra instructions after certain writes; e.g., note the call:
48 8D 49 08        lea         rcx,[rcx+8]
E8 7A E5 FF FF     call        0000064488002028

That barrier simply updates an entry in the card table, so the GC knows to look at that segment the next time it scans
the heap.  Most of the time this ends up as inlined assembly code, however it depends on the particulars of the
situation.  See this code for an
example of what this looks like for the CLR on x64.
It’s difficult for the compiler to optimize these away because the need for write barriers is “temporal” in nature.  We
did aggressively eliminate them for stack allocated objects, however.  And it’s possible to write, or transform code,
into less barrier hungry styles.  For example, consider two ways of writing the same API:
bool Test(out object o);
object Test(out bool b);

In the resulting Test method body, you will find a write barrier in the former, but not the latter.  Why?  Because the
former is writing a heap object reference (of type object), and the compiler has no idea, when analyzing this method
in isolation, whether that write is to another heap object.  It must be conservative in its analysis and assume the
worst.  The latter, of course, has no such problem, because a bool isn’t something the GC needs to scan.
Another aspect of GC that impacts code quality is the optional presence of more heavyweight concurrent read and write
barriers, when using concurrent collection.  A concurrent GC does some collection activities concurrent with the user
program making forward progress.  This is often a good use of multicore processors and it can reduce pause times and
help user code make more forward progress over a given period of time.
There are many challenges with building a concurrent GC, however one is that the cost of the resulting barriers is high.
The original concurrent GC by Henry Baker was a copying GC and had the notion
of “old” versus “new” space.  All reads and writes had to be checked and, anything operation against the old space had
to be forwarded to the new space.  Subsequent research for the DEC Firefly used hardware memory protection to reduce the
cost, but the faulting cases were still exceedingly expensive.  And, worst of all, access times to the heap were
unpredictable.  There has been a lot of good research into solving this problem, however we abandoned copying.
Instead, we used a concurrent mark-sweep compacting collector.  This means only write barriers are needed under normal
program execution, however some code was cloned so that read barriers were present when programs ran in the presence of
object movement.  Our primary GC guy’s research was published, so you can read all about it.  The
CLR also has a concurrent collector, but it’s not quite as good.  It uses copying to collect the youngest generation,
mark-sweep for the older ones, and the mark phase is parallelized.  There are unfortunately a few conditions that can
lead to sequential pauses (think of this like a big “lock”), sometimes over 10 milliseconds: 1) all threads must be
halted and scanned, an operation that is bounded only by the number of threads and the size of their stacks; 2) copying
the youngest generation is bounded only by the size of that generation (thankfully, in normal configurations, this is
small); and 3) under worst case conditions, compaction and defragmentation, even of the oldest generation, can happen.
Separate compilation
The basic model to start with is static linking.  In this model, you compile everything into a single executable.  The
benefits of this are obvious: it’s simple, easy to comprehend, conceptually straightforward to service, and less work
for the entire compiler toolchain.  Honestly, given the move to Docker containers as the unit of servicing, this model
makes more and more sense by the day.  But at some point, for an entire operating system, you’ll want separate
compilation.  Not just because compile times can get quite long when statically linking an entire operating system, but
also because the working set and footprint of the resulting processes will be bloated with significant duplication.
Separately compiling object oriented APIs is hard.  To be honest, few people have actually gotten it to work.  Problems
include the fragile base class problem, which is a real killer for
version resilient libraries.  As a result, most real systems use a dumbed down “C ABI” at the boundary between components.  This is why Windows,
for example, has historically used flat C Win32 APIs and, even in the shift to more object orientation via WinRT, uses
COM underneath it all.  At some runtime expense, the ObjectiveC runtime addressed this challenge.  As with most things
in computer science, virtually all problems can be solved with an extra level of indirection; this one can be too.
The design pivot we took in Midori was that whole processes were sealed.  There was no dynamic loading, so nothing that
looked like classical DLLs or SOs.  For those scenarios, we used the Asynchronous Everything programming model, which made it easy to dynamically
connect to and use separately compiled and versioned processes.
We did, however, want separately compiled binaries, purely as a developer productivity and code sharing (working set)
play.  Well, I lied.  What we ended up with was incrementally compiled binaries, where a change in a root node triggered
a cascading recompilation of its dependencies.  But for leaf nodes, such as applications, life was beautiful.  Over
time, we got smarter in the toolchain by understanding precisely which sorts of changes could trigger cascading
invalidation of images.  A function that was known to never have been inlined across modules, for example, could have its
implementation – but not its signature – changed, without needing to trigger a rebuild.  This is similar to the
distinction between headers and objects in a classical C/C++ compilation model.
Our compilation model was very similar to C++’s, in that there was static and dynamic linking.  The runtime model, of
course, was quite different.  We also had the notion of “library groups,” which let us cluster multiple logically
distinct, but related, libraries into a single physical binary.  This let us do more aggressive inter-module
optimizations like inlining, devirtualization, async stack optimizations, and more.
Parametric polymorphism (a.k.a., generics)
That brings me to generics.  They throw a wrench into everything.
The problem is, unless you implement an erasure
model – which utterly stinks performance-wise due
to boxing allocations, indirections, or both – there’s no way for you to possibly pre-instantiate all possible versions
of the code ahead-of-time.  For example, say you’re providing a List<T>.  How do you know whether folks using your
library will want a List<int>, List<string>, or List<SomeStructYouveNeverHeardOf>?
Solutions abound:

Do not specialize.  Erase everything.
Specialize only a subset of instantiations, and create an erased instantiation for the rest.
Specialize everything.  This gives the best performance, but at some complexity.

Java uses #1 (in fact, erasure is baked into the language).  Many ML compilers use #2.  .NET’s NGen compilation
model is sort of a variant of #2, where things that can be trivially specialized are specialized, and everything else is
JIT compiled.  .NET Native doesn’t yet have a solution to this problem, which means 3rd party libraries, separate
compilation, and generics are a very big TBD.   As with everything in Midori, we picked the hardest path, with the most
upside, which meant #3.  Actually I’m being a little glib; we had several ML compiler legends on the team, and #2 is
fraught with peril; just dig a little into some papers on how hard (and clever) this can
get.  It’s difficult to know a priori which instantiations are going to be performance critical to a program.  My own
experience trying to get C# code into the heart of Windows back in the Longhorn days also reinforced this; we didn’t
want JIT’ting and the rules for what generics you could and couldn’t use in that world were so mind boggling they
eventually led to greek formulas.
Anyway, Midori’s approach turned out to be harder than it sounded at first.
Imagine you have a diamond.  Library A exports a List<T> type, and libraries B and C both instantiate List<int>.  A
program D then consumes both B and C and maybe even passes List<T> objects returned from one to the other.  How do we
ensure that the versions of List<int> are compatible?
We called this problem the potentially multiply instantiated, or PMI for short, problem.
The CLR handles this problem by unifying the instantiations at runtime.  All RTTI data structures, vtables, and whatnot,
are built and/or aggressively patched at runtime.  In Midori, on the other hand, we wanted all such data structures to
be in readonly data segments and hence shareable across processes, wherever possible.
Again, everything can be solved with an indirection.  But unlike solution #2 above, solution #3 permits you to stick
indirections only in the rare places where you need them.  And for purposes of this one, that meant RTTI and accessing
static variables of just those generic types that might have been subject to PMI.  First, that affected a vast subset of
code (versus #2 which generally affects even loading of instance fields).  Second, it could be optimized away for
instantiations that were known not to be PMI, by attaching state and operations to the existing generic dictionary that
was gets passed around as a hidden argument already.  And finally, because of all of this, it was pay for play.
But damn was it complex.
It’s funny, but C++ RTTI for template instantiations actually suffers from many of the same problems.  In fact, the
Microsoft Visual C++ compiler resorts to a strcmp of the type names, to resolve diamond issues!  (Thankfully there are
well-known, more efficient ways to do this, which we are
actively pursuing for the next release of VC++.)
Virtual dispatch
Although I felt differently when first switching from Java to C#, Midori made me love that C# made methods non-virtual
by default.  I’m sure we would have had to change this otherwise.  In fact, we went even further and made classes
sealed by default, requiring that you explicitly mark them virtual if you wanted to facilitate subclasses.
Aggressive devirtualization, however, was key to good performance.  Each virtual means an indirection.  And more
impactfully, a lost opportunity to inline (which for small functions is essential).  We of course did global
intra-module analysis to devirtualize, but also extended this across modules, using whole program compilation, when
multiple binaries were grouped together into a library group.
Although our defaults were right, my experience with C# developers is that they go a little hog-wild with virtuals and
overly abstract code.  I think the ecosystem of APIs that exploded around highly polymorphic abstractions, like LINQ and
Reactive Extensions, encouraged this and instilled a bit of bad behavior (“gratuitous over-abstraction”).  I guess you
could make similar arguments about highly templated code in C++.  As you can guess, there wasn’t very much of it in the
lowest levels of our codebase – where every allocation and instruction mattered – but in higher level code, especially
in applications that tended to be dominated by high-latency asynchronous operations, the overheads were acceptable and
productivity benefits high.  A strong culture around identifying and trimming excessive fat helped to ensure features
like this were used appropriately, via code reviews, benchmarks, and aggressive static analysis checking.
Interfaces were a challenge.
There are just some poorly designed, inefficient patterns in the .NET Framework.  IEnumerator<T> requires two
interface dispatches simply to extract the next item!  Compare that to C++ iterators which can compile down a pointer
increment plus dereference.  Many of these problems could be addressed simply with better library designs.  (Our final
design for enumeration didn’t even involve interfaces at all.)
Plus invoking a C# interface is tricky.  Existing systems do not use pointer adjustment like
C++ does so usually an interface dispatch requires a table search.  First a level of indirection to get to the vtable,
then another level to find the interface table for the interface in question.  Some systems attempt to do callsite
caching for monomorphic invocations; that is, caching the latest invocation in the hope that the same object kind passes
through that callsite time and time again.  This requires mutable stubs, however, not to mention an incredibly complex
system of thunks and whatnot.  In Midori, we
never ever ever violated W^X; and we avoided mutable runtime data structures,
because they inhibit sharing, both in terms of working set, but also amortizing TLB and data cache pressure.
Our solution took advantage of the memory ordering model earlier.  We used so-called “fat” interface pointers.  A fat
interface pointer was two words: the first, a pointer to the object itself; the second, a pointer to the interface
vtable for that object.  This made conversion to interfaces slightly slower – because the interface vtable lookup had
to happen – but for cases where you are invoking it one or more times, it came out a wash or ahead.  Usually,
significantly.  Go does something like this, but it’s slightly different for two reasons.  First, they generate the
interface tables on the fly, because interfaces are duck typed.  Second, fat interface pointers are subject to tearing
and hence can violate memory safety in Go, unlike Midori thanks to our strong concurrency model.
The finally challenge in this category was generic virtual methods, or GVMs.  To cut to the chase, we banned them.
Even if you NGen an image in .NET, all it takes is a call to the LINQ query a.Where(...).Select(...), and you’re
pulling in the JIT compiler.  Even in .NET Native, there is considerable runtime data structure creation, lazily, when
this happens.  In short, there is no known way to AOT compile GVMs in a way that is efficient at runtime.  So, we didn’t
even bother offering them.  This was a slightly annoying limitation on the programming model but I’d have done it all
over again thanks to the efficiencies that it bought us.  It really is surprising how many GVMs are lurking in .NET.
Statics
I was astonished the day I learned that 10% of our code size was spent on static initialization checks.
Many people probably don’t realize that the CLI specification offers two static initialization modes.  There
is the default mode and beforefieldinit.  The default mode is the same as Java’s.  And it’s horrible. The static
initializer will be run just prior to accessing any static field on that type, any static method on that type, any
instance or virtual method on that type (if it’s a value type), or any constructor on that type.  The “when” part
doesn’t matter as much as what it takes to make this happen; all of those places now need to be guarded with explicit
lazy initialization checks in the resulting machine code!
The beforefieldinit relaxation is weaker.  It guarantees the initializer will run sometime before actually accessing
a static field on that type.  This gives the compiler a lot of leeway in deciding on this placement.  Thankfully the
C# compiler will pick beforefieldinit automatically for you should you stick to using field initializers only.  Most
people don’t realize the incredible cost of choosing instead to use a static constructor, however, especially for value
types where suddenly all method calls now incur initialization guards.  It’s just the difference between:
struct S {
    static int Field = 42;
}

and:
struct S {
    static int Field;
    static S() {
        Field = 42;
    }
}

Now imagine the struct has a property:
struct S {
    // As above...
    int InstanceField;
    public int Property { get { return InstanceField; } }
}

Here’s the machine code for Property if S has no static initializer, or uses beforefieldinit (automatically
injected by C# in the the field initializer example above):
; The struct is one word; move its value into EAX, and return it:
8B C2                mov         eax,edx
C3                   ret

And here’s what happens if you add a class constructor:
; Big enough to get a frame:
56                   push        rsi
48 83 EC 20          sub         rsp,20h
; Load the field into ESI:
8B F2                mov         esi,edx
; Load up the cctor's initialization state:
48 8D 0D 02 D6 FF FF lea         rcx,[1560h]
48 8B 09             mov         rcx,qword ptr [rcx]
BA 03 00 00 00       mov         edx,3
; Invoke the conditional initialization helper:
E8 DD E0 FF FF       call        2048
; Move the field from ESI into EAX, and return it:
8B C6                mov         eax,esi
48 83 C4 20          add         rsp,20h
5E                   pop         rsi

On every property access!
Of course, all static members still incur these checks, even if beforefieldinit is applied.
Although C++ doesn’t suffer this same problem, it does have mind-bending initialization ordering semantics.  And, like C# statics, C++11 introduced thread-safe
initialization, by way of the “magic statics” feature.
We virtually eliminated this entire mess in Midori.
I mentioned offhandedly earlier that Midori had no mutable statics.  More accurately, we extended the notion of const
to cover any kind of object.  This meant that static values were evaluated at compile-time, written to the readonly
segment of the resulting binary image, and shared across all processes.  More importantly for code quality, all runtime
initialization checks were removed, and all static accesses simply replaced with a constant address.
There were still mutable statics at the core of the system – in the kernel, for example – but these did not make their
way up into user code.  And because they were few and far between, we did not rely on the classical C#-style lazy
initialization checks for them.  They were manually initialized on system startup.
As I said earlier, a 10% reduction in code size, and lots of speed improvements.  It’s hard to know exactly how much
saved this was than a standard C# program because by the time we made the change, developers were well aware of the
problems and liberally applied our [BeforeFieldInit] attribute all over their types, to avoid some of the overheads.
So the 10% number is actually a lower bound on the savings we realized throughout this journey.
Async model
I already wrote a lot about our async model.  I won’t
rehash all of that here.  I will reiterate one point: the compiler was key to making linked stacks work.
In a linked stacks model, the compiler needs to insert probes into the code that check for available stack space.  In
the event there isn’t enough to perform some operation – make a function call, dynamically allocate on the stack, etc.
– the compiler needs to arrange for a new link to get appended, and to switch to it.  Mostly this amounts to some
range checking, a conditional call to a runtime function, and patching up RSP.  A probe looked something like:
; Check amount of stack space:
    lea     rax, [rsp-250h]
    cmp     rax, qword ptr gs:[0]
    ja      prolog
; If insufficient stack, link a new segment:
    mov     eax, 10029h
    call    ?g_LinkNewStackTrampoline
prolog:
; The real code goes here...

Needless to say, you want to probe as little as possible, for two reasons.  First, they incur runtime expense.  Second,
they chew up code size.  There are a few techniques we used to eliminate probes.
The compiler of course knew how to compute stack usage of functions.  As a result, it could be smart about the amount of
memory to probe for.  We incorporated this knowledge into our global analyzer.  We could coalesce checks after doing
code motion and inlining.  We hoisted checks out of loops.  For the most part, we optimized for eliminating checks,
sometimes at the expense of using a little more stack.
The most effective technique we used to eliminate probes was to run synchronous code on a classical stack, and to teach
our compiler to elide probes altogether for them.  This took advantage of our understanding of async in the type system.
Switching between the classical stack and back again again amounted to twiddling RSP:
; Switch to the classical stack:
move    rsp, qword ptr gs:[10h]
sub     rsp, 20h

; Do some work (like interop w/ native C/C++ code)...

; Now switch back:
lea     rsp, [rbp-50h]

I know Go abandoned linked stacks because of these switches.  At first they were pretty bad for us, however after about
a man year or two of effort, the switching time faded away into the sub-0.5% noise.
Memory ordering model
Midori’s stance on safe concurrency had truly one
amazing benefit: you get a sequentially consistent memory
ordering model for free.  You may wish to read that again.  Free!
Why is this so?  First, Midori’s process model ensured
single-threaded execution by default.  Second, any fine-grained parallelism inside of a process was governed by a finite
number of APIs, all of which were race-free.  The lack of races meant we could inject a fence at fork and join points,
selectively, without a developer needing to care or know.
Obviously this had incredible benefits to developer productivity.  The fact that Midori programmers never got bitten by
memory reordering problems was certainly one of my proudest outcomes of the project.
But it also meant the compiler was free to make more aggressive code motion optimizations, without any sacrifices to this
highly productive programming model.  In other words, we got the best of both worlds.
A select few kernel developers had to think about the memory ordering model of the underlying machine.  These were the
people implementing the async model itself.  For that, we eliminated C#’s notion of volatile – which is utterly
broken anyway – in favor of something more like C++
atomics.  That model is quite nice for two reasons.  First, what kind of
fence you need is explicit for every read and write, where it actually matters.  (ences affect the uses of a variable,
not its declaration.  Second, the explicit model tells the compiler more information about what optimizations can or
cannot take place, again at a specific uses, where it matters most.
Error model
Our error model journey was a long one and will be the topic of a future post.  In a nutshell, however, we experimented
with two ends of the spectrum – exceptions and return codes – and lots of points in betweeen.
Here is what we found from a code quality perspective.
Return codes are nice because the type system tells you an error can happen.  A developer is thus forced to deal with
them (provided they don’t ignore return values).  Return codes are also simple, and require far less “runtime magic”
than exceptions or related mechanisms like setjmp/longjmp.  So, lots to like here.
From a code quality persective, however, return codes suck.  They force you to execute instructions in hot paths that
wouldn’t have otherwise been executed, including when errors aren’t even happening.  You need to return a value from
your function – occupying register and/or stack space – and callers need to perform branches to check the results.
Granted, we hope that these are predicted correctly, but the reality is, you’re just doing more work.
Untyped exceptions suck when you’re trying to build a reliable system.  Operating systems need to be reliable.  Not
knowing that there’s a hidden control flow path when you’re calling a function is, quite simply, unacceptable.  They
also require heavier weight runtime support to unwind stacks, search for handlers, and so on.  It’s also a real pain in
the arse to model exceptional control flow in the compiler.  (If you don’t believe me, just read through this mail
exchange).  So, lots to hate here.
Typed exceptions – I got used to not saying checked exceptions for fear of hitting Java nerves – address some of these
shortcomings, but come with their own challenges.  Again, I’ll save detailed analysis for my future post.
From a code quality perspective, exceptions can be nice.  First, you can organize code segments so that the “cold”
handlers aren’t dirtying your ICACHE on successful pathways.  Second, you don’t need to perform any extra work during
the normal calling convention.  There’s no wrapping of values – so no extra register or stack pressure – and there’s
no branching in callers.  There can be some downsides to exceptions, however.  In an untyped model, you must assume
every function can throw, which obviously inhibits your ability to move code around.
Our model ended up being a hybrid of two things:

Fail-fast for programming bugs.
Typed exceptions for dynamically recoverable errors.

I’d say the ratio of fail-fast to typed exceptions usage ended up being 10:1.  Exceptions were generally used for I/O
and things that dealt with user data, like the shell and parsers.  Contracts were the biggest source of fail-fast.
The result was the best possible configuration of the above code quality attributes:

No calling convention impact.
No peanut butter associated with wrapping return values and caller branching.
All throwing functions were known in the type system, enabling more flexible code motion.
All throwing functions were known in the type system, giving us novel EH optimizations, like turning try/finally
blocks into straightline code when the try could not throw.

A nice accident of our model was that we could have compiled it with either return codes or exceptions.  Thanks to this,
we actually did the experiment, to see what the impact was to our system’s size and speed.  The exceptions-based system
ended up being roughly 7% smaller and 4% faster on some key benchmarks.
At the end, what we ended up with was the most robust error model I’ve ever used, and certainly the most performant one.
Contracts
As implied above, Midori’s programming language had first class contracts:
void Push(T element)
    requires element != null
    ensures this.Count == old.Count + 1
{
        ...
}

The model was simple:

By default, all contracts are checked at runtime.
The compiler was free to prove contracts false, and issue compile-time errors.
The compiler was free to prove contracts true, and remove these runtime checks.

We had conditional compilation modes, however I will skip these for now.  Look for an upcoming post on our language.
In the early days, we experimented with contract analyzers like MSR’s Clousot, to prove contracts.  For compile-time reasons, however, we had to
abandon this approach.  It turns out compilers are already very good at doing simple constraint solving and propagation.
So eventually we just modeled contracts as facts that the compiler knew about, and let it insert the checks wherever
necessary.
For example, the loop optimizer complete with range information above can already leverage checks like this:
void M(int[] array, int index) {
    if (index >= 0 && index < array.Length) {
        int v = array[index];
        ...
    }
}

to eliminate the redundant bounds check inside the guarded if statement.  So why not also do the same thing here?
void M(int[] array, int index)
        requires index >= 0 && index < array.Length {
    int v = array[index];
    ...
}

These facts were special, however, when it comes to separate compilation.  A contract is part of a method’s signature,
and our system ensured proper subtyping substitution,
letting the compiler do more aggressive optimizations at separately compiled boundaries.  And it could do these
optimizations faster because they didn’t depend on global analysis.
Objects and allocation
In a future post, I’ll describe in great detail our war with the garbage collector.  One technique that helped us win,
however, was to aggressively reduce the size and quantity of objects a well-behaving program allocated on the heap.
This helped with overall working set and hence made programs smaller and faster.
The first technique here was to shrink object sizes.
In C# and most Java VMs, objects have headers.  A standard size is a single word, that is, 4 bytes on 32-bit
architectures and 8 bytes on 64-bit.  This is in addition to the vtable pointer.  It’s typically used by the GC to mark
objects and, in .NET, is used for random stuff, like COM interop, locking, memozation of hash codes, and more.  (Even
the source code calls it the “kitchen sink”.)
Well, we ditched both.
We didn’t have COM interop.  There was no unsafe free-threading so there was no locking (and locking on random objects
is a bad idea anyway).  Our
Object didn’t define a GetHashCode.  Etc.  This saved a word per object with no discernable loss in the programming
model (actually, to the contrary, it was improved), which is nothing to shake a stick at.
At that point, the only overhead per object was the vtable pointer.  For structs, of course there wasn’t one (unless
they were boxed).  And we did our best to eliminate all of them.  Sadly, due to RTTI, it was difficult to be aggressive.
I think this is another area where I’d go back and entirely upend the C# type system, to follow a more C, C++, or even
maybe Go-like, model.  In the end, however, I think we did get to be fairly competitive with your average C++ program.
There were padding challenges.  Switching the struct layout from C#’s current default of sequential, to our
preferred default of auto, certainly helped.  As did optimizations like the well-known C++ empty base optimization.
We also did aggressive escape analysis in order to more efficiently allocate objects.  If an object was found to be
stack-confined, it was allocated on the stack instead of the heap.  Our initial implementation of this moved somewhere
in the neighborhood of 10% static allocations from the heap to the stack, and let us be far more aggressive about
pruning back the size of objects, eliminating vtable pointers and entire unused fields.  Given how conservative this
analysis had to be, I was pretty happy with these results.
We offered a hybrid between C++ references and Rust borrowing if developers wanted to give the compiler a hint while at
the same time semantically enforcing some level of containment.  For example, say I wanted to allocate a little array to
share with a callee, but know for sure the callee does not remember a reference to it.  This was as simple as saying:
void Caller() {
    Callee(new[] { 0, 1, ..., 9 });
}

void Callee(int[]& a) {
    ... guaranteed that `a` does not escape ...
}

The compiler used the int[]& information to stack allocate the array and, often, eliminate the vtable for it
entirely.  Coupled with the sophisticated elimination of bounds checking, this gave us something far closer to C
performance.
Lambdas/delegates in our system were also structs, so did not require heap allocation.  The captured display frame was
subject to all of the above, so frequently we could stack allocate them.  As a result, the following code was heap
allocation-free; in fact, thanks to some early optimizations, if the callee was inlined, it ran as though the actual
lambda body was merely expanded as a sequence of instructions, with no call over head either!
void Caller() {
    Callee(() => ... do something ... );
}

void Callee(Action& callback) {
    callback();
}

In my opinion, this really was the killer use case for the borrowing system.  Developers avoided lambda-based APIs in
the early days before we had this feature for fear of allocations and inefficiency.  After doing this feature, on the
other hand, a vibrant ecosystem of expressive lambda-based APIs flourished.
Throughput
All of the above have to do with code quality; that is, the size and speed of the resulting code.  Another important
dimension of compiler performance, however, is throughput; that is, how quickly you can compile the code.  Here too
a language like C# comes with some of its own challenges.
The biggest challenge we encountered has less to do with the inherently safe nature of a language, and more to do with
one very powerful feature: parametric polymorphism.  Or, said less pretentiously, generics.
I already mentioned earlier that generics are just a convenient copy-and-paste mechanism.  And I mentioned some
challenges this poses for code size.  It also poses a problem for throughput, however.  If a List<T> instantiation
creates 28 types, each with its own handful of methods, that’s just more code for the compiler to deal with.  Separate
compilation helps, however as also noted earlier, generics often flow across module boundaries.  As a result, there’s
likely to be a non-trivial impact to compile time.  Indeed, there was.
In fact, this is not very different from where most C++ compilers spend the bulk of their time.  In C++, it’s templates.
More modern C++ code-bases have similar problems, due to heavy use of templated abstractions, like STL, smart pointers,
and the like.  Many C++ code-bases are still just “C with classes” and suffer this problem less.
As I mentioned earlier, I wish we had banished RTTI.  That would have lessened the generics problem.  But I would guess
generics still would have remained our biggest throughput challenge at the end of the day.
The funny thing – in a not-so-funny kind of way – is that you can try to do analysis to prune the set of generics and,
though it is effective, this analysis takes time.  The very thing you’re trying to save.
A metric we got in the habit of tracking was how much slower AOT compiling a program was than simply C# compiling it.
This was a totally unfair comparison, because the C# compiler just needs to lower to MSIL whereas an AOT compler needs
to produce machine code.  It’d have been fairer to compare AOT compiling to JIT compiling.  But no matter, doing a great
job on throughput is especially important for a C# audience.  The expectation of productivity was quite high.  This was
therefore the key metric we felt customers would judge us on, and so we laser-focused on it.
In the early days, the number was ridiculously bad.  I remember it being 40x slower.  After about a year and half with
intense focus we got it down to 3x for debug builds and 5x for optimized builds.  I was very happy with this!
There was no one secret to achieving this.  Mostly it had to do with just making the compiler faster like you would any
program.  Since we built the compiler using Midori’s toolchain, however – and compiled it using itself – often this
was done by first making Midori better, which then made the the compiler faster.  It was a nice virtuous loop.  We had
real problems with string allocations which informed what to do with strings in our programming model.  We found crazy
generics instantiation closures which forced us to eliminate them and build tools to help find them proactively.  Etc.
Culture
A final word before wrapping up.  Culture was the most important aspect of what we did.  Without the culture, such an
amazing team wouldn’t have self-selected, and wouldn’t have relentlessly pursued all of the above achievements.  I’ll
devote an entire post to this.  However, in the context of compilers, two things helped:

We measured everything in the lab.  “If it’s not in the lab, it’s dead to me.”
We reviewed progress early and often.  Even in areas where no progress was made.  We were habitually self-critical.

Every sprint, we had a so-called “CQ Review” (where CQ stands for “code quality”).  The compiler team prepared for a few
days, by reviewing every benchmark – ranging from the lowest of microbenchmarks to compiling and booting all of Windows
– and investigating any changes.  All expected wins were confirmed (we called this “confirming your kill”), any
unexpected regressions were root cause analyzed (and bugs filed), and any wins that didn’t materialize were also
analyzed and reported on so that we could learn from it.  We even stared at numbers that didn’t change, and asked
ourselves, why didn’t they change.  Was it expected?  Do we feel bad about it and, if so, how will we change next
sprint?  We reviewed our competitors’ latest compiler drops and monitored their rate of change.  And so on.
This process was enormously healthy.  Everyone was encouraged to be self-critical.  This was not a “witch hunt”; it was
an opportunity to learn as a team how to do better at achieving our goals.
Post-Midori, I have kept this process.  I’ve been surprised at how contentious this can be with some folks.  They get
threatened and worry their lack of progress makes them look bad.  They use “the numbers aren’t changing because that’s
not our focus right now” as justification for getting out of the rhythm.  In my experience, so long as the code is
changing, the numbers are changing.  It’s best to keep your eye on them lest you get caught with your pants around your
ankles many months later when it suddenly matters most.  The discipline and constant drumbeat are the most important
parts of these reviews, so skipping even just one can be detrimental, and hence was verboten.
This process was as much our secret sauce as anything else was.
Wrapping Up
Whew, that was a lot of ground to cover.  I hope at the very least it was interesting, and I hope for the incredible
team who built all of this that I did it at least a fraction of justice.  (I know I didn’t.)
This journey took us over a decade, particularly if you account for the fact that both Bartok and Phoenix had existed
for many years even before Midori formed.  Merely AOT compiling C#, and doing it well, would have netted us many of the
benefits above.  But to truly achieve the magical native-like performance, and indeed even exceed it in certain areas,
required some key “whole system” architectural bets.  I hope that some day we can deliver safety into the world at this
level of performance.  Given the state of security all-up in the industry, mankind seriously needs it.
I’ve now touched on our programming language enough that I need to go deep on it.  Tune in next time!

Tweet











"
https://news.ycombinator.com/rss,Show HN: Otterkit – COBOL compiler for .NET,https://github.com/otterkit/otterkit,Comments,"








otterkit

/

otterkit

Public




 

Notifications



 

Fork
    1




 


          Star
 77
  









        Otterkit COBOL Compiler
      





otterkit.com


License





     Apache-2.0 license
    






77
          stars
 



1
          fork
 



 


          Star

  





 

Notifications












Code







Issues
1






Pull requests
0






Discussions







Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Discussions
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







otterkit/otterkit









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








4
branches





0
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




KTSnowy

[Update]: Parse SEARCH statement




        …
      




        eb47152
      

Jan 17, 2023





[Update]: Parse SEARCH statement


eb47152



Git stats







176

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github/ISSUE_TEMPLATE



[Update]: Update issue templates and security policy notice



Nov 19, 2022









Assets



[Update]: Added dotnet tool and project templates Nuget packages



Dec 24, 2022









OtterkitTemplatePack



[Update]: Update Otterkit.Templates version number



Dec 24, 2022









libotterkit @ a0256a7



[Update/Fix]: Apply Roslyn Analyzers fixes and change suggestions



Jan 4, 2023









src



[Update]: Parse SEARCH statement



Jan 17, 2023









.gitattributes



Create .gitattributes



Oct 13, 2022









.gitignore



[Update]: Added dotnet tool and project templates Nuget packages



Dec 24, 2022









.gitmodules



[Update]: Added libotterkit as a git submodule



Nov 2, 2022









LICENSE



[Update]: Added year and name to the LICENSE file



Dec 19, 2022









README.md



[Update]: Add project icon to the README header



Jan 11, 2023









SECURITY.md



[Update]: Change ""runtime"" to ""compiler"" in the security policy



Nov 19, 2022









THIRD-PARTY-LICENSES



[Update]: Added libmpdec license notice



Oct 23, 2022




    View code
 















 Otterkit COBOL Compiler
About
Installation
Quick Install
Build from Source
Standard Acknowledgement





README.md




 Otterkit COBOL Compiler
Otterkit is a free and open source compiler for the COBOL Programming Language on the .NET platform.
Warning: The project is currently in pre-release, so not all of the standard has been implemented.
About
COBOL was created in 1959 by the CODASYL Committee (With Rear Admiral Grace Hopper as a technical consultant to the committee), its design follows Grace Hopper's belief that programs should be written in a language that is close to English. It prioritizes readability, reliability, and long-term maintenance. The language has been implemented throughout the decades on many platforms with many dialects, and the Otterkit COBOL compiler is a free and open source implementation of the ISO COBOL 2022 Standard on the .NET platform.
Installation
Quick Install
Otterkit is available to install on the Nuget package manager (.NET 7 is required). To install, type into the command line:
dotnet tool install --global Otterkit --version 1.0.15-alpha

Build from Source
First, clone the git repo from https://github.com/otterkit/otterkit.git to get the source code. To access the libotterkit submodule inside, use the --recurse-submodules --remote-submodules flag on the clone command. To run, navigate into the src folder (for the compiler, not libotterkit) and then type dotnet run into the command line.
Standard Acknowledgement
Any organization interested in reproducing the COBOL standard and specifications in whole or in part,
using ideas from this document as the basis for an instruction manual or for any other purpose, is free
to do so. However, all such organizations are requested to reproduce the following acknowledgment
paragraphs in their entirety as part of the preface to any such publication (any organization using a
short passage from this document, such as in a book review, is requested to mention ""COBOL"" in
acknowledgment of the source, but need not quote the acknowledgment):
COBOL is an industry language and is not the property of any company or group of companies, or of any
organization or group of organizations.
No warranty, expressed or implied, is made by any contributor or by the CODASYL COBOL Committee
as to the accuracy and functioning of the programming system and language. Moreover, no
responsibility is assumed by any contributor, or by the committee, in connection therewith.
The authors and copyright holders of the copyrighted materials used herein:

FLOW-MATIC® (trademark of Sperry Rand Corporation), Programming for the 'UNIVAC® I and
II, Data Automation Systems copyrighted 1958,1959, by Sperry Rand Corporation;
IBM Commercial Translator Form No F 28-8013, copyrighted 1959 by IBM;
FACT, DSI 27A5260-2760, copyrighted 1960 by Minneapolis-Honeywell

Have specifically authorized the use of this material in whole or in part, in the COBOL specifications.
Such authorization extends to the reproduction and use of COBOL specifications in programming
manuals or similar publications.









About

      Otterkit COBOL Compiler
    





otterkit.com


Topics



  compiler


  dotnet


  cobol



Resources





      Readme
 
License





     Apache-2.0 license
    

Security policy





      Security policy
    



Stars





77
    stars

Watchers





3
    watching

Forks





1
    fork







    Releases

No releases published












    Contributors 3








KTSnowy
Gabriel Gonçalves

 






TriAttack238
Sean Vo

 






gabrielesilinic
Gabriele Silingardi

 





Languages










C#
100.0%











"
https://news.ycombinator.com/rss,Olympic medalists in art competitions,https://en.wikipedia.org/wiki/List_of_Olympic_medalists_in_art_competitions,Comments,"



List of Olympic medalists in art competitions - Wikipedia












































List of Olympic medalists in art competitions

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search


 Under the pseudonyms Georges Hohrod and Martin Eschbach, IOC founder Pierre de Coubertin won a gold medal in the literature category at the 1912 Summer Olympics.
There were 146 medalists in the art competitions that were part of the Olympic Games from 1912 until 1948. These art competitions were considered an integral part of the movement by International Olympic Committee (IOC) founder Pierre de Coubertin and necessary to recapture the complete essence of the Ancient Olympic Games. Their absence before the 1912 Summer Olympics, according to journalism professor Richard Stanton, stems from Coubertin ""not wanting to fragment the focus of his new and fragile movement"".[1] Art competitions were originally planned for inclusion in the 1908 Summer Olympics but were delayed after that edition's change in venue from Rome to London following the 1906 eruption of Mount Vesuvius.[2] By the 1924 Summer Olympics they had grown to be considered internationally relevant and potentially ""a milestone in advancing public awareness of art as a whole"".[3]
During their first three appearances, the art competitions were grouped into five broad categories: architecture, literature, music, painting, and sculpture. The Dutch Organizing Committee for the 1928 Summer Olympics split these into subcategories in the hopes of increasing participation.[4] Although it was a successful strategy, the 1932 Summer Olympics eliminated several of these subcategories, which led to fewer entries in the broader categories.[5] For the 1936 Summer Olympics, the German government proposed the addition of a film contest to the program, which was rejected.[6]
Following a final appearance at the 1948 Summer Olympics, art competitions were removed from the Olympic program. Planners of the 1952 Summer Olympics opposed their inclusion on logistical grounds, claiming that the lack of an international association for the event meant that the entire onus of facilitation was placed on the local organizing committee.[7] Concerns were also raised about the professionalism of the event, since only amateurs were allowed to participate in the sporting tournaments,[8] and the growing commercialization of the competitions, as artists had been permitted to sell their submissions during the course of the Games since 1928. In 1952 an art festival and exhibition was held concurrent with the Games, a tradition that has been maintained in all subsequent Summer Olympics.[9]
The IOC does not track medalists in Olympic art competitions in its database and thus the prize winners are only officially recorded in the original Olympic reports. Judges were not required to distribute first, second, and third place awards for every category, and thus certain events lack medalists in these placements. Since participants were allowed multiple submissions, it was also possible for artists to win more than one in a single event, as Alex Diggelmann of Switzerland did in the graphic arts category of the 1948 edition.[10] Diggelmann is tied with Denmark's Josef Petersen, who won second prize three times in literature, for the number of medals captured in the art competitions. Luxembourg's Jean Jacoby is the only individual to win two gold medals, doing so in painting in 1924 and 1928. Of the 146 medalists, 11 were women and only Finnish author Aale Tynni was awarded gold. Germany was the most successful nation, with eight gold, seven silver, and nine bronze medals,[11] although one was won by Coubertin himself, a Frenchman. He submitted his poem Ode to Sport under the pseudonyms Georges Hohrod and Martin Eschbach, as if it were a joint-entry, and won first prize in the 1912 literature category. The original report credits this medal to Germany.[12] Two individuals, Walter W. Winans[13] and Alfréd Hajós, won medals in both athletic and art competitions.[14]

Contents

1 Architecture

1.1 Mixed architecture
1.2 Mixed architecture, architectural designs
1.3 Town planning


2 Literature

2.1 Mixed literature
2.2 Dramatic works
2.3 Epic works
2.4 Lyric and speculative works


3 Music

3.1 Mixed music
3.2 Compositions for orchestra
3.3 Solo and chorus compositions
3.4 Instrumental and chamber
3.5 Vocal


4 Painting

4.1 Mixed painting
4.2 Drawings and water colors
4.3 Engravings and etchings
4.4 Graphic works
4.5 Paintings


5 Sculpturing

5.1 Mixed sculpturing
5.2 Medals
5.3 Medals and plaques
5.4 Reliefs and medallions
5.5 Reliefs
5.6 Statues


6 Statistics

6.1 Multiple medalists
6.2 Medals per year


7 References

7.1 General
7.2 Specific


8 Notes


Architecture[edit]
Mixed architecture[edit]
 Alfréd Hajós was one of two individuals to win medals in sport and art competitions.

Olympic medalists in mixed architecture


Games

Gold

Silver

Bronze


1912 Stockholm

 Eugène-Edouard Monod & Alphonse Laverrière (SUI)Building-plan of a modern Stadium

none awarded

none awarded


1920 Antwerp

none awarded

 Holger Sinding-Larsen (NOR)Project pour une Ecole de Gymnastique

none awarded


1924 Paris

none awarded

 Alfréd Hajós & Dezső Lauber (HUN)Plan d'un Stade

 Julien Médecin (MON)Stade pour Monte-Carlo

Mixed architecture, architectural designs[edit]
 Jan Wils, 1928 gold medalist in architectural designs

Olympic medalists in mixed architecture, architectural designs


Games

Gold

Silver

Bronze


1928 Amsterdam

 Jan Wils (NED)Olympic Stadium in Amsterdam

 Ejnar Mindedal Rasmussen (DEN)Swimming pool at Ollerup

 Jacques Lambert (FRA)Stadium at Versailles


1932 Los Angeles

 Gustave Saacké, Pierre Bailly, & Pierre Montenot (FRA)Design for a ""Cirque pour Toros""

 John Russell Pope (USA)Design for the Payne Whitney Gymnasium, New Haven, Conn.

 Richard Konwiarz (GER)Design for a ""Schlesierkampfbahn"" in the Sport Park of Breslau


1936 Berlin

 Hermann Kutschera (AUT)Skiing Stadium

 Werner March[note 1] (GER)Reich Sport Field

 Hermann Stiegholzer & Herbert Kastinger (AUT)Sporting Center in Vienna


1948 London

 Adolf Hoch (AUT)Skisprungschanze auf dem Kobenzl

 Alfred Rinesch (AUT)Watersports Centre in Carinthia

 Nils Olsson (SWE)Baths and Sporting Hall for Gothenburg

Town planning[edit]
 Charles Downing Lay, 1936 silver medalist in designs for municipal planning

Olympic medalists in town planning


Games

Gold

Silver

Bronze


1928 Amsterdam

 Alfred Hensel (GER)Stadium at Nuremberg

 Jacques Lambert (FRA)Stadium at Versailles

 Max Laeuger (GER)Municipal park at Hamburg


1932 Los Angeles

 John Hughes (GBR)Design for a Sports and Recreation Center with Stadium, for the City of Liverpool

 Jens Klemmensen (DEN)Design for a Stadium and Public Park

 André Verbeke (BEL)Design for a ""Maraton Park""


1936 Berlin[15]

 Werner March & Walter March (GER)Reich Sport Field

 Charles Downing Lay (USA)Marine Park, Brooklyn

 Theo Nussbaum (GER)Municipal Planning and Sporting Centre in Cologne


1948 London

 Yrjö Lindegren (FIN)The Centre of Athletics in Varkaus, Finland.

 Werner Schindler & Edy Knupfer (SUI)Swiss Federal Sports and Gymnastics Training Centre

 Ilmari Niemeläinen (FIN)The Athletic Centre in Kemi, Finland.

Literature[edit]
Mixed literature[edit]
 Oliver St. John Gogarty, 1924 bronze medalist in literature
 Kazimierz Wierzyński, 1928 gold medalist in lyric and speculative works

Olympic medalists in mixed literature


Games

Gold

Silver

Bronze


1912 Stockholm

 Georges Hohrod & Martin Eschbath (GER)Ode to sport

none awarded

none awarded


1920 Antwerp

 Raniero Nicolai (ITA)Canzoni Olimpioniche

 Theodore Andrea Cook (GBR)Olympic Games of Antwerp

 Maurice Bladel (BEL)La Louange des Dieux


1924 Paris

 Géo-Charles (FRA)Jeux Olympiques

 Margaret Stuart (GBR)Sword Songs

 Charles Gonnet (FRA)Vers le Dieu d’Olympie


 Josef Petersen (DEN) Euryale

 Oliver St. John Gogarty (IRL)Ode pour les Jeux de Tailteann


1932 Los Angeles

 Paul Bauer (GER)Am Kangehenzonga

 Josef Petersen (DEN)The Argonauts

none awarded

Dramatic works[edit]

Olympic medalists in dramatic works


Games

Gold

Silver

Bronze


1928 Amsterdam

none awarded

 Lauro De Bosis (ITA)Icaro

none awarded

Epic works[edit]

Olympic medalists in epic works


Games

Gold

Silver

Bronze


1928 Amsterdam

 Ferenc Mező (HUN)L’histoire des Jeux Olympiques

 Ernst Weiss (GER)Boetius von Orlamünde

 Carel Scharten & Margo Scharten-Antink (NED)De Nar uit de Maremmen


1936 Berlin

 Urho Karhumäki (FIN)Avoveteen

 Wilhelm Ehmer (GER)For the Top of the World

 Jan Parandowski (POL)Dysk Olimijski


1948 London

 Giani Stuparich (ITA)La Grotta

 Josef Petersen (DEN)The Olympic Champion

 Éva Földes (HUN)The Well of Youth

Lyric and speculative works[edit]

Olympic medalists in lyric and speculative works


Games

Gold

Silver

Bronze


1928 Amsterdam

 Kazimierz Wierzyński (POL)Laur Olimpijski

 Rudolf G. Binding (GER)Reitvorschrift fur eine Geliebte

 Johannes Weltzer (DEN)Symphonia Heroïca


1936 Berlin[16]

 Felix Dhünen-Sondinger (GER)The Runner

 Bruno Fattori (ITA)Profili Azzuri

 Hans Stoiber (AUT)The Discus


1948 London[17]

 Aale Tynni (FIN)Laurel of Hellas

 Ernst van Heerden (RSA)Six Poems

 Gilbert Prouteau (FRA)Rythme du Stade

Music[edit]
Mixed music[edit]
 Josef Suk, 1932 silver medalist in music

Olympic medalists in mixed music


Games

Gold

Silver

Bronze


1912 Stockholm

 Riccardo Barthelemy (ITA)Triumphal March

none awarded

none awarded


1920 Antwerp

 Georges Monier (BEL)Olympique

 Oreste Riva (ITA)Epinicion

none awarded


1932 Los Angeles

none awarded

 Josef Suk (TCH)Into a New Life

none awarded

Compositions for orchestra[edit]
 Werner Egk, 1936 gold medalist in compositions for orchestra

Olympic medalists in compositions for orchestra


Games

Gold

Silver

Bronze


1928 Amsterdam

none awarded

none awarded

 Rudolph Simonsen (DEN)Symphony No. 2 ""Hellas""


1936 Berlin

 Werner Egk (GER)Olympic Festive Music

 Lino Liviabella (ITA)The Victor

 Jaroslav Křička (TCH)Mountain Suite


1948 London[18]

 Zbigniew Turski (POL)Olympic Symphony

 Kalervo Tuukkanen (FIN)Karhunpyynti

 Erling Brene (DEN)Viguer

Solo and chorus compositions[edit]
 Kurt Thomas, 1936 silver medalist in solo and chorus compositions

Olympic medalists in solo and chorus compositions


Games

Gold

Silver

Bronze


1936 Berlin

 Paul Höffer (GER)Olympic Vow

 Kurt Thomas (GER)Olympic Cantata, 1936

 Harald Genzmer (GER)The Runner

Instrumental and chamber[edit]

Olympic medalists in the instrumental and chamber event


Games

Gold

Silver

Bronze


1948 London

none awarded

 Jean Weinzweig (CAN)Divertimenti for Solo Flute and Strings

 Sergio Lauricella (ITA)Toccata per Pianoforte

Vocal[edit]

Olympic medalists in the vocal event


Games

Gold

Silver

Bronze


1948 London

none awarded

none awarded

 Gabriele Bianchi (ITA)Inno Olimpionico

Painting[edit]
Mixed painting[edit]
 Jack Butler Yeats, 1924 silver medalist in painting

Olympic medalists in mixed painting


Games

Gold

Silver

Bronze


1912 Stockholm

 Carlo Pellegrini (ITA)Winter Sports

none awarded

none awarded


1920 Antwerp

none awarded

 Henriette Brossin de Mère-de Polanska (FRA)L'Elan

 Alfred Ost (BEL)Joueur de Football


1924 Paris

 Jean Jacoby (LUX)Etude de Sport

 Jack Butler Yeats (IRL)Natation

 Johan van Hell (NED)Patineurs

Drawings and water colors[edit]
 Rugby by Jean Jacoby, the winning entry in the 1928 drawings category

Olympic medalists in drawing and water colors


Games

Gold

Silver

Bronze


1928 Amsterdam[19]

 Jean Jacoby (LUX)Rugby

 Alex Virot (FRA)Gestes de Football

 Władysław Skoczylas (POL)Posters


1932 Los Angeles

 Lee Blair (USA)Rodeo

 Percy Crosby (USA)Jackknife

 Gerhard Westermann (NED)Horseman


1936 Berlin

none awarded

 Romano Dazzi (ITA)Four Sketches for Frescoes

 Sujaku Suzuki (JPN)Classical Horse Racing in Japan

Engravings and etchings[edit]

Olympic medalists in engravings and etchings


Games

Gold

Silver

Bronze


1948 London

 Albert Decaris (FRA)Swimming Pool

 John Copley (GBR)Polo Players

 Walter Battiss (RSA)Seaside Sport

Graphic works[edit]
 William Nicholson, 1928 gold medalist in graphic works

Olympic medalists in graphic works


Games

Gold

Silver

Bronze


1928 Amsterdam

 William Nicholson (GBR)Un Almanach de douze Sports

 Carl Moos (SUI)Posters

 Max Feldbauer (GER)Mailcoach


1932 Los Angeles[20]

 Joseph Golinkin (USA)Leg Scissors

 Janina Konarska (POL)Stadium

 Joachim Karsch (GER)Stabwechsel


1936 Berlin[21]

 Alex Diggelmann (SUI)Arosa I Placard

 Alfred Hierl (GER)International Automobile Race on the Avis

 Stanisław Ostoja-Chrostowski (POL)Yachting Club Certificate


1948 London[22]

none awarded

 Alex Diggelmann (SUI)World Championship for Cycling Poster

 Alex Diggelmann (SUI)World Championship for Ice Hockey Poster

Paintings[edit]
 David Wallin, 1932 gold medalist in painting

Olympic medalists in paintings


Games

Gold

Silver

Bronze


1928 Amsterdam

 Isaac Israëls (NED)Cavalier Rouge

 Laura Knight (GBR)Boxeurs

 Walther Klemm (GER)Patinage


1932 Los Angeles

 David Wallin (SWE)At the Seaside of Arild

 Ruth Miller (USA)Struggle

none awarded


1936 Berlin

none awarded

 Rudolf Eisenmenger (AUT)Runner at the Finishing Line

 Ryuji Fujita (JPN)Ice Hockey


1948 London

 Alfred Thomson (GBR)London Amateur Championships

 Giovanni Stradone (ITA)Le Pistard

 Letitia Marion Hamilton (IRL)Meath Hunt Point-to-Point Races

Sculpturing[edit]
Mixed sculpturing[edit]
 Walter W. Winans was one of two individuals to win medals in sport and art competitions.
 Frederick William MacMonnies, 1932 silver medalist in medals and reliefs
 Mahonri Young, 1932 gold medalist in statues

Olympic medalists in mixed sculpturing


Games

Gold

Silver

Bronze


1912 Stockholm

 Walter W. Winans (USA)An American trotter

 Georges Dubois (FRA)Model of the entrance to a modern Stadium

none awarded


1920 Antwerp

 Albéric Collin (BEL)La Force

 Simon Goossens (BEL)Les Patineurs

 Alphons De Cuyper (BEL)Lanceur de Poids et Coureur


1924 Paris

 Konstantinos Dimitriadis (GRE)Discobole Finlandais

 Frantz Heldenstein (LUX)Vers l'olympiade

 Jean René Gauguin (DEN)Le Boxeur


 Claude-Léon Mascaux (FRA)Cadre de Medailles


Medals[edit]

Olympic medalists in medals


Games

Gold

Silver

Bronze


1936 Berlin

none awarded

 Luciano Mercante (ITA)Medals

 Josue Dupon (BEL)Equestrian Medals

Medals and plaques[edit]

Olympic medalists in medals and plaques


Games

Gold

Silver

Bronze


1948 London

none awarded

 Oskar Thiede (AUT)Eight Sports Plaques

 Edwin Grienauer (AUT)Prize Rowing Trophy

Reliefs and medallions[edit]

Olympic medalists in reliefs and medallions


Games

Gold

Silver

Bronze


1928 Amsterdam

 Edwin Grienauer (AUT)Médailles

 Chris van der Hoef (NED)Médaille pour les Jeux Olympiques

 Edwin Scharff (GER)Plaquette


1932 Los Angeles[23]

 Józef Klukowski (POL)Sport Sculpture II

 Frederick William MacMonnies (USA)Lindbergh Medal

 R. Tait McKenzie (CAN)Shield of the Athletes

Reliefs[edit]

Olympic medalists in reliefs


Games

Gold

Silver

Bronze


1936 Berlin

 Emil Sutor (GER)Hurdlers

 Józef Klukowski (POL)Ball

none awarded


1948 London

none awarded

none awarded

 Rosamund Fletcher (GBR)The End of the Covert

Statues[edit]

Olympic medalists in statues


Games

Gold

Silver

Bronze


1928 Amsterdam

 Paul Landowski (FRA)Boxer

 Milo Martin (SUI)Athlète au repos

 Renée Sintenis (GER)Footballeur


1932 Los Angeles

 Mahonri Young (USA)The Knockdown

 Miltiades Manno (HUN)Wrestling

 Jakub Obrovský (TCH)Odysseus


1936 Berlin

 Farpi Vignoli (ITA)Sulky Driver

 Arno Breker (GER)Decathlon Athlete

 Stig Blomberg (SWE)Wrestling Youths


1948 London

 Gustaf Nordahl (SWE)Homage to Ling

 Chintamoni Kar (GBR)The Stag

 Hubert Yencesse (FRA)Nageuse

Statistics[edit]
Multiple medalists[edit]

Multiple medalists in Olympic art competitions


Athlete

Nation

Olympics

Gold

Silver

Bronze

Total


Alex Diggelmann

 Switzerland (SUI)

1936–1948

1
1
1
3


Josef Petersen

 Denmark (DEN)

1924, 1932, 1948

0
3
0
3


Jean Jacoby

 Luxembourg (LUX)

1924–1936

2
0
0
2


Józef Klukowski

 Poland (POL)

1932–1936

1
1
0
2


Werner March

 Germany (GER)

1928–1936

1
1
0
2


Edwin Grienauer

 Austria (AUT)

1928, 1948

1
0
1
2


Jacques Lambert

 France (FRA)

1928

0
1
1
2

Medals per year[edit]

Medals won by country by year


Nation

1912

1920

1924

1928

1932

1936

1948

Total


 Austria (AUT)

–
–
–
1
–
4
4
9


 Belgium (BEL)

–
6
–
–
1
1
–
8


 Canada (CAN)

–
–
–
–
1
–
1
2


 Denmark (DEN)

–
–
2
3
2
–
2
9


 Finland (FIN)

–
–
–
–
–
1
4
5


 France (FRA)

1
1
3
2
3
–
3
13


 Great Britain (GBR)

–
1
1
2
1
–
4
9


 Germany (GER)

1
–
–
8
3
12
–
24


 Greece (GRE)

–
–
1
–
–
–
–
1


 Hungary (HUN)

–
–
1
1
1
–
1
4


 Ireland (IRL)

–
–
2
–
–
–
1
3


 Italy (ITA)

2
2
–
1
–
5
4
14


 Japan (JPN)

–
–
–
–
–
2
–
2


 Luxembourg (LUX)

–
–
2
1
–
–
–
3


 Monaco (MON)

–
–
1
–
–
–
–
1


 Netherlands (NED)

–
–
1
4
1
–
–
6


 Norway (NOR)

–
1
–
–
–
–
–
1


 Poland (POL)

–
–
–
2
2
3
1
8


 South Africa (RSA)

–
–
–
–
–
–
2
2


 Switzerland (SUI)

1
–
–
2
–
1
3
7


 Sweden (SWE)

–
–
–
–
1
1
2
4


 Czechoslovakia (TCH)

–
–
–
–
2
1
–
3


 United States (USA)

1
–
–
–
7
1
–
9


References[edit]
General[edit]
""The Official Report of the Olympic Games of Stockholm 1912"" (PDF). Stockholm: Swedish Olympic Committee. 1913: 808–809. Archived from the original (PDF) on 2014-02-07. Retrieved 2017-05-14. {{cite journal}}: Cite journal requires |journal= (help)
""Olympic Games: Antwerp 1920: Official Report"" (PDF) (in French). Brussels: Belgian Olympic Committee. 1957: 80. Archived from the original (PDF) on 2018-10-07. Retrieved 2017-05-14. {{cite journal}}: Cite journal requires |journal= (help)
""Les Jeux de la VIIIE Olympiade"" (PDF) (in French). Paris: Comite Olympique Francais. 1924: 605–612. Archived from the original (PDF) on 2016-08-25. Retrieved 2017-05-14. {{cite journal}}: Cite journal requires |journal= (help)
""Official Report of the Olympic Games of 1928 Celebrated at Amsterdam"" (PDF). Amsterdam: Netherlands Olympic Committee. 1928: 892–901. Archived from the original (PDF) on 2013-09-23. Retrieved 2017-05-14. {{cite journal}}: Cite journal requires |journal= (help)
""The Games of the Xth Olympiad Los Angeles 1932"" (PDF). Los Angeles: Xth Olympiade Committee of the Games of Los Angeles. 1933: 756–765. Archived from the original (PDF) on 2016-08-19. Retrieved 2017-05-14. {{cite journal}}: Cite journal requires |journal= (help)
""The XIth Olympic Games Berlin, 1936: Official Report Volume II"" (PDF). Berlin: Organisationskomitee Fur Die XI. Olympiade Berlin 1936 E.V. 1937: 1119–1123. Archived from the original (PDF) on 2017-01-30. Retrieved 2017-05-14. {{cite journal}}: Cite journal requires |journal= (help)
""The Official Report of the Organising Committee for the XIV Olympiad London 1948"" (PDF). London: The Organising Committee for the XIV Olympiad. 1951: 535–537. Archived from the original (PDF) on 2013-09-27. Retrieved 2017-05-14. {{cite journal}}: Cite journal requires |journal= (help)
Specific[edit]


^ Stanton, Richard (2000). The Forgotten Olympic Art Competitions (1st ed.). Victoria: Trafford Publishing. p. 3. ISBN 1552126064.

^ Stanton, p. 18.

^ Stanton, p. 69.

^ Stanton, p. 95.

^ Stanton, p. 146.

^ Stanton, p. 158.

^ Stanton, p. 210.

^ Stanton, p. 211.

^ Stanton, p. 213.

^ ""The Official Report of the Organising Committee for the XIV Olympiad London 1948"" (PDF). London: The Organising Committee for the XIV Olympiad. 1951: 536. Archived from the original (PDF) on 2016-03-03. Retrieved 2012-03-06. {{cite journal}}: Cite journal requires |journal= (help)

^ Gjerde, Arild; Jeroen Heijmans; Bill Mallon; Hilary Evans (2011). ""Art Competitions"". Olympics. Sports Reference.com. Archived from the original on 2012-05-14. Retrieved 2012-03-07.

^ Gjerde, Arild; Jeroen Heijmans; Bill Mallon; Hilary Evans (2011). ""Pierre, Baron de Coubertin Biography and Olympic Results"". Olympics. Sports Reference.com. Archived from the original on 2012-05-04. Retrieved 2012-03-07.

^ Gjerde, Arild; Jeroen Heijmans; Bill Mallon; Hilary Evans (2011). ""Walter Winans Biography and Olympic Results"". Olympics. Sports Reference.com. Archived from the original on 2012-10-18. Retrieved 2012-03-08.

^ Gjerde, Arild; Jeroen Heijmans; Bill Mallon; Hilary Evans (2011). ""Alfréd Hajós Biography and Olympic Results"". Olympics. Sports Reference.com. Archived from the original on 2012-11-01. Retrieved 2012-03-08.

^ In 1936, this event was listed as ""Designs for Municipal Planning"". ""The XIth Olympic Games Berlin, 1936: Official Report Volume II"" (PDF). Berlin: Organisationskomitee Fur Die XI. Olympiade Berlin 1936 E.V. 1937: 1119. Archived from the original (PDF) on 2014-03-02. Retrieved 2017-05-14. {{cite journal}}: Cite journal requires |journal= (help)

^ In 1936, this event was listed as ""Lyric Works"". ""The XIth Olympic Games Berlin, 1936: Official Report Volume II"" (PDF). Berlin: Organisationskomitee Fur Die XI. Olympiade Berlin 1936 E.V. 1937: 1122. Archived from the original (PDF) on 2014-03-02. Retrieved 2017-05-14. {{cite journal}}: Cite journal requires |journal= (help)

^ In 1948, this event was listed as ""Lyrics"". ""The Official Report of the Organising Committee for the XIV Olympiad London 1948"" (PDF). London: The Organising Committee for the XIV Olympiad. 1951: 536. Archived from the original (PDF) on 2016-09-12. Retrieved 2017-05-14. {{cite journal}}: Cite journal requires |journal= (help)

^ In 1948, this event was listed as ""Choral and Orchestra"". ""The Official Report of the Organising Committee for the XIV Olympiad London 1948"" (PDF). London: The Organising Committee for the XIV Olympiad. 1951: 537. Archived from the original (PDF) on 2016-09-12. Retrieved 2017-05-14. {{cite journal}}: Cite journal requires |journal= (help)

^ In 1928, this event was listed as ""Drawings"". ""Official Report of the Olympic Games of 1928 Celebrated at Amsterdam"" (PDF). Amsterdam: Netherlands Olympic Committee. 1928: 898. Retrieved 2017-05-28. {{cite journal}}: Cite journal requires |journal= (help)

^ In 1932, this event was listed as ""Prints"".""The Games of the Xth Olympiad Los Angeles 1932"" (PDF). Los Angeles: Xth Olympiade Committee of the Games of Los Angeles. 1933: 758. Archived from the original (PDF) on 2014-03-02. Retrieved 2017-05-14. {{cite journal}}: Cite journal requires |journal= (help)

^ In 1936, this event was listed as ""Commercial Graphic Art"". ""The XIth Olympic Games Berlin, 1936: Official Report Volume II"" (PDF). Berlin: Organisationskomitee Fur Die XI. Olympiade Berlin 1936 E.V. 1937: 1122. Archived from the original (PDF) on 2014-03-02. Retrieved 2017-05-14. {{cite journal}}: Cite journal requires |journal= (help)

^ In 1948, this event was listed as ""Applied Art and Crafts"". ""The Official Report of the Organising Committee for the XIV Olympiad London 1948"" (PDF). London: The Organising Committee for the XIV Olympiad. 1951: 535–537. Archived from the original (PDF) on 2016-09-12. Retrieved 2017-05-14. {{cite journal}}: Cite journal requires |journal= (help)

^ In 1932, this event was listed as ""Medals and Reliefs"". ""The Games of the Xth Olympiad Los Angeles 1932"" (PDF). Los Angeles: Xth Olympiade Committee of the Games of Los Angeles. 1933: 758. Archived from the original (PDF) on 2014-03-02. Retrieved 2017-05-14. {{cite journal}}: Cite journal requires |journal= (help)


Notes[edit]


^ Data provided by Bill Mallon to Sports Reference.com lists Walter March as a co-medalist.


vteArt competitions at the Summer Olympics
1912
1920
1924
1928
1932
1936
1948

List of medalists

vteLists of Olympic medalistsSummer sports
Archery
Artistic swimming
Athletics
men
women
mixed
Badminton
Baseball
Basketball
Boxing
Canoeing
men
women
Cycling
men
women
Diving
Equestrian
Fencing
men
women
Field hockey
Football
Golf
Gymnastics
men
women
Handball
men
women
Judo
Karate
Modern pentathlon
Rowing
men
women
Rugby
Rugby sevens
Sailing
by discipline
by class
Shooting
Skateboarding
Sport climbing
Softball
Surfing
Swimming
men
women
mixed
Table tennis
Taekwondo
Tennis
Triathlon
Volleyball
Water polo
men
women
Weightlifting
Wrestling
freestyle
Greco-Roman
Winter sports
Alpine skiing
Biathlon
Bobsleigh
Cross-country skiing
Curling
Figure skating
Freestyle skiing
Ice hockey
Luge
Nordic combined
Short track speed skating
Skeleton
Ski jumping
Snowboarding
Speed skating
Discontinued sports
Basque pelota
Cricket
Croquet
Jeu de paume
Lacrosse
Polo
Rackets
Roque
Tug of war
Water motorsports
Unofficial sports
Art competitions

Olympic sports
Olympic medal
All-time Olympic Games medal table
 Olympic Games portal







Retrieved from ""https://en.wikipedia.org/w/index.php?title=List_of_Olympic_medalists_in_art_competitions&oldid=1123623812""
Categories: Lists of Summer Olympic medalists by sportArts awardsArchitecture awardsArt competitions at the Olympic GamesOlympic medalists in art competitionsHidden categories: CS1 errors: missing periodicalArticles with short descriptionShort description is different from WikidataArticles with hCardsCS1 French-language sources (fr)Featured lists



Navigation menu



Personal tools


Not logged inTalkContributionsCreate accountLog in





Namespaces


ArticleTalk





English









Views


ReadEditView history





More

























Navigation


Main pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate




Contribute


HelpLearn to editCommunity portalRecent changesUpload file




Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageWikidata item




Print/export


Download as PDFPrintable version




Languages


Français日本語Suomi
Edit links






 This page was last edited on 24 November 2022, at 19:18 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License 3.0;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Mobile view
Developers
Statistics
Cookie statement










"
https://news.ycombinator.com/rss,Elliptic Curves: The Great Mystery,https://www.cantorsparadise.com/elliptic-curves-the-great-mystery-61599a93c61d,Comments,"Published inCantor’s ParadiseKasper MüllerFollowJan 14·11 min read·Member-onlySaveElliptic Curves: The Great MysteryA surprisingly beautiful blend of algebra, geometry, and number theoryImage from Wikimedia CommonsThese curves defined by very simple equations are shrouded in mystery and elegance. In fact, the equations describing them are so simple that even high-school students would be able to understand them.However, a ton of simple questions about them remain unsolved despite tenacious efforts by some of the greatest mathematicians in the world. But that’s not all. As you will soon see, this theory connects various important fields of mathematics because it turns out that elliptic curves are more than just plane curves!Let’s grab a cup of coffee and start from the beginning.Introduction and MotivationIn mathematics, we often solve problems by stating them in a different setting than they originally occurred in. An example of this is that some geometric problems can be turned into algebraic problems and vice versa.A classical problem going back thousands of years is whether a positive integer n is the area of some right triangle with rational side lengths, that is such that the lengths of all three sides are expressible as fractions of whole numbers. In this case, n is called a congruent number). For instance, 6 is a congruent number because it is the area of the right triangle with side lengths 3, 4 and 5.In 1640, Fermat famously proved that 1 is not a congruent number. He did so using his famous method of proof by infinite descent.As the amazing mathematician, Keith Conrad notes about the result:This leads to a weird proof that √2 is irrational. If √2 were rational then √2, √2 and 2 would be the sides of a rational right triangle with area 1. This is a contradiction of 1 not being a congruent number!Since Fermat’s proof, the hunt for proving or disproving that numbers are congruent has been ongoing.Amazingly, one can show by elementary methods that for each triple of rational numbers (a, b, c) such that a² + b² = c² and 1/2 ab = n, we can find two rational numbers x and y such that y² = x³ - n²x and y ≠ 0 and conversely for each rational pair (x, y) such that y² = x³ - n²x and y ≠ 0, we can find three rational numbers a, b, c such that a² + b² = c² and 1/2 ab = n.That is, right triangles with area n correspond exactly to rational solutions to the equation y² = x³ - n²x with y ≠ 0 and vice versa. A mathematician would say that there is a bijection between the two sets.Therefore, a rational number n > 0 is congruent if and only if the equation y² = x³ - n²x has a rational solution (x, y) with y ≠ 0. For example, since 1 is not congruent, the only rational solutions to y² = x² - x have y = 0.For the interested reader, the exact correspondence is the following.If we try this correspondence on the triangle with side lengths 3, 4, and 5, and with area 6, then the corresponding solution is (x, y) = (12, 36).To me, this is absolutely amazing. One starts with a problem in number theory and geometry and through algebra, transforms it into a problem about rational points on plane curves!The equation y² = x³ - n²x is an example of an elliptic curve.Elliptic CurvesIn general, if f(x) denotes a third-degree polynomial with a non-zero discriminant (i.e. all the roots are distinct), then y² = f(x) describes an elliptic curve except for one important addition to this object, namely what is called a “point at infinity”.Basically, a point at infinity is a point where parallel lines can meet. It is out of the scope of this article to go into projective geometry to proper define it but this is a wonderful and exciting subject that I strongly encourage you to look up.Now, by a minor algebraic miracle, it turns out that we can make a suitable (rational) change of coordinates, and get a new curve on the form y² = x³ + ax + b such that rational points on the two curves are in one-to-one correspondence. The second transformed curve however is typically easier to work with.Because of this, we sometimes assume that an elliptic curve is on this form and so from now on we will assume that too, that is, when we say “elliptic curve”, we mean a curve on the form y² = x³ + ax + b together with a point 𝒪 at infinity.Throughout this article, unless otherwise stated, we’ll assume that the coefficients a and b are rational numbers.Elliptic curves take on two typical shapes which are graphed below.Image from Wikimedia CommonsHowever, if we consider x and y as complex variables, the curves will look entirely different. In fact, they will then take the form of a complex torus or doughnut!So why do we study elliptic curves and what can we do with them?First of all, many number theoretic problems can be translated into problems about Diophantine equations, secondly, it turns out that elliptic curves are related to discrete geometric objects called lattices and deeply connected to some very important objects called modular forms which are certain extremely symmetric complex functions with a lot of number theoretic information in them.Actually, the connection between elliptic curves and modular forms turned out to be the key to proving Fermat’s Last Theorem which Andrew Wiles achieved in the 90s through several years of intensive work on this connection.The story about this quest and the proof of the Theorem is in my opinion one of the most beautiful pursuits in all of science - unfortunately, as pointed out by my friend Kenneth Nielsen, the margin in this Medium post is too narrow to contain it!So I guess I’ll have to write another article.Elliptic curves are also used in cryptography to encrypt messages and online transactions.The most important feature of them, however, is the mind-blowing fact that they are more than just curves and more than just geometry. In fact, they have an algebraic structure on them called an Abelian group structure with respect to a cool geometric operation - a kind of geometric addition rule for how to add points on the curve together.If you don’t know what an Abelian group is, you can think about it as a set of objects with an operation defined on them such that they have the same kind of structure as the integers with respect to addition (except they can be finite).More specifically, for a group with the operation *, it needs to be stable with respect to the operation (i.e. if a is in the group and b is in the group, then a * b is in the group), there is an identity element e (0 for the integers) such that a * e = a for all elements a in the group, and for each element a, there is an inverse element c, such that when you operate them together you get back the identity element (a * c = e). Furthermore, the group operation has to be associative i.e. a * (b * c) = (a * b) * c. That is, it doesn’t matter which elements you add together first. If the commutative law holds ( i.e. a * b = b * a) then the group is called an Abelian group.Examples of Abelian groups are:The integers ℤ with respect to the operation +.The action of rotating a square clockwise by 90 degreesVector spaces with vectors as elements and vector addition as the operationThe fancy terminology for a curve with an Abelian group structure is Abelian variety.What is so amazing about elliptic curves is that we can define an operation (let’s denote it ⊕) between rational points on them (that is, both the x and y coordinates are rational numbers) such that the set of those points on the curve becomes an Abelian group with respect to the operation ⊕ and with identity element 𝒪 (the point at infinity).Let’s define the operation.If you take two rational points on the curve (for example P and Q) and consider a line through them, then the line intersects the curve at another rational point (possibly the point at infinity). Let’s call this point -R.Now, because the curve is symmetric about the x-axis, we get another rational point R when we reflect -R about it. There is a drawing of this below.Addition rule for elliptic curves - image from Wikimedia CommonsThis reflected point (R in the above image) is the addition of the two aforementioned points (P and Q above). We can write P ⊕ Q = R.One can prove (and this is actually not easy) that this operation is associative, which is really surprising, at least to me. Also, the point at infinity acts as a (unique) identity for this operation and each point has an inverse point (which is just the point you get by reflecting about the x-axis). It is also Abelian (i.e. P ⊕ Q = Q ⊕ P).The MysteryIt turns out that two different elliptic curves can have vastly different groups associated with them. An important invariant that in some sense is the most defining feature is what is known as the rank of the curve (or group).A curve can have a finite or an infinite number of rational points on them. This can be hard to handle, so what we are interested in, is how many points we need in order to generate all the others by the aforementioned addition rule. These generators are called basis points.The rank is a dimensionality measure like the dimension of a vector space and indicates how many independent basis points (on the curve) have infinite order (i.e. we can keep adding it without getting our starting point back). If the curve only contains a finite amount of rational points on it, then the rank is zero. There is still a group but it is finite.Calculating the rank of an elliptic curve is notoriously difficult but we have a nice result due to Mordell which tells us that the rank is always finite. That is, we only need a finite amount of basis points in order to generate all the rational points on the curve.One of the most important and interesting problems in number theory is called the Birch and Swinnerton-Dyer Conjecture and it is all about the rank of elliptic curves. In fact, it is so difficult and important that it is one of the Millenium Problems.You actually get a million dollars if you solve it!Finding rational points on elliptic curves with rational coefficients is hard. One way to approach this is by reducing the curve modulo p where p is a prime number. This means that instead of considering the rational solution set of the equation y² = x³ + ax + b, we consider the rational solution set of the congruence y² ≡ x³ + ax + b (mod p) where for this to make sense we might have to clear denominators by multiplying by an integer on both sides.So we are considering two numbers with the same remainder when divided by p to be equal in this new space. The great thing about this is that now there are only a finite number of things to check. Let’s denote the number of rational solutions to such a reduced curve modulo p, by Np.In the early 1960s, Peter Swinnerton-Dyer used the EDSAC-2 computer (not exactly a Macbook!) at the University of Cambridge Computer Laboratory to calculate the number of points modulo p on elliptic curves with known rank. He worked together with the mathematician Bryan John Birch in understanding elliptic curves and after the computer had crunched a bunch of products of the formfor growing x, they got the following output taken from data associated with the curve E: y² = x³ − 5x (as an example). I should note that the x-axis is really log log x and the y-axis is log y.The curve in question is y² = x³ − 5x. This is a curve of rank 1 and one of the curves originally looked at by Birch and Swinnerton-Dyer.Now, I am a mathematician and not a statistician but even I can see a clear trend and it does seem that the regression line has slope 1 on this plot.The curve E has rank 1 and when they tried different curves of varying ranks, they found the same pattern every time. The slope of the fitted regression line, it seemed, was always equal to the rank of the curve in question.More precisely, they stated the bold conjecture thatHere C is some constant.This computer crunching adventure combined with a great deal of far-sightedness led them to make a general conjecture about the behavior of a curve’s Hasse–Weil L-function L(E, s) at s = 1.This L-function is defined as follows. Letand let the discriminant of the curve be denoted Δ. Then we can define the L-function associated with E as the following Euler productWe view this as a function of the complex variable s.Their conjecture now has the form:Conjecture (Birch and Swinnerton-Dyer): Let E be any elliptic curve over ℚ. The rank of the abelian group E(ℚ) of rational points of the curve E is equal to the order of the zero of L(E, s) at s = 1.The reason why it was quite far-sighted is that, at the time, they didn’t even know if a so-called analytic continuation existed for all such L-functions. The problem was that L(E, s) as defined above only converges when Re(s) > 3/2.That they can all be evaluated at s = 1 by analytic continuation was first proved in 2001 again by using the close connection to modular forms that Andrew Wiles proved.Sometimes the conjecture is stated using the Taylor expansion of the L-function, but it is saying the same thing in a different way. The field of rational numbers can be replaced by a more general field but I didn’t want to go into more abstraction than necessary.The subject of Elliptic Curves is a beautiful dance between number theory, abstract algebra and geometry. There is a lot more to say about them than what I have sketched here, but I hope that you got a feeling or a glimpse of something that one could call astounding.We reached the end of this article…If you have any questions, comments or concerns, then please reach out.If you like to read articles like this one on Medium, you can get a membership for full access: simply click here.Thanks for reading.MathematicsMathScienceTechnologyHistory----8More from Cantor’s ParadiseFollowMedium’s #1 Math PublicationRead more from Cantor’s ParadiseRecommended from MediumPallavi SolaiappanUnderstanding Simpson’s ParadoxSunny LabhWhat is the Batman Equation?Sunny LabhThe Mathematics Book That Awakened Ramanujan’s GeniusMipsmonstaSet Matrix Zeros (LintCode/LeetCode)logicianTHE UNKNOWN DIVIDERPacific Institute for the Mathematical SciencesinThe Pacific Institute for the Mathematical SciencesFields Medalist Martin Hairer Sits Down With PIMSCristian Miguel LunaNewton’s MethodJesus NajerainCantor’s ParadiseThe Four-Color TheoremAboutHelpTermsPrivacyGet the Medium app"
https://news.ycombinator.com/rss,PHP in 2023,https://stitcher.io/blog/php-in-2023,Comments,"


PHP in 2023 - stitcher.io


































































    By continuing your visit to this site, you accept the use of cookies.
    Read more.



            « back — written by
            Brent on January 17, 2023
        

PHP in 2023

From its humble beginnings as a personal project in the mid-90s, PHP has grown to become one of the most popular languages for web development, powering everything from small blogs to large enterprise applications.
It's a language that has seen an astonishing transformation over the course of almost three decades. Even within the last 10 years, PHP has transformed in ways we couldn't imagine.
Every year, I write a post about the current state of PHP, where I look back and forward. Let's begin!






# The PHP Foundation
I usually start these posts with a summary of the latest PHP version, but this time I want focus on the PHP Foundation first. It's been a little over a year since the foundation was created: a collective of 10 volunteers and 6 developers being paid to work on PHP, the language.
Last year, I wrote this:

I'm a little worried now that Nikita has stepped down. He's definitely not the only person capable of working on PHP's core, but he did a tremendous amount of work these past years with PHP 8.0 and 8.1. I hope the PHP Foundation will be up to speed soon and that there are enough core developers who have time to work on PHP next year. PHP 8.2 is already in development, although there aren't many RFCs drafted yet.
I don't think 2022 will be the most mind blowing year for PHP, but rather a year of adding stability. Nothing wrong with that.

I think it's fair to say that the Foundation has delivered. They recently published their 2022 report, and it shows some pretty impressive numbers:

In total, $580,000 was raised in 2022
The Foundation pays 6 developers to work on PHP
Foundation members made almost half of all commits in php-src
They created 8 new RFCs, only one of those RFCs didn't make it

I think the Foundation is one of the best things to happen to PHP in a long time, and I hope they'll be able to improve the language even more in 2023. If you're working at a company that uses PHP, I'd highly recommend you consider donating.
# PHP 8.2
Moving on to PHP 8.2. It's generally regarded as a smaller release, but nevertheless it has a bunch of nice features. Just to name a couple:
Readonly classes:
readonly class PostData
{
    public function __construct(
        public string $title,
        public string $author,
        public string $body,
        public DateTimeImmutable $createdAt,
        public PostState $state,
    ) {}
}
A brand new randomizer:
$rng = $is_production
    ? new Random\Engine\Secure()
    : new Random\Engine\Mt19937(1234);
 
$randomizer = new Random\Randomizer($rng);

$randomizer->shuffleString('foobar');
Standalone null, true and false:
function alwaysFalse(): false
{
    return false;
}
Disjunctive normal form types:
function generateSlug((HasTitle&HasId)|null $post) 
{ /* … */ }
Redacted parameters:
function connect(
    string $user,
    #[\SensitiveParameter] string $password
) {
    // …
}
And more.
It's kind of crazy to realise how much PHP has evolved over the years. I made a little video comparison that clearly shows the difference:

# The ecosystem
Just like every year, I should mention Packagist, PHP's package manager: it now lists 361,000 packages; 60,000 more than last year:

One impressive number is the total amount of installations. Last year I mentioned this:

Oh, by the way, just recently Packagist passed the milestone of having handled over more than 50 billion installs. Congrats Packagist!

I just checked, and we're now at 74,492,061,634 installs. That's an increase of 24 billion installs in one year, 2 billion installs per month. All of that to say: the PHP ecosystem is growing a lot.

Twice a year, I publish my version stats post. In these posts, I analyze PHP version usage across the community based on Packagist's data. I wanted to share a graph from that post again: the timeline between 2013 and now, showing the per-version usage history.

While it's great to see PHP 8.* usage rise steeply, there's also a big chunk of people still stuck on older, slow and insecure PHP versions. My hope for 2023 is to see those old version numbers decline even more rapidly. I wrote it like this in my version stats post:

This data beautifully visualizes the division within the PHP community: one part is keeping up with modern PHP, while another one stays helplessly behind.

Speaking of upgrades, I want to mention one tool in particular: Rector. Rector is a free automation tool that helps upgrade your PHP codebase. All it takes is a tiny amount of configuration, and it'll do a huge amount of work for you.
I recently used it to update my community-driven content aggregator, Aggregate to PHP 8.2, and it was really fun and easy to use.
When, after publishing my version stats post, several people told me they hadn't updated yet and were stuck on PHP 7.*, I asked them why. They told me it was simply too much manual work. Interestingly enough, no one had even tried to use tools like Rector to help them…
I firmly believe that a ""programming language"" is so much more than a compiler: it's the tools and ecosystem that play an equal part in defining that ""programming language"", and I really think lots of people, projects and businesses would benefit if they looked into using automation tools like Rector.

Since I'm talking about the ecosystem, I can't go without mentioning PHP's two largest frameworks: Laravel and Symfony.
Over the past years, Laravel has grown tremendously. They now employ 8 full time developers to work on the framework and its ecosystem. On top of that, JetBrains' dev survey reports that 67% of PHP developers work with Laravel.
While Symfony as a framework might be less popular compared to Laravel these days, it's still one of the most mature and stable frameworks within the PHP community. It's more often used for enterprise app development, but its standalone components are popular throughout the whole PHP ecosystem — Laravel also has a couple of dependencies on Symfony components. No surprise that more than a handful of Symfony packages make it into Packagist's top package list.
I should also mention WordPress. I'll be honest, I have a love/hate relationship with it. As a user, WordPress is great. It's so easy to install and use, and I think it has earned every bit of its popularity over the years.
As a developer though, WordPress makes me sad. The inability to stay up to date with modern and safe PHP versions casts a shadow on the whole PHP community.
Right now, WordPress only has beta support for PHP 8.0. Now, to be clear: PHP 8.0 was released in 2020, and is now end of life, three years later — and WordPress doesn't yet support it…
Of course, there are reasons for not properly supporting newer PHP versions. Up to you to decide whether they are good or not. My personal opinion is that the decision to hold on to backwards compatibility as much as WordPress does is mostly business driven: a big part of WordPress is the commercial part, and a big part of their customer base is running old PHP versions. It's a vicious circle where both parties are holding each other back and, by extent, hold back the PHP community as a whole.
On the other hand, we should recognise the fact that not many software projects are able to stay as popular and relevant as WordPress after almost 20 years, so maybe their strategy about backwards compatibility is the right one?
# Superset
Finally, I can't help but mention my long-term dream for PHP. I write about it, I speak about it, and I hope that one day it'll become true: a superset of PHP, with proper IDE and static analyser support.
There are a bunch of reasons why I want it to happen, you can read and hear about them if you want to; but I really hope it'll become a reality. I doubt we'll get to see a widely accepted and supported superset in 2023, but some baby steps are already being made. I'm definitely keeping a close eye on PXP, which might take things in the right direction.

With all of that being said, I hope that you'll enjoy 2023! And just in case you're new here: I'm Brent, developer advocate at JetBrains, I write and vlog about PHP; and I'd really appreciate it if you checked out the YouTube channel I've been working on lately. Take a look, and maybe consider subscribing? Thanks!
If you're not into videos but still want to follow me, you can join 15k newsletter subscribers instead, I hope to see you there!


        👍
    

        👍
    

        👍 0



Footnotes

The PHP Foundation


PHP Annotated, my YouTube channel


PHP 8.2 in 8 code blocks


PHP version stats: January, 2023


PHP in 2022


What's new in PHP 8.2






                    Next up: 
                                            PHP version stats: January, 2023
                                    


    Follow me:
    Twitter —
    RSS —
    Newsletter —
    Patreon —
    GitHub


    © 2023 stitcher.io — Cookies & Privacy









"
https://news.ycombinator.com/rss,A Solid State of Progress – Fairchild Camera and Instrument Corporation (1979),https://archive.org/details/bitsavers_fairchildAs19731979_143551798,Comments,"








    fairchild :: A Solid State Of Progress 1973 1979  

    Item Preview
  























remove-circle 
Share or Embed This Item







Share to Twitter



Share to Facebook



Share to Reddit



Share to Tumblr



Share to Pinterest



Share via email








EMBED







EMBED (for wordpress.com hosted blogs and archive.org item <description> tags)
[archiveorg bitsavers_fairchildAs19731979_143551798 width=560 height=384 frameborder=0 webkitallowfullscreen=true mozallowfullscreen=true]




                Want more?
                Advanced embedding details, examples, and help!
              


















Favorite 




Share 



Flag

Flag this item for



                  Graphic Violence                



                  Explicit Sexual Content                



                  Hate Speech                



                  Misinformation/Disinformation                



                  Marketing/Phishing/Advertising                



                  Misleading/Inaccurate/Missing Metadata                


 
 
 


texts

fairchild :: A Solid State Of Progress 1973 1979














Publication date


1979




Topics
isoplanar, mos, logic, device, ttl, integrated, planar, circuit, semiconductor, monolithic, integrated circuit, linear integrated

 Collection
bitsavers_unsorted; bitsavers; additional_collections



 Language
English

 

From the bitsavers.org collection, a scanned-in computer-related document.fairchild :: A Solid State Of Progress 1973 1979



Addeddate
2013-01-10 22:56:18


Bitsavers-filename
/pdf/fairchild/A_Solid_State_Of_Progress_1973_1979.pdf


Identifier
bitsavers_fairchildAs19731979_143551798


Identifier-ark
ark:/13960/t4mk7n90w


Ocr
ABBYY FineReader 8.0


Ppi
400





plus-circle            Add Review

comment
        Reviews
      

          There are no reviews yet. Be the first one to
          write a review.
        






470

        Views      

1
Favorite




        DOWNLOAD OPTIONS
      




download                1 file              



              ABBYY GZ              download 





download                1 file              



              DAISY              download 
For print-disabled users





download                1 file              



              EPUB              download 





download                1 file              



              FULL TEXT              download 





download                1 file              



              ITEM TILE              download 





download                1 file              



              KINDLE              download 





download                1 file              



              PDF              download 





download                1 file              



              PDF WITH TEXT              download 





download                1 file              



              SINGLE PAGE PROCESSED JP2 ZIP              download 





download                1 file              



              TORRENT              download 




download 15 Files
            
download                7 Original

SHOW ALL





IN COLLECTIONS

Bitsavers: General and Unsorted Items





The BITSAVERS.ORG Documents Library





Additional Collections








        Uploaded by
                  
            Jason Scott          
        
                  on January 10, 2013









SIMILAR ITEMS (based on metadata)







Terms of Service (last updated 12/31/2014)

"
https://news.ycombinator.com/rss,Project Mage is an effort to build a power-user environment in Common Lisp,https://project-mage.org,Comments,"




Project Mage













Project Mage
Home
About
Campaign
Code
Contact
RSS









nil


Project Mage: The Elevator Pitch →



Hi, and welcome! Project Mage is an effort to build a power-user environment and a set of applications in Common Lisp. For a comprehensive discussion, see The Power of Structure. But first it's probably best to get a brief outlook: Project Mage: The Elevator Pitch. Otherwise, the essays listed below may be read in any order.








Project Mage: The Elevator Pitch (17 January 2023) The Power of Structure (12 January 2023) Emacs is Not Enough (12 January 2023) On Flexibility and Software Temples (12 January 2023) Overcoming the Print-Statement (12 January 2023) Data-Supplied Web as a Model of Free Web (12 January 2023) Isn't It Obvious That C Programmers Wrote Git? (12 January 2023) Epilogue (12 January 2023) [Appendix] Why Common Lisp for This Project (12 January 2023) [Appendix] All Else is Not Enough (12 January 2023)







...proudly created, delivered and presented to you by: some-mthfka. Mthfka!



"
https://news.ycombinator.com/rss,"Twitter kicking off a developer API campaign on January 16, 2023",https://carhenge.club/@mattdsteele/109700383808551139,Comments,"























Matt Steele: ""Live your life with unbridled audacity, like Twit…"" - carhenge.club


































To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.



















"
https://news.ycombinator.com/rss,Apple Unveils MacBook Pro Featuring M2 Pro and M2 Max,https://www.apple.com/newsroom/2023/01/apple-unveils-macbook-pro-featuring-m2-pro-and-m2-max/,Comments,"


opens in new window




					Apple Newsroom needs your permission to enable desktop notifications when new articles are published
				










PRESS RELEASE
January 17, 2023






                
                
                    Apple unveils MacBook Pro featuring M2 Pro and M2 Max, with more game-changing performance and the longest battery life ever in a Mac
                
            




            
            
                New MacBook Pro features up to 6x faster performance than fastest Intel-based MacBook Pro and support for up to 96GB of unified memory for demanding pro workflows
            
        























































                            Today Apple introduced the new MacBook Pro with M2 Pro and M2 Max.
                        






CUPERTINO, CALIFORNIA Apple today announced the new 14- and 16-inch MacBook Pro featuring M2 Pro and M2 Max, Apple’s next-generation pro silicon that brings even more power-efficient performance and battery life to pro users. With M2 Pro and M2 Max — the world’s most powerful and efficient chip for a pro laptop — MacBook Pro tackles demanding tasks, like effects rendering, which is up to 6x faster than the fastest Intel-based MacBook Pro, and color grading, which is up to 2x faster.1 Building on the unprecedented power efficiency of Apple silicon, battery life on MacBook Pro is now up to 22 hours — the longest battery life ever in a Mac.2 For enhanced connectivity, the new MacBook Pro supports Wi-Fi 6E,3 which is up to twice as fast as the previous generation, as well as advanced HDMI, which supports 8K displays for the first time. With up to 96GB of unified memory in the M2 Max model, creators can work on scenes so large that PC laptops can’t even run them.4 Rounding out the unrivaled features of MacBook Pro are its renowned Liquid Retina XDR display, extensive array of connectivity, 1080p FaceTime HD camera, six-speaker sound system, and studio-quality mics. When combined with macOS Ventura, the MacBook Pro user experience is unrivaled. Customers can order the new 14- and 16-inch MacBook Pro today, with availability beginning Tuesday, January 24.

“MacBook Pro with Apple silicon has been a game changer, empowering pros to push the limits of their workflows while on the go and do things they never thought possible on a laptop,” said Greg Joswiak, Apple’s senior vice president of Worldwide Marketing. “Today the MacBook Pro gets even better. With faster performance, enhanced connectivity, and the longest battery life ever in a Mac, along with the best display in a laptop, there’s simply nothing else like it.”

















                Available in 16- and 14-inch models, MacBook Pro delivers more performance, advanced connectivity, and the longest battery life ever in a Mac.
              






Unrivaled Power-Efficient Performance with M2 Pro and M2 Max

With M2 Pro and M2 Max, MacBook Pro is capable of transforming pro workflows across a wide range of disciplines, from art to science to app development. Users looking to upgrade from Intel-based Mac models will experience even more dramatic improvements in performance, battery life, connectivity, and overall productivity. MacBook Pro also maintains performance whether users are plugged in or on battery.

















                MacBook Pro with M2 Pro and M2 Max transforms pro workflows, empowering users to push their creativity even further.
              






MacBook Pro with M2 Pro features a 10- or 12-core CPU with up to eight high-performance and four high-efficiency cores for up to 20 percent greater performance over M1 Pro. With 200GB/s of unified memory bandwidth — double the amount in M2 — and up to 32GB of unified memory, users can tackle large projects and run multiple pro apps with blazing speed. A next-generation GPU with up to 19 cores delivers up to 30 percent more graphics performance, and the Neural Engine is 40 percent faster, speeding up machine learning tasks like video analysis and image processing. The powerful media engine in M2 Pro also tears through the most popular video codecs, dramatically accelerating video playback and encoding while using very little power.

With M2 Pro on MacBook Pro:


Rendering titles and animations in Motion is up to 80 percent faster1 than the fastest Intel-based MacBook Pro and up to 20 percent faster5 than the previous generation.
Compiling in Xcode is up to 2.5x faster1 than the fastest Intel-based MacBook Pro and nearly 25 percent faster5 than the previous generation.
Image processing in Adobe Photoshop is up to 80 percent faster1 than the fastest Intel-based MacBook Pro and up to 40 percent faster5 than the previous generation.









Motion is shown on MacBook Pro with M2 Pro.


Xcode is shown on MacBook Pro with M2 Pro.


Adobe Photoshop is shown on MacBook Pro with M2 Pro.


















With M2 Pro, animations in Motion render 80 percent faster than the Intel-based MacBook Pro, and 20 percent faster than the previous generation.



















With M2 Pro, compiling in Xcode is up to 2.5x faster than the fastest Intel-based MacBook Pro, and nearly 25 percent faster than the previous generation.



















With M2 Pro, image processing in Adobe Photoshop is up to 80 percent faster than the fastest Intel-based MacBook Pro, and up to 40 percent faster than the previous generation.











previous




next








MacBook Pro with M2 Max pushes workflows to the extreme with a much larger GPU featuring up to 38 cores and delivering up to 30 percent greater graphics performance over M1 Max, and also includes 400GB/s of unified memory bandwidth — twice that of M2 Pro. With up to 96GB of unified memory, MacBook Pro once again pushes the limits of graphics memory in a laptop to enable intensive graphics workloads, such as creating scenes with extreme 3D geometry and textures, or merging massive photographic panoramas. M2 Max has a next-gen 12-core CPU with up to eight high-performance and four high-efficiency cores that delivers up to 20 percent greater performance over M1 Max, and a more powerful media engine than M2 Pro, with twice the ProRes acceleration to dramatically speed up media playback and transcoding.

With M2 Max on MacBook Pro:


Effects rendering in Cinema 4D is up to 6x faster1 than the fastest Intel-based MacBook Pro and up to 30 percent faster6 than the previous generation.
Color grading in DaVinci Resolve is up to 2x faster1 than the fastest Intel-based MacBook Pro and up to 30 percent faster6 than the previous generation.









Cinema 4D is shown on MacBook Pro.


DaVinci Resolve is shown on MacBook Pro.


















With M2 Max, effects rendering in Cinema 4D is up to 6x faster than the fastest Intel-based MacBook Pro and up to 30 percent faster than the previous generation.



















With M2 Max, color grading in DaVinci Resolve is up to 2x faster than the fastest Intel-based MacBook Pro and up to 30 percent faster than the previous generation.











previous




next








Enhanced Connectivity

MacBook Pro now features Wi-Fi 6E3 for faster wireless connectivity, as well as more advanced HDMI, to support 8K displays up to 60Hz and 4K displays up to 240Hz. These new capabilities build on the versatile connectivity options already in MacBook Pro, including three Thunderbolt 4 ports for high-speed connection to peripherals, an SDXC card slot, and MagSafe 3 charging.








MagSafe 3, Thunderbolt 4, and the headphone jack are shown on MacBook Pro.


An SDXC card slot, Thunderbolt 4 port, and HDMI port are shown on MacBook Pro.


















MacBook Pro features three Thunderbolt 4 ports to connect high-speed peripherals, MagSafe 3 charging, and a headphone jack that supports high-impedance headphones.



















MacBook Pro incudes an SDXC card slot for fast access to media and an HDMI port for conveniently connecting to TVs and displays, including 8K displays for the first time.











previous




next








macOS Ventura

With macOS Ventura, MacBook Pro delivers even more performance and productivity. Powerful updates like Continuity Camera bring videoconferencing features to any Mac, including Desk View, Center Stage, Studio Light, and more. Handoff in FaceTime allows users to start a FaceTime call on their iPhone or iPad and fluidly pass it over to their Mac, or vice versa. And tools like Stage Manager automatically organize apps and windows, so users can concentrate on the task at hand and still see everything in a single glance.

Messages and Mail are better than ever, while Safari — the world’s fastest browser on Mac — ushers in a passwordless future with passkeys. With iCloud Shared Photo Library, users can now create and share a separate photo library among up to six family members, and the new Freeform app provides a flexible canvas that helps users be more productive and expressive, whether they are planning or brainstorming on their own, or together with others. With the power and popularity of Apple silicon, and new developer tools in Metal 3, gaming on Mac has never been better.








Stage Manager is shown in macOS Ventura on MacBook Pro.


Freeform is shown in macOS Ventura on MacBook Pro.


















Features like Stage Manager on macOS Ventura help users stay focused and get more done.



















With macOS Ventura, users get access to the new Freeform app, which makes visual collaboration easier than ever.











previous




next








MacBook Pro and the Environment

MacBook Pro is designed to minimize its impact on the environment, including 100 percent of the following recycled materials: aluminum in the enclosure, rare earth elements in all magnets, tin in the solder of the main logic board, and gold in the plating of multiple printed circuit boards. It also features 35 percent or more recycled plastic in multiple components, and meets Apple’s high standards for energy efficiency. MacBook Pro is free of numerous harmful substances, and 97 percent of the packaging is fiber based, bringing Apple closer to its goal of completely removing plastic from its packaging by 2025. 

Today, Apple is carbon neutral for global corporate operations, and by 2030, plans to be 100 percent carbon neutral across the entire manufacturing supply chain and all product life cycles. This means that every Apple device sold, from component manufacturing, assembly, transport, customer use, charging, all the way to recycling and material recovery, will have net-zero climate impact.





Pricing and Availability


The new MacBook Pro models with M2 Pro and M2 Max are available to order today, January 17, on apple.com/store and in the Apple Store app in 27 countries and regions, including the US. They will begin arriving to customers and will be in Apple Store locations and Apple Authorized Resellers starting Tuesday, January 24.
MacBook Pro with M2 Pro and M2 Max will be available in Australia, China, Hong Kong, Japan, Macau, New Zealand, and Singapore beginning Friday, February 3.
The new 14-inch MacBook Pro with M2 Pro starts at $1,999 (US), and $1,849 (US) for education; and the 16-inch MacBook Pro with M2 Pro starts at $2,499 (US), and $2,299 (US) for education. Additional technical specifications, configure-to-order options, and accessories are available at apple.com/mac.
Every customer who buys a Mac from Apple can enjoy a free Online Personal Session with an Apple Specialist, get their product set up in select stores including help with data transfer, and receive guidance on how to make their new Mac work the way they want.
With Apple Trade In, customers can trade in their current computer and get credit toward a new Mac. Customers can visit apple.com/shop/trade-in to see what their device is worth.
AppleCare+ for Mac provides expert technical support and additional hardware coverage from Apple, including up to two incidents of accidental damage protection every 12 months, each subject to a fee.






Share article

















































Text of this article


January 17, 2023
PRESS RELEASE
Apple unveils MacBook Pro featuring M2 Pro and M2 Max, with more game-changing performance and the longest battery life ever in a Mac
New MacBook Pro features up to 6x faster performance than fastest Intel-based MacBook Pro and support for up to 96GB of unified memory for demanding pro workflows
CUPERTINO, CALIFORNIA Apple today announced the new 14- and 16-inch MacBook Pro featuring M2 Pro and M2 Max, Apple’s next-generation pro silicon that brings even more power-efficient performance and battery life to pro users. With M2 Pro and M2 Max — the world’s most powerful and efficient chip for a pro laptop — MacBook Pro tackles demanding tasks, like effects rendering, which is up to 6x faster than the fastest Intel-based MacBook Pro, and color grading, which is up to 2x faster.1 Building on the unprecedented power efficiency of Apple silicon, battery life on MacBook Pro is now up to 22 hours — the longest battery life ever in a Mac.2 For enhanced connectivity, the new MacBook Pro supports Wi-Fi 6E,3 which is up to twice as fast as the previous generation, as well as advanced HDMI, which supports 8K displays for the first time. With up to 96GB of unified memory in the M2 Max model, creators can work on scenes so large that PC laptops can’t even run them.4 Rounding out the unrivaled features of MacBook Pro are its renowned Liquid Retina XDR display, extensive array of connectivity, 1080p FaceTime HD camera, six-speaker sound system, and studio-quality mics. When combined with macOS Ventura, the MacBook Pro user experience is unrivaled. Customers can order the new 14- and 16-inch MacBook Pro today, with availability beginning Tuesday, January 24.
“MacBook Pro with Apple silicon has been a game changer, empowering pros to push the limits of their workflows while on the go and do things they never thought possible on a laptop,” said Greg Joswiak, Apple’s senior vice president of Worldwide Marketing. “Today the MacBook Pro gets even better. With faster performance, enhanced connectivity, and the longest battery life ever in a Mac, along with the best display in a laptop, there’s simply nothing else like it.”
Unrivaled Power-Efficient Performance with M2 Pro and M2 Max
With M2 Pro and M2 Max, MacBook Pro is capable of transforming pro workflows across a wide range of disciplines, from art to science to app development. Users looking to upgrade from Intel-based Mac models will experience even more dramatic improvements in performance, battery life, connectivity, and overall productivity. MacBook Pro also maintains performance whether users are plugged in or on battery.
MacBook Pro with M2 Pro features a 10- or 12-core CPU with up to eight high-performance and four high-efficiency cores for up to 20 percent greater performance over M1 Pro. With 200GB/s of unified memory bandwidth — double the amount in M2 — and up to 32GB of unified memory, users can tackle large projects and run multiple pro apps with blazing speed. A next-generation GPU with up to 19 cores delivers up to 30 percent more graphics performance, and the Neural Engine is 40 percent faster, speeding up machine learning tasks like video analysis and image processing. The powerful media engine in M2 Pro also tears through the most popular video codecs, dramatically accelerating video playback and encoding while using very little power.
With M2 Pro on MacBook Pro:

Rendering titles and animations in Motion is up to 80 percent faster1 than the fastest Intel-based MacBook Pro and up to 20 percent faster5 than the previous generation.
Compiling in Xcode is up to 2.5x faster1 than the fastest Intel-based MacBook Pro and nearly 25 percent faster5 than the previous generation.
Image processing in Adobe Photoshop is up to 80 percent faster1 than the fastest Intel-based MacBook Pro and up to 40 percent faster5 than the previous generation.

MacBook Pro with M2 Max pushes workflows to the extreme with a much larger GPU featuring up to 38 cores and delivering up to 30 percent greater graphics performance over M1 Max, and also includes 400GB/s of unified memory bandwidth — twice that of M2 Pro. With up to 96GB of unified memory, MacBook Pro once again pushes the limits of graphics memory in a laptop to enable intensive graphics workloads, such as creating scenes with extreme 3D geometry and textures, or merging massive photographic panoramas. M2 Max has a next-gen 12-core CPU with up to eight high-performance and four high-efficiency cores that delivers up to 20 percent greater performance over M1 Max, and a more powerful media engine than M2 Pro, with twice the ProRes acceleration to dramatically speed up media playback and transcoding.
With M2 Max on MacBook Pro:

Effects rendering in Cinema 4D is up to 6x faster1 than the fastest Intel-based MacBook Pro and up to 30 percent faster6 than the previous generation.
Color grading in DaVinci Resolve is up to 2x faster1 than the fastest Intel-based MacBook Pro and up to 30 percent faster6 than the previous generation.

Enhanced Connectivity
MacBook Pro now features Wi-Fi 6E3 for faster wireless connectivity, as well as more advanced HDMI, to support 8K displays up to 60Hz and 4K displays up to 240Hz. These new capabilities build on the versatile connectivity options already in MacBook Pro, including three Thunderbolt 4 ports for high-speed connection to peripherals, an SDXC card slot, and MagSafe 3 charging.
macOS Ventura
With macOS Ventura, MacBook Pro delivers even more performance and productivity. Powerful updates like Continuity Camera bring videoconferencing features to any Mac, including Desk View, Center Stage, Studio Light, and more. Handoff in FaceTime allows users to start a FaceTime call on their iPhone or iPad and fluidly pass it over to their Mac, or vice versa. And tools like Stage Manager automatically organize apps and windows, so users can concentrate on the task at hand and still see everything in a single glance.
Messages and Mail are better than ever, while Safari — the world’s fastest browser on Mac — ushers in a passwordless future with passkeys. With iCloud Shared Photo Library, users can now create and share a separate photo library among up to six family members, and the new Freeform app provides a flexible canvas that helps users be more productive and expressive, whether they are planning or brainstorming on their own, or together with others. With the power and popularity of Apple silicon, and new developer tools in Metal 3, gaming on Mac has never been better.
MacBook Pro and the Environment
MacBook Pro is designed to minimize its impact on the environment, including 100 percent of the following recycled materials: aluminum in the enclosure, rare earth elements in all magnets, tin in the solder of the main logic board, and gold in the plating of multiple printed circuit boards. It also features 35 percent or more recycled plastic in multiple components, and meets Apple’s high standards for energy efficiency. MacBook Pro is free of numerous harmful substances, and 97 percent of the packaging is fiber based, bringing Apple closer to its goal of completely removing plastic from its packaging by 2025. 
Today, Apple is carbon neutral for global corporate operations, and by 2030, plans to be 100 percent carbon neutral across the entire manufacturing supply chain and all product life cycles. This means that every Apple device sold, from component manufacturing, assembly, transport, customer use, charging, all the way to recycling and material recovery, will have net-zero climate impact.
Pricing and Availability

The new MacBook Pro models with M2 Pro and M2 Max are available to order today, January 17, on apple.com/store and in the Apple Store app in 27 countries and regions, including the US. They will begin arriving to customers and will be in Apple Store locations and Apple Authorized Resellers starting Tuesday, January 24.
MacBook Pro with M2 Pro and M2 Max will be available in Australia, China, Hong Kong, Japan, Macau, New Zealand, and Singapore beginning Friday, February 3.
The new 14-inch MacBook Pro with M2 Pro starts at $1,999 (US), and $1,849 (US) for education; and the 16-inch MacBook Pro with M2 Pro starts at $2,499 (US), and $2,299 (US) for education. Additional technical specifications, configure-to-order options, and accessories are available at apple.com/mac.
Every customer who buys a Mac from Apple can enjoy a free Online Personal Session with an Apple Specialist, get their product set up in select stores including help with data transfer, and receive guidance on how to make their new Mac work the way they want.
With Apple Trade In, customers can trade in their current computer and get credit toward a new Mac. Customers can visit apple.com/shop/trade-in to see what their device is worth.
AppleCare+ for Mac provides expert technical support and additional hardware coverage from Apple, including up to two incidents of accidental damage protection every 12 months, each subject to a fee.

 About Apple
 Apple revolutionized personal technology with the introduction of the Macintosh in 1984. Today, Apple leads the world in innovation with iPhone, iPad, Mac, Apple Watch, and Apple TV. Apple’s five software platforms — iOS, iPadOS, macOS, watchOS, and tvOS — provide seamless experiences across all Apple devices and empower people with breakthrough services including the App Store, Apple Music, Apple Pay, and iCloud. Apple’s more than 100,000 employees are dedicated to making the best products on earth, and to leaving the world better than we found it.
 
Results are compared to previous-generation 2.4GHz 8-core Intel Core i9-based 16-inch MacBook Pro systems with Radeon Pro 5600M graphics with 8GB HBM2, 64GB of RAM, and 8TB SSD.
Testing was conducted by Apple in November and December 2022 using preproduction 16-inch MacBook Pro systems with Apple M2 Pro, 12-core CPU, 19-core GPU, 16GB of RAM, and 1TB SSD. The Apple TV app movie playback test measures battery life by playing back HD 1080p content with display brightness set to eight clicks from bottom. Battery life varies by use and configuration. See apple.com/batteries for more information.
Wi‑Fi 6E is not available in China mainland. It requires macOS 13.2 or later in Japan.
Testing was conducted by Apple in November and December 2022 using preproduction 16-inch MacBook Pro systems with Apple M2 Max, 12-core CPU, 38-core GPU, 96GB of RAM, and 8TB SSD, as well as a production Intel Core i9-based PC system with NVIDIA Quadro RTX 6000 graphics with 24GB GDDR6 and the latest version of Windows 11 Pro available at the time of testing, and a production Intel Core i9-based PC system with NVIDIA GeForce RTX 3080 Ti graphics with 16GB GDDR6 and the latest version of Windows 11 Home available at the time of testing. OTOY Octane X 2022.1 on preproduction 16-inch MacBook Pro systems and OTOY OctaneRender 2022.1 on Windows systems were tested using a scene that requires over 40GB of graphics memory when rendered. Performance tests are conducted using specific computer systems and reflect the approximate performance of MacBook Pro.
Results are compared to previous-generation 16-inch MacBook Pro systems with Apple M1 Pro, 10-core CPU, 16-core GPU, 32GB of RAM, and 8TB SSD.
Results are compared to previous-generation 16-inch MacBook Pro systems with Apple M1 Max, 10-core CPU, 32-core GPU, 64GB of RAM, and 8TB SSD.

Press Contacts
Starlayne Meza
Apple
starlayne_meza@apple.com
Michelle Del Rio
Apple
mr_delrio@apple.com
(408) 862-1478
Apple Media Helpline
media.help@apple.com
(408) 974-2042

Copy text








Images in this article

Download all images









About Apple

Apple revolutionized personal technology with the introduction of the Macintosh in 1984. Today, Apple leads the world in innovation with iPhone, iPad, Mac, Apple Watch, and Apple TV. Apple’s five software platforms — iOS, iPadOS, macOS, watchOS, and tvOS — provide seamless experiences across all Apple devices and empower people with breakthrough services including the App Store, Apple Music, Apple Pay, and iCloud. Apple’s more than 100,000 employees are dedicated to making the best products on earth, and to leaving the world better than we found it.







Results are compared to previous-generation 2.4GHz 8-core Intel Core i9-based 16-inch MacBook Pro systems with Radeon Pro 5600M graphics with 8GB HBM2, 64GB of RAM, and 8TB SSD.
Testing was conducted by Apple in November and December 2022 using preproduction 16-inch MacBook Pro systems with Apple M2 Pro, 12-core CPU, 19-core GPU, 16GB of RAM, and 1TB SSD. The Apple TV app movie playback test measures battery life by playing back HD 1080p content with display brightness set to eight clicks from bottom. Battery life varies by use and configuration. See apple.com/batteries for more information.
Wi‑Fi 6E is not available in China mainland. It requires macOS 13.2 or later in Japan.
Testing was conducted by Apple in November and December 2022 using preproduction 16-inch MacBook Pro systems with Apple M2 Max, 12-core CPU, 38-core GPU, 96GB of RAM, and 8TB SSD, as well as a production Intel Core i9-based PC system with NVIDIA Quadro RTX 6000 graphics with 24GB GDDR6 and the latest version of Windows 11 Pro available at the time of testing, and a production Intel Core i9-based PC system with NVIDIA GeForce RTX 3080 Ti graphics with 16GB GDDR6 and the latest version of Windows 11 Home available at the time of testing. OTOY Octane X 2022.1 on preproduction 16-inch MacBook Pro systems and OTOY OctaneRender 2022.1 on Windows systems were tested using a scene that requires over 40GB of graphics memory when rendered. Performance tests are conducted using specific computer systems and reflect the approximate performance of MacBook Pro.
Results are compared to previous-generation 16-inch MacBook Pro systems with Apple M1 Pro, 10-core CPU, 16-core GPU, 32GB of RAM, and 8TB SSD.
Results are compared to previous-generation 16-inch MacBook Pro systems with Apple M1 Max, 10-core CPU, 32-core GPU, 64GB of RAM, and 8TB SSD.





Press Contacts


Starlayne Meza

Apple

starlayne_meza@apple.com





Michelle Del Rio

Apple

mr_delrio@apple.com


                        (408) 862-1478
                    


Apple Media Helpline



media.help@apple.com


                        (408) 974-2042
                    







Latest News










PRESS RELEASE
Apple unveils M2 Pro and M2 Max: next-generation chips for next-level workflows
January 17, 2023













PRESS RELEASE
Apple introduces new Mac mini with M2 and M2 Pro — more powerful,  capable, and versatile than ever
January 17, 2023













UPDATE
Introducing Apple Business Connect
January 11, 2023










Apple Newsroom
The latest news and updates, direct from Apple.
Read more




"
https://news.ycombinator.com/rss,New Mac Mini,https://www.apple.com/mac-mini/,Comments,"






								Save on Mac mini in our Education Store.*
Shop





































Mac mini
More muscle. More hustle.


Mac mini with M2 packs the speed you need to get more done faster. And M2 Pro takes it to a whole new level — bringing a pro chip to Mac mini for the first time. Add to that a versatile array of ports and you’ve got a desktop ready to flex in any setup, no matter which chip you choose.



Available starting 1.24




Watch the announcement 

See Mac mini in AR







Supercharged by














8‑core CPU
10‑core GPU
Up to 24GB unified memory
100GB/s memory bandwidth














Up to 12‑core CPU
Up to 19‑core GPU
Up to 32GB unified memory
200GB/s memory bandwidth



The next generation of Apple silicon makes this the hardest‑working Mac mini we’ve ever built. From rich presentations to immersive gaming, M2 flies through work and play. And M2 Pro crushes compute‑intensive tasks like editing massive images and 8K ProRes video.



Apple silicon’s next chapter is here. M2 and M2 Pro scale up our breakthrough system on a chip (SoC) architecture, which combines the CPU, GPU, unified memory, and Neural Engine on a single power‑efficient chip.
M2 brings a faster, next‑generation CPU and GPU to Mac mini, along with much higher memory bandwidth. M2 Pro delivers even more CPU and GPU cores, with double the memory bandwidth of M2. And both chips feature an industry‑leading media engine that rips through ProRes, H.264, and HEVC video so you can edit multiple streams even at 8K resolution.

M2
M2 brings the next generation of Apple silicon to Mac mini, packing in over 20 billion transistors — 25 percent more than M1. And with 50 percent more memory bandwidth than M1, M2 lets you cruise through workflows and seamlessly switch between apps.















Up to24GBunified memory


100GB/smemory bandwidth



M2 Pro
 M2 Pro pushes Mac mini to new heights with over 40 billion transistors — twice that of M2. And with 50 percent more memory and two times the memory bandwidth of M2, M2 Pro can handily tackle large projects and juggle pro workflows, whether you’re editing a large batch of selects in Lightroom Classic or color grading 8K video in DaVinci Resolve.















Up to32GBunified memory


200GB/smemory bandwidth



 More power at your command. Apple silicon allows Mac mini to outperform desktops many times its size. The next‑generation 8‑core CPU in M2 is up to 18 percent faster than M1, and the new 10‑core GPU delivers up to 35 percent faster graphics performance than M1. The 12‑core CPU in M2 Pro offers up to 1.9 times faster performance than M1, and its 19‑core GPU is up to 2.6 times faster.1














8-core CPU
10-core GPU
16-coreNeural Engine
















Up to12-core CPU
Up to19-core GPU
16-coreNeural Engine






Go inside M2 and M2 Pro





Speed and capability



Compared to Intel-based Mac mini







Photo editing

/

Image upscaling

/

Video editing

/

Video transcoding

/

Code compiling

/

Productivity










Faster filter and function performance in Photoshop2






Mac mini with M2 Pro




4.7
x








Mac mini with M2




3.5
x








Mac mini with M1





iMac 27‑inch with Core i7 and Radeon Pro 5500 XT





Mac mini with Core i7 (baseline)





Faster ML image upscaling performance in Pixelmator Pro3






Mac mini with M2 Pro




24.0
x








Mac mini with M2




22.0
x








Mac mini with M1





iMac 27‑inch with Core i7 and Radeon Pro 5500 XT





Mac mini with Core i7 (baseline)





Faster complex timeline render performance in Final Cut Pro4






Mac mini with M2 Pro




18.8
x








Mac mini with M2




9.8
x








Mac mini with M1





iMac 27‑inch with Core i7 and Radeon Pro 5500 XT





Mac mini with Core i7 (baseline)





Faster ProRes transcode performance in Final Cut Pro5






Mac mini with M2 Pro




12.6
x








Mac mini with M2




7.2
x








Mac mini with M1





iMac 27‑inch with Core i7 and Radeon Pro 5500 XT





Mac mini with Core i7 (baseline)





Faster project build performance in Xcode6






Mac mini with M2 Pro




3.2
x








Mac mini with M2




1.9
x








Mac mini with M1





iMac 27‑inch with Core i7 and Radeon Pro 5500 XT





Mac mini with Core i7 (baseline)





Faster spreadsheet performance in Excel7






Mac mini with M2 Pro




1.9
x








Mac mini with M2




1.8
x








Mac mini with M1





iMac 27‑inch with Core i7 and Radeon Pro 5500 XT





Mac mini with Core i7 (baseline)












Compatibility



Mini does that.
With macOS Ventura, your go‑to apps run lightning fast on Mac mini — from Microsoft 365 to Adobe Creative Cloud to Zoom. And over 15,000 apps and plug-ins are optimized for M2 and M2 Pro. Organize your apps and windows with Stage Manager, so you can stay focused while seamlessly moving between tasks. Dive into a multiplayer gaming session with SharePlay. Or start a to‑do list on your iPhone and pick it up on Mac mini without missing a beat.













Microsoft Excel / Affinity Designer 2




Design
Whether you choose M2 or M2 Pro, the performance and efficiency of Apple silicon allow Mac mini to blow away desktops many times its size — all in an iconic 7.7‑inch‑square frame.



Towering performance. Minus the tower.




























Unified memory













Memory you won’t forget. Unified memory on Mac does more than traditional RAM. A single pool of high-bandwidth, low‑latency memory allows Apple silicon to move data more efficiently — so everything you do is fluid. Configure your Mac mini with between 8GB and 24GB of memory on M2, and 16GB or 32GB on M2 Pro.















Fast SSD storage
Mac mini comes with all-flash storage — up to a whopping 8TB SSD for all your photo and video libraries, files, and apps.8 That’s up to four times as much as the previous generation.


















Privacy and security
M2 and macOS Ventura give Mac mini industry‑leading privacy and security features beyond anything in its class, including built‑in protections against malware and viruses. And the next‑generation Secure Enclave helps keep your system and your data protected.





Keep it secure. Keep it safe.












Versatility
All systems go.
With its next-level performance, wide array of ports, and compact size, this mini desktop is perfect for an impressive range of uses — and ready to take on whatever you do.










Productivity


Gaming


Audio and video production


Software development










Zoom / Microsoft PowerPoint / Notes










Mac mini turns any desk into a powerful workspace. Add a display, keyboard, and mouse or trackpad, and you’re ready to create, code, and collaborate.

























No Man’s Sky










Jump into graphics-intensive AAA games like No Man’s Sky and Resident Evil Village with fluid frame rates and high fidelity — all enabled by the incredible GPU performance of M2 or M2 Pro.


Final Cut Pro / Logic Pro










Mac mini is a powerful desktop for content creation. Run more plug‑ins and layer more audio tracks in Logic Pro. And edit multiple streams of 4K and 8K ProRes video in Final Cut Pro faster with the help of the high‑performance media engine.


Xcode










Whether you choose M2 or M2 Pro, Mac mini gives you the tools you need to create powerful apps or games.










Connectivity
Take it all in.
With two or four Thunderbolt 4 ports, two USB‑A ports, HDMI, Wi‑Fi 6E, and Gigabit Ethernet, Mac mini connects to just about anything.9 And if you want even faster networking speeds, you can configure Mac mini with 10Gb Ethernet for up to 10 times the throughput.










M2


M2 Pro






















Side view of silver Mac mini M2 with I/O ports highlighted image

Ethernet
Thunderbolt 4
HDMI
USB-A
Headphone jack

















Side view of silver Mac mini M2 Pro with I/O ports highlighted image

Ethernet
Thunderbolt 4
HDMI
USB-A
Headphone jack












Thunderbolt 4
Transfer data at up to 40Gb/s.10 Charge and power external devices. And expand your workspace with external displays that can output video at up to 6K resolution and 60Hz with Thunderbolt 4. HDMI unlocks up to 8K video output.














Wi-Fi 6E
Another first for Mac mini, Wi‑Fi 6E delivers better all‑around performance and up to 2.4Gb/s of throughput — for superfast file transfers or backing up to Time Machine.













Wi‑Fi 6E
up to 2.4Gb/s throughput10








Accessories
Deck out your desk.
With its breathtaking 27-inch 5K Retina display, 12MP camera with Center Stage, and high‑fidelity six‑speaker sound system, Studio Display helps take Mac mini to the max. Add in convenient wireless Magic accessories like Magic Keyboard with Touch ID to complete the package.
Learn more about Studio Display
Shop Magic accessories





























Use AR to see Mac mini in your workspace.
AR

Open this page using Safari on your iPhone or iPad.



See Mac mini in AR





View with accessories























Apple Card
Get 3% Daily Cash back with Apple Card.
And pay over time, interest-free when you choose to check out with Apple Card Monthly Installments.†
 
Learn more




















Apple Trade In
Get credit toward a new Mac mini.
Just trade in your eligible computer for credit toward a new one or recycle it for free. It’s good for you and the planet.**
 
Learn more




















Which desktop is right for you?
Compare all Mac models






















New Mac mini
									
◊

Apple M2 or Apple M2 Pro chip
8- to 12-core CPU
10- to 19-core GPU
8GB to 32GB unified memory
256GB to 8TB storage8


											Order now
										






















										Mac Studio
									
◊

Apple M1 Max or Apple M1 Ultra chip
10- or 20-core CPU
24- to 64-core GPU
32GB to 128GB unified memory
512GB to 8TB storage8

Buy


Learn more 





















										iMac
									
◊

Apple M1 chip
8-core CPU
7- or 8-core GPU
8GB or 16GB unified memory
256GB to 2TB storage8
4.5K Retina display 218 ppi

Buy


Learn more 







Accessories
Explore Mac accessories.

Shop





























Designed with the earth in mind.
We designed Mac mini to minimize environmental impact and support our net-zero carbon goal — from using 100 percent recycled aluminum in its enclosure to ensuring that almost all wood fiber in its packaging comes from responsibly managed forests.
 
Learn more about Apple and the environment








Continuity
All your devices. One seamless experience.
 
Learn more




















Apple and Education
Empowering educators and students to move the world forward.
 
Learn about Apple and Education




















Apple at Work
Get the power to take your business to the next level.

 
Learn about Apple at Work


 
Learn more about Mac for business





















"
https://news.ycombinator.com/rss,Ruby 3.2’s YJIT is Production-Ready,https://shopify.engineering/ruby-yjit-is-production-ready,Comments,"






Ruby 3.2’s YJIT is Production-Ready




            by Maxime Chevalier-Boisvert


Development




Jan 17, 2023


          9 minute read
        




  Email
  Facebook
  Twitter
  LinkedIn






Shopify and YJIT
Back in July 2020, I joined the Ruby & Rails Infrastructure (R&RI) team at Shopify. Our team focuses on making sure that Ruby as well as Ruby on Rails, central to the infrastructure behind all Shopify stores and much of the modern web, run as smoothly and efficiently as possible.
As part of the R&RI team, I got to meet skilled engineers that were doing open source work, directly contributing patches to CRuby itself. Since my background is in compiler design, I started to discuss with my manager the possibility that we could build a relatively simple Just-In-Time (JIT) compiler for Ruby. To my surprise, my manager and two colleagues were immediately on board with this idea, and what would become the  YJIT project was born.
Building YJIT was hard work. There were many long, intense debugging sessions involved, but within just over a year, we’d managed to deliver roughly  20% speedups on  railsbench. Following that, the CRuby core contributors invited us to  upstream YJIT, and so YJIT was released as an official part of Ruby 3.1 in December of 2021. Upstreaming YJIT had been an aspirational goal for the team from the start, but we had never thought it would happen this fast. I’ll take this opportunity to say that I’m very thankful to Shopify for letting us take on some risks, and to the Ruby community for being so open-minded.
Major YJIT Improvements in Ruby 3.2
A lot has happened for YJIT in 2022. For one thing, we’ve expanded the team. We wrote about job openings in the YJIT team on this blog last year, and we were flooded with applications from people excited to work on a Ruby JIT, all of them with impressive CVs and a long list of systems programming skills. We ended up recruiting three skilled engineers which became part of the YJIT dream team. One of these new recruits is no other than Takashi Kokubun, long-time CRuby core member and maintainer of MJIT.
The YJIT team has made multiple improvements to YJIT which are now available  as part of Ruby 3.2. The good news is that, as you might expect, the new version of YJIT brings better performance, both on benchmarks and on real workloads, but I would say that the broader theme for 3.2 has been to make YJIT more robust, more maintainable, and generally more production-ready.
Rewriting YJIT to Rust
To start 2021, we decided to  port YJIT from C99 to Rust. The motivation for this was twofold. Rust provides additional safety guarantees that C doesn’t, which is important when doing low-level systems programming with many constraints, as in a JIT compiler. The secondary motivating factor was that we felt that, as the complexity of YJIT increases, we needed better tools to manage that complexity. Writing C code, we had to resort to implementing our own dynamic arrays in terms of C macros, which felt both unsafe and awkward. Rust provides a much richer standard library and many nice and fast abstractions. It took Noah Gibbs, Alan Wu, and me about three months to port YJIT to Rust, and I’m happy to say that our new Rust codebase does feel much easier to maintain.
Improved Memory Usage
One of the challenges with JIT compilers is that they always incur some amount of memory overhead over interpreters. At the most basic level, a JIT compiler needs to generate executable machine code, which an interpreter doesn’t, so JIT compilers must use more memory than interpreters. On top of that, however, JIT compilers also need to allocate memory for auxiliary data structure (metadata), which can also add quite a bit of extra memory overhead.
We were unhappy with how much extra memory YJIT used in Ruby 3.1. We felt that the amount of memory needed back then made it difficult to deploy in production at Shopify, and so we’ve made multiple improvements to reduce memory usage. The good news is that, thanks to the hard work of Alan and Takashi, the overhead has been cut down to approximately one third of what it was for 3.1, which helps make YJIT a lot more usable in production. To achieve this, we’ve optimized how much space our metadata takes, we’ve implemented a garbage collector for machine code that is no longer used, and we’ve made it so YJIT will lazily allocate memory pages for machine code as opposed to allocating and initializing a large block of memory up front.
Improved Performance

YJIT’s performance vs the interpreter in Ruby 3.2 (higher is better).

YJIT 3.2 doesn’t just use less memory though, it’s also faster. We now  speed up railsbench by about 38% over the interpreter, but this is on top of the Ruby 3.2 interpreter, which is already faster than the interpreter from Ruby 3.1. According to  the numbers gathered by Takashi, the cumulative improvement makes YJIT 57% faster than the Ruby 3.1.3 interpreter. It’s not just our numbers  that show that the  new YJIT delivers great  performance, the Ruby community has done  their own benchmarking as well.

 
Source: @rafael_falco on Twitter.

ARM64 Support
Another major change in YJIT 3.2 is that we now have  a new backend that can generate machine code for multiple CPU platforms, which enables us to support ARM64 CPUs. In 3.1, we only supported x86-64 on Mac and Linux. With developers at Shopify migrating to Apple M1/M2 laptops, we found ourselves in the awkward situation where we could only run YJIT locally through emulation with Rosetta. With Ruby 3.2, it’s now possible to run YJIT natively on Apple M1 & M2, AWS Graviton 1 & 2, and even on  Raspberry Pis! Interestingly, YJIT gets an even bigger speedup on Mac M1 hardware than it does on Intel x86-64 CPUs. We hope that this will encourage people to try out YJIT locally on their development machines.
Additional Improvements
Ruby 3.2 also includes another major change that has been in the works for a while. Jemma Issroff and Aaron Patterson have done an impressive amount of work in order to reimplement Ruby’s internal representation for objects, which is now based on the concept of  object shapes. This allows both the interpreter and YJIT to benefit from faster instance variable accesses.
In addition to this,  Eileen Uchitelle implemented a tool to trace YJIT exits, Jimmy Miller worked on improving YJIT support for various types of Ruby method calls, and Kevin Newton implemented a  finer-grained constant cache invalidation mechanism. This change was brought about to address a situation we had seen in production where constants being redefined would cause YJIT to recompile a lot of code. 
Last but not least, Peter Zhu and Matthew Valentine-House have made improvements to Ruby 3.2’s garbage collector, and made it possible to allocate  variable-sized objects. This improves Ruby’s memory usage and also significantly improves the interpreter’s performance. It also makes it possible to allocate larger objects which are more cache-friendly.
Running YJIT in Production
The main reason why Shopify chose to invest in the development of YJIT is of course that Shopify runs a large amount of infrastructure built on top of Ruby and Ruby on Rails. Multiple large clusters of servers distributed across the world, capable of serving over  75 million requests per minute. From the start, the objective was to eventually be able to use YJIT to improve the efficiency of Shopify’s Storefront Renderer (SFR).
Given that YJIT 3.1 had significant memory overhead and was still marked as experimental, we didn’t want to deploy it globally right away. However, starting about a year ago, we’ve started to run a few SFR nodes using YJIT. This has been extremely valuable to us, because it’s enabled us to gather statistics and see how YJIT and our codebase behave under a real-world deployment with real traffic, which has exposed some performance issues we couldn’t see on benchmarks. 

This year, with Ruby 3.2, YJIT has improved enough that we’ve deemed it production-ready, and Shopify has proceeded to deploy it globally on its entire SFR infrastructure. We’re able to measure real speedups ranging from 5% to 10% (depending on time of day) on our total end-to-end request completion time measurements.

YJIT speedup over the Ruby 3.2 interpreter on our SFR deployment.

I want to be honest and say that YJIT is still not perfect. It still has some memory overhead, but we think it’s worth the speedups, and of course, we intend on improving the situation further. One of the key advantages of YJIT is its very fast compilation times. At Shopify, we deploy continuously, often multiple times every day, sometimes multiple times in a single hour. That means YJIT has to be able to compile code very quickly, otherwise some Shopify customers might see their request time out whenever a deployment occurs. It’s not just the speed of the code we compile that matters, it’s also how fast we can compile code.
We’ve successfully deployed YJIT in production at Shopify, but the YJIT team has relatively little visibility into how many people are using YJIT in practice outside of interacting with people on Twitter or at conferences. If you’re using YJIT in production, for your dev environment, or even for a hobby project, please  let us know and share your feedback! We’d love to hear your YJIT success stories (or pain points, for that matter).
Future Plans
The year 2023 has just begun and we already have a long list of new improvements we want to bring to YJIT. Since we’ve just deployed YJIT, I think it’s important that we continue to remain grounded and use statistics from our real-world deployment to address the biggest pain points. YJIT’s biggest flaw is still its memory footprint, and this is something we need to continue working to further improve.
In terms of the biggest opportunities for speedups, Ruby is method calls all the way down. That is, loop iteration as well as most basic operations in Ruby are method calls, and typical Ruby code contains many calls to small Ruby methods. As such, the most obvious area for potential improvements would be to make method calls faster. There are a few avenues we're exploring to achieve this, such as potentially implementing a more efficient calling convention, and also inlining method calls.
In addition to optimizing the performance of method calls, we’d also like to better optimize the machine code that YJIT generates. We still don’t have a proper register allocator, and we don’t really optimize across basic blocks. Finally, we may also want to optimize the way YJIT and CRuby perform various hash and string operations, as these are very common in web workloads.
More About YJIT
If you’re interested in trying out Ruby 3.2, the release notes and tarball packages can be found  here, it’s also possible to directly install Ruby 3.2 via brew if you’re on macOS, or using the  ruby-install tool. In order to make sure that YJIT is available, you just need to make sure that you have rustc 1.58.0 or newer (or  the Rust toolchain) installed on your machine before you install/build Ruby using your favorite tool (brew, ruby-build, ruby-install, etc.). You can then run Ruby with YJIT enabled by passing the --yjit command-line flag to Ruby, or by setting the RUBY_YJIT_ENABLE environment variable.
For more information on YJIT’s design or how to use it, you can check out our  documentation, or one of the resources below.

Alan Wu’s RubyKaigi 2022 keynote: Stories from developing YJIT (RubyKaigi 2022)
Building a Lightweight IR and Backend for YJIT (RubyKaigi 2022)
 Optimizing Ruby’s Memory Layout: Variable Width Allocation by Peter Zhu
YJIT - Building a new JIT Compiler inside CRuby (RubyConf 2021)
MJIT, YJIT, and HAML with Takashi Kokubun - Ruby Rogues #573
Parsers, Interpreters, and YJIT with Kevin Newton

YJIT: Building a New JIT Compiler for CRuby 
Our Experience Porting the YJIT Ruby Compiler to Rust

I’d like to conclude with a big thank you to the YJIT team, and everyone that has contributed to this project’s success, including: Alan Wu, Aaron Patterson, Jemma Issroff, Eileen Uchitelle, Kevin Newton, Noah Gibbs, Jimmy Miller, Takashi Kokubun, Ufuk Kayserilioglu, Mike Dalessio, Jean Boussier, John Hawthorn, Rafael França, and more!

Maxime Chevalier-Boisvert obtained a PhD in compiler design at the University of Montreal in 2016, where she developed Basic Block Versioning (BBV), a JIT compiler architecture optimized for dynamically-typed programming languages. She leads the YJIT project at Shopify.

Open source software plays a vital and integral part at Shopify. If being a part of an Engineering organization that’s committed to the support and stewardship of open source software sounds exciting to you, visit our Engineering career page to find out about our open positions and learn about Digital by Design.


  Email
  Facebook
  Twitter
  LinkedIn

  Get stories like this in your inbox!Stories from the teams who build and scale Shopify. The commerce platform powering millions of businesses worldwide.Email addressYes, sign me up!

Share your email with us and receive monthly updates.
Thanks for subscribing.
You’ll start receiving free tips and resources soon.













Search articles

Search


  Get stories like this in your inbox!Stories from the teams who build and scale Shopify. The commerce platform powering millions of businesses worldwide.Email addressYes, sign me up!

Share your email with us and receive monthly updates.
Thanks for subscribing.
You’ll start receiving free tips and resources soon.








  
    Resources
  


            Our Tech Stack
 Curious about what’s in our tech stack.

            Sponsorship
 We’re looking to partner with you.

            Working Anywhere at Shopify
 Learn about Digital by Design

            Shopify Partner Developers
 Become a Shopify developer and earn money by building apps or working with businesses

            Shopify Engineering on Twitter
 Connect with us on Twitter

            Shopify Engineering YouTube
 Connect with us on YouTube




  
    Popular
  


            Migrating our Largest Mobile App to React Native
 
            Shopify Embraces Rust for Systems Programming
 
            Mixing It Up: Remix Joins Shopify to Push the Web Forward
 
            From Ruby to Node: Overhauling Shopify’s CLI for a Better Developer Experience
 
            A Flexible Framework for Effective Pair Programming
 
            10 Tips for Building Resilient Payment Systems
 
            Five Common Data Stores and When to Use Them
 
            Deconstructing the Monolith: Designing Software that Maximizes Developer Productivity
 
            Under Deconstruction: The State of Shopify’s Monolith
 
            Reducing BigQuery Costs: How We Fixed A $1 Million Query
 



  
    Latest
  


            Ruby 3.2’s YJIT is Production-Ready
 
            How Good Documentation Can Improve Productivity
 
            From Ruby to Node: Overhauling Shopify’s CLI for a Better Developer Experience
 
            Reliving Your Happiest HTTP Interactions with Ruby’s VCR Gem
 
            Monte Carlo Simulations: Separating Signal from Noise in Sampled Success Metrics
 
            React Native Skia: A Year in Review and a Look Ahead
 
            Migrating our Largest Mobile App to React Native
 
            Optimizing Ruby’s Memory Layout: Variable Width Allocation
 
            Year in Review 2022: Tenderlove's Ruby and Rails Reflections and Predictions
 
            Automatically Rotating GitHub Tokens (So You Don’t Have To)
 



"
https://news.ycombinator.com/rss,Byte Magazine 1975-1995,https://worldradiohistory.com/Byte_Magazine.htm,Comments,"




BYTE MAGAZINE: Early computer publication

























			|

































 Byte Magazine 
							1975-1995


							Search 
							Byte Magazine









					 Early Personal Computing magazine










Byte magazine was an 
									early microcomputer magazine, influential 
									in the late 1970s and throughout the 1980s 
									because of its wide-ranging editorial 
									coverage. Byte started in 1975, 
									shortly after the first personal computers 
									appeared as kits which were advertised in the back of 
									electronics magazines.





													Issues are Globally 
													Searchable










													Best of Byte
													1977 Compendium
													Over 300 pages of articles
 from the first three years of Byte











													Byte Book of Computer Music
													1979








									1975

									1976

									1977


1978


1979




Sep

Oct


Nov

Dec


Jan

Feb

Mar

Apr

May

Jun

Jul

Aug

Sep

Oct

Nov

Dec


Jan

Feb

Mar

Apr

May

Jun

Jul

Aug

Sep

Oct

Nov

Dec


Jan

Feb

Mar

Apr

May

Jun

Jul

Aug

Sep

Oct

Nov

Dec


Jan

Feb

Mar

Apr

May

Jun

Jul

Aug

Sep

Oct

Nov

Dec




1980



1981



1982



1983



1984





Jan


Feb

Mar

Apr

May

Jun

Jul

Aug

Sep

Oct

Nov

Dec


Jan

Feb

Mar

Apr

May

Jun

Jul

Aug

Sep

Oct

Nov

Dec


Jan

Feb

Mar

Apr

May

Jun

Jul

Aug

Sep

Oct

Nov

Dec


Jan

Feb

Mar

Apr

May

Jun

Jul

Aug

Sep

Oct

Nov

Dec



Jan

Feb

Mar


Apr

May

Jun

Jul

Aug

Sep


Sep



IBM Special



Oct


Nov



Dec




1985



1986



1987




1988



1989




Jan

Feb

Mar

Apr

May

Jun

Jul

Aug

Sep
10th 
									Anniversary



Oct

Nov

Nov II



									IBM Special



Dec



Jan

Feb

Mar

Apr

May

Jun

Jul

Aug

Sep


									Oct

Nov

Dec


Jan

Feb

Mar

Apr

May

Jun

Jul

Aug

Sep

Oct

Nov

									Nov II
IBM Special

									Dec



									Jan 
 
									Feb

Mar

Apr


									May
Jun
Jul
Aug
Macintosh Issue

									Sep

									Oct

									Nov

									Nov
IBM Issue
Dec

Jan

									Feb

									Mar

									Apr

May

Jun

Jul

									Aug

									Sep

Oct

Nov


IBM Special


Dec



									1990

									1991

									1992

1993

									1994




Jan


									Feb

Mar

Apr

May

Jun

Jul


									Aug

Sep

Oct

Nov



Nov II




									IBM Special

Dec

Dec II


Jan

									Feb

Mar

									Apr

									May

									Jun

Jul

Aug

Sep

Oct

Nov

Dec

1992  

Outlook 
									Special




									Jan



									Feb



									Mar


									Apr

									May



									Jun



									Jul


									Aug


									Sep

Oct



									Nov

Dec

									Special
Portable 
									Computing




Jan


Feb

Mar




									Spring Windows 


									Special 





									Apr
May

									Jun

									Jul

									Aug

									Sep

									Oct

									Nov

									Dec


Jan

Feb

Mar

Apr

May



									1995

									1996

									1997

									1998

									1999



									 

									 

									 

									 

									 

































"
https://news.ycombinator.com/rss,Ask HN: Books that teach programming by building a series of small projects?,https://news.ycombinator.com/item?id=34412069,Comments,"

Ask HN: Books that teach programming by building a series of small projects? | Hacker News

Hacker News
new | past | comments | ask | show | jobs | submit 
login




 Ask HN: Books that teach programming by building a series of small projects?
100 points by newsoul 1 hour ago  | hide | past | favorite | 49 comments 

It is common knowledge that when first learning programming, one should start with small projects to build something real rather than learning rules and syntax of the language only.Which are some of the best books that take a project based approach in teaching programming to a beginner? 
 
  
 
twawaaay 6 minutes ago  
             | next [–] 

My favourite is ""Practical Common Lisp"" by Peter Seibel (https://gigamonkeys.com/book/)Not only is the book free to read (although I would suggest to pay for it if you like it!) The code to parse binary files actually ""inspired"" my design of an actual production application which was very flexible, succinct and, most importantly,  so fast I had to spend a lot of effort convincing people the numbers are actually true.It taught me some important lessons about how you can achieve performance with Lisp languages and the real reasons for the power of macros. Not too shabby for the first book on Lisp I red!
 
reply



  
 
kennyfrc 1 hour ago  
             | prev | next [–] 

There’s quite a few:- Zed Shaw’s Learn More Python the Hard Way[1]- Brian Hogan’s Exercises for Programmers (best for beginners or for learning a new language)[2]- Hal Fulton’s The Ruby Way[3]- Chris Ferdinandi’s Vanilla JS Academy[4]- Marc-Andre Cournoyer’s Great Code Club (it’s old, and the community doesn’t exist anymore, but i think he still maintains it)[5]- A few python books from No Starch Press (notably those authored by Al Sweigart)I learned the most as a beginner  from Zed Shaw’s work, and from reading open source code.Once you’re done with the initial “learn from tutorials” phase, there’s no better resource than reading open source code.[1] https://www.amazon.com/Learn-More-Python-Hard-Way/dp/0134123...[2] https://www.amazon.com/Exercises-Programmers-Challenges-Deve...[3] https://www.amazon.com/Ruby-Way-Programming-Addison-Wesley-P...[4] https://vanillajsacademy.com/[5] https://www.greatcodeclub.com/
 
reply



  
 
lazyant 43 minutes ago  
             | parent | next [–] 

> - A few python books from No Starch Press (notably those authored by Al Sweigart)Yes for example https://nostarch.com/inventwithpython and https://nostarch.com/big-book-small-python-projects
 
reply



  
 
Zhyl 44 minutes ago  
             | parent | prev | next [–] 

Specifically ""Automate the Boring Stuff with Python"" is my favourite Al Sweigart book.
 
reply



  
 
7speter 53 minutes ago  
             | parent | prev | next [–] 

> and from reading open source code.Do you just search for projects in the language your using on github?
 
reply



  
 
kennyfrc 43 minutes ago  
             | root | parent | next [–] 

Yes, and I would typically search for simple projects using keywords such as “micro”, “small”, “tiny”, or “pico”.I would then try to re-implement the “getting started” code in the readme from scratch, like a little programming puzzle. If I can’t figure it out, I‘ll add in some debugger breakpoints, inspect the stack trace to understand how it works, then code the needed methods / classes as I go.If you’re a ruby programmer, soveran’s work in github can be read in a day or two. I specifically like cuba (micro webframework) and mote (microtemplate).
 
reply



  
 
darreninthenet 1 hour ago  
             | parent | prev | next [–] 

Is Shaw's first Python book any good?
 
reply



  
 
danpalmer 30 minutes ago  
             | root | parent | next [–] 

Shaw has had some strong opinions that have gone significantly against the mainstream viewpoint in Python. Whether he's right or not doesn't really matter - as an educator that is targeting the mainstream it's fairly important to stay on that and not introduce personal bias around these things. Unfortunately his criticism was often not very constructive.
 
reply



  
 
kennyfrc 1 hour ago  
             | root | parent | prev | next [–] 

If you’re a pure beginner with zero programming experience, yes. If you’ve programmed a few scripts and have done a flask web app, his first book might be too easy.
 
reply



  
 
cameron_b 54 minutes ago  
             | root | parent | prev | next [–] 

I found his Learn Python3 The Hard Way to be a great series of exercises if you're starting out or starting again. You won't find -everything- on any topic, but the exercises are complete enough to get you rolling and seed the sort of patterns that will make you successful in looking for more material.I had a little bit of Python(2) under my belt from general curiosity, and I've done some C++ in high school so I have some ""CS Theory""There is a ""more"" python version out now 
https://learncodethehardway.org/more-python-book/Amazon ( the ""newer"" version proposed is not, it's his python2 book )
https://www.amazon.com/Learn-More-Python-Hard-Way/dp/0134123...
 
reply



  
 
__warlord__ 1 hour ago  
             | prev | next [–] 

Not a book, but check [Build your own X](https://github.com/codecrafters-io/build-your-own-x), a compilation of well-written, step-by-step guides for re-creating our favorite technologies from scratch.
 
reply



  
 
modernerd 23 minutes ago  
             | prev | next [–] 

For Swift:https://www.apple.com/swift/playgrounds/ (I've had more success introducing programming with this than with any of the links below; it's a very compelling intro for those who already own an iPad/Mac, and the core concepts are generalisable to other languages/environments even if it's specific to Apple's APIs and hardware.)~~~For Python:https://automatetheboringstuff.comhttps://nostarch.com/big-book-small-python-projects~~~For JS:https://eloquentjavascript.net (project-based and for beginners)https://javascript30.com (not for total beginners or self-study, would need a friend/tutor)
 
reply



  
 
tjpnz 37 minutes ago  
             | prev | next [–] 

MUD Game Programming. Not sure how easy obtaining a copy would be in 2023 but it has two projects which I recall being a lot of fun. I never did build a successful MUD but did learn a fair amount about network programming.https://www.goodreads.com/en/book/show/927128.Mud_Game_Progr...
 
reply



  
 
rahuldan 1 hour ago  
             | prev | next [–] 

There is this excellent Github repo for this: https://github.com/codecrafters-io/build-your-own-xIt has a collection of blogs for building various small projects to learn different languages.
 
reply



  
 
asicsp 1 hour ago  
             | parent | next [–] 

Similar resources:* Projectbook https://projectbook.code.brettchalupa.com/* Project Based Learning https://github.com/practical-tutorials/project-based-learnin...
 
reply



  
 
kriro 48 minutes ago  
             | prev | next [–] 

""Hands on Rust"" teaches Rust by building a rogue like game step by step. Before that there's a chapter where you rebuild Flappy Bird from scratch that teaches the ""basics"" before diving into more advanced concepts in the rogue like. I liked the approach and recommend the book but it's fast paced and expects quite a bit from the reader (it's excellent if you have some programming experience already but probably daunting as a true first book imo).
 
reply



  
 
myers 35 minutes ago  
             | parent | next [–] 

I second this one.  This was a great book.There are some follow ups to the final project I likeCompile to WASM so you can run on the web: <https://hands-on-rust.com/2021/11/06/run-your-rust-games-in-...>Port to the Bevy ECS: <https://saveriomiroddi.github.io/learn_bevy_ecs_by_ripping_o...>
 
reply



  
 
forrestbrazeal 19 minutes ago  
             | prev | next [–] 

My Cloud Resume Challenge project [0] and book [1] uses a set of small, stackable mini-projects to introduce beginners to many of the pragmatic skills used in cloud software engineering.[0] https://cloudresumechallenge.dev/docs/the-challenge/
[1] https://cloudresumechallenge.dev/book/
 
reply



  
 
tmtvl 39 minutes ago  
             | prev | next [–] 

Common Lisp: A Gentle Introduction to Symbolic Computation(https://www.cs.cmu.edu/~dst/LispBook/) has you build a number of small applications, like a substitution decipher application, a plotting function, and more, which it calls ""keyboard exercises"".
 
reply



  
 
larve 55 minutes ago  
             | prev | next [–] 

Peter norvig’s paradigms of artificial intelligence programming, despite its age, is a delight.https://norvig.github.io/paip-lisp/#/
 
reply



  
 
newsoul 54 minutes ago  
             | parent | next [–] 

But is that suitable for a beginner?
 
reply



  
 
nesarkvechnep 47 minutes ago  
             | root | parent | next [–] 

Yes, it is. It’s not about machine learning, neural networks but the more approachable AI of the 80s.
 
reply



  
 
globalise83 40 minutes ago  
             | root | parent | next [–] 

Haven't tried his, but can GPT3 write code that results in 1980s style AI?
 
reply



  
 
dartharva 48 minutes ago  
             | prev | next [–] 

I don't know why jtimiclat's comment recommending Al Sweigart's books got downvoted and faded out, they are indeed quite good for beginners wanting to get their hands dirty with programming instead of cramming it in an academic way.Not to mention they're all freely available and give excellent value for both time and money: https://inventwithpython.com/
 
reply



  
 
skizm 14 minutes ago  
             | prev | next [–] 

While on the topic, how about any books or courses that teach building a 3D game engine from scratch? I'm language agnostic, so any language is fine.
 
reply



  
 
theshrike79 1 hour ago  
             | prev | next [–] 

If we're talking a complete beginner with maybe only ""hello world"" level skills I recommend the Lego BOOST set: https://www.lego.com/en-fi/product/boost-creative-toolbox-17...It teaches you step by step the basics of programming (loops, function calls etc) using a scratch-like programming language.
 
reply



  
 
ajoseps 47 minutes ago  
             | prev | next [–] 

This book is still in progress but I've gone through some of the chapters and have enjoyed it. Rust from the Ground Up: https://leanpub.com/rust-from-the-ground-upI was looking for a book that had offline projects I can work on while on flights, and this book focuses on rebuilding linux utilities using rust. The other nice part is that you get a better understanding of linux internals.I believe the author is also responsive on the rust subreddit.
 
reply



  
 
phowat 48 minutes ago  
             | prev | next [–] 

http://landoflisp.com/Was a really fun read.
 
reply



  
 
rg111 28 minutes ago  
             | parent | next [–] 

A similar great book is Realm of Racket.
 
reply



  
 
throwawaybnb123 1 hour ago  
             | prev | next [–] 

Codecademy has projects page: https://www.codecademy.com/projects
 
reply



  
 
olkyts 25 minutes ago  
             | prev | next [–] 

Learning Rust, I use these 2:
1. Command-Line Rust: A Project-Based Primer for Writing Rust CLIs
2. Zero To Production In Rust (it's actually one project)
 
reply



  
 
elias94 22 minutes ago  
             | prev | next [–] 

I would suggest Practical Common Lisphttps://gigamonkeys.com/book/You will create a MP3 database, a web server, a spam filter, a HTML generator. Really practical!
 
reply



  
 
gen_greyface 1 hour ago  
             | prev | next [–] 

check thishttps://news.ycombinator.com/item?id=22299180
Ask HN: What are some books where the reader learns by building projects?
 
reply



  
 
boredemployee 58 minutes ago  
             | prev | next [–] 

>> one should start with small projects to build something real rather than learning rules and syntax of the language only.Yes, but don't overlook the great learning that is gained, at the beginning of any learning, by studying from many different sources (aka good books). It seems that something magical happens when you do this: the confrontation of ideas from different sources can make you better absorb ideas and resolve any doubts that you didn't understand before.
 
reply



  
 
LastTrain 50 minutes ago  
             | prev | next [–] 

When I was young, I went about it the opposite way. I thought of some small project and researched how to implement it, then moved on to something bigger. Everybody is different of course, but I felt like I learned more having to figure out how to go about implementing each myself.
 
reply



  
 
werber 1 hour ago  
             | prev | next [–] 

https://eloquentjavascript.net/ is nice and project based, it builds up from 0
 
reply



  
 
bartvk 49 minutes ago  
             | prev | next [–] 

Learn Swift in 100 days. It start with the basics using Swift Playgrounds. But as soon as possible, it starts with building apps.https://www.hackingwithswift.com/100
 
reply



  
 
saperyton 1 hour ago  
             | prev | next [–] 

The Advanced Beginner blog post series by Robert Heaton is great.
 
reply



  
 
asicsp 1 hour ago  
             | parent | next [–] 

Link: https://robertheaton.com/2018/12/08/programming-projects-for...
 
reply



  
 
devd00d 52 minutes ago  
             | prev | next [–] 

Not quite programming but certainly covers the ""small projects"" part. This is what taught me the basics of Unreal Engine: https://www.youtube.com/watch?v=k-zMkzmduqI
 
reply



  
 
pixelmonkey 1 hour ago  
             | prev | next [–] 

Not a book, but CS50x is a freely available course with high quality videos and lecture notes. The lecture exercises are programming projects of sorts. It teaches programming from scratch with C and then Python. Plus, since a lot of students take it, there are lots of online resources with tips, like the CS50 subreddit.https://cs50.harvard.edu/x/2023/
 
reply



  
 
cehrlich 52 minutes ago  
             | parent | next [–] 

CS50x and CS50web are fantastic. web is a bit outdated but it doesn't matter because the projects are so good.
 
reply



  
 
nemoniac 1 hour ago  
             | prev | next [–] 

Nand2tetris https://www.nand2tetris.org/
 
reply



  
 
tsm 1 hour ago  
             | prev | next [–] 

Software Design by Example (https://www.routledge.com/Software-Design-by-Example-A-Tool-...)Available free online at https://third-bit.com/sdxjs/
 
reply



  
 
basicallydan 1 hour ago  
             | prev | next [–] 

If they don't exist, this is a good idea for anybody looking for a project to write a programming book
 
reply



  
 
poszlem 1 hour ago  
             | prev | next [–] 

I can recommend ""Programming: Principles and Practice Using C++"", although the fact that it uses C++ as a first language can be a downside to some.
 
reply



  
 
kgwxd 25 minutes ago  
             | prev | next [–] 

Programming Games for Atari 2600 https://forums.atariage.com/topic/339819-upcoming-book-on-at...
 
reply



  
 
rg111 25 minutes ago  
             | prev | next [–] 

I recently finished the book that teaches programming by developing games using DragonRubyGameToolkit. Really loved this book._____Python Crash Course by Eric Matthes has a section dedicated to projects.I really liked this book.This is what taught me Python.I knew C before.
 
reply



  
 
hgsgm 48 minutes ago  
             | prev [–] 

checkio.com for increasing challenges in Python or JS.But also pretty much every book does that.
 
reply







Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
Search:  


"
https://news.ycombinator.com/rss,New Mac Mini (M2) and MacBook Pro (M2 Pro/Max),https://www.apple.com/mac/,Comments,"
Mac





Get 3% Daily Cash back with Apple Card. And pay for your new Mac over 12 months, interest‑free when you choose to check out with Apple Card Monthly Installments.* Learn more







Introducing the new MacBook Pro and Mac mini.


Watch the announcement 





New
MacBook Pro

Mover. Maker. Boundary breaker.






Order now



Learn more


Available starting 1.24










New
Mac mini

More muscle. More hustle.






Order now



Learn more


Available starting 1.24














A gift for every wish.
Find something for everyone this Lunar New Year.

Shop the gift guide 






Which Mac is right for you?




Laptop
Desktop













														MacBook Air
														M1 chip





Buy
Learn more



View in AR








13.3”
Retina display1



Apple M1 chip


8-core
CPU



7-core
GPU



Up to 16GB unified memory


2TB
Maximum configurable storage2



Up to 18 hours battery life3



720p FaceTime HD camera



Three‑mic array Stereo speakers


2.8 lb.
Weight



Touch ID








														MacBook Air
														M2 chip





Buy
Learn more



View in AR








13.6”
Liquid Retina display1



Apple M2 chip


8-core
CPU


Up to
10-core
GPU



Up to 24GB unified memory


2TB
Maximum configurable storage2



Up to 18 hours battery life4



1080p FaceTime HD camera



Three‑mic array Four-speaker sound system with Spatial Audio


2.7 lb.
Weight



Touch ID








														MacBook Pro 13”
														





Buy
Learn more



View in AR








13.3”
Retina display1



Apple M2 chip


8-core
CPU


10-core
GPU



Up to 24GB unified memory


2TB
Maximum configurable storage2



Up to 20 hours battery life5



720p FaceTime HD camera



Studio-quality three‑mic array Stereo speakers with Spatial Audio


3.0 lb.
Weight



Touch Bar and Touch ID







New 
														MacBook Pro 14” and 16”
														





Order now
Learn more



View in AR








14.2” or 16.2”
Liquid Retina XDR display1



Apple M2 Pro or Apple M2 Max chip


Up to
12-core
CPU


Up to
38-core
GPU



Up to 96GB unified memory


8TB
Maximum configurable storage2



Up to 22 hours battery life6



1080p FaceTime HD camera



Studio-quality three‑mic array Six-speaker sound system with Spatial Audio


3.5 lb. or 4.7 lb.
Weight



Touch ID












														iMac
														





Buy
Learn more



View in AR









Apple M1 chip


8-core
CPU


Up to
8-core
GPU



Up to 16GB unified memory
For increased performance and power efficiency


2TB
Maximum configurable storage2


4.5K
Retina display7
218 ppi



1080p FaceTime HD camera
With the image signal processor of M1 for drastically improved performance



Configurable with Magic Keyboard with Touch ID and Numeric Keypad







New 
														Mac mini
														





Order now
Learn more



View in AR









Apple M2 or Apple M2 Pro chip


Up to
12-core
CPU


Up to
19-core
GPU



Up to 32GB unified memory
For increased performance and power efficiency


8TB
Maximum configurable storage2


—Not available


—Not available


—Not available








														Mac Studio
														





Buy
Learn more



View in AR









Apple M1 Max orApple M1 Ultra chip


Up to
20-core
CPU


Up to
64-core
GPU



Up to 128GB unified memory
For increased performance and power efficiency


8TB
Maximum configurable storage2


—Not available


—Not available


—Not available








														Mac Pro
														





Buy
Learn more



View in AR









Intel Xeon W processor


Up to
28-core
CPU


Up to
AMD
Radeon Pro W6800X Duo GPU



Up to 1.5TB memory


8TB
Maximum configurable storage2


—Not available


—Not available



Magic Keyboard with Numeric Keypad






Compare all Mac models
Shop Mac









Behind the Mac
Hear the Force.
Learn how the pros at Skywalker Sound push the limits of sonic storytelling.
Watch the film 

















Get 3% Daily Cash back with Apple Card.
And pay for your new Mac over 12 months, interest‑free when you choose to check out with Apple Card Monthly Installments.*
Learn more 








Accessories
Explore Mac accessories.


Shop







Apple Trade In
Get credit toward a new Mac.
Just trade in your eligible computer for credit or recycle it for free. It’s good for you and the planet.8


Find your trade‑in value 











Fast delivery or pickup

Enjoy two‑hour delivery from an Apple Store, free delivery, or easy pickup.
Learn more about free delivery




Pay monthly at 0% APR

You can pay over time when you choose to check out with Apple Card Monthly Installments.*
Learn more about Monthly Installments




Get help buying

Have a question? Call a Specialist or chat online. Call 1‑800-MY‑APPLE.
Contact us







What makes a Mac a Mac?






Why Mac
Incredible power. Incredibly simple.

Learn more 






Continuity
All your devices.One seamless experience.

Learn moreabout your devices working together 






macOS Ventura
Works smarter.Plays harder.Goes further.

Learn more 













Built-in Apps
Powerful creativity and productivity tools live inside every Mac — apps that help you explore, connect, and work more efficiently.









											Safari
										




											Photos
										




											iMovie
										




											GarageBand
										




											Pages
										




											Numbers
										




											Keynote
										








Safari


Safari has innovative features that let you enjoy more of the web. In even more ways. Built-in privacy features help protect your information and keep your Mac secure. An updated start page helps you easily and quickly save, find, and share your favorite sites. And Siri suggestions surface bookmarks, links from your reading list, iCloud Tabs, links you receive in Messages, and more.
Learn more about Safari





Photos


Keep your growing library organized and accessible. Perfect your images and create beautiful gifts for sharing. And with iCloud Photos, you can store a lifetime’s worth of photos and videos in the cloud.
Learn more about Photos





iMovie


Tell stories like never before. A simple design and intuitive editing features make it easy to create beautiful 4K movies and Hollywood-style trailers.
Learn more about iMovie





GarageBand


The easiest way to create great-sounding songs on your Mac. With an intuitive interface and access to a complete sound library, it’s never been easier to learn, play, record, and share music like a pro.
Learn more about GarageBand





Pages


This powerful word processor gives you everything you need to create documents that look beautiful. And read beautifully. It lets you work seamlessly between Mac, iOS, and iPadOS devices. And work effortlessly with people who use Microsoft Word.
Learn more about Pages





Numbers


Create sophisticated spreadsheets with dramatic interactive charts, tables, and images that paint a revealing picture of your data. Work seamlessly between Mac, iOS, and iPadOS devices. And work effortlessly with people who use Microsoft Excel.
Learn more about Numbers





Keynote


Bring your ideas to life with beautiful presentations. Employ powerful tools and dazzling effects that keep your audience engaged. Work seamlessly between Mac, iOS, and iPadOS devices. And work effortlessly with people who use Microsoft PowerPoint.
Learn more about Keynote








Safari




Photos




iMovie




GarageBand




Pages




Numbers




Keynote





































Pro Apps
For professionals ready to push their creativity, these industry-leading apps offer maximum control over editing, processing, and output of music and film.









											Logic Pro
										




											MainStage
										




											Final Cut Pro
										




											Motion
										




											Compressor
										








Logic Pro


Logic Pro puts a complete recording and MIDI production studio on your Mac, with everything you need to write, record, edit, and mix like never before. And with a huge collection of full-featured plug-ins along with thousands of sounds and loops, you’ll have everything you need to go from first inspiration to final master, no matter what kind of music you want to create.
Learn more about Logic Pro





MainStage


Take your Mac to the stage with a full-screen interface optimized for live performance, flexible hardware control, and a massive collection of plug-ins and sounds that are fully compatible with Logic Pro.
Learn more about MainStage





Final Cut Pro


Built to meet the needs of today’s creative editors, Final Cut Pro offers revolutionary video editing, powerful media organization, and incredible performance optimized for Mac computers and macOS.
Learn more about Final Cut Pro





Motion


Motion is a powerful motion graphics tool that makes it easy to create cinematic 2D and 3D titles, fluid transitions, and realistic effects in real time.
Learn more about Motion





Compressor


Add power and flexibility for exporting projects from Final Cut Pro. Customize output settings, work faster with distributed encoding, and easily package your film for the iTunes Store.
Learn more about Compressor








Logic Pro




MainStage




Final Cut Pro




Motion




Compressor































The Mac App Store features rich editorial content and great apps for Mac.	Explore the Mac App Store







Get more out of Mac






Apple One
Bundle up to six Apple services. And enjoy more for less.


Try it free9 

Learn more 













Apple TV Plus
Get 3 months of Apple TV+ freewhen you buy a Mac.


Try it free10 

Learn moreabout Apple TV Plus 




















Apple Pay
The safer way to make secure, contactless purchases in stores and online.


Learn moreabout Apple Pay 










Apple Arcade
Get 3 months of Apple Arcade free when you buy a Mac.


Try it free11 

Learn more 










Apple News Plus
Get 3 months of Apple News+ free when you buy a Mac.12


Learn moreabout Apple News Plus 







Apple Gift Card
For everything and everyone.


Learn moreabout Apple Gift Card 

Buy 










Apple at Work
Get the power to take your business to the next level.


Learn about Apple at Work 

Learn more about Mac for business 










Apple and Education
Empowering educators and students to move the world forward.


Learn about Apple and Education 













Upgrade to start your free trial.


Get the latest iOS 

Get the latest iPadOS 

Get the latest tvOS 

Get the latest macOS 



"
https://news.ycombinator.com/rss,The FBI Identified a Tor User,https://www.schneier.com/blog/archives/2023/01/the-fbi-identified-a-tor-user.html,Comments,"





The FBI Identified a Tor User - Schneier on Security






































							Schneier on Security						






						Menu						

Blog
Newsletter
Books
Essays
News
Talks
Academic
About Me
 



Search

Powered by DuckDuckGo





Blog

Essays

Whole site

Subscribe



 



HomeBlog 


The FBI Identified a Tor User
No details, though:
According to the complaint against him, Al-Azhari allegedly visited a dark web site that hosts “unofficial propaganda and photographs related to ISIS” multiple times on May 14, 2019. In virtue of being a dark web site—­that is, one hosted on the Tor anonymity network—­it should have been difficult for the site owner’s or a third party to determine the real IP address of any of the site’s visitors.
Yet, that’s exactly what the FBI did. It found Al-Azhari allegedly visited the site from an IP address associated with Al-Azhari’s grandmother’s house in Riverside, California. The FBI also found what specific pages Al-Azhari visited, including a section on donating Bitcoin; another focused on military operations conducted by ISIS fighters in Iraq, Syria, and Nigeria; and another page that provided links to material from ISIS’s media arm. Without the FBI deploying some form of surveillance technique, or Al-Azhari using another method to visit the site which exposed their IP address, this should not have been possible.
There are lots of ways to de-anonymize Tor users. Someone at the NSA gave a presentation on this ten years ago. (I wrote about it for the Guardian in 2013, an essay that reads so dated in light of what we’ve learned since then.) It’s unlikely that the FBI uses the same sorts of broad surveillance techniques that the NSA does, but it’s certainly possible that the NSA did the surveillance and passed the information to the FBI.

Tags: dark web, de-anonymization, FBI, hacking, NSA, privacy, surveillance, Tor 

Posted on January 17, 2023 at 7:02 AM			•
			3 Comments 



Comments



thorvold •

					
						January 17, 2023 8:27 AM					

The filing mentions that it is referencing a purported Top Secret document “Exhibit 2” from the timeframe of 2013.  Based on that info, I am assuming this is a document purportedly from the Edward Snowden leak.  The current policy of the government is that a classified document that is leaked is still classified until officially de-classified at a later date. Public access != Unclassified.  The government is not going to acknowledge that the document is indeed classified in an open context because that would then confirm that the information contained in the document is likely true.  Potentially the “fact of” information that the lawyer obtained in that document and then references in his motion may also be classified.
This would make the motion a derivatively classified document based on the inclusion of classified information in it.  If the government managed to convince the judge that the information was still classified, then that would show the need required to seal the motion, without actually stating in open writing that the document was indeed true.






Will •

					
						January 17, 2023 9:29 AM					

The gist of the article is that the US government could have compromised the website, or the website may have been a honeypot, or they may have ways of unmasking TOR traffic generally.
But isn’t it more likely that they compromised the machine he used to access the dark web instead?






Winter •

					
						January 17, 2023 9:44 AM					

A known way to re-identify an IP address over Tor is when the user enables javascript support. If you do so, it is advised to use the browser in a VM with the IP address shielded. This is especially true when the user does not use the Tor browser, but accesses Tor using SOCKS5 on a regular browser.
If the FBI already had access to the dark web site, it could install Javascript code to get at the IP address.
Another, fairly unlikely way, is to look for searches on public fora in the open for certain websites just before the access.
A real killer would be asking for translating the offending page in open Google Translate just after you accessed it via Tor. Google can be fickle when used over Tor, it generally blocks access from Tor.






			Subscribe to comments on this entry		


Leave a comment Cancel replyLoginName 
Email 
URL: 
 Remember personal info?


		Fill in the blank: the name of this blog is Schneier on ___________ (required):	



Comments:





Allowed HTML
	<a href=""URL""> • <em> <cite> <i> • <strong> <b> • <sub> <sup> • <ul> <ol> <li> • <blockquote> <pre>
	Markdown Extra syntax via https://michelf.ca/projects/php-markdown/extra/




 

Δ 

← Hacked Cellebrite and MSAB Software Released 
Sidebar photo of Bruce Schneier by Joe MacInnis.



About Bruce SchneierI am a public-interest technologist, working at the intersection of security, technology, and people. I've been writing about security issues on my blog since 2004, and in my monthly newsletter since 1998. I'm a fellow and lecturer at Harvard's Kennedy School, a board member of EFF, and the Chief of Security Architecture at Inrupt, Inc. This personal website expresses the opinions of none of those organizations.
Related Entries

Friday Squid Blogging: How to Buy Fresh or Frozen SquidFriday Squid Blogging: Squid FetishFriday Squid Blogging: Grounded Fishing Boat Carrying 16,000 Pounds of SquidFriday Squid Blogging: Injured Giant Squid and PaddleboarderFriday Squid Blogging: Squid in ConcertFriday Squid Blogging: China Bans Taiwanese Squid Imports

Featured Essays

The Value of EncryptionData Is a Toxic Asset, So Why Not Throw It Out?How the NSA Threatens National SecurityTerrorists May Use Google Earth, But Fear Is No Reason to Ban ItIn Praise of Security TheaterRefuse to be TerrorizedThe Eternal Value of PrivacyTerrorists Don't Do Movie Plots 
More EssaysBlog Archives

Archive by Month100 Latest Comments
Blog Tags3d printers9/11A Hacker's MindAaron Swartzacademicacademic papersaccountabilityACLUactivismAdobeadvanced persistent threatsadwareAESAfghanistanair marshalsair travelairgapsal QaedaalarmsalgorithmsalibisAmazonAndroidanonymityAnonymousantivirusApacheAppleApplied Cryptographyartificial intelligenceMore TagsLatest BookMore Books




 




Blog
Newsletter
Books
Essays
News
Talks
Academic
About Me
 

















"
https://news.ycombinator.com/rss,Dijkstra Maps Visualized,http://www.roguebasin.com/index.php/Dijkstra_Maps_Visualized,Comments,"



Dijkstra Maps Visualized - RogueBasin



















Dijkstra Maps Visualized

From RogueBasin



Jump to navigation
Jump to search
Contents

1 Dijkstra Maps Visualized

1.1 -- The basics --
1.2 -- Multiple sources --
1.3 -- Variable strengths, and what distance really means --
1.4 -- Fleeing AI --
1.5 -- Automatic exploration --
1.6 -- Cheap mouse pathing --
1.7 -- Moving into optimal range --
1.8 -- Hazard avoidance --




Dijkstra Maps Visualized
Dijkstra maps are awesome. They can teach your AI some clever new tricks - and cheaply, too, because the same map can be used for any number of actors. And AI is just the beginning: Dijkstra maps can facilitate automatic exploration, pathfind-to-cursor, and dungeon generation, too.
So, what is a Dijkstra map? Take a look:

At its most basic, it indicates distances. In the above image, the map shows how far from the player ('@') each cell is. (The colors here are just for demonstrative purposes. The numbers are the important part.) Some notes on the implementation can be found here.
Now, I'd like to encourage you to read this fantastic article by Pender(Brian Walker), the creator of Brogue. This article is based heavily on the ideas presented there, and I'll go into more detail on how they can be achieved.
Finished reading that one? Sounds amazing, right? Let's see if we can get those examples working.

-- The basics --
One of the simplest applications of Dijkstra maps is making enemies beeline for the player, taking the shortest path at all times. The image above is all you need. These goblins - 

- can, each turn, simply check each cell adjacent to them, and step to any that has the lowest value.
 
This map only needs to be updated when the player moves, like this:


-- Multiple sources --
They work great! If you have multiple sources, the resulting map will lead toward whichever is closest. In this example, we add gold. Goblins now want to attack the player AND collect gold:

(The map will be updated whenever the player moves or gold is collected, so the goblins don't aim for a target that's no longer there.)

-- Variable strengths, and what distance really means --
So, in the previous example, our goblins were happy to collect gold or attack the player, whichever was closest, just by seeking lower values on the map. But what if we want our goblins to be exceptionally greedy, willing to walk farther to reach gold even if the player is actually closer? Here's how.
Instead of starting all of our sources at a value of 0, give the more desirable ones a lower value - let's use -4 for the gold while keeping the player at 0. Like this:

(Those gray numbers are negatives, so a goblin seeking lower values would move from a 0 to a -1 and so forth.)
Why does this work? By starting the gold at a value of -4, we're treating it as though it were closer than it really is. If a goblin is 7 cells away from gold, and 3 cells away from the player, the -4 modifier means that the goblin will see them as equidistant, and be equally likely to approach either one.

-- Fleeing AI --
Now that we know how to make enemies approach, let's look at making them flee. What happens if we take the approach map and have them move toward higher numbers? (i.e., darker colors?)

They just end up in the corners, and can't escape even when the player is right next to them. Here's how to fix that:
First, take the existing approach map and multiply each value by a number close to -1.2. This effectively flips the map so that moving toward the lower numbers (lighter colors) takes you away from the source(s) instead of toward the source(s).

Then, we rescan that map. It's the same as the basic scan, but we use whatever values happen to be in the map already. After that, we end up with this:

Observe what happened to the spaces around corners and doorways.
Note also that what we've effectively done is to create a new map using the farthest tiles as the sources, and giving them a bonus for being farther away - the bonus is the 0.2 part of the -1.2 multiplication.
Changing that coefficient from -1.2 to a stronger number can have a big effect on the result: The bigger the coefficient, the ""closer"" a distant cell will be, and the more it'll ""pull"" fleeing monsters toward it. Here's what a coefficient of -1.6 looks like:

If the coefficient is too high, the most distant cells will dominate the whole map - a fleeing monster will ONLY want to move to the FARTHEST cell if this happens. If the coefficient is too low, the map won't change very much from the ""bump into corners"" version. Experiment with different values to see which you like!

-- Automatic exploration --
Autoexplore is really easy! Just use every unseen cell as a source, and you get this:

The player can now move toward lower values and automatically uncover new territory. Keep doing this each turn until a key is pressed or until something happens (an enemy comes into view; a message is generated; the player takes damage).
(Note that this even accounts for the extra turn required to open a door ('+') before you move through it, by assigning it a cost of 2 instead of 1.)

-- Cheap mouse pathing --
Let's say you want to show a visible path onscreen as the player moves the mouse (or other) cursor across the map. Dijkstra maps can provide an optimization so you don't need to run a normal pathfinder (like A* or regular Dijkstra pathfinding) every time the cursor moves to a new cell.

By calculating a single Dijkstra map when the cursor moves onto the map (for the first time on each turn), you can simply roll from the cursor to the player to find a path (then, of course, reverse that path so it leads from the player to the cursor).
(I recommend having a way to ensure that the path you see is the path you take - I prefer a deterministic pathfinding routine for the player, and a randomized one for enemies.)
Like autoexplore, this one treats unexplored cells like they're regular floors, so it's possible to choose an impossible path. That's fine - you'll just need to stop the player once it's clear that the chosen path leads into a wall.


-- Moving into optimal range --
Want your ranged enemies to stay at a certain range? Start with a Dijkstra map from the player, and then create another one - the source cells will be the cells on the first map with the desired range:

But, you probably want to go one step further, and say that the source cells will be the cells on the first map with the desired range...to which the player has line of sight:


-- Hazard avoidance --

If the player can swim, but only for a few turns, you can use a map like this one, calculated just once when the map is created. Track the remaining swimming turns, and if the player tries to move to a cell with a value greater than the remaining turns, you can prevent that move and give a warning.

I hope these examples help to illustrate just how useful Dijkstra maps can be. Good luck!





Retrieved from ""http://roguebasin.com/index.php?title=Dijkstra_Maps_Visualized&oldid=45312""
Category: Developing



Navigation menu




Personal tools


Create accountLog in






Namespaces


PageDiscussion






Variants










Views


ReadView sourceView history






More







Search



















Navigation


Main PageCommunity portalRecent changesRandom pageHelp





Tools


What links hereRelated changesSpecial pagesPrintable versionPermanent linkPage information






 This page was last edited on 24 May 2017, at 16:22.


Privacy policy
About RogueBasin
Disclaimers






"
https://news.ycombinator.com/rss,The Next Button,https://maxhodak.com/nonfiction/2023/01/14/the-next-button.html,Comments,"




Max Hodak - The Next button









Max Hodak — 
Writings

The Next button
January 2023

With the enormous advances in machine learning over the last few years, it feels like we are now not only on the cusp of big things, but that the world is already a genuinely different place than it was just a little while ago. I mean, how does even just assigning homework work in a world of LLMs?
There’s one product I haven’t seen discussed yet that I’m really looking forward to. I call it the Next button. It’s an app one might install on their computer that can access the keyboard and mouse and which puts just a single button in the system tray. When you click it, the model does whatever it thinks is most useful to do next, conditioned on things like the contents of one’s email accounts, calendar, files on my computer and cloud storage, text messages — ideally, any digital content associated with me.
Actions the Next button might take include:

Generically, opening Gmail, picking an active thread, and drafting a reply for me to send.
Generating a pull request to fix some bugs or maybe pick off an Asana task in one of the web app projects.
Opening Google Docs and drafting that CBER/CDRH jurisdictional clarification letter I’ve been dragging my feet on sending to FDA.
Opening Keynote and drafting a few slides using assets from my computer and Google Drive for that real estate developer I’ve been discussing a new facility with, who is waiting on a company overview.
Generating a Google Sheet with a handful of itinerary options for a complex trip I have to do sometime in the spring for campus recruiting, and then booking it.
Reading the fact that there are a bunch of job offers to go out off of my Asana inbox, and opening tabs with the DocuSign offers I need to sign.

It doesn’t take over any of the actual creative parts of my job — those still require input from me — and at every step there is an opportunity for human review and proofreading. I would ultimately have to take the actions the model proposes. But it sure makes most things a whole lot easier and faster.
I suspect all of this will be possible in the next 12-18 months. Tab completion for your work life is probably coming. The future is going to be weird.


      © 2008 - 2023
    







"
https://news.ycombinator.com/rss,Memory Safety Approaches Speed Up and Slow Down Development Velocity,https://verdagon.dev/blog/when-to-use-memory-safe-part-2,Comments,"







How Memory Safety Approaches Speed Up and Slow Down Development Velocity





































Languages ∩ Architecture


RSS
Github




            Sponsor us on GitHub!
          
r/Vale
Twitter
Discord








How Memory Safety Approaches Speed Up and Slow Down Development Velocity
Part 2 of the Memory Safety Expedition

Jan 16, 2023
 — 
Evan Ovadia
 — 
Sponsor us on GitHub!




Every March, developers worldwide each try to make a roguelike game in less than 168 hours as part of the 7DRL Challenge. Roguelike games are hard, involving procedural level generation, pathfinding, line-of-sight algorithms, and more dijkstra maps than you can imagine.









Most people don't survive the week. 0




Years of 7DRL challenges has taught me that development velocity is the most important thing to optimize for. It doesn't matter how perfect your code is if it doesn't make it into the hands of the players in time.




A few weeks ago I wrote the first part of our little expedition, which explored the various memory safety approaches. Today, let's talk about how they might help or harm development velocity!




You might learn something new, like:



Modern non-memory-safe languages can use recent techniques to zero in on memory errors so quickly, that they can be fast to develop in.


Reference counting has a hidden superpower that can help track down logic problems.


Borrow checking can sometimes help and sometimes hurt development velocity, even after the initial learning curve.





 Why it's important



Developer velocity is important. Not just for delivering a complete game in 7 days, but in a lot of every day software engineering situations too:



A startup needs to get a working product into the hands of its users before the investment money dries up.


One needs developer velocity to be able to respond to unforeseeable events, such as market opportunities or new regulatory requirements. 1


A company needs to reduce costs to stay in business, including reducing development costs.





Some napkin math to illustrate that last one:



Google used 15.5 terawatt hours of electricity in 2020, most which went to data centers. We'll conservatively assume rather expensive electricity ($0.199/kwh for CA). That comes out to $3.085 billion. 


Google has 27,169 software engineers. Some are higher, but let's conservatively use the entry level average yearly compensation which is currently $178,751. That comes out to $4.856 billion. 2





As you can see, software development can be much more expensive than power usage, so it can make sense to primarily optimize for development velocity. 3 4




 The Language Factor

The choice of language, and its memory safety approaches, is a big factor in development velocity.




There are generally four approaches to memory safety:



Manual memory management (MMM), like in C, Ada, Zig, Odin, etc.


Borrow checking, like in Rust, Cone, Cyclone, etc.


Garbage collection (GC), like in Java, Go, Python, Javascript, etc. 5


Reference counting (RC), like in Swift, Nim, Lobster, etc.





There's also a fifth approach, generational references with regions which we'll talk about elsewhere; this series is comparing the more traditional approaches.




But by the end of this, you'll have a much better idea of which is best for a particular situation, whether it be a game development challenge, web server, or anything else.




 What Actually Is Developer Velocity?



Development velocity is a nebulous concept, but I would say it's how fast we can expand, modify, and maintain our codebase to deliver value to the user and not cause too much collateral damage.




The ""deliver value to the user"" is very important here. It means doing something that will help the user in a specific way. Without that clear goal, we can get caught up in making our features absolutely perfect according to some arbitrary artistic criteria. As anyone who has launched a product can tell you, it's better to have two solid, tested, flexible features instead of one absolutely perfect one.




The ""too much"" is also important. There is often a tradeoff between moving fast and keeping things absolutely correct. A big part of software engineering is weighing the value of new features against the risk and relative severity of any bugs that might slip into production for a while.




With that in mind, let's see how the various approaches do!




 Manual Memory Management (MMM) can be slow



People that come to C from higher-level languages often think that they should code like Java but manually inserting malloc and free calls. Let's call this style ""naive C"".




One can become pretty skilled in mentally tracking where these calls should go, but the approach tends to fall apart after a while: it isn't resilient to changes and refactoring. It's common to accidentally change the code and break some previous assumptions, resulting in a memory bug.




Memory bugs are notoriously difficult to solve. Naive C doesn't have great development velocity.




 It can also be fast

...and that's why experienced C developers don't really use this ""naive C"" style of coding.




Instead, they:



Use safer practices which drastically reduce memory problems, such as Architected MMM where we use arrays and arenas instead of malloc and free, always copying in/out of tagged unions instead of pointing into them, and so on.


Use static analysis to enforce safer practices, such as SPARK.


Use things like Address Sanitizer and Valgrind to detect buffer overflows, dangling pointers, memory leaks, and double-frees.


Use special allocators that don't reuse memory, to better detect problems.


If using C++, using RAII to automatically prevent double-free and memory leaks at compile-time.





With these, developer velocity can be stellar even with an MMM language. Modern MMM languages even include these kinds of mechanisms, such as Zig's Release-Safe mode.




There is a slight cost to these memory approaches. Address Sanitizer increases run-time by 73%. This makes it almost as slow as garbage collection. 6




However, that doesn't matter if we can just enable it in debug mode to get the error detection improvements for developer velocity and turn them off in release mode, just like assertions. As we talked about in Part 1, we wouldn't want this for a public-facing server, but it's a great tradeoff for many games and settings like webapps and mobile apps where any security issues can be sandboxed away. 7




Google Earth proved that this strategy can work well. In an average Google Earth quarter, only 3-5% of bug reports were traceable to memory safety problems, and Address Sanitizer made them trivial to reproduce in development mode.




Another developer velocity benefit from MMM languages is that they aim to be as simple as possible to keep compile times low. Languages like C, Zig, and Odin tend to compile much faster than more complex languages like Scala and Rust. This greatly helps developer velocity. 8 On top of that, Zig's simplicity helps its ability to hot-reload code changes which could make it one of the fastest native languages to develop in.





How Memory Safety Approaches Speed Up and Slow Down Development Velocity


 Why it's important


 The Language Factor


 What Actually Is Developer Velocity?


 Manual Memory Management (MMM) can be slow


 It can also be fast


 Future MMM Improvements


 Borrow Checking




 Drawback: Prototyping and Iterating


 Benefit: Concurrency


 Drawback: Unsupported Patterns


 Benefit: Top-Down, Flattened Architectures


 Drawback: Leaky Abstractions and API Stability




 Borrow Checking and Beyond


 Garbage Collection


 GC'd code can be more correct


 GC can be slow in specific domains


 Garbage Collection's Future


 Reference Counting


 Conclusions




Side Notes
(interesting tangential thoughts)




Notes [–]
Notes [+]
0
1
2
3
4
5
6
7
8



0


By this I mean, most people don't have a finished game by the end. But they still celebrate with the rest of us, after an honorable struggle!




1


We also need developer velocity to develop fast enough to compensate for product managers that wildly underestimate how long it will take to make something.




2


Just to keep it simple, let's not include benefits or bonuses, which make it even higher.




3


This of course varies by company and team; a startup will have much higher development costs, and a company like Oracle would probably have more server costs.




4


Note that there are many aspects of development velocity that aren't related to memory safety, and also many uses of electricity that aren't doing garbage collection or reference counting.




5


By ""garbage collection"" I'm specifically referring to tracing garbage collection.




6


 From TheNewStack:



Java: 1.85x run time


Go: 2.83x run time


Haskell: 3.55x run time







7


This works particularly well for apps that only talk to a trusted first-party server, which includes most apps. It doesn't work as well for programs where clients indirectly send each other data, such as multiplayer first person shooter games.




8


I can speak from experience; every time I have a project that takes more than five seconds to compile, I tend to get distracted by Reddit or something shiny.











 Future MMM Improvements

There are a few approaches on the horizon that improve upon MMM even more.




To address more of the memory unsafety in MMM languages, CHERI detects any memory safety problems at run-time using capabilities.




From Microsoft Security Response Center:


We’ve assessed the theoretical impact of CHERI on all the memory safety vulnerabilities we received in 2019, and concluded that in its current state, and combined with other mitigations, it would have deterministically mitigated at least two thirds of all those issues.




In other words, CHERI can reduce memory-unsafety related slowdowns by two thirds, which is pretty incredible. Arm CPUs are even starting to have hardware support for it, bringing its run-time overhead down to 6.8%.




That remaining one third is solved by Vale which takes this approach even further in the form of its generational references. By building it into the language itself, it can know exactly when it does or doesnt need to perform a check, and by adding native region support it can theoretically skip the vast majority of them, bringing its run-time overhead even closer to zero, within the noise of any C or C++ program. With that kind of speed, we can leave the protections enabled even in release mode.










 Borrow Checking

Borrow checking is another mechanism that can be added to a low-level language to protect against memory problems, similar to SPARK. Rust is the main language with borrow checkers, though there are some newer languages that are designing borrow checkers with better development velocity.




Its main advantages are that:



It detects memory safety bugs, like the aforementioned 3-5% of Google Earth bugs, at compile time. That means that instead of getting 80 bug reports, they might have only 77 if they used Rust.


The borrow checker saves time when using concurrency.


It tends to default us into architectures that, in certain situations, help us develop faster.





In a lot of situations, Rust can be pretty stellar for development velocity. There are plenty of folks (such as here and here) which praise Rust's velocity, especially against languages like C and Python.




But that's not that surprising; neither have as strong of a static type system or the generics that almost every modern GC'd and MMM 9 language has. So how does borrow checking fare against more modern languages and practices?




On the other side, borrow checking has some considerable developer velocity drawbacks which make it much slower than languages with garbage collection, and sometimes even slower than MMM. 10




I'll link to some quotes and examples (green to emphasize that they're just anecdotes) to complete the picture. Note that these aren't data, just everyday people's experiences. 11 Keep in mind that these are mostly comparing borrow checking to garbage collection.




From Matt Welsh:


Rust has made the decision that safety is more important than developer productivity. This is the right tradeoff to make in many situations — like building code in an OS kernel, or for memory-constrained embedded systems — but I don’t think it’s the right tradeoff in all cases, especially not in startups where velocity is crucial.




In the words of another software architect:


It gives me such a great feeling of safety and control over your process, but at the end of the day it can be very tedious. The real killer is when I'm trying to prototype. If I don't already know what my interfaces are going to look like, Rust really slows me down. Compile times don't help here either.




We often write it off as just a learning curve problem, but it's apparently true even for more experienced rustaceans.




These weaknesses are can be subtle, and often don't appear in small projects when it's still easy to change a program's architecture. They start to reveal themselves more in larger projects, as a project starts scaling across multiple developers. As one user says, ""Six weeks into the whole thing and people spend more time un-breaking builds than writing code.""




Note that this doesn't mean borrow checking is a bad thing. It just means that to get its benefits, you'll have to pay some development velocity costs.




We don't often hear about the underlying reasons, 12 but knowledge of language design and software architecture helps us identify some of the root causes:



The borrow checker adds extra constraints that are usually unnecessary in other paradigms 13 which causes artificial complexity and extra refactoring, which slows developer velocity.


Forced coupling, which means that refactors and changes in one area will unnecessarily cause refactors and changes in neighboring code.


It doesn't support a lot of the patterns which enable better decoupling and looser architectures.


The borrow checker makes mutability inherently leaky, causing it to leak through abstractions.





These are rather tricky concepts to grasp, so let's explore these a little more.







Notes [–]
Notes [+]
9
10
11
12
13



9


Zig has comptime and Odin has parametric polymorphism.




10


Keep in mind that a lot of Rust's developer velocity (especially compared to C) doesn't come from borrow checking, but from having a much stronger static type system and generics, which GC languages and most newer MMM languages also have nowadays.




11


I would love to see an actual experiment measuring the actual developer velocity between different languages. I'm not sure how they would factor out the learning curve costs, but it's probably possible.




12


And there's a surprising lack of articles covering the tradeoffs of Rust online. I'm not sure why this is the case.




13


 Some folks think that the borrow checker's rules are inherent to programming and should be followed in any paradigm. This is false. Aliasability-xor-mutability is just a rough approximation of the real rule, dereference-xor-destroyed. Shared mutability doesn't always cause memory unsafety; dereferencing destroyed data is the real danger.












 Drawback: Prototyping and Iterating



Some say that Rust is terrible language for prototyping, it makes draft coding very difficult, and that other languages are much faster to change.




This is largely because it imposes extra constraints compared to other paradigms. Universally, the more constraints and rules you add to a problem, the more rigid the solution space is. When you want to make a change, you can't just do the simplest change, you need to find a change that also satisfies the extra constraints of the borrow checker.




One user says, ""Rust’s complexity regularly slows things down when you’re working on system design/architecture, and regularly makes things faster when you’re implementing pieces within a solid design (but if it’s not solid, it may just grind you to a total halt).""




Borrow checking tends to make refactoring more extensive because of the borrow checker's forced coupling: as a codebase's components become more interconnected and coupled, then changes in one will require changes in another. In borrow checking, every function is much more coupled to its callers and callees by mutability constraints.




From Using Rust at a startup: A cautionary tale: 14


What really bites is when you need to change the type signature of a load-bearing interface and find yourself spending hours changing every place where the type is used only to see if your initial stab at something is feasible. And then redoing all of that work when you realize you need to change it again.




Another user says, ""In general I find refactoring to be a far more excruciating process in Rust than in other languages."" 15




It's also a little worse because borrow checker also requires you to satisfy its constraints up-front for every single iteration of your code, long before it actually matters. In the words of one user, it ""forces me to put the cart before the horse"". Most code goes through multiple iterations before it's complete, 16 so we're paying this cost more often than other paradigms.




Once you're done prototyping, it can be difficult to change your program, like moving through molasses. Refactoring can be a massive pain, especially if you change the way you store some data.




And as one user says, ""In theory ... it's possible to avoid the escape hatches if you are careful to structure access to your data juuuuust right. That works out fine if you're the only person working on an infrequently-changing program with a small model that you understand very well. If everyone needs to hook into the first two or three function call levels of the main loop, and if your model is anything but small, that doesn't work out anymore.""




Of course, these users' experiences aren't universal, it might depend greatly on the domain. Stateless programs like command line tools or low-abstraction domains like embedded programming will have less friction with the borrow checker than domains with a lot of interconnected state like apps, stateful programs, or complex turn-based games. This is probably why some people find the borrow checker to be slower to work with, while some people think it's just fine.







Notes [–]
Notes [+]
14
15
16



14


Some interesting discussion on this quote.




15


The user continues on to say ""It is also far more satisfying."" There's nothing quite like the feeling of being done with a refactor, especially when it improves one's code.




16


Some people are brilliant and can get everything completely right on the first try, but alas, I am not one of them, and I haven't met one either.











 Benefit: Concurrency

The borrow checker may cause slower development velocity when prototyping and iteration, but it also helps in some ways if our program is using concurrency.




The borrow checker helps protect against data races. A data race is when: 17



Two or more threads [or async tasks] concurrently accessing a location of memory.


One or more of them is a write.


One or more of them is unsynchronized.





The reading CPU will get a partial, inconsistent view of the data because the writing CPU hasn't finished writing to it yet. These bugs are very difficult to detect, because they depend on the scheduling of the threads, which is effectively random.




These bugs can take days to track down, slowing developer velocity for programs that use concurrency.




Most languages, including C, Java, and Swift, offer no protections against data races. Go offers partial protection against data races by encouraging and defaulting to message passing, but one can still suffer the occasional data race.




Rust uses the borrow checker to protects us from data races at compile time, by isolating one thread's memory from another thread's memory, and offering standard library tools that let us safely share data between threads, such as Mutex<T>.




Not all programs use (or should use) concurrency, but if your program does, the borrow checker may improve your developer velocity in these areas of your program.







Notes [–]
Notes [+]
17



17


From The Rustonomicon.











 Drawback: Unsupported Patterns



The borrow checker is a very effective static analysis mechanism, but it still tends to be incompatible with a lot of simple, useful, and safe patterns:



Graphs; it generally only thrives with a strict tree hierarchy of data.


It has a lot of trouble with observers and any callback-based code.


Back-references; you cannot get from a child to a parent.


Dependency references, such as a HomeViewController's reference to a NetworkRequester.


Intrusive data structures.


Many forms of RAII 18 plus higher RAII.


Delegates, like in iOS GUI widgets.





All of these are generally impossible within the rules of borrow checker. There are workarounds for some, but they have their own complexities and limitations.




When the easiest solution is one of these, then one has to spend extra time to find workarounds or an entire different approach that satisfies the borrow checker. This is artificial complexity, and can slow down development velocity. For example, I once saw someone who brought in an entire Rust framework (Yew) as a dependency because they couldn't make an observer work within the borrow checker.




Luckily, one can avoid some of these problems by leaning more heavily on workarounds like Rc or RefCell, though many in the Rust community say that these are a last resort, avoided, and should be refactored out whenever possible, but these workarounds can help avoid forced coupling and help improve developer velocity if used well.







Notes [–]
Notes [+]
18



18


RAII is about automatically affecting the world outside our object. To do that, the borrow checker often requires us to take a &mut parameter or return a value, but we can't change drop's signature. To see this in action, try to make a handle that automatically removes something from a central collection.











 Benefit: Top-Down, Flattened Architectures

The borrow checker often influences us into a top-down architecture, which can help us maintain assumptions and invariants in our programs.




In short, a top-down architecture is where you organize your program's functions into a tree (or a directed acyclic graph), such that a parent can call a child, but a child cannot call a parent.




This is a very subtle but powerful effect for your program. Explaining it would take an entire three articles on its own, but check out this video by Brian Will where he talks about the benefits of this kind of ""procedural"" style. 19




GC'd languages like Pony, and functional languages like Haskell and Clojure also have this benefit. In Rust it's a natural side-effect of (or rather, requirement for) its memory safety approach.




The borrow checker also influences us toward a ""flatter"" organization, where all of our program's long-term state is held in central collections, reminiscent of a relational database. This leads naturally into certain architectures like ECS. This architecture works pretty well for a lot of programs.




Note that it can also be a bad fit for some programs. More complex turn-based games, such as roguelikes, are better with other architectures. ECS isn't as flexible or extensible in those situations.




 Drawback: Leaky Abstractions and API Stability

There are occasional features in programming languages that are inherently leaky, in that they tend to leak through abstractions.




An example that illustrates the concept, unrelated to memory safety, is async/await.


If foo() calls an async function bar(), then we need to make foo itself async as well, and a lot of foo's callers as well. It's usually doable... unless we need to change a function signature that overrides a method on an trait. When a feature forces us to change a trait method's signature, it is an inherently leaky feature.


Leakiness can make refactors much more widespread, and can even lead to an architectural deadlock when we run into an trait method we can't change, such as one in a public API.




In a similar way, the borrow checker is also inherently leaky. For example, when our method expects a &mut reference to some data, it imposes a global constraint that nobody else has a shared reference to that data. 20 This often manifests as a &mut in our callers and all of their callers. This ""caller-infectious requirement"" makes borrow checking inherently leaky, and can be a particularly thorny problem when it runs into classes that have fixed interfaces, where there is no way to pass extra data through them.




I'll repeat Matt Welsh's quote, since it applies here as well:


What really bites is when you need to change the type signature of a load-bearing interface and find yourself spending hours changing every place where the type is used only to see if your initial stab at something is feasible. And then redoing all of that work when you realize you need to change it again.




When this occurs, one user says you can do ""one of three things: 1) Refactor into an unrecognizable mess. 2) Add a lot of RwLock or RefCell. 3) Abandon the project. The issue is that to maintain the same interface, it gets to be so hacky that maintence becomes borderline impossible.""




Leaky features directly conflict with the main benefits of decoupling and abstraction, which help isolate changes in one area from affecting another. Maintaining decoupling is a very important principle for development velocity, no matter what paradigm you're using.




Some people say that ""abstractions are bad anyway"". I'll take this opportunity to point out some of the most successful and beneficial abstractions in programming history, such as file descriptors, the UDP protocol, and the Linux operating system. 21




To sum up, the constraints we add for memory safety are sometimes in conflict with the necessary constraints of API stability and beneficial abstraction, and there's sometimes a limit to how many constraints you can add to a problem before it becomes impossible to satisfy them all. 22




Don't get me wrong, one can get sufficient developer velocity in Rust, especially compared to naive C or C++. These drawbacks are real, but they aren't crippling. Plenty of projects have been completed in a reasonable time with Rust... it's just often slower than other approaches.







Notes [–]
Notes [+]
19
20
21
22



19


It's a great video, explaining a good architecture, though I would disagree with his conclusion that object-oriented coding is therefore bad. One can easily use a top-down object-oriented architecture. In an iOS app, simply never do any mutation if calling a delegate method, only when called from above. Many React apps are architected this way, and so is Google Earth.




20


This is also true of the converse; a & reference puts a constraint on all other code that they don't have any &mut references.




21


 More good programming abstractions:



If statements


Docker


JVM bytecode



And some from real life:



Amazon.com


Driving


Spoken language!







22


A quote from Harry Potter and the Methods of Rationality. Shout-out to my fellow Ravenclaws!











 Borrow Checking and Beyond

A lot of languages are working on borrow checking blends that are better for development velocity.




Some languages are using it under the hood:



Lobster is using borrowing and other static analysis techniques under the hood to eliminate a lot of reference counting overhead.


Kind is using a blend of borrowing and cloning to fuel a purely functional language.





Vale is building something similar to borrow checking but at the regions level, to largely eliminate memory safety overhead without introducing aliasing restrictions. Its opt-in nature means the user can use it where it makes sense and doesn't hinder development velocity.




Verona and Forty2 are experimenting with mixing regions and garbage collection.




Some languages are also putting borrow checkers on top of simpler and more flexible foundations:



Austral is building one on top of linear types.


Val is working on one built on top of Mutable Value Semantics.





Cone is particularly interesting because it builds a borrow checker on top of any user-specified memory management strategy.




 Garbage Collection



Garbage collection is probably the best approach for developer velocity. It completely decouples your goals from the constraints of memory management. You are free to solve your problem without tracking extra requirements, such as C++'s single ownership or Rust's borrow checking.




This is a good thing, because most of your code doesn't need to care about memory management. In most programs, profiling shows only a small portion of code that's performance sensitive, requiring more precise control of memory. 23




I particularly liked this quote from the Garbage Collection Handbook:


Above all, memory management is a software engineering issue. Well-designed programs are built from components [...] that are highly cohesive and loosely coupled. [...] modules should not have to know the rules of the memory management game played by other modules. [...] GC uncouples the problem of memory management from interfaces.




In garbage collection, we don't have to satisfy the move-to-move constraint, or borrow-to-borrow constraint. We dont have to worry about matching pointers versus values. There's just one kind of reference, rather than Rust's 5 or C++'s 7. 24




GC also doesn't cause any refactoring to satisfy single ownership (like C++'s unique_ptr) or mutability requirements (like in Rust), because those concepts never existed to begin with. 25




 GC'd code can be more correct



There are certain errors that arise in borrow checked and MMM languages, which don't happen in GC languages.




Often, instead of holding a reference to an object like in a GC'd language, the borrow checker will force us to hold a key into a central collection, such as an ID into a hash map. If we try to ""dereference"" 26 the ID of an object that no longer exists, we often get a run-time error (a None or Err usually) that we have to handle or propagate.




Pony is a great example of how a garbage collected language can reach further towards correctness than other languages. Pony is one of the only languages that literally cannot cause a run-time error. Like Erlang, it is incredibly resilient. 27




Garbage collection can also be better with privacy, since objects are never accidentally reused or mixed up with each other. For example, the borrow checker can turn memory safety problems into privacy problems if one's not careful.




 GC can be slow in specific domains

If one has unusually constrained latency requirements, such as in high frequency trading or a real-time first-person shooter game, it can take quite some time to refactor and tune the program to not have unwelcome latency spikes.




For example, to avoid the Java garbage collector firing in a specific scope, one has to completely avoid the new keyword in that scope, and avoid calling any functions that might sneakily use new. Coding without new in Java is a particularly arcane challenge.




It's a little easier in C#, where we can use the struct keyword to make a class without any heap allocation. High-performance Unity games sometimes use this style, but it's still rather difficult.




 Garbage Collection's Future

Garbage collection is more than fast enough for most situations. And even for those situations where it's latency spikes are too burdensome, there are solutions on the horizon.




Cone and Verona will allow us to explicitly separate GC regions from each other, such that we can create and destroy a temporary short-lived region before its first collection even needs to happen. By using regions wisely, one can probably avoid the vast majority of collections. 28




Cone aims to take that even further by blending in a borrow checker, plus allowing more allocation strategies such as arenas, reference counting, or even custom ones from the users themselves.




With these advances, we might be able to get GC's development velocity advantages without the usual performance drawbacks.




 Reference Counting



Reference counting, like in Swift, generally has the benefits that garbage collection does.




It does have one drawback: any cycle of references pointing at each other could cause a memory leak, wasting the available memory. This can be largely mitigated with good tooling that detects these cycles in development.




However, reference counting has three nice benefits: weak references, deterministic destruction, and the ability to make constraint references.




A weak reference is a mechanism for determining if the pointed-at object is still alive. This can be useful for a program's logic. For example, a Rocket might check if the target Spaceship has already been destroyed, to know whether it should safely fall into the planet's ocean.




Reference-counted objects are destroyed deterministically, which helps us have finer control over our program's performance.




One can also specify where they expect a reference-counted object to be destroyed, simply by asserting that they have the last reference (in other words, asserting the reference count is 1 at the end of the scope). I call this a constraint reference 29 and it can help us detect our program's logic bugs in a way that no other paradigm can.




 Conclusions

MMM, borrow checking, GC, and RC each have their strengths and weaknesses. However, in the dimension of development velocity, my general conclusions would be:



Garbage collection is often the best, since it decouples memory concerns from the actual problem at hand.


Reference counting is almost as good as garbage collection, and possibly even better if one uses its specific unique abilities.


MMM can be pretty good, as long as one uses proper architectures and tools to detect memory problems.


Borrow checking is almost as good as the others, if we augment it with Rust's other aspects like RefCell.





But that's too general! Here's some rough guidelines to help with more specific situations:



When making a game, default to something GC or RC, like Swift or C#. They offer great development velocity, plus value types (struct) for the more performance-sensitive areas. If one need even more performance, then:



If working on a single player game, an MMM language with proper architecture and memory safety mitigations is a stellar option.


If working on a multiplayer game that has particular security risks, then Rust is a good option.



When making a web server, default to a memory-safe language with RC or GC (such as C#), and bonus points if it has good concurrency support like Pony or Go.



If working on something extremely latency-sensitive, Rust could be better. MMM isn't really advisable for something exposed to the network.



When making a mobile app, stick to Swift, Kotlin, or Typescript. GC is particularly good here because its pauses can happen in between user actions.


If working on something that needs high reliability and correctness, look into something like Pony. It's run-time is guaranteed to never cause a run-time error that's not explicitly created by the user.





There are also a lot of other languages improving, blending, and even creating new memory safety paradigms.




Of course, one should consider all this in context. Developer velocity is one of the most important factors in language choice, but not the only factor:



An open-source project might want a language that encourages simplicity more, for a healthier community of contributors.


A real-time first-person shooter game would still want an approach with more predictable performance, even if it takes longer to code.


A safety critical application would still want an approach that encourages more correctness, even at the cost of developer velocity.





I hope this post has given you a broader perspective on how various memory safety approaches affect development velocity!




If you're interested in this kind of thing, then check out Part 1 which talks about memory safety in unsafe languages, and keep an eye out for the next parts on our RSS feed, twitter, discord server, or subreddit!




If you found this interesting or entertaining, please consider sponsoring me:







     Sponsor me on GitHub!
  



With your help, I can write this kind of nonsense more often!




Cheers,


- Evan Ovadia









Notes [–]
Notes [+]
23
24
25
26
27
28
29



23


This is also why C# offers value types (struct) for more precise control over memory.




24


In C++, there's Ship, Ship&, Ship*, const Ship*, const Ship&, unique_ptr<Ship>, shared_ptr<Ship>. In Rust, there's Ship, &Ship, &mut Ship, Box<Ship>, Rc<Ship>. Cell<T> and RefCell<T> might also count, bringing Rust to 7 too, perhaps.




25


Though I wouldn't turn down some sort of single ownership / RAII being added to a GC'd language. It would prevent us ever forgetting to call .dispose() on a class's child.




26


By this I mean use the index or ID to look up an object in the central hash map.




27


Note that not all garbage collected languages strive for correctness... most of them have some flavor of null or nil.




28


I suspected Pony could do something similar, but after a quick discussion with one of the developers, it turns out not to be the case.




29


Vale used to be based on these constraint references, check out this article from long ago for more.















        Copyright © 2022 Evan Ovadia  
      




"
https://news.ycombinator.com/rss,Show HN: Cosh – concatenative command-line shell,https://github.com/tomhrr/cosh,Comments,"








tomhrr

/

cosh

Public




 

Notifications



 

Fork
    1




 


          Star
 30
  









        Concatenative command-line shell
      
License





     BSD-3-Clause license
    






30
          stars
 



1
          fork
 



 


          Star

  





 

Notifications












Code







Issues
2






Pull requests
0






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







tomhrr/cosh









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





1
tag







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




tomhrr

[main] Fix test issue (2).




        …
      




        52a05c2
      

Jan 17, 2023





[main] Fix test issue (2).


52a05c2



Git stats







289

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github/workflows



[main] Try update.



Oct 1, 2022









bin



[gh-21] Document benchmark programs, fix implicit call problem, add /…



Aug 4, 2022









doc



[main] Remove benchmark content for now, since the results are not co…



Jan 10, 2023









lib



[main] Miscellaneous fixes and documentation updates.



Jan 10, 2023









src



[main] Fix problem with command output collection.



Jan 17, 2023









test-data



[main] Handle big integers properly when parsing JSON (closes #5).



Jan 28, 2022









test-misc



[main] Fix problem with command output collection.



Jan 17, 2023









tests



[main] Fix test issue (2).



Jan 17, 2023









.gitignore



[gh-11-t2] gitignore updates.



Jul 3, 2022









Cargo.toml



[gh-43] Add basic hashing functions.



Jan 5, 2023









LICENCE



[main] Initial commit.



Jan 8, 2022









Makefile



[main] Use the libdir from the Makefile when loading rt.chc.



Jan 11, 2023









README.md



[main] Documentation updates.



Jan 11, 2023




    View code
 


















cosh
Why?
Install
Dependencies
Supported systems
Building
Running
Documentation
Licence





README.md




cosh


cosh is a concatenative command-line shell.
Why?
Basic shell operations like ls, ps, stat, and so on are
implemented as functions that return first-class values, as opposed to
relying on executables that return text streams.  This makes working
with the results simpler:


Find files matching a path, and search them for data:

sh:     find . -iname '*test*' -print0 | xargs -0 grep data
cosh: lsr; [test m] grep; [f<; [data m] grep] map



Find the total size of all files in the current directory:

sh:     ls | xargs stat -c %s | awk '{s+=$1} END {print s}' -
cosh: ls; [is-dir; not] grep; [stat; size get] map; sum



A small set of versatile primitives means that less needs to be
remembered when compared with typical shells (see e.g. the various
flags for cut(1)), though some commands may be longer as a result:


Get the second and third columns from each row of a CSV file:

sh:     cut -d, -f2,3 test-data/csv
cosh: test-data/csv f<; [chomp; , split; (1 2) get] map



Sort files by modification time:

sh:     ls -tr
cosh: ls; [[stat; mtime get] 2 apply; <=>] sortp



Arithmetical operators and XML/JSON/CSV encoding/decoding functions
reduce the number of times that it becomes necessary to use a more
full-featured programming language or a third-party executable:


Increment floating-point numbers in file:

sh:     sed 's/$/+10/' nums | bc
cosh: nums f<; [chomp; 10 +] map;



Get the first value from the ""zxcv"" array member of a JSON file:

sh:     jq .zxcv[0] test-data/json2
cosh: test-data/json2 f<; from-json; zxcv get; 0 get



It also integrates with external executable calls, where that is
necessary:

Print certificate data:

bash: for i in `find . -iname '*.pem'`; do openssl x509 -in $i -text -noout; done
cosh: lsr; [pem$ m] grep; [{openssl x509 -in {} -text -noout}] map;



Install
Dependencies

Rust

Supported systems
This has been tested on Linux (Debian 12), but should work on any
Linux/macOS/BSD system where Rust can be built.
Building
make
make test
sudo make install

Apart from the core cosh executable, this will also install a
compiled library of core functions (rt.chc).
Running
user@host:/$ cosh
/$ hello println;
hello

Documentation
Documentation
Licence
See LICENCE.









About

      Concatenative command-line shell
    
Resources





      Readme
 
License





     BSD-3-Clause license
    



Stars





30
    stars

Watchers





2
    watching

Forks





1
    fork







    Releases





1
tags







    Packages 0


        No packages published 





Languages












Rust
97.7%







xBase
2.2%







Other
0.1%











"
https://news.ycombinator.com/rss,Hubble finds black hole twisting captured star into donut shape,https://phys.org/news/2023-01-hubble-hungry-black-hole-captured.html,Comments,"400 Bad request
Your browser sent an invalid request.

"
https://news.ycombinator.com/rss,Show HN: Otterkit – COBOL compiler for .NET,https://github.com/otterkit/otterkit,Comments,"








otterkit

/

otterkit

Public




 

Notifications



 

Fork
    1




 


          Star
 86
  









        Otterkit COBOL Compiler
      





otterkit.com


License





     Apache-2.0 license
    






86
          stars
 



1
          fork
 



 


          Star

  





 

Notifications












Code







Issues
1






Pull requests
0






Discussions







Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Discussions
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







otterkit/otterkit









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








4
branches





0
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




KTSnowy

[Update]: Parse SEARCH statement




        …
      




        eb47152
      

Jan 17, 2023





[Update]: Parse SEARCH statement


eb47152



Git stats







176

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github/ISSUE_TEMPLATE



[Update]: Update issue templates and security policy notice



Nov 19, 2022









Assets



[Update]: Added dotnet tool and project templates Nuget packages



Dec 24, 2022









OtterkitTemplatePack



[Update]: Update Otterkit.Templates version number



Dec 24, 2022









libotterkit @ a0256a7



[Update/Fix]: Apply Roslyn Analyzers fixes and change suggestions



Jan 4, 2023









src



[Update]: Parse SEARCH statement



Jan 17, 2023









.gitattributes



Create .gitattributes



Oct 13, 2022









.gitignore



[Update]: Added dotnet tool and project templates Nuget packages



Dec 24, 2022









.gitmodules



[Update]: Added libotterkit as a git submodule



Nov 2, 2022









LICENSE



[Update]: Added year and name to the LICENSE file



Dec 19, 2022









README.md



[Update]: Add project icon to the README header



Jan 11, 2023









SECURITY.md



[Update]: Change ""runtime"" to ""compiler"" in the security policy



Nov 19, 2022









THIRD-PARTY-LICENSES



[Update]: Added libmpdec license notice



Oct 23, 2022




    View code
 















 Otterkit COBOL Compiler
About
Installation
Quick Install
Build from Source
Standard Acknowledgement





README.md




 Otterkit COBOL Compiler
Otterkit is a free and open source compiler for the COBOL Programming Language on the .NET platform.
Warning: The project is currently in pre-release, so not all of the standard has been implemented.
About
COBOL was created in 1959 by the CODASYL Committee (With Rear Admiral Grace Hopper as a technical consultant to the committee), its design follows Grace Hopper's belief that programs should be written in a language that is close to English. It prioritizes readability, reliability, and long-term maintenance. The language has been implemented throughout the decades on many platforms with many dialects, and the Otterkit COBOL compiler is a free and open source implementation of the ISO COBOL 2022 Standard on the .NET platform.
Installation
Quick Install
Otterkit is available to install on the Nuget package manager (.NET 7 is required). To install, type into the command line:
dotnet tool install --global Otterkit --version 1.0.15-alpha

Build from Source
First, clone the git repo from https://github.com/otterkit/otterkit.git to get the source code. To access the libotterkit submodule inside, use the --recurse-submodules --remote-submodules flag on the clone command. To run, navigate into the src folder (for the compiler, not libotterkit) and then type dotnet run into the command line.
Standard Acknowledgement
Any organization interested in reproducing the COBOL standard and specifications in whole or in part,
using ideas from this document as the basis for an instruction manual or for any other purpose, is free
to do so. However, all such organizations are requested to reproduce the following acknowledgment
paragraphs in their entirety as part of the preface to any such publication (any organization using a
short passage from this document, such as in a book review, is requested to mention ""COBOL"" in
acknowledgment of the source, but need not quote the acknowledgment):
COBOL is an industry language and is not the property of any company or group of companies, or of any
organization or group of organizations.
No warranty, expressed or implied, is made by any contributor or by the CODASYL COBOL Committee
as to the accuracy and functioning of the programming system and language. Moreover, no
responsibility is assumed by any contributor, or by the committee, in connection therewith.
The authors and copyright holders of the copyrighted materials used herein:

FLOW-MATIC® (trademark of Sperry Rand Corporation), Programming for the 'UNIVAC® I and
II, Data Automation Systems copyrighted 1958,1959, by Sperry Rand Corporation;
IBM Commercial Translator Form No F 28-8013, copyrighted 1959 by IBM;
FACT, DSI 27A5260-2760, copyrighted 1960 by Minneapolis-Honeywell

Have specifically authorized the use of this material in whole or in part, in the COBOL specifications.
Such authorization extends to the reproduction and use of COBOL specifications in programming
manuals or similar publications.









About

      Otterkit COBOL Compiler
    





otterkit.com


Topics



  compiler


  dotnet


  cobol



Resources





      Readme
 
License





     Apache-2.0 license
    

Security policy





      Security policy
    



Stars





86
    stars

Watchers





3
    watching

Forks





1
    fork







    Releases

No releases published












    Contributors 3








KTSnowy
Gabriel Gonçalves

 






TriAttack238
Sean Vo

 






gabrielesilinic
Gabriele Silingardi

 





Languages










C#
100.0%











"
https://news.ycombinator.com/rss,Dr V’s Magical Putter (2014),https://grantland.com/features/a-mysterious-physicist-golf-club-dr-v/,Comments,"






 » Dr. V’s Magical Putter















































 

 















Search
Search



Search




Menu
Search




Features
The Triangle
The Hollywood Prospectus
Contributors
Podcasts
Video
Quarterly
ESPN.com














➤
						





 Previous
Why Not the Warriors?









➤
						





 Next
Western Promises











Facebook
Twitter
Print





Sports
Dr. V’s Magical Putter

					The remarkable story behind a mysterious inventor who built a ""scientifically superior"" golf club						
by Caleb Hannan on January 15, 2014





A letter from Grantland editor-in-chief Bill Simmons on the origins of this story and how it came to be published can be read here. A guest editorial from Christina Kahrl detailing the problems with this piece as they relate to transgender issues can be found here.
Strange stories can find you at strange times. Like when you’re battling insomnia and looking for tips on your short game.
It was well past midnight sometime last spring and I was still awake despite my best efforts. I hadn’t asked for those few extra hours of bleary consciousness, but I did try to do something useful with them.
I play golf. Sometimes poorly, sometimes less so. Like all golfers, I spend far too much time thinking of ways to play less poorly more often. That was the silver lining to my sleeplessness — it gave me more time to scour YouTube for tips on how to play better. And it was then, during one of those restless nights, that I first encountered Dr. Essay Anne Vanderbilt, known to friends as Dr. V.
She didn’t appear in the video. As I would later discover, it’s almost impossible to find a picture, let alone a moving image, of Dr. V on the Internet. Instead, I watched a clip of two men discussing the radical new idea she had brought to golf. Gary McCord did most of the talking. A tournament announcer for CBS with the mustache of a cartoon villain, McCord is one of the few golf figures recognizable to casual sports fans because he’s one of the few people who ever says anything interesting about the sport.
The video was shot in March of last year, when McCord was in California for an event on the Champions Tour, the 50-and-over circuit on which he occasionally plays. In it, he explained that he had helped Dr. V get access to the nearby putting green, where he said she was currently counseling a few players. She was an aeronautical physicist from MIT, he continued, and the woman who had “built that Yar putter with zero MOI.” The credentials were impressive, but the name “Yar” and the acronym were unfamiliar.
According to McCord, before building her putter Dr. V had gone back and reviewed all the patents associated with golf, eventually zeroing in on one filed in 1966 by Karsten Solheim. As the creator of Ping clubs, Solheim is the closest thing the game has to a lovable grandfather figure. He was an engineer at General Electric before becoming one of the world’s most famous club designers, and his greatest gift to the sport was his idea to shift the weight in a club’s face from the middle to its two poles. This innovation may sound simple, but at the time it was revolutionary enough to make Solheim one of the richest men in America and the inventor of one of the most copied club designs in history. In Dr. V’s estimation, however, Solheim was nothing but a hack. “The whole industry followed [that patent],” she told McCord. “You’re using pseudoscience from the ’50s in golf!”
As the video went on, McCord told the story of how he had arranged a meeting between Dr. V and an executive at TaylorMade, the most successful clubmaker in the world, whose products McCord also happened to endorse. The gist of that meeting: This previously unknown woman had marched up to one of the most powerful men in golf and told him that everything his company did was wrong. “She just hammered them on their designs,” McCord said. “Hammered them.”
I was only half-awake when I watched the clip, but even with a foggy brain I could grasp its significance. McCord is one of golf’s most candid talkers — his method of spiking the truth with a dash of humor famously cost him the chance to continue covering the Masters after the schoolmarms who run the tournament objected to his description of one green as so fast that it looked like it had been “bikini-waxed.” This respected figure was saying that this mysterious physicist had a valuable new idea. But the substance of that idea wasn’t yet clear — over time, I would come to find out that nothing about Dr. V was, and that discovery would eventually end in tragedy. That night, however, all I knew was that I wanted to know more.

♦♦♦
No athletes rely on their equipment quite like golfers. Picking which sneakers to wear or what bat to swing are relatively simple choices compared with selecting 14 clubs. Variables like grip material, shaft strength, and club length further complicate the process, and that’s without even considering which ball to use. The market for selling this equipment is enormously competitive, and it reflects a reality that goes against the current perception of the game.
Since Tiger Woods joined the PGA Tour in 1996, broadcast golf has enjoyed a decade-and-a-half-long financial boom. That same year only nine players earned more than $1 million. By 2012, that number had ballooned to 100. But even as the money in televised golf has grown, participation has shrunk. The sport loses about 1 million players per year. That dwindling pool of paying customers has made the competition to sell them equipment ever more fierce.
Barney Adams, the founder of Adams Golf, the last truly successful independent club manufacturer, is unapologetically pessimistic about other small companies’ odds of survival. “We got lucky,” he says. “Our success was tied to one club.” Adams had been a custom club fitter constantly on the brink of bankruptcy until he built a club called Tight Lies. Adams’s creation was billed as a fairway wood, but many consider it to be the first hybrid, a half-iron half-wood that combined the best features of both. Adams exploited his finder’s advantage for as long as he could, but today every clubmaker has its own line of hybrids. In 2012, Adams Golf was sold to Adidas, which already owns TaylorMade.
Adams’s assessment of golf’s demographics and his conclusions about what they mean for the business are brutal. “Look at the average age of today’s golfer,” he says. “Half are over 40. How does that forecast into the future? If you look out 50 years, golf becomes squash.” The outlook is more grim, says Adams, for designers who make only putters. “In the history of the golf industry there’s never been an independent putter company that hasn’t gone broke,” he says. The only path to success involves being bought by a larger company. And to do that, Adams says, you need a story to sell. A story that can usually be reduced to five simple words: “Mad scientist invents great product.”
♦♦♦
I wanted to know more about Dr. V, so I sent her an email and received one in return that confused the hell out of me. It was early April, and I was trying to set up an appointment to speak with her on the phone. First, however, she insisted that our discussion and any subsequent article about her putter focus on the science and not the scientist. The reason for this stipulation seemed dire.
“I have no issues as long as the following protocols are followed because of my association with classified documents,” she wrote. “Allow me to elucidate; I have the benefits under the freedom of information act the same privileges as federal judges, my anonymity is my security as well as my livelihood, since I do numerous active projects … If the aforementioned is agreeable to you, please respond to this communique at your convenience so we can schedule our lively nuncupative off the record collogue.”
The words caught my eye first. Communique! Nuncupative! Collogue! I hadn’t heard of any of them, and it wasn’t until I looked up their definitions that I understood what she was saying. Everything about her email suggested she might be a tough interview. So, instead of trying to get a straight answer out of Dr. V, I reached out to McCord. He’s the one who first told me how she came to build her putter.
Yar Golf — Dr. V’s company — had begun seven years ago, he said, at an Arizona country club where she was attending the wedding of a colleague’s daughter. In the ladies’ locker room she met Gerri Jordan, a retired Bank of America senior analyst, who had just come from the course. Jordan was slamming her putter against a locker when Dr. V walked over and asked how she could help. Jordan asked her what she knew about putting, and Dr. V answered honestly — nothing. What she did know, however, was physics. She told Jordan that if the goal was to roll the ball smoothly, then the tool she was using was wrong for the job. This encounter is what eventually led to the creation of Yar, whose name comes from a nautical term that roughly translates to “easy to maneuver.” McCord’s cameo in the story was still a few years in the making.
By the time he met Dr. V she had already built her putter. She called it the Oracle GX1 — “G” for Gerri, as in Jordan; “X” for NASA Hyper-X, the hypersonic flight research program. It looked different from any other putter on the market. It had a small face and a large circular cutout in the back, giving it the appearance of a steel-shafted cup holder. It was also built using a principle that ran contrary to what had come to be golf’s conventional wisdom when it came to putters.
McCord explained that MOI, the acronym that had baffled me a month earlier, was Dr. V’s primary focus. It stood for “moment of inertia,” a concept that was by scientific standards fairly easy to understand. McCord explained that moments of inertia are a body’s resistance to changes in its rotation. “The higher the MOI,” he said, “the more the body has to resist.” Golf manufacturers were making putters with higher and higher levels of MOI, and advertising that fact as a benefit — it was supposed to make the club more forgiving, so that if a player didn’t hit the ball right on the sweet spot the stroke would still be pure. But McCord said Dr. V thought the whole idea was crazy. “What she said to me was that what we’ve been doing with putters is not science,” he said. “We’re going the wrong way. Zero MOI, that’s where golf should be going.” And that’s precisely what Dr. V said she had achieved with the Oracle.
But it wasn’t just the science behind Dr. V’s putter that intrigued McCord. It was the scientist, too. For starters, she was a woman in the male-dominated golf industry. She also cut a striking figure, standing 6-foot-3 with a shock of red hair. What’s more, she was a Vanderbilt, some link in the long line descending from Cornelius, the original Commodore. All of this would have been more than enough to capture McCord’s attention, but what he found most remarkable about Dr. V was where she had been before she started making putters. She told him she had spent most of her career as a private contractor for the Department of Defense, working on projects so secretive — including the stealth bomber — that her name wasn’t listed on government records. “Isn’t that about as clandestine as you can get?” McCord asked me.
He had his own peculiar way of verifying this information. McCord said he was on friendly terms with a few retired four-star generals. He told me that they not only knew of Dr. V, but also that one had even called her “one of us.” Dan Quayle was also an acquaintance. Unable to help himself, McCord once put the former vice-president on the phone with Dr. V and watched as they chatted about old Pentagon projects. McCord clearly enjoyed showing off his discovery, this exotic new addition to the world of golf. But he wouldn’t have stuck his neck out for Dr. V, whom he just called “Doc,” if he didn’t also believe in her product. Yar hadn’t made McCord a paid sponsor, but it didn’t matter — the Oracle was so good that he used it anyway. “It’s the only one I’ll have in my bag now,” he told me. It was why he had set up the meeting between Dr. V and the company whose products he was paid to endorse, TaylorMade. “I just wanted to make sure they saw her first,” he said.
McCord also had an explanation for Dr. V’s strange vocabulary: This was just how scientists talked. He told me not to take it personally and not to be intimidated. Dr. V made fun of him and the “primitive information base” in golf all the time, he said. It was all in good fun! He even offered to arrange a phone call between us. “She will talk to you about the science and not the scientist,” he said after confirming with her that it was OK. Then he left me with a lighthearted warning: “Call Doc and hang on.”
♦♦♦
Golf may be unlike other sports in the way its athletes rely on equipment, but it is very much like every other sport when it comes to the best way to sell that equipment: Put it in the hands of the pros. This is especially true for club designers who make putters. For them, the line between obscurity and fame is so thin a single weekend of golf can make it disappear.
By the time Karsten Solheim died in 2000, he was widely considered a genius. But before Julius Boros won the 1967 Phoenix Open with Ping’s Anser putter, Solheim was still working his day job at GE. Bobby Grace was an independent manufacturer with middling success until Nick Price won the 1994 PGA Championship with one of Grace’s mallet putters. In the eight weeks after Price’s win, Grace took orders for $6 million worth of clubs. It’s a similar story for Scotty Cameron, the biggest name in putters. Cameron and his wife had barely founded their golf company before Bernhard Langer won the 1993 Masters with one of Cameron’s blades. After Langer’s win, Cameron struck it rich.
Anyone who plays sports understands this phenomenon. We want to use the same clubs, shoes, balls, bats, and everything else as the pros because they’re the best, and we want to give ourselves every chance to play as well as them. It’s as much about confidence as it is quality equipment.
This isn’t just common sense — social scientists have actually studied how using “professional” gear affects amateurs’ performance. In 2011, researchers at the University of Virginia laid out a putting mat, a ball, and a putter, and invited 41 undergraduates to take part in an experiment. The students were asked to do two things: Take 10 test putts and then try to draw the hole to scale. Half were told nothing about the putter’s origins. The rest were told it once belonged to a PGA Tour player. You already know what happened next. The students who thought they were using a pro’s club sank more putts and drew the hole larger than the control group. The social scientists running the experiment must have known that what they were witnessing was pure superstition. How else to describe the process by which years of practice and skill can be transmitted from an expert to an amateur through the simple transfer of an object? But because they’re academics, they use a different term — positive contagion. It’s like the placebo effect for sports.
On May 4, 2012, McCord bestowed the blessing of positive contagion on Dr. V’s Oracle putter. While calling the second round of the Wells Fargo Championship, he singled out the club being used by golfer Aaron Baddeley. “Now, this is one of the greatest putters in the world,” he said. McCord then gave a quick sketch of Yar’s origins — Dr. V, rocket science, zero MOI. Even though Baddeley unhelpfully missed his putt, McCord was acting as Yar’s most vocal unpaid booster. He raved about the putter so much that his fellow announcers teased that he was filming an infomercial.
McCord never mentioned the name of the company that made the putter. And Baddeley, statistically one of the tour’s best putters, didn’t even play very well — he finished the tournament one over par, tied for 65th place. But none of that mattered to the golf fans who had listened to McCord’s plug. All they heard was one of the sport’s most trusted voices enthusiastically recommending a club being used by one of the world’s most skilled putters. The word was out. Within an hour, Dr. V told McCord, Yar’s website crashed after some 90,000 people rushed to see what all the fuss was about.

♦♦♦
By the time I actually spoke with Dr. V, she had managed to add a few more quirks to her character. She had begun our correspondence by signing off emails with “Ciao.” Then she moved on to “Cheerio and Toodle Pip.” I didn’t know what to expect when she answered the phone in her Arizona lab. She told me she would “notify the switchboard personnel” to direct the call to her office, as if she were living in some bygone era. But when I finally called, the person on the other end of the line seemed normal. She asked about my dog, which was barking in the background. She complained about the lack of scientific expertise in club design — “There are no physicists in golf that I know of” — and she made things I knew to be hard sound simple. “A golf club is just a source of kinetic energy,” she said. “It just has to transfer it to a ball. It really is that easy.” All the big words she had used in her emails were replaced with smaller ones. She may have written like a mad scientist, but she spoke like someone who wanted to be understood. She also added a few new layers to her story.
Though she had insisted that she would only talk if the focus was on her putter and not herself, Dr. V willingly volunteered some background information. She had been born in Pennsylvania and later moved to Georgia. She had lived in Boston while attending MIT, and she had also spent more than a decade in Washington, D.C., while working on top-secret projects. All that moving around had resulted in what she called a “mutated accent.” The pitch of her voice was strange, too — deeper than expected. She said it was the result of a collapsed larynx she had suffered in a car crash. She also revealed why she avoided the golf course, preferring the life of a “lab rat.” The woman who had invented the newest, greatest putter not only didn’t play golf very often, she also was practically allergic to the sun. If she spent more than a few hours outside, she said, she got crippling migraines.
Dr. V’s time in Washington also helped explain the inspiration behind her putter’s strange look. She said she had been a regular volunteer at Walter Reed Army Medical Center. There, she discovered that golf had been used as therapy for some injured veterans. So to help those veterans on the course, the Oracle was designed to allow its user to retrieve the ball from the hole without bending down. That meant the resemblance to a cup holder wasn’t a tacky design choice but a triumph of ergonomics.
Dr. V also shared details of the chilly reception Yar had received from the rest of the golf industry. In six months there had been nine attempted break-ins at Yar’s office in Tempe, she said. Dr. V didn’t know who the burglars were, but she presumed someone was trying to steal her secrets. “A company would rather destroy Yar than buy us.” She had also seen firsthand how other golf companies reacted when tour pros they paid to use their clubs used Yar’s instead. Baddeley had used the Oracle for a few weeks, she said, and in that time had risen in the rankings of the PGA Tour’s best putters. But then, suddenly and without explanation, he stopped using it. Dr. V believed a competitor had convinced Baddeley to go back to one of its putters. McCord was less conspiratorial. He told me that Baddeley had complained that he couldn’t use the Oracle on certain greens. “Now, if that’s the real reason,” said McCord, his voice trailing off. “When you start talking golfers and you start talking contracts with club companies, I don’t know.”
The story of Dr. V and her putter was getting stranger by the second. An aeronautical physicist with a sun allergy builds the world’s greatest putter by rejecting conventional wisdom, then watches as deep-pocketed competitors try to steal her secrets and shut her out of the market. Just the explanation for the hole in the putter itself was outlandish. Dr. V had somehow found a way to turn an injury aid into a superior product. The strangest fact of all: The putter worked! Why else would Baddeley or McCord use it if they weren’t being paid? Clearly, there was only one thing left for me to do.
♦♦♦
John Tomac
A few weeks after my first talk with Dr. V, I received a package. Inside was an Oracle putter with my name engraved on the back of its face. Dr. V had spent an hour on the phone getting my specifications — the length of my fingers, the distance between my wrist and the ground, which of my eyes was dominant. She then spent another half-hour talking me through drills to show me how to use the club. The concept of zero MOI had remained abstract until the moment I first swung the Oracle. While other putters twisted when you pulled them back, Dr. V’s didn’t — a reflection, perhaps, of the stability needed to design wings for the stealth bomber, which she often said was her inspiration when building the putter. It seemed as if all I had to do was hold the club, pick a line, and hit the ball, then watch it roll smoothly in that direction. The club didn’t fight me.
I then went to a public course to try the Oracle on some actual greens. I didn’t make every putt — far from it. But I did seem to sink more than usual. And like McCord, the more I used the putter the more I became its unofficial pitchman. I began to look forward to the “oohs” and “ahhs” from strangers when they would first see me use the club to pick the ball out of the hole. I enjoyed telling the wild story behind the putter’s invention. It turned a normal round of golf into an act of seduction. And it was all because Dr. V’s club had me putting with a lot more confidence.
I was ready to proclaim her an unknown genius with an idea that could revolutionize golf. All that was left to do was make sure the stories about engineering accolades and top-secret defense projects were legit. It was, I thought, just a formality.
I started with Dr. V’s biggest accomplishment — her work on the stealth bomber. The Department of Defense could not confirm her employment without a Social Security number, and I figured that Dr. V wouldn’t want to share hers. So I contacted Aviation Week senior international defense editor Bill Sweetman, who had written a book on the plane. Sweetman said there was no way to confirm Dr. V’s work without forcing her into a compromising position, since stealth workers signed lifetime nondisclosure agreements. “It would not be surprising if she worked on the B-2,” he wrote in an email, “and that she would not want to talk about it if she did.”
He was certainly right about that. I emailed Dr. V to tell her how much I loved her putter. I also told her that an equation-heavy document she had sent me called “The Inertia Matrix,” which further explained how to use the Oracle, had looked too confusing for me to follow. Finally, I asked if she could help me confirm a few facts about her past life. When I heard back, the patient woman I had spoken to on the phone had been replaced by an angry, mocking scientist. She wrote:
As I clearly stated at the onset of your unsolicited probing, your focus must be on the benefits of the Science for the Golfer not the scientist, however, at this juncture you are in reversal of your word, as well as neophytic in your modus operandi of understanding the science of Yar. If you were observant or should I state; had the mental aptitude of ratiocination you would have gleaned or inquired about the advantages of the Inertia Matrix … If you are what you presented yourself to [Gary McCord], as a golf nut, then you should be in shock and awe that someone has given the golfing milieu a scientific breakthrough as revolutionary as the two-piece rubber core golf ball was a hundred years ago!!!
The email was a surprise. Dr. V’s initial requests for privacy had seemed reasonable. Now, however, they felt like an attempt to stop me from writing about her or the company she’d founded. But why?
It didn’t take long to uncover some serious discrepancies in her story. I contacted the registrar’s office at MIT. It had no record of anyone named Essay Anne Vanderbilt attending. The registrar at the University of Pennsylvania confirmed the same thing. Whatever Dr. V’s actual credentials, they didn’t include a business degree from Wharton, where she had supposedly gotten her MBA. This was significant but inconclusive. After all, Dr. V could have attended the schools under a different name. But why wouldn’t she have mentioned that?
The deeper I looked, the stranger things got. It seemed as if there was no record of Dr. V’s existence prior to the early 2000s. And what little I managed to find didn’t exactly align with the image she projected of a world-class scientist. I couldn’t find any record of her ever living in Boston. The same went for Washington, D.C. And when I contacted Walter Reed, I was told the hospital had no way to prove she had ever worked there.
I also found a lawsuit filed against the town of Gilbert, Arizona, in July 2007. The plaintiff’s name: Essay Vanderbilt, who had accused the town and three of its employees of sexual discrimination. The suit alleged that the previous year Vanderbilt was working as a “vehicle service writer” in Gilbert’s Fleet Management Division. In other words, at the same time that Dr. V claimed to have been working on top-secret government projects in D.C., she was actually coordinating car repairs for a Phoenix suburb. Vanderbilt didn’t win her case. And in 2011, a civil court in Maricopa County, Arizona, ordered her to pay nearly $800,000 to a commercial developer. That judgment may have been the reason why, later that year, Vanderbilt filed for bankruptcy, listing assets of less than $50,000 and liabilities of more than $1 million.
At this point I was still hoping everything I’d found was all a big misunderstanding. I wanted to believe Dr. V’s story. After all, the putter worked. People who knew a lot more than me about golf swore by the club. There were even logical explanations for much of what I had uncovered: Dr. V could have gone to school under a different name; she could have mixed up the dates while telling the story of when she founded Yar; she could have taken the job in Gilbert as an extra source of income to pay her bills; and she may have filed for bankruptcy simply because the golf club business can be cutthroat, and Yar had struggled financially before catching a hot streak in the past year.
I was still clinging to these threads when Leland Frische came along and snipped them all.
♦♦♦
Frische is the risk manager for Gilbert, and he had been there when Vanderbilt first came to work for the town. He said she was hired in April 2006 and there were problems almost immediately. Vanderbilt had applied to be the manager of the fleet services division, but she lost out on that job to someone else. She believed she was more qualified, however, and others complained that she did not try to hide that. “She would confront her boss in open meetings,” Frische told me. “She would talk down to people. She really didn’t give us many options.” The town eventually fired Vanderbilt. Not long after, she filed her lawsuit. And that’s when something weird happened, Frische told me.
The town’s lawyers began investigating her background. Like me, they found some big holes, namely an education history she claimed to have but didn’t. The town’s lawyers also suspected that at one point she might have been known by a different name, and they asked her to reveal it. When she refused, the judge asked her to sign an affidavit saying she had always gone by Essay Anne Vanderbilt. She refused that request, too, and with it forfeited the right to continue her lawsuit. Frische said Gilbert’s search had ended there. But while we spoke on the phone, he started saying things that sounded odd to me. “Have you ever seen her in person?” he asked. “What I really hope for you is that you could meet her someday,” he said at another point in the conversation, from what seemed out of nowhere.
He was clearly trying to tell me something, which is why he began emphasizing certain words. Every time he said “she” or “her” I could practically see him making air quotes. Finally it hit me. Cliché or not, a chill actually ran up my spine.
“Are you trying to tell me that Essay Anne Vanderbilt was once a man?”
It took a moment for him to respond.
“I cannot confirm or deny anything on that,” he said, sounding once again like a risk manager. “But let me ask you a question. How far have you looked into her background?”
♦♦♦
Here is what I now know about Dr. Essay Anne Vanderbilt, inventor of the Oracle GX1 putter.
She was born a boy on July 12, 1953, in Philadelphia. She was given the name Stephen Krol, a person who has not received degrees from MIT or the University of Pennsylvania. She has been married at least twice, and the brother of one of Krol’s ex-wives says Dr. V has two children, possibly more. She was once a mechanic at a Sunoco station that she also may have run in Bucks County, Pennsylvania. She filed and subsequently dropped a lawsuit against Sunoco. She moved to Arizona at some point after marrying her second wife in 1997. She ended up in Bonney Lake, Washington, a short time later. She filed a “petition for change of name” on October 14, 2003, in the Pierce County, Washington, District Court. She scratched out an unsuccessful first attempt at writing “Essay” on that petition. She wrote “OLD NAME DOES NOT MATCH ME” where the court paperwork asked why she no longer wanted to be known as Stephen Krol. She worked as general manager at Trax Bar and Grill, an LGBT bar in Kent, Washington. She was the subject of three separate harassment claims from her time there, including one from a male coworker who said she made “inappropriate comments about her breasts and genitalia.” She moved to Arizona again sometime later. She met Gerri Jordan. She built a putter. She met Gary McCord. She told me the focus should be on the science and not the scientist.
What little else I know about Stephen Krol in the years before and after he changed his name comes from people who knew him, but didn’t know him well. My attempts to get in touch with members of his family and his ex-wives were unsuccessful. Some people didn’t pick up or return my calls. Others, like Ewa Kroll, whose name showed up alongside his in searches and whose relationship to Stephen I still haven’t been able to parse, hustled me off the phone as quickly as possible. “I have not talked to him for years,” she said. “I’m just going to have to say ‘good-bye’ now.”
The darkest discovery was something that occurred after Krol had decided to live as Dr. V. In 2008, she tried to kill herself with an overdose of prescription drugs and carbon monoxide poisoning from closing herself in a garage with her car running. A police report offered some explanations for why she might have tried to take her own life — Yar’s business was slow and Dr. V’s romantic relationship was on the ropes. She had recently fought with her girlfriend, Gerri Jordan, president of Yar Golf. Jordan told police that she and Dr. V were in a monogamous relationship and that they had gotten into an argument two days before. She had found Dr. V in the passenger seat of her car after the suicide attempt and tried to keep her awake. Jordan had also presumably been the first person to read the suicide note Dr. V had taped to the window of the car door, which read in part, “Tell Gorgeous Gerri that I love her.”
♦♦♦
What began as a story about a brilliant woman with a new invention had turned into the tale of a troubled man who had invented a new life for himself. Yet the biggest question remained unanswered: Had Dr. V created a great golf club or merely a great story?
She had faked the credentials that made the science behind her club seem legitimate. But the more I talked to people in the world of club design, the more I came to understand that many believed the physics behind the Oracle putter were solid, even if the “scientist” was not. I found Kelvin Miyahira, a golf instructor in Hawaii with no ties to Yar who nonetheless had become one of its biggest fans. Miyahira had used a high-speed camera to compare the Oracle with other, more popular putters. In slow-motion videos he posted to YouTube, he showed that when he used the Oracle, it was more stable and rolled the ball more smoothly and with less sidespin than any of the other clubs he tried.

Champions Tour player David Frost had once received an hour-long putting lesson from Dr. V and four days later had won a tournament by tying the lowest score ever recorded on that course. The information Dr. V had imparted to him was so valuable, Frost told me, that he wasn’t even willing to share it. Maybe if I’d had the same access, the Oracle would have remained as effective for me. But positive contagion, at least in my case, only seemed to work when I believed I was still infected. When I was under the impression that Dr. V was a brilliant engineer, my putting improved dramatically. As soon as I learned she had simply been a struggling mechanic, the magic was lost. Today, Dr. V’s Oracle is collecting dust in my garage.
The other question to consider was if any of the lies actually mattered. Yes, Dr. V had fabricated a résumé that helped sell the Oracle putter under false pretenses. But she was far from the first clubmaker to attach questionable scientific value to a piece of equipment just to make it more marketable. Sure, her lies were more audacious than the embellishments found in late-night infomercials. But her ultimate intent — to make a few bucks, or, maybe, to be known as a genius — remained the same. Whatever the answers, Gary McCord would not be able to help me find them. The man who had once been so willing to talk stopped responding to my emails. Finally, a spokesperson at CBS told me that McCord had “nothing more to add to the story.” That left Jordan and Dr. V.
I called them both, and realized that they had given me the same phone number. Dr. V had said the number was for her lab with the “switchboard personnel.” This time, though, no one answered and I heard the outgoing message. What sounded like a young girl’s voice filled the receiver: “Thank you for calling Essay Vanderbilt and family …” The next day I tried again. No answer still, but the recording had changed. Instead of a young girl, the voice was Jordan’s: “Hello, you’ve reached the offices of Yar Golf …”
♦♦♦
John Tomac
I was under the belief that what had transpired at Yar was ultimately harmless until I heard from a mysterious “silent investor” whom both Jordan and Dr. V had alluded to in our previous talks. His name was Phil Kinney. He was a retiree from Pittsburgh and he said he wasn’t the only one who had put money into the company. He had invested $60,000 — money that he believed he’d never see again.
It wasn’t that Kinney didn’t love Yar’s putter or have high hopes for its future. He had loved it from the moment he met Dr. V at a convention four years ago. (Before I told him about her past, he told me that because of her height and vivid red hair, it was hard to miss the “pretty woman walking toward me in a miniskirt.”) He still loved the club enough to sell it to friends and clients, too. But he had also come to know the frustrations of working with Dr. V.
Kinney had heard his own share of incredible claims. Dr. V had told him that she was a $1,000-an-hour consultant. She said she was one of the original designers of Bluetooth technology. She even suggested that her status as a Vanderbilt provided access to some exclusive company who could help Yar’s business. Kinney said Dr. V told him she was good friends with the Hilton family, and that the relationship would pay off in the form of putters sold at their hotels. Kinney also recalled a trip he had taken to Arizona where, in Dr. V’s house, she had shown him a computer that she said mirrored the one in Phoenix’s airport traffic control tower.
For all her wild stories, though, what Dr. V was most, Kinney said, was a difficult person to deal with. “She would just explode. If you’re disagreeing with her while she had one of her headaches, you were in trouble.” And Kinney often disagreed with Dr. V. He tried to get her to change the design of the putter. She wouldn’t budge. He tried to get her to change Yar’s confusing website. She had the same reaction. He even tried to convince Dr. V to let well-known club designers like Bobby Grace, whom Kinney said wanted to invest, buy into the company. “She just told me, ‘We don’t need him.'” It seemed unlikely that Yar would ever deliver a return on Kinney’s investment.
Maybe the most surprising thing about my conversation with Kinney was how calmly he took the news that the woman he thought was an aerospace engineer had once been a man, and a mechanic. “I’m pretty dang gullible, I guess,” he said. For all the hassle that came with his partnership with Dr. V, what had kept him going was the putter. That was what Kinney couldn’t understand. If Yar had simply been a scam, the story would have been much simpler. But the Oracle worked. And Dr. V seemed more interested in achieving fame as a club designer than in getting rich.
“She could have took my money and ran,” he said. “But she didn’t. She took it and built a great product.”
Kinney said he was worried that the putter’s excellence would be lost in the strange tale of Dr. V. He genuinely believed the Oracle was a superior product. But at one point near the end of our conversation, he had a thought that seemed to trouble him. “Maybe I liked it because she convinced me before I even hit it,” he said. “Maybe it’s not as good as I think. Someone tells me a story, I believe it.”
♦♦♦
The last time I heard from Dr. V she warned me that I was about to commit a hate crime. But before that, I received a voice mail from Jordan.
Neither of them had contacted me in months, since I had sent an email trying to confirm what I had discovered, and Jordan wrote back to deny everything. “Your attack tale should be published in the National Enquirer,” Jordan wrote, “right next to the article on Martians … If I am to believe your diatribe, what you are telling golfers is that the most scientifically advanced Near Zero MOI putter, and the science of the Inertia Matrix was invented by a lesbian auto mechanic.”
Now, Jordan’s message said she was calling to propose a deal. When I phoned her back, Jordan explained the offer. I could fly to Arizona and meet with Dr. V at her attorney’s office, where she would show me proof of her degrees from both MIT and the University of Pennsylvania. Dr. V then got on the phone and added another detail. Once I saw the documents I would have to sign a nondisclosure agreement barring me from revealing any of the details I’d learned about Dr. V’s past.
The “deal” was one I could not accept, and when I explained this Dr. V got upset. “What is your intention?” she asked. “Are you being paid by someone to destroy Yar?” Dr. V’s anger made it so that what she said came out fast and with almost no interruption. I tried to record everything she said and ask the occasional question, but it was like yelling into a wind tunnel. When she finally had said her piece, she handed the phone back to Jordan. “Well, I guess you’re just going to print what you’re going to print,” Jordan said. “Try to lead a decent life. Have a good one.” Then she hung up.
A few days later, Dr. V sent one final email. It had her signature mix of scattered punctuation and randomly capitalized words. Once upon a time I had brushed off these grammatical quirks, but now they seemed like outward expressions of the inner chaos she struggled to contain. “To whom this may concern,” it read. “I spoke with Caleb Hannan last Saturday his deportment is reminiscent to schoolyard bullies, his sole intention is to injure or bring harm to me … Because of a computer glitch, some documents that are germane only to me, were visible to web-viewers, government officials have now rectified this egregious condition … Caleb Hannan came into possession of documents that were clearly marked: MADE NON-PUBLIC (Restricted) … Exposing NON-PUBLIC Documents is a Crime, and prosecution of such are under the auspices of many State and Federal Laws, including Hate Crimes Legislation signed into Law by President Obama.”
Over the course of what was now eight months of reporting, Dr. V had accused me of being everything from a corporate spy to a liar and a fraud. She had also threatened me. One of the quotes I was able to type down during our last conversation was this: “You have no idea what I have done and what I can do.” It’s not all that menacing when transcribed, but her tone made it clear she believed she could harm me. Yet despite all that, the main emotion I felt while reading her desperate, last-ditch email was sadness. Although there were times when I had been genuinely thrilled with the revelation that Dr. V’s official narrative didn’t line up with reality, there was nothing satisfying about where the story had ended up. People had been hurt by Dr. V’s lies, but she was the person who seemed to be suffering most.
Not long after she sent her email, I got a call from a Pennsylvania phone number that I didn’t recognize. It was Dr. V’s ex-brother-in-law, who represented the closest I had gotten to finding someone who could tell me what she’d been like in her previous life. “Well, there’s one less con man in the world now,” he said. Even though he hated his former family member, this seemed like an especially cruel way to tell me that Dr. V had died. All he could tell me was what he knew — that it had been a suicide. A few weeks later a police report filled in the details.
Around 11 a.m. on October 18, Jordan walked into the home office she shared with Dr. V and found pieces from her business partner’s jewelry collection laid out on a desk next to some handwritten letters. Each letter explained which friend or family member was to get which piece of jewelry in case of Dr. V’s death. Jordan then noticed that Dr. V’s car was missing. At first, Jordan explained to the police, she didn’t think much of the missing vehicle. Jordan prepared some breakfast and then drove to her nearby apartment. When Jordan arrived and reached her bedroom, she found Dr. V lying on the floor curled in a fetal position with a white plastic bag over her head; an empty bottle of pills sat on the kitchen counter.
Writing a eulogy for a person who by all accounts despised you is an odd experience. What makes it that much harder is that Dr. V left so few details — on purpose, of course. Those who knew her in her past life refused to talk about her. Those who knew her in the life she had created were helpful right up to the point where that new life began to look like a lie. The only person who can provide this strange story with its proper ending is the person who started it. The words she spoke came during our last conversation, when she was frantically trying to convince me of things I knew couldn’t possibly be true. Yet though they may have been spoken by a desperate person at one of the most desperate times in a life that had apparently seen many, it’s hard to argue with Dr. V’s conclusions. “Nobody knows my life but me,” she said. “You don’t know what the truth is.”
 



Facebook
Twitter
Print



Share




Filed Under: Sports, Golf, Gary McCord, Karsten Solheim, PGA Tour



 

Archive







More From 
More Sports
More Features



More from 


Dr. V’s Magical Putter  January 15, 2014


See all from 


More Sports


Medieval Times: The Armored Combat League Makes Sport Out of Swords and Shields  August 11, 2015


Your Non-Major Sports Heroes of Summer  July 22, 2015


‘Rembert Explains’ Podcast: D’Angelo, and Reviewing the Year in Sports With Clinton Yates  December 18, 2014


‘B.S. Report’: Chris Rock and Bill’s Dad  December 18, 2014


Arsenio Hall’s Black Curling Bit vs. Steve Harvey’s Black Curling Bit  February 12, 2014


See all Sports


More Features


Blades of Glory  October 28, 2015


Russell, the Creator  October 28, 2015


30 for 30 Shorts: Every Day  October 28, 2015


The 15 Biggest Plays in Baseball History  October 27, 2015


The Laughs, Pathos, and Overwhelming Talent of Jan Hooks  October 20, 2015


See all Features








Top Stories






The Weight of the Interim Label

by Bill Barnwell 







World Series Preview: Five Questions for Games Three and Four

by Ben Lindbergh 







50 Scenes That Do Not Appear in the ‘X-Files’ Revival

by Brian Phillips 



Click here to watch ‘Jalen & Jacoby,’ Grantland Features, and video podcasts!


 

 

Top Stories






The Weight of the Interim Label

by Bill Barnwell 







World Series Preview: Five Questions for Games Three and Four

by Ben Lindbergh 







50 Scenes That Do Not Appear in the ‘X-Files’ Revival

by Brian Phillips 



Click here to watch ‘Jalen & Jacoby,’ Grantland Features, and video podcasts!









 

Most Popular














Features




The Triangle




Hollywood Prospectus




















Twitter
Facebook
 
Features
The Triangle
The Hollywood Prospectus
Contributors
Podcasts
Video
Quarterly
ESPN.com
 
Terms of Use and Privacy Policy and Safety Information/Your California Privacy Rights/Children's Online Privacy Policy are applicable to you.
© 2023 ESPN Internet Ventures. All rights reserved. Interest-Based Ads. Cookie Policy.
Powered by WordPress VIP










 


"
https://news.ycombinator.com/rss,The Scandal of Financial Nihilism,https://read.lukeburgis.com/p/the-scandal-of-financial-nihilism,Comments,"























The Scandal of Financial Nihilism - by Luke Burgis













SubscribeSign inShare this postThe Scandal of Financial Nihilismread.lukeburgis.comCopy linkTwitterFacebookEmailThe Scandal of Financial NihilismThe Wheels Keep on Turning—and the anti-mimetic breaks are breaking. Luke BurgisJan 14933Share this postThe Scandal of Financial Nihilismread.lukeburgis.comCopy linkTwitterFacebookEmailArticle voiceover1×0:00-16:29Audio playback is not supported on your browser. Please upgrade.“It is because you don't know the end and purpose of things that you think the wicked and the criminal have power and happiness.” — Boethius, On the Consolation of PhilosophyNobody has a precise idea how much wealth anyone else has. There is a good chance that you don’t even know your own. We are all engaged in an elaborate game of assumptions, even about our own future. The fact that men are posting gain and loss porn on Reddit is not a sign of transparency, but opacity. The narratives only go skin deep. Beneath the narratives, people are silently crying out for order—a link between cause and effect, or some indication that hard work still matters, or the assurance that they will be rewarded for good decisions and not for reckless ones.When you make and/or lose a lot of money quickly, without the ability to see exactly why and how the windfall or bust happened, it can lead to financial nihilism. When you see—or think you see—other people doing the same, it can lead to financial nihilism. When you work very hard and do All the Right Things (as Dave Ramsey, or some other financial guru, would have you do) and then proceed to lose most of your net worth in the course of a month due to unexpected circumstances, it can lead to financial nihilism. There are a few different ways of defining this particular form of nihilism. Each of them points to a different aspect of it.My friend Demetri Kofinas has referred to financial nihilism as “a philosophy that treats the objects of speculation as though they were intrinsically worthless.” It is a philosophy that fully embraces the view that reality is completely subjective. I should note that the Subjective Theory of Value in economics does not, in any way, imply that reality itself is subjective—but rather that subjects determine the value of an object, and ultimately its price, through their collective decisions. Each of these actors could be acting with a greater or lesser vision of and adherence to reality. But for the financial nihilist, the subjective completely subsumes the objective. The truth is that both exist. For example: beauty, at least classically understood, is objective—it has certain characteristics, like claritas (clarity), consonantia (proportion), integritas (integrity/coherence)—but I perceive it subjectively. Conflating the two leads to problems. Financial nihilism is usually downstream from ontological nihilism. I don’t think people are nihilistic in a domain-specific sense; I believe that nihilism is diffusive and easily bleeds over into all aspects of life, from relationships to one’s health. (For instance: you hear about a friend who takes extremely good care of himself but is nevertheless unexpectedly diagnosed with cancer, and so you think: ‘screw it, what’s the use of this discipline?’). We have entered a truly nihilistic age, which is reflected in our politics as much if not more than our markets. It’s as if it doesn’t actually matter what people say or do anymore. This spirit of political nihilism was summed up by Donald Trump saying “I could stand in the middle of Fifth Avenue and shoot somebody, and I wouldn't lose any voters, OK? It's, like, incredible."" Yes, it is incredible. And that’s why politics is no longer credible. Nihilism, like all of life, ultimately comes down to the fundamental question of belief. People don’t know who they can trust or who they can believe in, but they must believe in something. There is a time-bound aspect to financial nihilism. If in one second you can lose or make a lot of money in a manner that feels completely detached from any amount of “value” that you believe you created  (the idea that all people who have money must have created a ton of “value” is problematic, because most people don’t understand what value means in this specific case—which is subjective value), then you begin to get the sense that the quality of your work has no connection to what people will pay for it, or to financial success. Not even your intelligence can save you. You can analyze a single stock for six months (as one long-only hedge fund manager told me) and buy it with a complete conviction that it’s undervalued, and then watch as it loses 80% in the course of a single year. If the bears are making money based purely on mimesis and momentum, what’s the use of pouring yourself out doing analysis for 40+ hours? Eventually, you come to place here you simply wait, second by second, to see if anything changes. Having lost a firm belief in your own agency, you place bets, you wait, you see what happens. If nothing matters, then what good is there in being good? Abandonment to non-divine providence is the only thing left.If you’re unable to transcend the transactional—to exit the logic of the game (the gamification of the stock market, the NFT swindles, the latest crypto narrative)—you simply enter deeply into it for fear of being left out or left behind. The small amount of nihilism that led to your decision to play breeds more nihilism once you’re deep inside of the game. Now, in the moment, you may feel that you truly believe in the game, but you don’t really. You can’t even be an honest nihilist because you have had to make friends with dishonest wealth.Lastly, a financial nihilist can be described as someone who has no skin in the game. No doubt we could list more, but I’ll leave it at that for now. SubscribeFinancial nihilism comes down to an even more fundamental problem. It is an epistemic problem. Many people simply see nothing worth believing in and so they are, in the words of Neil Postman, amusing themselves to death.Unfortunately, I think it’s even darker than that. Financial nihilism very often stems from a basic (unacknowledged) belief that life is simply not worth living. If we’re bankrupt a few years from now, who cares? We don’t want to be here in the first place. If a person lacks the will to live and engage deeply with the world, what is there to fear about financial loss? The pain of loss may in fact be the only way they have left to feel anything at all. Nihilism lies at the heart of many of our health problems, political problems, and relationship problems. When there is no hope for the future, why do we expect people to care about decisions that affect the future? For lack of vision people perish. Lastly, we lack a rich understanding of the difference between wealth and money. Wealth, as Paul Graham has written, is what we want: food, clothes, a home, the ability to travel and learn about the world, and so forth. Money is simply a system we’ve devised for helping us get the things that we want, or think we want, in a specialized world. Obviously, though, we want much more than food, clothes, houses, cars, and things. We want loving relationships, lives full of beauty, nourishing conversations, and many more immaterial things that we often say we “can’t put a price on.”But of course we can. How much would you pay, right now, to be magically transported to a wonderful meal with a handful of wise people who are interested in you and wanted to engage in conversation about all of the topics that were most important to you? I would pay a handsome price.   I can’t buy that, though. Nobody can. (Some extraordinarily rich people do attempt to buy these kinds of experiences and relationships, but those purchases carry one big caveat: genuine, loving care and attention is not something anyone can buy. Many extremely rich people go their entire lives wondering if anyone is really their friend—someone interested in them for who they are, rather than their bank account. I can’t imagine that kind of existential loneliness on account of money.) What I can do, though, is invest in things that I feel matter—and resist the temptation to invest in things that I don’t think matter merely so that I’m part of the game. I can refuse to play a nihilistic game. That is one freedom that each of has. We get to choose.Financial nihilists of the speculative variety aren’t necessarily greedy people who seek money at all costs. They are people who have ceased to believe that money means anything—so it ends up meaning everything. The financial nihilists actually have something right: money doesn’t matter as much as most people think it does. Their downfall is their inability to believe that a greater reality exists for which money is simply one layer. The economic layer is an important layer, no doubt, but it is not the Layer Zero of life. If money is ordered to nothing else, then it becomes ordered to itself—to its own propagation and self-replication. And that is precisely the definition of a meme. That is why we have a meme market and a meme economy. SubscribeWhen I was a kid, I begged my parents for a Nintendo Entertain System for well over two years before they finally gave in. Well, my dad gave in. All of my friends had one. My mimetic desire was raging. One day, I sat down and made a list of “100 reasons why I should have a Nintendo.” (A miraculous accomplishment, when I think about it). I wish it was still around somewhere because it would be hilarious to read. I would’ve gladly posted it here for your amusement.I don’t remember a single item on the list, but I know that I came up with 100. I’m sure some of them were noble (I had pledged to buy learning games that would help develop my brain—that much I do remember). But many of my “reasons” were likely suspect, to say the least.My dad took one look at the list and decided that he had had enough. It was not that the list had convinced him that I should have a Nintendo. I can’t imagine that it was all that persuasive. It was the fact that I had actually taken the time to make it. He just couldn’t take it anymore. I think he thought that his kid, his only son and only child, had finally lost it.He told me to get in the car and drove me to the nearest Best Buy where he bought me a Nintendo and told me to pick out three games (I would’ve been happy with one). When he got home, he got in a little spat with my mom who couldn’t believe that he had succumbed. I started feeling bad. Had I manipulated him into buying it for me? My dad was a truck driver. He worked his ass off for every penny he ever made. He had no idea what a Nintendo even was. After seeing my mom’s concern, and my own distress, my dad turned to me and said: “Son, it’s only money.” He smiled, sat down, cracked open a beer, and started watching a Detroit Tigers game.I’ve never forgotten that. My dad wasn’t a financial nihilist. He—perhaps more than anybody I’ve ever known—knew what money is. It was something real to him because it led to, or made possible, real things. But he had perspective, and he had order. He could say “it’s only money” without a hint of the cool cynicism of the man who blows his Bitcoin windfall on a new car that he doesn’t need. My dad said “it’s only money” because he secretly knew that he was wealthy.  3Share this postThe Scandal of Financial Nihilismread.lukeburgis.comCopy linkTwitterFacebookEmailPrevious3 CommentsLuke BurgisJan 15AuthorFriends, there were a couple of types in the original piece. I apologize for that. I'm still a one-man shop at this point (but looking for help...!). Thanks to the few of you who pointed them out. I corrected what I was able to find, but always welcome you pointing them out. Thank you, as always, for reading. Expand full commentReplyCollapseAndrew VierraJan 16Really liking the article voiceover enjoying how it works best for me by using my phone/bluetooth and reading along on the laptop for pausing purposes. The article was excellent and highlights for me something Gil Bailie talked about  when he mentioned a virtical relationship and a horizontal relationship in a talk with the Dominican School on Youtube. Or the talk I had with my gf this morning about the 1st and the 10th Commandment. We will always worship something whether its a partner, money, or a car but to have a primary relationship with something greater will be more fruitful if it comes first. The article also reminds me of the book The Noonday Devil by Jean Charles Nault OSB in writing about Acedia the author outlines one who flees their cell spreading scandal in an empty attempt to avoid ones primary duty. Beautiful how the article ends with something I will always try to silently remember to myself that fondness inside of each of us that is wealth. Expand full commentReplyCollapse1 more comments…TopNewCommunityNo postsReady for more?Subscribe© 2023 Luke BurgisPrivacy ∙ Terms ∙ Collection notice Publish on SubstackGet the appSubstack is the home for great writing

















        This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts
    



"
https://news.ycombinator.com/rss,Tell HN: Gmail's spam filters have gone bonkers,https://news.ycombinator.com/item?id=34411009,Comments,"

Tell HN: Gmail's spam filters have gone bonkers | Hacker News

Hacker News
new | past | comments | ask | show | jobs | submit 
login




 Tell HN: Gmail's spam filters have gone bonkers
26 points by thyrox 4 hours ago  | hide | past | favorite | 14 comments 

I don't know what is happening with Gmail but lately Gmail's aggressive spam filtering created a huge problem for me.It was a reply I sent to an email yet Gmail somehow figured out it was spam and the other person never got it (which I learned after 1 week)Two weeks before they sent the property tax notice (from the govt) in spam and had I missed that email there would  have been a  penalty.This is a very old Gmail address almost 10 years or more so I don't know what's happening but I've stopped trusting Gmail altogether after this.This spam thing is such a black box, so don't be like me and check your spam folder just like you check your inbox if you're on Gmail.P.S. Also looking for a good alternative to gmail if anybody has some suggestions. Right now I've just created a filter to send all email to inbox. 
 
  
 
MandieD 48 minutes ago  
             | next [–] 

My home state’s (Texas) online ballot printing site for small counties got its mails to my GMail sent to spam in 2018 and 2020, and not even that far in 2022.That domain doesn’t have SPF or DKIM configured, and the link to click for generating the ballot is HTTP and an IP address (not even a proper DNS name), so no wonder.However, I’d expect those mails to be delivered anyway since I marked the address as safe each time, but here we are.
 
reply



  
 
gameshot911 41 minutes ago  
             | parent | next [–] 

>However, I’d expect those mails to be delivered anyway since I marked the address as safe each time, but here we are.Exactly - this is the biggest thing for me.  I understand that some mail may erroneously be identified as spam, but when I explicitly mark it as Not Spam, I would expect my mail provider to respect that decision (for my inbox personally, at least).I had a re-occuring issue where the payment receipt from my small gym kept going to spam.  I think after marking it Not Spam for like 6 months it eventually started appearing in the Inbox sportatically.
 
reply



  
 
h2odragon 2 hours ago  
             | prev | next [–] 

They marked one of my two ""Google Calendar"" notifications as spam, this weekend.I think that ""providing service to customers"" was never very high on the list of priorities there, and has fallen off completely in the past few years.Anybody working there who still has that as a priority is probably facing an uphill battle against management process and social issues to even open the conversation.
 
reply



  
 
kioleanu 2 hours ago  
             | prev | next [–] 

We've had our problems, but Fastmail does a pretty good job. Not perfect, sure, but I don't remember having to send an e-mail to spam in the last 2 years.Or not receiving e-mails. Or recipients not receiving my emails. I had my doubts, because of the custom domain, but it's been ok until now.I have it set up to load my gmail e-mails also and it filters spam messages that gmail missed. Meaning, I log in to gmail and see spam messages in the inbox, that never reacahed my Fastmail inbox. In the past months, with about 100% success rate.
 
reply



  
 
gandalfian 1 hour ago  
             | prev | next [–] 

Still better than the filters that just invisibly destroy spam without ever telling you. (I'm looking at you namecheap and numerous others. And my aunt's isp that vaporised any incoming email containing the word chicken and had no one who could help. Most British isps have given up having email at all, the expertise has gone).
 
reply



  
 
rgavuliak 1 hour ago  
             | prev | next [–] 

At the same time for me Spam gets through that pretends to be tied to loyalty programs.
 
reply



  
 
frogperson 1 hour ago  
             | prev | next [–] 

I suspect it has something to do with Gmail's policy change on nov 2022 when they started requiring spf, dkim, and dmarc.There are probably a ton of misconfigured domains and non-google email servers that interpret the new configurations slightly differently.
 
reply



  
 
warrenm 53 minutes ago  
             | prev | next [–] 

>This is a very old Gmail address almost 10 years or more so I don't know what's happening but I've stopped trusting Gmail altogether after this.I haven't ""trusted"" Gmail (except to stay online) for a lot longer than 10 years (and my 'main' Gmail address goes back to when it was invite-only almost 19 years ago)
 
reply



  
 
prepend 2 hours ago  
             | prev | next [–] 

I’ve noticed gmail getting worse lately too. Hotmail as well, and I think even much more.I’m not sure what’s happening and wonder if it’s just spammers being smarter.I’ve had gmail since a few months after launch and spam seems worst this last year. Today I got two spam messages in my inbox offering to eliminate my energy bill.And I’ve had OTP codes sent to spam.This is particularly risky as my banks and pretty much everything are pushing paperless. And I had a mortgage company that wouldn’t ever send paper and was completely digital.
 
reply



  
 
neilsimp1 3 hours ago  
             | prev | next [–] 

I have the opposite issue - a LOT of spam makes it's way into my Inbox.
 
reply



  
 
Demonsult 1 hour ago  
             | prev | next [–] 

The ""user as the product"" model works well for the user for a while, when business is booming. I now pay Microsoft for email simply because they have someone answering the phone over there, competent or not. I get Office as part of the deal.
 
reply



  
 
lgl 3 hours ago  
             | prev | next [–] 

A perfect anti-spam solution doesn't exist.Even if you switch e-mail provider, as long as they're also using some sort of anti-spam system, false positives will always happen occasionally.The obvious recommendation is to always add any trusted senders to your contacts or, as you've already done, create some filters.Alternatively, switch to a provider that doesn't use any spam filtering at all. But that will open up a whole different can of worms.
 
reply



  
 
intc 2 hours ago  
             | parent | next [–] 

We use server side spam filtering at c1 (see: https://c1.fi/about/). So far things are working pretty well but certainly false positives may occur. On the positive side client can contact yours truly and we can usually do something to sort these out.
 
reply



  
 
readonthegoapp 32 minutes ago  
             | prev [–] 

my gmail filters started blowing up some time ago -- a year or two?i variously deactivated some/all/etc., re-activated, re-created, broke-out, aggregated, etc.none of it really seems to matter.i don't actually get a lot of mail, i just stopped checking all day every day, so i started missing a lot of stuff b/c i would just never catch up.
 
reply







Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
Search:  


"
https://news.ycombinator.com/rss,MacBook Pro featuring M2 Pro and M2 Max,https://www.apple.com/newsroom/2023/01/apple-unveils-macbook-pro-featuring-m2-pro-and-m2-max/,Comments,"


opens in new window




					Apple Newsroom needs your permission to enable desktop notifications when new articles are published
				










PRESS RELEASE
January 17, 2023






                
                
                    Apple unveils MacBook Pro featuring M2 Pro and M2 Max, with more game-changing performance and the longest battery life ever in a Mac
                
            




            
            
                New MacBook Pro features up to 6x faster performance than fastest Intel-based MacBook Pro and support for up to 96GB of unified memory for demanding pro workflows
            
        























































                            Today Apple introduced the new MacBook Pro with M2 Pro and M2 Max.
                        






CUPERTINO, CALIFORNIA Apple today announced the new 14- and 16-inch MacBook Pro featuring M2 Pro and M2 Max, Apple’s next-generation pro silicon that brings even more power-efficient performance and battery life to pro users. With M2 Pro and M2 Max — the world’s most powerful and efficient chip for a pro laptop — MacBook Pro tackles demanding tasks, like effects rendering, which is up to 6x faster than the fastest Intel-based MacBook Pro, and color grading, which is up to 2x faster.1 Building on the unprecedented power efficiency of Apple silicon, battery life on MacBook Pro is now up to 22 hours — the longest battery life ever in a Mac.2 For enhanced connectivity, the new MacBook Pro supports Wi-Fi 6E,3 which is up to twice as fast as the previous generation, as well as advanced HDMI, which supports 8K displays for the first time. With up to 96GB of unified memory in the M2 Max model, creators can work on scenes so large that PC laptops can’t even run them.4 Rounding out the unrivaled features of MacBook Pro are its renowned Liquid Retina XDR display, extensive array of connectivity, 1080p FaceTime HD camera, six-speaker sound system, and studio-quality mics. When combined with macOS Ventura, the MacBook Pro user experience is unrivaled. Customers can order the new 14- and 16-inch MacBook Pro today, with availability beginning Tuesday, January 24.

“MacBook Pro with Apple silicon has been a game changer, empowering pros to push the limits of their workflows while on the go and do things they never thought possible on a laptop,” said Greg Joswiak, Apple’s senior vice president of Worldwide Marketing. “Today the MacBook Pro gets even better. With faster performance, enhanced connectivity, and the longest battery life ever in a Mac, along with the best display in a laptop, there’s simply nothing else like it.”

















                Available in 16- and 14-inch models, MacBook Pro delivers more performance, advanced connectivity, and the longest battery life ever in a Mac.
              






Unrivaled Power-Efficient Performance with M2 Pro and M2 Max

With M2 Pro and M2 Max, MacBook Pro is capable of transforming pro workflows across a wide range of disciplines, from art to science to app development. Users looking to upgrade from Intel-based Mac models will experience even more dramatic improvements in performance, battery life, connectivity, and overall productivity. MacBook Pro also maintains performance whether users are plugged in or on battery.

















                MacBook Pro with M2 Pro and M2 Max transforms pro workflows, empowering users to push their creativity even further.
              






MacBook Pro with M2 Pro features a 10- or 12-core CPU with up to eight high-performance and four high-efficiency cores for up to 20 percent greater performance over M1 Pro. With 200GB/s of unified memory bandwidth — double the amount in M2 — and up to 32GB of unified memory, users can tackle large projects and run multiple pro apps with blazing speed. A next-generation GPU with up to 19 cores delivers up to 30 percent more graphics performance, and the Neural Engine is 40 percent faster, speeding up machine learning tasks like video analysis and image processing. The powerful media engine in M2 Pro also tears through the most popular video codecs, dramatically accelerating video playback and encoding while using very little power.

With M2 Pro on MacBook Pro:


Rendering titles and animations in Motion is up to 80 percent faster1 than the fastest Intel-based MacBook Pro and up to 20 percent faster5 than the previous generation.
Compiling in Xcode is up to 2.5x faster1 than the fastest Intel-based MacBook Pro and nearly 25 percent faster5 than the previous generation.
Image processing in Adobe Photoshop is up to 80 percent faster1 than the fastest Intel-based MacBook Pro and up to 40 percent faster5 than the previous generation.









Motion is shown on MacBook Pro with M2 Pro.


Xcode is shown on MacBook Pro with M2 Pro.


Adobe Photoshop is shown on MacBook Pro with M2 Pro.


















With M2 Pro, animations in Motion render 80 percent faster than the Intel-based MacBook Pro, and 20 percent faster than the previous generation.



















With M2 Pro, compiling in Xcode is up to 2.5x faster than the fastest Intel-based MacBook Pro, and nearly 25 percent faster than the previous generation.



















With M2 Pro, image processing in Adobe Photoshop is up to 80 percent faster than the fastest Intel-based MacBook Pro, and up to 40 percent faster than the previous generation.











previous




next








MacBook Pro with M2 Max pushes workflows to the extreme with a much larger GPU featuring up to 38 cores and delivering up to 30 percent greater graphics performance over M1 Max, and also includes 400GB/s of unified memory bandwidth — twice that of M2 Pro. With up to 96GB of unified memory, MacBook Pro once again pushes the limits of graphics memory in a laptop to enable intensive graphics workloads, such as creating scenes with extreme 3D geometry and textures, or merging massive photographic panoramas. M2 Max has a next-gen 12-core CPU with up to eight high-performance and four high-efficiency cores that delivers up to 20 percent greater performance over M1 Max, and a more powerful media engine than M2 Pro, with twice the ProRes acceleration to dramatically speed up media playback and transcoding.

With M2 Max on MacBook Pro:


Effects rendering in Cinema 4D is up to 6x faster1 than the fastest Intel-based MacBook Pro and up to 30 percent faster6 than the previous generation.
Color grading in DaVinci Resolve is up to 2x faster1 than the fastest Intel-based MacBook Pro and up to 30 percent faster6 than the previous generation.









Cinema 4D is shown on MacBook Pro.


DaVinci Resolve is shown on MacBook Pro.


















With M2 Max, effects rendering in Cinema 4D is up to 6x faster than the fastest Intel-based MacBook Pro and up to 30 percent faster than the previous generation.



















With M2 Max, color grading in DaVinci Resolve is up to 2x faster than the fastest Intel-based MacBook Pro and up to 30 percent faster than the previous generation.











previous




next








Enhanced Connectivity

MacBook Pro now features Wi-Fi 6E3 for faster wireless connectivity, as well as more advanced HDMI, to support 8K displays up to 60Hz and 4K displays up to 240Hz. These new capabilities build on the versatile connectivity options already in MacBook Pro, including three Thunderbolt 4 ports for high-speed connection to peripherals, an SDXC card slot, and MagSafe 3 charging.








MagSafe 3, Thunderbolt 4, and the headphone jack are shown on MacBook Pro.


An SDXC card slot, Thunderbolt 4 port, and HDMI port are shown on MacBook Pro.


















MacBook Pro features three Thunderbolt 4 ports to connect high-speed peripherals, MagSafe 3 charging, and a headphone jack that supports high-impedance headphones.



















MacBook Pro incudes an SDXC card slot for fast access to media and an HDMI port for conveniently connecting to TVs and displays, including 8K displays for the first time.











previous




next








macOS Ventura

With macOS Ventura, MacBook Pro delivers even more performance and productivity. Powerful updates like Continuity Camera bring videoconferencing features to any Mac, including Desk View, Center Stage, Studio Light, and more. Handoff in FaceTime allows users to start a FaceTime call on their iPhone or iPad and fluidly pass it over to their Mac, or vice versa. And tools like Stage Manager automatically organize apps and windows, so users can concentrate on the task at hand and still see everything in a single glance.

Messages and Mail are better than ever, while Safari — the world’s fastest browser on Mac — ushers in a passwordless future with passkeys. With iCloud Shared Photo Library, users can now create and share a separate photo library among up to six family members, and the new Freeform app provides a flexible canvas that helps users be more productive and expressive, whether they are planning or brainstorming on their own, or together with others. With the power and popularity of Apple silicon, and new developer tools in Metal 3, gaming on Mac has never been better.








Stage Manager is shown in macOS Ventura on MacBook Pro.


Freeform is shown in macOS Ventura on MacBook Pro.


















Features like Stage Manager on macOS Ventura help users stay focused and get more done.



















With macOS Ventura, users get access to the new Freeform app, which makes visual collaboration easier than ever.











previous




next








MacBook Pro and the Environment

MacBook Pro is designed to minimize its impact on the environment, including 100 percent of the following recycled materials: aluminum in the enclosure, rare earth elements in all magnets, tin in the solder of the main logic board, and gold in the plating of multiple printed circuit boards. It also features 35 percent or more recycled plastic in multiple components, and meets Apple’s high standards for energy efficiency. MacBook Pro is free of numerous harmful substances, and 97 percent of the packaging is fiber based, bringing Apple closer to its goal of completely removing plastic from its packaging by 2025. 

Today, Apple is carbon neutral for global corporate operations, and by 2030, plans to be 100 percent carbon neutral across the entire manufacturing supply chain and all product life cycles. This means that every Apple device sold, from component manufacturing, assembly, transport, customer use, charging, all the way to recycling and material recovery, will have net-zero climate impact.





Pricing and Availability


The new MacBook Pro models with M2 Pro and M2 Max are available to order today, January 17, on apple.com/store and in the Apple Store app in 27 countries and regions, including the US. They will begin arriving to customers and will be in Apple Store locations and Apple Authorized Resellers starting Tuesday, January 24.
MacBook Pro with M2 Pro and M2 Max will be available in Australia, China, Hong Kong, Japan, Macau, New Zealand, and Singapore beginning Friday, February 3.
The new 14-inch MacBook Pro with M2 Pro starts at $1,999 (US), and $1,849 (US) for education; and the 16-inch MacBook Pro with M2 Pro starts at $2,499 (US), and $2,299 (US) for education. Additional technical specifications, configure-to-order options, and accessories are available at apple.com/mac.
Every customer who buys a Mac from Apple can enjoy a free Online Personal Session with an Apple Specialist, get their product set up in select stores including help with data transfer, and receive guidance on how to make their new Mac work the way they want.
With Apple Trade In, customers can trade in their current computer and get credit toward a new Mac. Customers can visit apple.com/shop/trade-in to see what their device is worth.
AppleCare+ for Mac provides expert technical support and additional hardware coverage from Apple, including up to two incidents of accidental damage protection every 12 months, each subject to a fee.






Share article

















































Text of this article


January 17, 2023
PRESS RELEASE
Apple unveils MacBook Pro featuring M2 Pro and M2 Max, with more game-changing performance and the longest battery life ever in a Mac
New MacBook Pro features up to 6x faster performance than fastest Intel-based MacBook Pro and support for up to 96GB of unified memory for demanding pro workflows
CUPERTINO, CALIFORNIA Apple today announced the new 14- and 16-inch MacBook Pro featuring M2 Pro and M2 Max, Apple’s next-generation pro silicon that brings even more power-efficient performance and battery life to pro users. With M2 Pro and M2 Max — the world’s most powerful and efficient chip for a pro laptop — MacBook Pro tackles demanding tasks, like effects rendering, which is up to 6x faster than the fastest Intel-based MacBook Pro, and color grading, which is up to 2x faster.1 Building on the unprecedented power efficiency of Apple silicon, battery life on MacBook Pro is now up to 22 hours — the longest battery life ever in a Mac.2 For enhanced connectivity, the new MacBook Pro supports Wi-Fi 6E,3 which is up to twice as fast as the previous generation, as well as advanced HDMI, which supports 8K displays for the first time. With up to 96GB of unified memory in the M2 Max model, creators can work on scenes so large that PC laptops can’t even run them.4 Rounding out the unrivaled features of MacBook Pro are its renowned Liquid Retina XDR display, extensive array of connectivity, 1080p FaceTime HD camera, six-speaker sound system, and studio-quality mics. When combined with macOS Ventura, the MacBook Pro user experience is unrivaled. Customers can order the new 14- and 16-inch MacBook Pro today, with availability beginning Tuesday, January 24.
“MacBook Pro with Apple silicon has been a game changer, empowering pros to push the limits of their workflows while on the go and do things they never thought possible on a laptop,” said Greg Joswiak, Apple’s senior vice president of Worldwide Marketing. “Today the MacBook Pro gets even better. With faster performance, enhanced connectivity, and the longest battery life ever in a Mac, along with the best display in a laptop, there’s simply nothing else like it.”
Unrivaled Power-Efficient Performance with M2 Pro and M2 Max
With M2 Pro and M2 Max, MacBook Pro is capable of transforming pro workflows across a wide range of disciplines, from art to science to app development. Users looking to upgrade from Intel-based Mac models will experience even more dramatic improvements in performance, battery life, connectivity, and overall productivity. MacBook Pro also maintains performance whether users are plugged in or on battery.
MacBook Pro with M2 Pro features a 10- or 12-core CPU with up to eight high-performance and four high-efficiency cores for up to 20 percent greater performance over M1 Pro. With 200GB/s of unified memory bandwidth — double the amount in M2 — and up to 32GB of unified memory, users can tackle large projects and run multiple pro apps with blazing speed. A next-generation GPU with up to 19 cores delivers up to 30 percent more graphics performance, and the Neural Engine is 40 percent faster, speeding up machine learning tasks like video analysis and image processing. The powerful media engine in M2 Pro also tears through the most popular video codecs, dramatically accelerating video playback and encoding while using very little power.
With M2 Pro on MacBook Pro:

Rendering titles and animations in Motion is up to 80 percent faster1 than the fastest Intel-based MacBook Pro and up to 20 percent faster5 than the previous generation.
Compiling in Xcode is up to 2.5x faster1 than the fastest Intel-based MacBook Pro and nearly 25 percent faster5 than the previous generation.
Image processing in Adobe Photoshop is up to 80 percent faster1 than the fastest Intel-based MacBook Pro and up to 40 percent faster5 than the previous generation.

MacBook Pro with M2 Max pushes workflows to the extreme with a much larger GPU featuring up to 38 cores and delivering up to 30 percent greater graphics performance over M1 Max, and also includes 400GB/s of unified memory bandwidth — twice that of M2 Pro. With up to 96GB of unified memory, MacBook Pro once again pushes the limits of graphics memory in a laptop to enable intensive graphics workloads, such as creating scenes with extreme 3D geometry and textures, or merging massive photographic panoramas. M2 Max has a next-gen 12-core CPU with up to eight high-performance and four high-efficiency cores that delivers up to 20 percent greater performance over M1 Max, and a more powerful media engine than M2 Pro, with twice the ProRes acceleration to dramatically speed up media playback and transcoding.
With M2 Max on MacBook Pro:

Effects rendering in Cinema 4D is up to 6x faster1 than the fastest Intel-based MacBook Pro and up to 30 percent faster6 than the previous generation.
Color grading in DaVinci Resolve is up to 2x faster1 than the fastest Intel-based MacBook Pro and up to 30 percent faster6 than the previous generation.

Enhanced Connectivity
MacBook Pro now features Wi-Fi 6E3 for faster wireless connectivity, as well as more advanced HDMI, to support 8K displays up to 60Hz and 4K displays up to 240Hz. These new capabilities build on the versatile connectivity options already in MacBook Pro, including three Thunderbolt 4 ports for high-speed connection to peripherals, an SDXC card slot, and MagSafe 3 charging.
macOS Ventura
With macOS Ventura, MacBook Pro delivers even more performance and productivity. Powerful updates like Continuity Camera bring videoconferencing features to any Mac, including Desk View, Center Stage, Studio Light, and more. Handoff in FaceTime allows users to start a FaceTime call on their iPhone or iPad and fluidly pass it over to their Mac, or vice versa. And tools like Stage Manager automatically organize apps and windows, so users can concentrate on the task at hand and still see everything in a single glance.
Messages and Mail are better than ever, while Safari — the world’s fastest browser on Mac — ushers in a passwordless future with passkeys. With iCloud Shared Photo Library, users can now create and share a separate photo library among up to six family members, and the new Freeform app provides a flexible canvas that helps users be more productive and expressive, whether they are planning or brainstorming on their own, or together with others. With the power and popularity of Apple silicon, and new developer tools in Metal 3, gaming on Mac has never been better.
MacBook Pro and the Environment
MacBook Pro is designed to minimize its impact on the environment, including 100 percent of the following recycled materials: aluminum in the enclosure, rare earth elements in all magnets, tin in the solder of the main logic board, and gold in the plating of multiple printed circuit boards. It also features 35 percent or more recycled plastic in multiple components, and meets Apple’s high standards for energy efficiency. MacBook Pro is free of numerous harmful substances, and 97 percent of the packaging is fiber based, bringing Apple closer to its goal of completely removing plastic from its packaging by 2025. 
Today, Apple is carbon neutral for global corporate operations, and by 2030, plans to be 100 percent carbon neutral across the entire manufacturing supply chain and all product life cycles. This means that every Apple device sold, from component manufacturing, assembly, transport, customer use, charging, all the way to recycling and material recovery, will have net-zero climate impact.
Pricing and Availability

The new MacBook Pro models with M2 Pro and M2 Max are available to order today, January 17, on apple.com/store and in the Apple Store app in 27 countries and regions, including the US. They will begin arriving to customers and will be in Apple Store locations and Apple Authorized Resellers starting Tuesday, January 24.
MacBook Pro with M2 Pro and M2 Max will be available in Australia, China, Hong Kong, Japan, Macau, New Zealand, and Singapore beginning Friday, February 3.
The new 14-inch MacBook Pro with M2 Pro starts at $1,999 (US), and $1,849 (US) for education; and the 16-inch MacBook Pro with M2 Pro starts at $2,499 (US), and $2,299 (US) for education. Additional technical specifications, configure-to-order options, and accessories are available at apple.com/mac.
Every customer who buys a Mac from Apple can enjoy a free Online Personal Session with an Apple Specialist, get their product set up in select stores including help with data transfer, and receive guidance on how to make their new Mac work the way they want.
With Apple Trade In, customers can trade in their current computer and get credit toward a new Mac. Customers can visit apple.com/shop/trade-in to see what their device is worth.
AppleCare+ for Mac provides expert technical support and additional hardware coverage from Apple, including up to two incidents of accidental damage protection every 12 months, each subject to a fee.

 About Apple
 Apple revolutionized personal technology with the introduction of the Macintosh in 1984. Today, Apple leads the world in innovation with iPhone, iPad, Mac, Apple Watch, and Apple TV. Apple’s five software platforms — iOS, iPadOS, macOS, watchOS, and tvOS — provide seamless experiences across all Apple devices and empower people with breakthrough services including the App Store, Apple Music, Apple Pay, and iCloud. Apple’s more than 100,000 employees are dedicated to making the best products on earth, and to leaving the world better than we found it.
 
Results are compared to previous-generation 2.4GHz 8-core Intel Core i9-based 16-inch MacBook Pro systems with Radeon Pro 5600M graphics with 8GB HBM2, 64GB of RAM, and 8TB SSD.
Testing was conducted by Apple in November and December 2022 using preproduction 16-inch MacBook Pro systems with Apple M2 Pro, 12-core CPU, 19-core GPU, 16GB of RAM, and 1TB SSD. The Apple TV app movie playback test measures battery life by playing back HD 1080p content with display brightness set to eight clicks from bottom. Battery life varies by use and configuration. See apple.com/batteries for more information.
Wi‑Fi 6E is not available in China mainland. It requires macOS 13.2 or later in Japan.
Testing was conducted by Apple in November and December 2022 using preproduction 16-inch MacBook Pro systems with Apple M2 Max, 12-core CPU, 38-core GPU, 96GB of RAM, and 8TB SSD, as well as a production Intel Core i9-based PC system with NVIDIA Quadro RTX 6000 graphics with 24GB GDDR6 and the latest version of Windows 11 Pro available at the time of testing, and a production Intel Core i9-based PC system with NVIDIA GeForce RTX 3080 Ti graphics with 16GB GDDR6 and the latest version of Windows 11 Home available at the time of testing. OTOY Octane X 2022.1 on preproduction 16-inch MacBook Pro systems and OTOY OctaneRender 2022.1 on Windows systems were tested using a scene that requires over 40GB of graphics memory when rendered. Performance tests are conducted using specific computer systems and reflect the approximate performance of MacBook Pro.
Results are compared to previous-generation 16-inch MacBook Pro systems with Apple M1 Pro, 10-core CPU, 16-core GPU, 32GB of RAM, and 8TB SSD.
Results are compared to previous-generation 16-inch MacBook Pro systems with Apple M1 Max, 10-core CPU, 32-core GPU, 64GB of RAM, and 8TB SSD.

Press Contacts
Starlayne Meza
Apple
starlayne_meza@apple.com
Michelle Del Rio
Apple
mr_delrio@apple.com
(408) 862-1478
Apple Media Helpline
media.help@apple.com
(408) 974-2042

Copy text








Images in this article

Download all images









About Apple

Apple revolutionized personal technology with the introduction of the Macintosh in 1984. Today, Apple leads the world in innovation with iPhone, iPad, Mac, Apple Watch, and Apple TV. Apple’s five software platforms — iOS, iPadOS, macOS, watchOS, and tvOS — provide seamless experiences across all Apple devices and empower people with breakthrough services including the App Store, Apple Music, Apple Pay, and iCloud. Apple’s more than 100,000 employees are dedicated to making the best products on earth, and to leaving the world better than we found it.







Results are compared to previous-generation 2.4GHz 8-core Intel Core i9-based 16-inch MacBook Pro systems with Radeon Pro 5600M graphics with 8GB HBM2, 64GB of RAM, and 8TB SSD.
Testing was conducted by Apple in November and December 2022 using preproduction 16-inch MacBook Pro systems with Apple M2 Pro, 12-core CPU, 19-core GPU, 16GB of RAM, and 1TB SSD. The Apple TV app movie playback test measures battery life by playing back HD 1080p content with display brightness set to eight clicks from bottom. Battery life varies by use and configuration. See apple.com/batteries for more information.
Wi‑Fi 6E is not available in China mainland. It requires macOS 13.2 or later in Japan.
Testing was conducted by Apple in November and December 2022 using preproduction 16-inch MacBook Pro systems with Apple M2 Max, 12-core CPU, 38-core GPU, 96GB of RAM, and 8TB SSD, as well as a production Intel Core i9-based PC system with NVIDIA Quadro RTX 6000 graphics with 24GB GDDR6 and the latest version of Windows 11 Pro available at the time of testing, and a production Intel Core i9-based PC system with NVIDIA GeForce RTX 3080 Ti graphics with 16GB GDDR6 and the latest version of Windows 11 Home available at the time of testing. OTOY Octane X 2022.1 on preproduction 16-inch MacBook Pro systems and OTOY OctaneRender 2022.1 on Windows systems were tested using a scene that requires over 40GB of graphics memory when rendered. Performance tests are conducted using specific computer systems and reflect the approximate performance of MacBook Pro.
Results are compared to previous-generation 16-inch MacBook Pro systems with Apple M1 Pro, 10-core CPU, 16-core GPU, 32GB of RAM, and 8TB SSD.
Results are compared to previous-generation 16-inch MacBook Pro systems with Apple M1 Max, 10-core CPU, 32-core GPU, 64GB of RAM, and 8TB SSD.





Press Contacts


Starlayne Meza

Apple

starlayne_meza@apple.com





Michelle Del Rio

Apple

mr_delrio@apple.com


                        (408) 862-1478
                    


Apple Media Helpline



media.help@apple.com


                        (408) 974-2042
                    







Latest News










PRESS RELEASE
Apple unveils M2 Pro and M2 Max: next-generation chips for next-level workflows
January 17, 2023













PRESS RELEASE
Apple introduces new Mac mini with M2 and M2 Pro — more powerful,  capable, and versatile than ever
January 17, 2023













UPDATE
Introducing Apple Business Connect
January 11, 2023










Apple Newsroom
The latest news and updates, direct from Apple.
Read more




"
https://news.ycombinator.com/rss,Ruby 3.2’s YJIT is Production-Ready,https://shopify.engineering/ruby-yjit-is-production-ready,Comments,"






Ruby 3.2’s YJIT is Production-Ready




            by Maxime Chevalier-Boisvert


Development




Jan 17, 2023


          10 minute read
        




  Email
  Facebook
  Twitter
  LinkedIn






Shopify and YJIT
Back in July 2020, I joined the Ruby & Rails Infrastructure (R&RI) team at Shopify. Our team focuses on making sure that Ruby as well as Ruby on Rails, central to the infrastructure behind all Shopify stores and much of the modern web, run as smoothly and efficiently as possible.
As part of the R&RI team, I got to meet skilled engineers that were doing open source work, directly contributing patches to CRuby itself. Since my background is in compiler design, I started to discuss with my manager the possibility that we could build a relatively simple Just-In-Time (JIT) compiler for Ruby. To my surprise, my manager and two colleagues were immediately on board with this idea, and what would become the  YJIT project was born.
Building YJIT was hard work. There were many long, intense debugging sessions involved, but within just over a year, we’d managed to deliver roughly  20% speedups on  railsbench. Following that, the CRuby core contributors invited us to  upstream YJIT, and so YJIT was released as an official part of Ruby 3.1 in December of 2021. Upstreaming YJIT had been an aspirational goal for the team from the start, but we had never thought it would happen this fast. I’ll take this opportunity to say that I’m very thankful to Shopify for letting us take on some risks, and to the Ruby community for being so open-minded.
Major YJIT Improvements in Ruby 3.2
A lot has happened for YJIT in 2022. For one thing, we’ve expanded the team. We wrote about job openings in the YJIT team on this blog last year, and we were flooded with applications from people excited to work on a Ruby JIT, all of them with impressive CVs and a long list of systems programming skills. We ended up recruiting three skilled engineers which became part of the YJIT dream team. One of these new recruits is no other than Takashi Kokubun, long-time CRuby core member and maintainer of MJIT.
The YJIT team has made multiple improvements to YJIT which are now available  as part of Ruby 3.2. The good news is that, as you might expect, the new version of YJIT brings better performance, both on benchmarks and on real workloads, but I would say that the broader theme for 3.2 has been to make YJIT more robust, more maintainable, and generally more production-ready.
Rewriting YJIT to Rust
To start 2021, we decided to  port YJIT from C99 to Rust. The motivation for this was twofold. Rust provides additional safety guarantees that C doesn’t, which is important when doing low-level systems programming with many constraints, as in a JIT compiler. The secondary motivating factor was that we felt that, as the complexity of YJIT increases, we needed better tools to manage that complexity. Writing C code, we had to resort to implementing our own dynamic arrays in terms of C macros, which felt both unsafe and awkward. Rust provides a much richer standard library and many nice and fast abstractions. It took Noah Gibbs, Alan Wu, and me about three months to port YJIT to Rust, and I’m happy to say that our new Rust codebase does feel much easier to maintain.
Improved Memory Usage
One of the challenges with JIT compilers is that they always incur some amount of memory overhead over interpreters. At the most basic level, a JIT compiler needs to generate executable machine code, which an interpreter doesn’t, so JIT compilers must use more memory than interpreters. On top of that, however, JIT compilers also need to allocate memory for auxiliary data structure (metadata), which can also add quite a bit of extra memory overhead.
We were unhappy with how much extra memory YJIT used in Ruby 3.1. We felt that the amount of memory needed back then made it difficult to deploy in production at Shopify, and so we’ve made multiple improvements to reduce memory usage. The good news is that, thanks to the hard work of Alan and Takashi, the overhead has been cut down to approximately one third of what it was for 3.1, which helps make YJIT a lot more usable in production. To achieve this, we’ve optimized how much space our metadata takes, we’ve implemented a garbage collector for machine code that is no longer used, and we’ve made it so YJIT will lazily allocate memory pages for machine code as opposed to allocating and initializing a large block of memory up front.
Improved Performance
 
YJIT’s performance vs the interpreter in Ruby 3.2 (higher is better). Image source: speed.yjit.org

YJIT 3.2 doesn’t just use less memory though, it’s also faster. We now  speed up railsbench by about 38% over the interpreter, but this is on top of the Ruby 3.2 interpreter, which is already faster than the interpreter from Ruby 3.1. According to  the numbers gathered by Takashi, the cumulative improvement makes YJIT 57% faster than the Ruby 3.1.3 interpreter. It’s not just our numbers  that show that the  new YJIT delivers great  performance, the Ruby community has done  their own benchmarking as well.

 
Source: @rafael_falco on Twitter.

ARM64 Support
Another major change in YJIT 3.2 is that we now have  a new backend that can generate machine code for multiple CPU platforms, which enables us to support ARM64 CPUs. In 3.1, we only supported x86-64 on Mac and Linux. With developers at Shopify migrating to Apple M1/M2 laptops, we found ourselves in the awkward situation where we could only run YJIT locally through emulation with Rosetta. With Ruby 3.2, it’s now possible to run YJIT natively on Apple M1 & M2, AWS Graviton 1 & 2, and even on  Raspberry Pis! Interestingly, YJIT gets an even bigger speedup on Mac M1 hardware than it does on Intel x86-64 CPUs. We hope that this will encourage people to try out YJIT locally on their development machines.
Additional Improvements
Ruby 3.2 also includes another major change that has been in the works for a while. Jemma Issroff and Aaron Patterson have done an impressive amount of work in order to reimplement Ruby’s internal representation for objects, which is now based on the concept of  object shapes. This allows both the interpreter and YJIT to benefit from faster instance variable accesses.
In addition to this,  Eileen Uchitelle implemented a tool to trace YJIT exits, Jimmy Miller worked on improving YJIT support for various types of Ruby method calls, and Kevin Newton implemented a  finer-grained constant cache invalidation mechanism. This change was brought about to address a situation we had seen in production where constants being redefined would cause YJIT to recompile a lot of code. 
Last but not least, Peter Zhu and Matthew Valentine-House have made improvements to Ruby 3.2’s garbage collector, and made it possible to allocate  variable-sized objects. This improves Ruby’s memory usage and also significantly improves the interpreter’s performance. It also makes it possible to allocate larger objects which are more cache-friendly.
Running YJIT in Production
The main reason why Shopify chose to invest in the development of YJIT is of course that Shopify runs a large amount of infrastructure built on top of Ruby and Ruby on Rails. Multiple large clusters of servers distributed across the world, capable of serving over  75 million requests per minute. From the start, the objective was to eventually be able to use YJIT to improve the efficiency of Shopify’s Storefront Renderer (SFR).
Given that YJIT 3.1 had significant memory overhead and was still marked as experimental, we didn’t want to deploy it globally right away. However, starting about a year ago, we’ve started to run a few SFR nodes using YJIT. This has been extremely valuable to us, because it’s enabled us to gather statistics and see how YJIT and our codebase behave under a real-world deployment with real traffic, which has exposed some performance issues we couldn’t see on benchmarks. 

This year, with Ruby 3.2, YJIT has improved enough that we’ve deemed it production-ready, and Shopify has proceeded to deploy it globally on its entire SFR infrastructure. We’re able to measure real speedups ranging from 5% to 10% (depending on time of day) on our total end-to-end request completion time measurements.

YJIT speedup over the Ruby 3.2 interpreter on our SFR deployment.

I want to be honest and say that YJIT is still not perfect. It still has some memory overhead, but we think it’s worth the speedups, and of course, we intend on improving the situation further. One of the key advantages of YJIT is its very fast compilation times. At Shopify, we deploy continuously, often multiple times every day, sometimes multiple times in a single hour. That means YJIT has to be able to compile code very quickly, otherwise some Shopify customers might see their request time out whenever a deployment occurs. It’s not just the speed of the code we compile that matters, it’s also how fast we can compile code.
We’ve successfully deployed YJIT in production at Shopify, but the YJIT team has relatively little visibility into how many people are using YJIT in practice outside of interacting with people on Twitter or at conferences. If you’re using YJIT in production, for your dev environment, or even for a hobby project, please  let us know and share your feedback! We’d love to hear your YJIT success stories (or pain points, for that matter).
Future Plans
The year 2023 has just begun and we already have a long list of new improvements we want to bring to YJIT. Since we’ve just deployed YJIT, I think it’s important that we continue to remain grounded and use statistics from our real-world deployment to address the biggest pain points. YJIT’s biggest flaw is still its memory footprint, and this is something we need to continue working to further improve.
In terms of the biggest opportunities for speedups, Ruby is method calls all the way down. That is, loop iteration as well as most basic operations in Ruby are method calls, and typical Ruby code contains many calls to small Ruby methods. As such, the most obvious area for potential improvements would be to make method calls faster. There are a few avenues we're exploring to achieve this, such as potentially implementing a more efficient calling convention, and also inlining method calls.
In addition to optimizing the performance of method calls, we’d also like to better optimize the machine code that YJIT generates. We still don’t have a proper register allocator, and we don’t really optimize across basic blocks. Finally, we may also want to optimize the way YJIT and CRuby perform various hash and string operations, as these are very common in web workloads.
More About YJIT
If you’re interested in trying out Ruby 3.2, the release notes and tarball packages can be found  here, it’s also possible to directly install Ruby 3.2 via brew if you’re on macOS, or using the  ruby-install tool. In order to make sure that YJIT is available, you just need to make sure that you have rustc 1.58.0 or newer (or  the Rust toolchain) installed on your machine before you install/build Ruby using your favorite tool (brew, ruby-build, ruby-install, etc.). You can then run Ruby with YJIT enabled by passing the --yjit command-line flag to Ruby, or by setting the RUBY_YJIT_ENABLE environment variable.
For more information on YJIT’s design or how to use it, you can check out our  documentation, or one of the resources below.

Alan Wu’s RubyKaigi 2022 keynote: Stories from developing YJIT (RubyKaigi 2022)
Building a Lightweight IR and Backend for YJIT (RubyKaigi 2022)
 Optimizing Ruby’s Memory Layout: Variable Width Allocation by Peter Zhu
YJIT - Building a new JIT Compiler inside CRuby (RubyConf 2021)
MJIT, YJIT, and HAML with Takashi Kokubun - Ruby Rogues #573
Parsers, Interpreters, and YJIT with Kevin Newton

YJIT: Building a New JIT Compiler for CRuby 
Our Experience Porting the YJIT Ruby Compiler to Rust

I’d like to conclude with a big thank you to the YJIT team, and everyone that has contributed to this project’s success, including: Alan Wu, Aaron Patterson, Jemma Issroff, Eileen Uchitelle, Kevin Newton, Noah Gibbs, Jimmy Miller, Takashi Kokubun, Ufuk Kayserilioglu, Mike Dalessio, Jean Boussier, John Hawthorn, Rafael França, and more!

Maxime Chevalier-Boisvert obtained a PhD in compiler design at the University of Montreal in 2016, where she developed Basic Block Versioning (BBV), a JIT compiler architecture optimized for dynamically-typed programming languages. She leads the YJIT project at Shopify.

Open source software plays a vital and integral part at Shopify. If being a part of an Engineering organization that’s committed to the support and stewardship of open source software sounds exciting to you, visit our Engineering career page to find out about our open positions and learn about Digital by Design.


  Email
  Facebook
  Twitter
  LinkedIn

  Get stories like this in your inbox!Stories from the teams who build and scale Shopify. The commerce platform powering millions of businesses worldwide.Email addressYes, sign me up!

Share your email with us and receive monthly updates.
Thanks for subscribing.
You’ll start receiving free tips and resources soon.













Search articles

Search


  Get stories like this in your inbox!Stories from the teams who build and scale Shopify. The commerce platform powering millions of businesses worldwide.Email addressYes, sign me up!

Share your email with us and receive monthly updates.
Thanks for subscribing.
You’ll start receiving free tips and resources soon.








  
    Resources
  


            Our Tech Stack
 Curious about what’s in our tech stack.

            Sponsorship
 We’re looking to partner with you.

            Working Anywhere at Shopify
 Learn about Digital by Design

            Shopify Partner Developers
 Become a Shopify developer and earn money by building apps or working with businesses

            Shopify Engineering on Twitter
 Connect with us on Twitter

            Shopify Engineering YouTube
 Connect with us on YouTube




  
    Popular
  


            Migrating our Largest Mobile App to React Native
 
            Shopify Embraces Rust for Systems Programming
 
            Mixing It Up: Remix Joins Shopify to Push the Web Forward
 
            From Ruby to Node: Overhauling Shopify’s CLI for a Better Developer Experience
 
            A Flexible Framework for Effective Pair Programming
 
            10 Tips for Building Resilient Payment Systems
 
            Five Common Data Stores and When to Use Them
 
            Deconstructing the Monolith: Designing Software that Maximizes Developer Productivity
 
            Under Deconstruction: The State of Shopify’s Monolith
 
            Reducing BigQuery Costs: How We Fixed A $1 Million Query
 



  
    Latest
  


            Ruby 3.2’s YJIT is Production-Ready
 
            How Good Documentation Can Improve Productivity
 
            From Ruby to Node: Overhauling Shopify’s CLI for a Better Developer Experience
 
            Reliving Your Happiest HTTP Interactions with Ruby’s VCR Gem
 
            Monte Carlo Simulations: Separating Signal from Noise in Sampled Success Metrics
 
            React Native Skia: A Year in Review and a Look Ahead
 
            Migrating our Largest Mobile App to React Native
 
            Optimizing Ruby’s Memory Layout: Variable Width Allocation
 
            Year in Review 2022: Tenderlove's Ruby and Rails Reflections and Predictions
 
            Automatically Rotating GitHub Tokens (So You Don’t Have To)
 



"
https://news.ycombinator.com/rss,Diskless infrastructure in beta (System Transparency: stboot),https://mullvad.net/en/blog/2022/1/12/diskless-infrastructure-beta-system-transparency-stboot/,Comments,"




















Diskless infrastructure in beta (System Transparency: stboot) - Blog | Mullvad VPN



















































About
Policies
Blog
Pricing
Servers
Apps
Help


Account

Get started









Diskless infrastructure in beta (System Transparency: stboot)
12 January 2022 
        
            SYSTEM TRANSPARENCY


Diskless infrastructure using stboot (in beta) is now available on a pair of WireGuard servers in Sweden.
Today we are introducing our first VPN servers booted with our new bootloader - stboot. This marks the start of our long-running public-facing journey to make our VPN infrastructure transparent and user-auditable.
Diskless infrastructure for VPN servers
Today we announce an early beta release of a part of our System Transparency technology running on one VPN server in Gothenburg and one in Stockholm, Sweden. Both of these servers are listed in a “System Transparency [BETA]” city in our server list, viewable within our app as well as on our website.
You can find these servers by selecting: Switch Location → Sweden → System Transparency [BETA]
Make sure you are using the WireGuard protocol (applies to desktop app only).
This means that we now have two servers running entirely on RAM, without any disks in use.
What does “without any disks in use” mean?

If the computer is powered off, moved or confiscated, there is no data to retrieve.
We get the operational benefits of having fewer breakable parts. Disks are among the components that break often. Therefore, switching away from them makes our infrastructure more reliable.
The operational tasks of setting up and upgrading package versions on servers become faster and easier.
Running the system in RAM does not prevent the possibility of logging. It does however minimise the risk of accidentally storing something that can later be retrieved.

Where do you pull data from if you have no disks to store it on?
For these servers we make use of provisioning servers in order to download an “OS Package”. These provisioning servers have disks but they contain only the signed images and some base configuration data that our System Transparency (or stbooted) servers will use.
Our VPN servers launch the System Transparency bootloader (stboot) which downloads the OS package from a provisioning server and verifies that it originates from relevant Mullvad VPN staff by checking its signatures. If the OS package is valid, the OS is booted. The server then waits for an authorised member of staff to provision and deploy it for customer use.
By and large, these servers will be configured in a similar manner to our other WireGuard servers, except we use no disks, and RAM is the only location where data is kept.

Debug output stboot starting up

Debug output OS package signatures verified
What happens when the server is restarted?
At this point, the server would boot up, unaware about its past history due to using no disk. The process would be the same as in the previous step (download, verify, wait for authorisation).
In other words, we have amnesia for servers.
If this is the first of many steps, what happens next?
We get your feedback, if any, on how well it works!
We will continue to develop our provisioning and deployment process of stbooted VPN servers, starting with the ones providing WireGuard tunnels. We will start adding more servers in different locations as we get more comfortable and the projects' moving parts become more mature.
End goal: Trustworthiness through transparency
We are continuously striving to strengthen the trustworthiness of all aspects of our service. This is why our VPN apps have been open source since we started over 12 years ago. Achieving transparency on the server side is a very different challenge, as merely open sourcing our server software is not enough. We want our users to be able to verify and audit what is currently running on the VPN server they are connected to. This is our end goal with System Transparency.
Note
During this beta, WireGuard keys will be wiped on each server restart. If you are using configuration files to connect to the servers you will need to download new ones each time this happens. This does not affect the Mullvad App.
Read more

System Transparency is the future
Open Source Firmware is the future
System Transparency home page









Mullvad

About
Help
Servers
Pricing
Blog
What is privacy?
Why Mullvad VPN?
What is a VPN?
Download
Press


Jobs






Policies

Open source
Privacy policy
Cookies
Terms of service
Partnerships and resellers
Reviews, ads and affiliates
Reporting a bug or vulnerability



Address

Mullvad VPN AB
Box 53049
400 14 Gothenburg
Sweden





support@mullvad.net





GPG key








Onion service






Follow us




@mullvadnet





@mullvadnet





MullvadNet





Mullvad VPN





mullvad




Language





English








العربيّة
Dansk
Deutsch
English
Español
فارسی
Suomi
Français
Italiano
日本語
한국어
Nederlands
Norsk
Polski
Português
Русский
Svenska
ภาษาไทย
Türkçe
繁體中文
简体中文










"
https://news.ycombinator.com/rss,"Tesla video promoting self-driving was staged, engineer testifies",https://www.reuters.com/technology/tesla-video-promoting-self-driving-was-staged-engineer-testifies-2023-01-17/,Comments,"Technology4 minute readJanuary 17, 20235:46 PM UTCLast Updated  agoTesla video promoting self-driving was staged, engineer testifiesBy Hyunjoo JinA Tesla Model 3 electric vehicle is shown in this picture illustration taken in Moscow, Russia July 23, 2020. REUTERS/Evgenia Novozhenina/File PhotoJan 17 (Reuters) - A 2016 video that Tesla (TSLA.O) used to promote its self-driving technology was staged to show capabilities like stopping at a red light and accelerating at a green light that the system did not have, according to testimony by a senior engineer.The video, which remains archived on Tesla’s website, was released in October 2016 and promoted on Twitter by Chief Executive Elon Musk as evidence that “Tesla drives itself.”But the Model X was not driving itself with technology Tesla had deployed, Ashok Elluswamy, director of Autopilot software at Tesla, said in the transcript of a July deposition taken as evidence in a lawsuit against Tesla for a 2018 fatal crash involving a former Apple (AAPL.O) engineer.The previously unreported testimony by Elluswamy represents the first time a Tesla employee has confirmed and detailed how the video was produced.The video carries a tagline saying: “The person in the driver’s seat is only there for legal reasons. He is not doing anything. The car is driving itself.”Elluswamy said Tesla’s Autopilot team set out to engineer and record a “demonstration of the system’s capabilities” at the request of Musk.Elluswamy, Musk and Tesla did not respond to a request for comment. However, the company has warned drivers that they must keep their hands on the wheel and maintain control of their vehicles while using Autopilot.The Tesla technology is designed to assist with steering, braking, speed and lane changes but its features “do not make the vehicle autonomous,” the company says on its website.To create the video, the Tesla used 3D mapping on a predetermined route from a house in Menlo Park, California, to Tesla’s then-headquarters in Palo Alto, he said.Drivers intervened to take control in test runs, he said. When trying to show the Model X could park itself with no driver, a test car crashed into a fence in Tesla’s parking lot, he said.“The intent of the video was not to accurately portray what was available for customers in 2016. It was to portray what was possible to build into the system,” Elluswamy said, according to a transcript of his testimony seen by Reuters.When Tesla released the video, Musk tweeted, “Tesla drives itself (no human input at all) thru urban streets to highway to streets, then finds a parking spot.”Tesla faces lawsuits and regulatory scrutiny over its driver assistance systems.The U.S. Department of Justice began a criminal investigation into Tesla’s claims that its electric vehicles can drive themselves in 2021, after a number of crashes, some of them fatal, involving Autopilot, Reuters has reported.The New York Times reported in 2021 that Tesla engineers had created the 2016 video to promote Autopilot without disclosing that the route had been mapped in advance or that a car had crashed in trying to complete the shoot, citing anonymous sources.When asked if the 2016 video showed the performance of the Tesla Autopilot system available in a production car at the time, Elluswamy said, ""It does not.""Elluswamy was deposed in a lawsuit against Tesla over a 2018 crash in Mountain View, California, that killed Apple engineer Walter Huang.Andrew McDevitt, the lawyer who represents Huang’s wife and who questioned Elluswamy’s in July, told Reuters it was “obviously misleading to feature that video without any disclaimer or asterisk.”The National Transportation Safety Board concluded in 2020 that Huang’s fatal crash was likely caused by his distraction and the limitations of Autopilot. It said Tesla’s “ineffective monitoring of driver engagement” had contributed to the crash.Elluswamy said drivers could “fool the system,” making a Tesla system believe that they were paying attention based on feedback from the steering wheel when they were not. But he said he saw no safety issue with Autopilot if drivers were paying attention.Reporting by Hyunjoo Jin; Editing by Kevin Krolicki and Lisa ShumakerOur Standards: The Thomson Reuters Trust Principles.Read NextTechnologycategoryDavos 2023: CEOs buzz about ChatGPT-style AI at World Economic Forum, article with image8:32 AM UTCTechnologycategoryApple launches faster M2 chips, powerful laptops in rare January launch, article with image4:51 PM UTCTechnologycategoryMicrosoft to expand ChatGPT access as OpenAI investment rumors swirl, article with image4:18 AM UTCFuture of MoneycategoryCryptoverse: Bitcoin is back with a bonk, article with image11:10 AM UTC"
https://news.ycombinator.com/rss,My grandfather was almost shot down by ground to air missiles at the Whitehouse (2018),https://nones-leonard.medium.com/flying-high-8536fa403324,Comments,"Leonard and Sondra NonesFollowJan 12, 2018·4 min readFlying HighWhen I owned Piper N47943, a four seat single engine airplane, I thought it prudent not to fly myself to assignments. I did not want the stress of being the pilot and the photographer. But, when I got an assignment to go to Washington DC to simply take straight on photographs of several monuments, I thought no stress why not fly, take the pictures and fly home. It was a perfect summer day. CAVU all the way. (Cieling and Visability Unlimited). I filed a visual flight plan and we were on our way. I was very naive in my selection of airports in the DC area. My choise was Washington National, right in the middle of town, and a very busy place.The trip down was beautiful and I made a smooth landing on the 6000 foot runway. As I was taxing to my parking place, I received a radio call from ground control. The caller advised me to make a reservation for my departure. He gave me a phone number and instructed me to call one hour before we would be ready to take off. The art director and I rented a car and we drove around Washington. I took the photographs and we went to lunch.When we were ready to head back to Teterboro Airport, I got to a phone, [cell phones had not not yet been invented], and called Departure. A very busy air traffic controller spit out departure instructions. I read them back and we were on our way to the airport.As instructed, I contacted departure control when we were ready to taxi. When finally I was able to break into the frantic jumble of pilots also trying to reach departure, a hectic voice said “we have an amendment to your departure are you ready to copy”. While still in my parking space I read back the departure instructions.I was told to start my engine. The jumble of voices on the radio of pilots waiting to taxi was without interruption. Then ground control was on the radio saying “N47943 taxi to the active runway, we have an amendment to your departure, are you ready to copy”. I acknowledged my latest departure instructions and was ready to roll.Seat belts fastened, I rolled forward made a right turn and taxied to runway 18 and took my place behind a 727, a large commercial airliner. When I looked back I saw another 727 roll onto the taxiway behind me and then as I slowly rolled forward there was another one behind that one. I was about twelth for departure. The radio traffic was constant. Like Jeopardy contestents the one that was quickest on the button got heard and those guys in the big planes were really fast. Then I heard “N47943 we have an amendment to your departure”. I wrote it down on my knee pad, read it back and squeezed along between the big airliners.Finally, I was number one for departure. Just as I was making my turn onto runway 18 through all the radio chatter I heard “N47493 we have an amendment to your departure“. Without pause, the controller announced “maintain runway heading, climb to 1500 feet and then follow the river”. When I reached 1500 feet I looked down. I was crossing the river. It went right and left. I tried to break into the radio clatter, but was unsuccessful. I had to make a decision with 50–50 chance of making the right decision. I turned left.After about 45 seconds an agitated controller’s voice was on the radio. “November47943 execute immediate right turn, immediate right turn”. I made the turn and then was cleared to climb to 3000 feet, was handed off to air traffic control and headed home to sleepy Teterboro.Some two weeks later I received a phone call from the FAA in Oklahoma City. What did I think I was doing in one of the busiest airports in the country, was his first question. Then how much flying time did I have? Did I consider myself a weekend pilot? Did I think I could handle the stress at an airport like National? Then he told me the left turn was going to take me into restricted zone-357, which was directly over the White House. I listened as a large lump formed in my throat and I was having difficulty swollowing. Then came the worst of it. Jets had been scrambled and ground to air missels were aimed at ME. He said “when I made the right turn the alert was cancelled”. After berating me for some time he finally let up and said “stay away from big commercial airports” I learned that day that there is more to flying than smooth take offs and landings and straight and level flight. Airline pilots are the best at what they do and have spent years honing their craft. My hat is off to them.Aviation----More from Leonard and Sondra NonesFollowRecommended from MediumLisa TisdaleEscape to Italy: Finding a Way to HealJosé Olascoaga OrtegaAs I wander: The traveler artist in emotional troubleLes MillerThere are things that go ‘bump’ in the night.D.O.T. MarceoFeeling of The AngelsMeenakshi BharathanWater Everywhere- A Story of LossChris Lowry5 Free Things to do in White CountyToiletopsToilet Luxury, Hygiene, & Usability - A Toiletops InfographicCourtney BurryinWorld Traveler’s BlogThe Lowdown on Three Very Different Sides to JapanAboutHelpTermsPrivacyGet the Medium app"
https://news.ycombinator.com/rss,GNU make 4.4 adds --shuffle to help find parallel build issues,https://trofi.github.io/posts/238-new-make-shuffle-mode.html,Comments,"




trofi's blog: New make --shuffle mode









/
/archive
/atom
/rss


New make --shuffle mode
TL;DR:
I implemented new --shuffle option for GNU make to simulate
non-deterministic build order in parallel makefiles.
UPDATE: --shuffle mode was released as part of GNU make 4.4 and
can be used as:

make --shuffle ... from the command line.
GNUMAKEFLAGS=--shuffle via environment variables for wider scale
builds.

Options Summary
has more details.
It already found bugs in 30+ packages
like gcc, vim, ghc, subversion, strace, ispell and others.
Background
About 11 years ago I was a year old Gentoo dev who just started getting
downstream bug reports on mysterious ghc build failures like
https://bugs.gentoo.org/326347.
The symptoms were seemingly simple: some file was inaccessible while
it was being written to, or executed.
Years later I mastered the intricacies of ghc’s build system on how
to debug it effectively. But at that time I did not really know what to
do. My main working machine was a Core 2 duo HP laptop which could not do
more than -j2. And even that required a bit of swap for ghc’s
linking stage. Throwing more parallelism was not really an option to
trigger such bugs.
Makefile target ordering
Makefiles are fundamentally simple: it’s a graph of dependencies
with a sequence of shell commands attached to a node. There are numerous
caveats, but they should not break this model too much.
In theory you can topologically sort the graph and execute the
dependencies in various conforming orders and expect the same result.
Modulo missing dependencies in the graph.
In practice GNU make happens to traverse the graph in very specific
topological order: it maintains syntactic order as much as possible.
Here is an example Makefile:
# cat Makefile
all: a b c

b: b1 b2 b3
a: a1 a2 a3
c: c1 c2 c3

a b c a1 a2 a3 b1 b2 b3 c1 c2 c3:; @echo $@ && sleep 1
(I added sleep 1 to make it more visible when next goal schedules).
Here is the sequential execution by GNU make:
$ make
a1
a2
a3
a
b1
b2
b3
b
c1
c2
c3
c
The seen order is exactly all’s prerequisites left-to-right
recursively.
Adding parallelism does not change the order too much: make still
traverses prerequisites in the same order and starts as many targets
with satisfied dependencies as possible.
Parallel example:
$ make -j4
a1
a2
a3
b1

b2
b3
c1
c2

c3
a
b

c
I added newlines where 1-second pauses visibly happen.
Note that in this example a1 does not depend on
c2. But c2 practically always starts execution after a1
finishes.
The “only” way to run a1 and c2 in parallel is to run
make with at least -j8. Which is a lot.
Or do something with the system that stalls task execution for
indefinite amount of time (like, adding various nice levels
or put system under high memory or CPU pressure).
Very occasionally already stressed system naturally gets into
unusual task execution order. You get the one-off failure and
struggle to repeat it ever again. Which makes it very hard to
test the fix unless you know where exactly to put the sleep
command to make it more reproducible.
An old idea
Even then it was clear that CPU count per device will only increase
and it will be increasingly painful to work with sequentially built
projects :) Bugs will come back again and again on you the more cores
you throw at the Makefiles.
I had a silly idea back then (post in russian):
what if we arbitrarily reorder the prerequisites in Makefiles? Or
maybe even trace spawned processes to know for sure what files targets
access? That might allow us to weed out most of the parallel bugs with
some sort of stress test on a low-core machine.
Fast forward 11 years I attempted to enable build parallelism by default
in nixpkgs.
A few packages still had some issues.
I recalled the idea and tried to implement target random shuffle within
GNU make!
Better reproducer: make --shuffle
Initial idea was very simple: pick target order at Makefile
parse time and reshuffle the lists randomly. To pick an example
above one of the example shuffles would be:
$ cat Makefile
all: c b a

b: b2 b1 b3
a: a3 a2 a1
c: c1 c2 c3

a b c a1 a2 a3 b1 b2 b3 c1 c2 c3:; @echo $@ && sleep 1
I wrote the proof of concent and
proposed
it to GNU make community.
The example run of patched make shows less determinism now:
$ ~/dev/git/make/make --shuffle -j4
c2
c3
c1
a2

a1
b2
a3
b3

b1

c
b
a
Paul did not seem to object too much to the idea and pointed out
that implementation will break more complex Makefiles as there
is a simple way to refer to individual prerequisites by number.
To pick Paul’s example:
%.o : %.c
	$(CC) $(CFLAGS) -c -o $@ $<

foo.o: foo.c foo.h bar.h baz.h

#

foo%: arg%1 arg%2 arg%3 arg%4
	bld $< $(word 3,$^) $(word 2,$^) $(word 4,$^)
In both cases syntactic reshuffling breaks the build rules
by passing wrong file name.
To fix it I came up with a way to store two orders at the same time:
syntactic and shuffled and posted patch as
https://lists.gnu.org/archive/html/bug-make/2022-02/msg00042.html.
Running make --shuffle on real projects
While I was waiting for the feedback I ran the build tests against
nixpkgs packages.
First, I almost instantly got build failures on the projects that
already explicitly disable parallel builds in nixpkgs to avoid known
failures: groff, source-highlight, portaudio, slang, gnu-efi,
bind, pth, libomxil, dhcp, directfb, doxygen, gpm, judy
and a few others. That was a good sign.
A bit later I started getting failures I did not encounter before in
ghc(!), gcc(!!), automake(!!!), pulseaudio,
libcanberra, many ocaml and some perl packages.
All the failures looked genuine missing dependencies. For example
gcc’s libgfortran is missing a libquadmath build dependency.
It is natural not to encounter it in real world as libquadmath is
usually built along with other small runtimes way before g++ or
gfortran is ready.
Fun fact: while running the build I stumbled on a GNU make bug
not related to my change:
https://lists.gnu.org/archive/html/bug-make/2022-02/msg00037.html.
The following snippet tricks GNU make to loop for a while until
it crashes with argument list exhaustion (or inode exhaustion in
/tmp):
$ printf 'all:\n\techo $(CC)' | ./make -sf -
<hung>
This bug is not present in any releases yet. And hopefully will not be.
I’d like to land the --shuffle change upstream in some form before
sending bug reports and trivial fixes to upstream projects.
How you can test it
If you are keen to try this shuffling mode on your make-based
projects (be it manually written, automake-based or cmake-based)
here is a rough instruction to do it:

install GNU make 4.4
use it as make --shuffle <your-typical-make-arguments>
against your project (or set it via GNUMAKEFLAGS=--shuffle
environment variable)
check if the build succeeds, run it a few times

Both sequential and parallel modes should work fine. I suggest trying
both. The shuffling overhead should be negligible.
How do failures look like
When build fails it reports the shuffling mode and seed used. Let’s try
it on a concrete cramfsswap example:
$ git clone https://github.com/julijane/cramfsswap.git
$ cd cramfsswap

$ ~/dev/git/make/make clean && ~/dev/git/make/make
rm -f cramfsswap
gcc -Wall -g -O -o cramfsswap -lz cramfsswap.c
strip cramfsswap

$ ~/dev/git/make/make clean && ~/dev/git/make/make
rm -f cramfsswap
strip cramfsswap
strip: 'cramfsswap': No such file
make: *** [Makefile:10: strip] Error 1 --shuffle=1645603370
Here we see a successful run and a failed run. Failed run reports
specific seed that might trigger the failure: --shuffle=1645603370.
We can use this seed explicitly:
$ ~/dev/git/make/make --shuffle=1645603370
strip cramfsswap
strip: 'cramfsswap': No such file
make: *** [Makefile:10: strip] Error 1 --shuffle=1645603370
$ ~/dev/git/make/make --shuffle=1645603370
strip cramfsswap
strip: 'cramfsswap': No such file
make: *** [Makefile:10: strip] Error 1 --shuffle=1645603370
$ ~/dev/git/make/make --shuffle=1645603370
strip cramfsswap
strip: 'cramfsswap': No such file
make: *** [Makefile:10: strip] Error 1 --shuffle=1645603370
Note how ordering is preserved across the runs with fixed seed.
Parting words
Implementing the shuffling idea took a weekend. I should have tried
it earlier :) The result instantly found existing and new missing
dependencies in a small subset of real projects. Some of these failures
are very hard to trigger otherwise.
UPDATE: --shuffle was upstreamed and released as part of
GNU make 4.4 release (see announcement).
Have fun!

    Posted on February 23, 2022 by trofi. Email,
    pull requests or comments
    are welcome!



            powered by hakyll



"
https://news.ycombinator.com/rss,Laser desorption mass spectrometry – Alien detector that fits in your hand,https://www.nature.com/articles/s41550-022-01866-x,Comments,"




Article
Published: 16 January 2023

Laser desorption mass spectrometry with an Orbitrap analyser for in situ astrobiology
Ricardo Arevalo Jr 
            ORCID: orcid.org/0000-0002-0558-50901, Lori Willhite1, Anais Bardyn1, Ziqin Ni1, Soumya Ray 
            ORCID: orcid.org/0000-0002-9591-93801, Adrian Southard2, Ryan Danell 
            ORCID: orcid.org/0000-0003-4863-19983, Andrej Grubisic4, Cynthia Gundersen5, Niko Minasola5, Anthony Yu4, Molly Fahey 
            ORCID: orcid.org/0000-0001-8551-96344, Emanuel Hernandez4, Christelle Briois 
            ORCID: orcid.org/0000-0002-5616-01806, Laurent Thirkell6, Fabrice Colin6 & …Alexander Makarov7 Show authors

Nature Astronomy

                         (2023)Cite this article




467 Altmetric


Metrics details





Subjects

Analytical chemistryPlanetary science




AbstractLaser desorption mass spectrometry (LDMS) enables in situ characterization of the organic content and chemical composition of planetary materials without requiring extensive sample processing. Coupled with an Orbitrap analyser capable of ultrahigh mass-resolving powers and accuracies, LDMS techniques facilitate the orthogonal detection of a wide range of biomarkers and classification of host mineralogy. Here an Orbitrap LDMS instrument that has been miniaturized for planetary exploration is shown to meet the performance standards of commercial systems and exceed key figures of merit of heritage spaceflight technologies, including those baselined for near-term mission opportunities. Biogenic compounds at area densities relevant to prospective missions to ocean worlds are identified unambiguously by redundant measurements of molecular ions (with and without salt adducts) and diagnostic fragments. The derivation of collision cross-sections serves to corroborate assignments and inform on molecular structure. Access to trace elements down to parts per million by weight levels provide insights into sample mineralogy and provenance. These analytical capabilities position the miniaturized LDMS described here for a wide range of high-priority mission concepts, such as those focused on life detection objectives (for example, Enceladus Orbilander) and progressive exploration of the lunar surface (for example, via the NASA Artemis Program).





Access through your institution




Buy or subscribe







This is a preview of subscription content, access via your institution


Access options





Access through your institution









Access through your institution




Change institution




Buy or subscribe



Subscribe to Nature+Get immediate online access to Nature and 55 other Nature journal$29.99monthlySubscribeSubscribe to JournalGet full journal access for 1 year$119.00only $9.92 per issueSubscribeAll prices are NET prices. VAT will be added later in the checkout.Tax calculation will be finalised during checkout.Buy articleGet time limited or full article access on ReadCube.$32.00BuyAll prices are NET prices.

Additional access options:


Log in


Learn about institutional subscriptions




Fig. 1: The highly miniaturized LDMS instrument described here leverages an Orbitrap mass analyser to achieve ultrahigh mass resolution and accuracy.Fig. 2: In both negative and positive mode, the miniaturized Orbitrap LDMS instrument achieves mass-resolving powers (m/Δm > 105, FWHM at m/z 100) comparable to commercial standards.Fig. 3: A single mass spectrum of an ocean world analogue sample illustrates the capability to detect and identify organic and inorganic components of planetary materials.Fig. 4: After successful injection into the Orbitrap analyser, the axial motions of the analyte ions are detected via image current in the time domain transient.Fig. 5: The Orbitrap LDMS instrument can detect trace elements down to ppmw concentrations, as illustrated by the measurement of REEs in NIST SRM610.


Data availability
Source data are provided with this paper. All other data presented in this study are available in the Supplementary Information.
ReferencesJohnson, S. S., Anslyn, E. V., Graham, H. V., Mahaffy, P. R. & Ellington, A. D. Fingerprinting non-terran biosignatures. Astrobiology 18, 915–922 (2018).Article 
    ADS 
    
                    Google Scholar 
                Marshall, S. M., Murray, A. R. G. & Cronin, L. A probabilistic framework for identifying biosignatures using Pathway Complexity. Philos. Trans. R. Soc. Lond. A 375, 20160342 (2017).ADS 
    
                    Google Scholar 
                Chan, M. A. et al. Deciphering biosignatures in planetary contexts. Astrobiology 19, 1075–1102 (2019).Article 
    ADS 
    
                    Google Scholar 
                Neveu, M., Hays, L. E., Voytek, M. A., New, M. H. & Schulte, M. D. The ladder of life detection. Astrobiology 18, 1375–1402 (2018).Article 
    ADS 
    
                    Google Scholar 
                Lukmanov, R. A. et al. On topological analysis of fs-LIMS data. Implications for in situ planetary mass spectrometry. Front. Artif. Intell. https://doi.org/10.3389/frai.2021.668163 (2021).Johnston, S., Gehrels, G., Valencia, V. & Ruiz, J. Small-volume U–Pb zircon geochronology by laser ablation-multicollector-ICP-MS. Chem. Geol. 259, 218–229 (2009).Article 
    ADS 
    
                    Google Scholar 
                Sagdeev, R. Z. & Zakharov, A. V. Brief history of the Phobos mission. Nature 341, 581–585 (1989).Article 
    ADS 
    
                    Google Scholar 
                Managadze, G. G. et al. Study of the main geochemical characteristics of Phobos’ regolith using laser time-of-flight mass spectrometry. Sol. Syst. Res. 44, 376–384 (2010).Article 
    ADS 
    
                    Google Scholar 
                Goesmann, F. et al. The Mars Organic Molecule Analyzer (MOMA) instrument: characterization of organic material in Martian sediments. Astrobiology 17, 655–685 (2017).Article 
    ADS 
    
                    Google Scholar 
                Grubisic, A. et al. Laser desorption mass spectrometry at Saturn’s moon Titan. Int. J. Mass Spectrom. 470, 116707 (2021).Article 
    
                    Google Scholar 
                Chumikov, A. E., Cheptsov, V. S., Managadze, N. G. & Managadze, G. G. LASMA-LR laser-ionization mass spectrometer onboard Luna-25 and Luna-27 missions. Sol. Syst. Res. 55, 550–561 (2021).Article 
    ADS 
    
                    Google Scholar 
                Briois, C. et al. Orbitrap mass analyser for in situ characterisation of planetary environments: performance evaluation of a laboratory prototype. Planet. Space Sci. 131, 33–45 (2016).Article 
    ADS 
    
                    Google Scholar 
                Willhite, L. et al. CORALS: a laser desorption/ablation Orbitrap mass spectrometer for in situ exploration of Europa. In 2021 IEEE Aerospace Conference 50100, 1–13 (2021).Makarov, A. A. Mass spectrometer US patent 5,886,346 (1999).Arevalo, R. Jr, Ni, Z. & Danell, R. M. Mass spectrometry and planetary exploration: a brief review and future projection. J. Mass Spectrom. 55, e4454 (2020).Article 
    ADS 
    
                    Google Scholar 
                Makarov, A. Electrostatic axially harmonic orbital trapping: a high-performance technique of mass analysis. Anal. Chem. 72, 1156–1162 (2000).Article 
    
                    Google Scholar 
                Arevalo, R. Jr et al. An Orbitrap-based laser desorption/ablation mass spectrometer designed for spaceflight. Rapid Commun. Mass Spectrom. https://doi.org/10.1002/rcm.8244 (2018).Article 
    
                    Google Scholar 
                Yu, A. W. et al. The Lunar Orbiter Laser Altimeter (LOLA) laser transmitter. In 2011 IEEE International Geoscience and Remote Sensing Symposium 3378–3379 (2011).Malloci, G., Mulas, G. & Joblin, C. Electronic absorption spectra of PAHs up to vacuum UV. Astron. Astrophys. 426, 105–117 (2004).Article 
    ADS 
    
                    Google Scholar 
                Cloutis, E. A. et al. Ultraviolet spectral reflectance properties of common planetary minerals. Icarus 197, 321–347 (2008).Article 
    ADS 
    
                    Google Scholar 
                Fahey, M. et al. Ultraviolet laser development for planetary lander missions. In 2020 IEEE Aerospace Conference 1–11 (2020).Büttner, A. et al. Optical design and characterization of the MOMA laser head flight model for the ExoMars 2020 mission. In Proc. SPIE 11180, International Conference on Space Optics—ICSO 2018, 111805H (12 July 2019); https://doi.org/10.1117/12.2536116Jenner, F. E. & O’Neill, H. S. C. Major and trace analysis of basaltic glasses by laser-ablation ICP-MS. Geochem. Geophys. Geosyst. https://doi.org/10.1029/2011GC003890 (2012).Humayun, M., Davis, F. A. & Hirschmann, M. M. Major element analysis of natural silicates by laser ablation ICP-MS. J. Anal. Spectrom. 25, 998–1005 (2010).Article 
    
                    Google Scholar 
                Longerich, H. P., Günther, D. & Jackson, S. E. Elemental fractionation in laser ablation inductively coupled plasma mass spectrometry. Fresenius J. Anal. Chem. 355, 538–542 (1996).Article 
    
                    Google Scholar 
                Alterman, M. A., Gogichayeva, N. V. & Kornilayev, B. A. Matrix-assisted laser desorption/ionization time-of-flight mass spectrometry-based amino acid analysis. Anal. Biochem. 335, 184–191 (2004).Article 
    
                    Google Scholar 
                Sarracino, D. & Richert, C. Quantitative MALDI-TOF MS of oligonucleotides and a nuclease assay. Bioorg. Med. Chem. Lett. 6, 2543–2548 (1996).Article 
    
                    Google Scholar 
                Chumbley, C. W. et al. Absolute quantitative MALDI imaging mass spectrometry: a case of rifampicin in liver tissues. Anal. Chem. 88, 2392–2398 (2016).Article 
    
                    Google Scholar 
                Zubarev, R. A. & Makarov, A. Orbitrap mass spectrometry. Anal. Chem. 85, 5288–5296 (2013).Article 
    
                    Google Scholar 
                Makarov, A., Denisov, E., Lange, O. & Horning, S. Dynamic range of mass accuracy in LTQ Orbitrap hybrid mass spectrometer. J. Am. Soc. Mass Spectrom. 17, 977–982 (2006).Hoegg, E. D. et al. Isotope ratio characteristics and sensitivity for uranium determinations using a liquid sampling–atmospheric pressure glow discharge ion source coupled to an Orbitrap mass analyzer. J. Anal. Spectrom. 31, 2355–2362 (2016).Article 
    
                    Google Scholar 
                Hofmann, A. E. et al. Using Orbitrap mass spectrometry to assess the isotopic compositions of individual compounds in mixtures. Int. J. Mass Spectrom. 457, 116410 (2020).Article 
    
                    Google Scholar 
                Hardouin, J. Protein sequence information by matrix-assisted laser desorption/ionization in-source decay mass spectrometry. Mass Spectrom. Rev. 26, 672–682 (2007).Article 
    ADS 
    
                    Google Scholar 
                Franchi, M., Ferris, J. P. & Gallori, E. Cations as mediators of the adsorption of nucleic acids on clay surfaces in prebiotic environments. Orig. Life Evol. Biosph. 33, 1–16 (2003); https://doi.org/10.1023/A:1023982008714Trumbo, S. K., Brown, M. E. & Hand, K. P. Sodium chloride on the surface of Europa. Sci. Adv. 5, eaaw7123 (2019).Article 
    ADS 
    
                    Google Scholar 
                Postberg, F., Schmidt, J., Hillier, J. et al. A salt-water reservoir as the source of a compositionally stratified plume on Enceladus. Nature 474, 620–622 (2011).De Sanctis, M. C. et al. Fresh emplacement of hydrated sodium chloride on Ceres from ascending salty fluids. Nat. Astron. 4, 786–793 (2020).Article 
    ADS 
    
                    Google Scholar 
                Hand, K. P. et al. Report of the Europa Lander Science Definition Team (NASA, 2017).Hendrix, A. R. et al. The NASA Roadmap to Ocean Worlds. Astrobiology 19, 1–27 (2018); https://doi.org/10.1089/ast.2018.1955MacKenzie, S. M. et al. The Enceladus Orbilander mission concept: balancing return and resources in the search for life. Planet. Sci. J. 2, 77 (2021).Article 
    
                    Google Scholar 
                Waite, J. H. Jr et al. Liquid water on Enceladus from observations of ammonia and 40Ar in the plume. Nature 460, 487–490 (2009).Article 
    ADS 
    
                    Google Scholar 
                Altwegg, K., Balsiger, H. & Fuselier, S. A. Cometary chemistry and the origin of icy solar system bodies: the view after Rosetta. Annu. Rev. Astron. Astrophys. 57, 113–155 (2019).Article 
    ADS 
    
                    Google Scholar 
                Guzman, M. et al. Collecting amino acids in the Enceladus plume. Int. J. Astrobiol. 18, 47–59 (2018).Article 
    ADS 
    
                    Google Scholar 
                Takayama, M. In-source decay characteristics of peptides in matrix-assisted laser desorption/ionization time-of-flight mass spectrometry. J. Am. Soc. Mass Spectrom. 12, 420–427 (2001).Article 
    ADS 
    
                    Google Scholar 
                Katta, V., Chow, D. T. & Rohde, M. F. Applications of in-source fragmentation of protein ions for direct sequence analysis by delayed extraction MALDI-TOF mass spectrometry. Anal. Chem. 70, 4410–4416 (1998).Article 
    
                    Google Scholar 
                Sanders, J. D. et al. Determination of collision cross-sections of protein ions in an Orbitrap mass analyzer. Anal. Chem. 90, 5896–5902 (2018).Article 
    
                    Google Scholar 
                Makarov, A. & Denisov, E. Dynamics of ions of intact proteins in the Orbitrap mass analyzer. J. Am. Soc. Mass Spectrom. 20, 1486–1495 (2009).Article 
    
                    Google Scholar 
                Anupriya, Jones, C. A. & Dearden, D. V. Collision cross sections for 20 protonated amino acids: Fourier transform ion cyclotron resonance and ion mobility results. J. Am. Soc. Mass Spectrom. 27, 1366–1375 (2016).Article 
    ADS 
    
                    Google Scholar 
                Chyba, C. & Sagan, C. Endogenous production, exogenous delivery and impact-shock synthesis of organic molecules: an inventory for the origins of life. Nature 355, 125–132 (1992).Article 
    ADS 
    
                    Google Scholar 
                Poppe, A. R. An improved model for interplanetary dust fluxes in the outer Solar System. Icarus 264, 369–386 (2016).Article 
    ADS 
    
                    Google Scholar 
                Taylor, S. R. & McLennan, S. M. in Handbook on the Physics and Chemistry of Rare Earths Vol. 11, 485–578 (eds Gschneidner, K. A. J. & Eyring, l.) (Elsevier, 1988).Jawin, E. R. et al. Lunar science for landed missions workshop findings report. Earth Space Sci. 6, 2–40 (2019).Article 
    ADS 
    
                    Google Scholar 
                National Academies of Sciences, Engineering, and Medicine. Origins, Worlds, and Life: A Decadal Strategy for Planetary Science and Astrobiology 2023–2032 (National Academies Press, 2022).Artemis III Science Definition Team Report (NASA, 2020).Steinbrügge, G. et al. Brine migration and impact-induced cryovolcanism on Europa. Geophys. Res. Lett. 47, e2020GL090797 (2020).Article 
    ADS 
    
                    Google Scholar 
                Danell, R. et al. A full featured, flexible, and inexpensive 2D and 3D ion trap control architecture and software package. In Proc. 58th ASMS Conference on Mass Spectrometry and Allied Topics 283889 (2010).Download referencesAcknowledgementsThis study was supported by the University of Maryland Faculty Incentive Program (PI: R.A. Jr), NASA Goddard Space Flight Center Internal Research and Development Program (PIs: A.G. and A.Y.), NASA ROSES ICEE 2 Grant 80NSSC19K0610 (PI: R.A. Jr), ROSES DALI Grant 80NSSC19K0768 (PI: R.A. Jr) and CRESST II Award Number 80GSFC21M0002 (PI: A.S.).Author informationAuthors and AffiliationsUniversity of Maryland, College Park, MD, USARicardo Arevalo Jr, Lori Willhite, Anais Bardyn, Ziqin Ni & Soumya RayCRESST II, College Park, MD, USAAdrian SouthardDanell Consulting, Winterville, NC, USARyan DanellNASA Goddard Space Flight Center, Greenbelt, MD, USAAndrej Grubisic, Anthony Yu, Molly Fahey & Emanuel HernandezAMU Engineering, Miami, FL, USACynthia Gundersen & Niko MinasolaLaboratoire de Physique et Chimie de l’Environnement et de l’Espace, Orléans, FranceChristelle Briois, Laurent Thirkell & Fabrice ColinThermo Fisher Scientific, Bremen, GermanyAlexander MakarovAuthorsRicardo Arevalo JrView author publicationsYou can also search for this author in
                        PubMed Google ScholarLori WillhiteView author publicationsYou can also search for this author in
                        PubMed Google ScholarAnais BardynView author publicationsYou can also search for this author in
                        PubMed Google ScholarZiqin NiView author publicationsYou can also search for this author in
                        PubMed Google ScholarSoumya RayView author publicationsYou can also search for this author in
                        PubMed Google ScholarAdrian SouthardView author publicationsYou can also search for this author in
                        PubMed Google ScholarRyan DanellView author publicationsYou can also search for this author in
                        PubMed Google ScholarAndrej GrubisicView author publicationsYou can also search for this author in
                        PubMed Google ScholarCynthia GundersenView author publicationsYou can also search for this author in
                        PubMed Google ScholarNiko MinasolaView author publicationsYou can also search for this author in
                        PubMed Google ScholarAnthony YuView author publicationsYou can also search for this author in
                        PubMed Google ScholarMolly FaheyView author publicationsYou can also search for this author in
                        PubMed Google ScholarEmanuel HernandezView author publicationsYou can also search for this author in
                        PubMed Google ScholarChristelle BrioisView author publicationsYou can also search for this author in
                        PubMed Google ScholarLaurent ThirkellView author publicationsYou can also search for this author in
                        PubMed Google ScholarFabrice ColinView author publicationsYou can also search for this author in
                        PubMed Google ScholarAlexander MakarovView author publicationsYou can also search for this author in
                        PubMed Google ScholarContributionsThe dataset presented in this study was collected and analysed by R.A. Jr, L.W., A.B., Z.N. and S.R. The system-level architecture of the miniaturized instrument and the operational sequence of the experiments conducted were defined by R.A. Jr, A.S., R.D., A.G., C.B., L.T., F.C. and A.M. Requirements for the ion optics and SIMION models of ion transmission were provided by A.S. The mechanical design of the mass analyser assembly and custom series of ion optics were led by C.G. and N.M. The design and build of the prototype UV laser system was led by A.Y. and M.F. All authors contributed to the interpretation of the results and editing of the manuscript.Corresponding authorCorrespondence to
                Ricardo Arevalo Jr.Ethics declarations
Competing interests
A.M. is an employee of Thermo Fisher Scientific, the manufacturer of the Orbitrap device leveraged in the miniaturized instrument described here.
Peer review
Peer review information
Nature Astronomy thanks Marek Tulej and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.
Additional informationPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Supplementary informationSupplementary InformationSupplementary discussion, Figs. 1–8 and Table 1.Source dataSource Data Fig. 2Raw time domain transients for Fig. 2.Source Data Fig. 3Raw time domain transient for Fig. 3.Source Data Fig. 5Raw time domain transient for Fig. 5.Rights and permissionsSpringer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.Reprints and PermissionsAbout this articleCite this articleArevalo, R., Willhite, L., Bardyn, A. et al. Laser desorption mass spectrometry with an Orbitrap analyser for in situ astrobiology.
                    Nat Astron  (2023). https://doi.org/10.1038/s41550-022-01866-xDownload citationReceived: 03 January 2022Accepted: 16 November 2022Published: 16 January 2023DOI: https://doi.org/10.1038/s41550-022-01866-xShare this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        




"
https://news.ycombinator.com/rss,"Patterns, the fastest way to build, scale cloud apps, is hiring",https://www.patterns.app/,Comments,"











Patterns | Build next-gen AI systems | Patterns





Skip to main contentMarketplacePricingBlogDocsSign upSearchScale next-gen cloud appsRun and deploy web apps, task queues, massively parallel compute jobs, machine learning models, GPUs, and much more with a single unified framework for code and infrastructure.Start BuildingExplore Patterns Accelerate development with powerful building blocksPatterns provides powerful data abstractions called nodes that are automatically orchestrated by a reactive graph. Build data systems 10x faster. Focus on developing code specific to the problem you’re solving, not messy implementation details.About Node TypesPYScript with PythonYour browser does not support the video tag.SQLBuild pipelines with SQLYour browser does not support the video tag.Store data in TablesYour browser does not support the video tag.Setup endpoints with WebhooksYour browser does not support the video tag.Execution with a reactive GraphYour browser does not support the video tag.Visualize with ChartsYour browser does not support the video tag.Document with MarkdownYour browser does not support the video tag.Develop anywhere Patterns apps are fully defined by git-backable code and files. Develop locally with our local devkit or in the UI, version control with confidence.Your browser does not support the video tag.Check out our Development KitProduction-grade infrastructure Patterns abstracts away micro-management of compute and storage and provides easy to configure system controls so that you can focus on shipping apps that create business value.GraphsAbstract away orchestration messiness and build production grade pipelines with our functional reactive execution protocol.CodeBuild sophisticated pipelines in languages you already know - Python and SQL. Version control with GitHub, and release with CI/CD.Compute & StorageRun your apps on any infrastructure. Turn up processing with any CPU/GPU and store data in the best database for the job at hand.  Leverage open-source components Components encapsulate a sub-graph of nodes that contain commonly reused data workflows such as integrations to third-party APIs for ETL, Reverse ELT, enrichment APIs, ML models, and can even contain logic for entire data pipelines.  Clone and contribute to our open-source marketplace of components.About ComponentsVersion ControlReceive updated code from the Component developer with zero-effort. No need to pull git and redeploy a container.Clone via the marketplaceClone Components directly from the marketplace into your App for immediate use.Build and share your ownDon't see a connector you need? Use our devkit complete with helper methods helper methods and a testing framework to develop a robust API integration yourself.App examples Apps are composable, defined by code, and easily cloned across accounts. Browse select apps below, explore more from our marketplace, build your own to share through the marketplace.App MarketplaceEng Advice GPT3 Slack Botslackopen_aibotsSet up this app in a few minutes and have a new engineering voice in the room.             This app utilizes Slack and OpenAI to bring a chatbot experience to any Slack Channel.             Clone and customize for your own use case or use in a production environment.             The OpenAI component in this App allows you to construct prompts to fit your desired completions.Crypto Wallet Tracker with Web3.pyweb3.pycryptoethThis App uses web3.py to query the Ethereum blockchain and extract data for a set of relevant wallets. Paramaterize this app with wallet addresses and set up a monitoring system to track wallets for changes.Lead Enrichment, Scoring, and Routingpeople data labssalesrevopsThis app powers sales, marketing, and growth teams with fast and accruate data about their leads.             Clone this app, customize it, and integrate with your marketing website and CRM to fully automate how you score, route, and prioritize your leads. rate with your existing data tools or directly with your business systems.             This is an example of a simple configuration of lead scoring using a webhook as input and Slack channel as output. This template can be configured differently to ingest data from a database, CRM, or any API that contains your lead data. Additionally, you can also configure Patterns to export enriched and scored leads to your CRM, email marketing tool, or back to the database of your choice.Activity Schema Analyticscdpproduct analyticsIngest data from any source and model into an activity schema - an event based data structure that             simplifies customer data analytics. This app has pre-built data pipelines to calculate leveraging an activity schema such as --             customer cohort retained revenue, active revenue, and cohort retention percentage.Twitter Sentiment and Stock Trading ApptwitterintrinioIngest data from Twitter and Intrinio and conduct sentiment analysis with textblob.py.            Get current stock market data and analyze price changes alongside sentiment. Eventually turn into a stock trading bot!  Hacker News Analysis and Notificationshacker newsslackIngest data from the Hacker News API, do an analysis on the most common domains, chart a plot of the most recent and top stories. Build a monitoring app that will check for keywords in posts and send messages to Slack when found.Scalable and Secure Patterns is cloud native and can be used as a fully-managed product, alternatively bring-your-own database and use alongside Modern Data Stack products.SOC2 Type II CertificationRest assured your data is in secure hands, we’ve obtained a SOC2 Type II certification that employs best practices in data security, management, and infrastructure. Loved by buildersThe ability to use code when we need it and UI when we don’t makes it quick and easy to create data workflows in patterns. We’re excited to partner with patterns to make sophisticated data use cases more accessible to our customers.Marion NammackDirector of Product Management at BrazeWhen I came across Patterns it promised to 10x our data science. A month later we’ve integrated all major adtech platforms and are using Patterns for production pipelines, that’s more like a 100x.Philippe AssefCo-founder @ BilyBlendJet offers free worldwide shipping using a variety of carriers. 
 Patterns was able to find and ingest all of the data from our mishmash of carriers, make it uniform, and visualize it in a heat map of the world. We’re now setting up automated alerts on the order level to provide proactive customer support when we see a package is lost or late. This is a huge win for our customers and our customer service team. 
 This is game changing software, which essentially lets you solve a problem you might have. I am extremely impressed, and you will be, too!Ryan PamplinCEO at BlendjetWe were able to prototype and rapidly iterate parts of our data pipeline in a fraction of the time it would have taken to build out and orchestrate elsewhere. Felt like one person could 10x themselves and get a data-driven backend up and running in an afternoon. Being able to visualize and manipulate the pipeline with straightforward but powerful base components rewrote our own product roadmap and makes many formerly far off features implementable today.Allen RomanoCo-founder and Chief Product Officer at LogoiOn Patterns we integrated, built data pipelines, and dashboards for all of our ads + marketing data. What we thought would take us a month, we shipped in a week. Abstracting away orchestration and data storage really did accelerate our development process.Casey DonahueFounder at OptiwattPatterns makes building pipelines easy. It enables the speed and flexibility to iterate and get immediate customer feedback.Ryan WallaceSoutions Architect at Lemay.aiAboutDocsBlogCareersGitHubTwitterLinkedInTerms of ServicePrivacy PolicyContactCopyright © 2023 Patterns Data Systems, Inc.



"
https://news.ycombinator.com/rss,Ask HN: Books that teach programming by building a series of small projects?,https://news.ycombinator.com/item?id=34412069,Comments,"

Ask HN: Books that teach programming by building a series of small projects? | Hacker News

Hacker News
new | past | comments | ask | show | jobs | submit 
login




 Ask HN: Books that teach programming by building a series of small projects?
242 points by newsoul 4 hours ago  | hide | past | favorite | 127 comments 

It is common knowledge that when first learning programming, one should start with small projects to build something real rather than learning rules and syntax of the language only.Which are some of the best books that take a project based approach in teaching programming to a beginner? 
 
  
 
azemetre 1 hour ago  
             | next [–] 

While not for beginners, if you'd like to learn rust I recently finished ""Command line Rust"" [1].It was my first introduction to rust and the book was quite enjoyable. It starts off with teaching you the very basics of a command line (what it means to exit, true, or false, etc) and each chapter has you recreate a popular command line tool (like grep, cal, tail, wc) while introducing a new rust concept.The book also does TDD, test driven design, by first teaching you how to create these tests then in subsequent chapters having the tests prewritten for you.It's definitely worth a look, the author has a great writing style as well that isn't as monotonous as most programming books I've read.[1] https://www.oreilly.com/library/view/command-line-rust/97810...
 
reply



  
 
dbalatero 15 minutes ago  
             | parent | next [–] 

Just to clarify, when you say ""not for beginners"" do you mean beginners to Rust or programming in general?edit: Based on your ""It was my first introduction to rust"" comment I think maybe you meant programming beginners?
 
reply



  
 
lifewallet_dev 38 minutes ago  
             | parent | prev | next [–] 

Thank you! I needed this one, how fun, I did  this when learning Haskell, now I can do it with Rust.
 
reply



  
 
adolph 1 hour ago  
             | parent | prev | next [–] 

Came here to mention just that book. It is excellent.
 
reply



  
 
twawaaay 3 hours ago  
             | prev | next [–] 

My favourite is ""Practical Common Lisp"" by Peter Seibel (https://gigamonkeys.com/book/)Not only is the book free to read (although I would suggest to pay for it if you like it!) The code to parse binary files actually ""inspired"" my design of an actual production application which was very flexible, succinct and, most importantly,  so fast I had to spend a lot of effort convincing people the numbers are actually true.It taught me some important lessons about how you can achieve performance with Lisp languages and the real reasons for the power of macros. Not too shabby for the first book on Lisp I red!
 
reply



  
 
halkony 1 hour ago  
             | parent | next [–] 

I've been learning lisp this past week. I think I get the main idea of ""code is data and data is code"", but I struggle to understand how it's more powerful than, say, making a function factory in Python.
 
reply



  
 
majormunky 14 minutes ago  
             | root | parent | next [–] 

I'm also learning lisp, mainly as a part of learning emacs, so I may not be super accurate here.  In python, when you have a reference to a function, its a bit opaque, you can tell the thing is a function, but I don't think you can really go into the function object, and alter it lets say.  You may be able to using the AST or something, but its not a ""first-class"" thing you can do.In lisp (and code is data, data is code), the function is also a list, so you would be able to use all the normal programming tools to iterate over the list, add new items to the list, etc, which can change how the function runs.
 
reply



  
 
kfoley 9 minutes ago  
             | root | parent | prev | next [–] 

I used to have the same issue understanding the benefit.  One simple example that helped it clicked for me was considering the `when` or `unless` macro.They're just very simple macros based around the `if` special form.  Because they're macros, they can just treat the body as data, i.e. it's not executed as part of argument resolution.If you wanted to make a `when` function in Python, the ""body"" would have to be a callable like a lambda or named function.I know this probably isn't the most compelling use case but I found it to be a really simple way to understand some of the benefits of macros and how they could be used elsewhere.
 
reply



  
 
cess11 10 minutes ago  
             | root | parent | prev | next [–] 

Not sure what you mean by ""function factory"", could you elaborate? Isn't it metaclasses and decorators that are used for metaprogramming in Python?In my opinion the 'everything is an expression' is the really tasty part of the lisps, which goes well with quoting.
 
reply



  
 
agumonkey 29 minutes ago  
             | root | parent | prev | next [–] 

maybe a function factory is just an OO pattern for return closures, it's difficult to not perceive things from 'now', but i'll be frank i 1) never liked DP 2) couldn't spend more than 5 minutes reading about them as it seemed more verbose and redundant
 
reply



  
 
kennyfrc 4 hours ago  
             | prev | next [–] 

There’s quite a few:- Zed Shaw’s Learn More Python the Hard Way[1]- Brian Hogan’s Exercises for Programmers (best for beginners or for learning a new language)[2]- Hal Fulton’s The Ruby Way[3]- Chris Ferdinandi’s Vanilla JS Academy[4]- Marc-Andre Cournoyer’s Great Code Club (it’s old, and the community doesn’t exist anymore, but i think he still maintains it)[5]- A few python books from No Starch Press (notably those authored by Al Sweigart)I learned the most as a beginner  from Zed Shaw’s work, and from reading open source code.Once you’re done with the initial “learn from tutorials” phase, there’s no better resource than reading open source code.[1] https://www.amazon.com/Learn-More-Python-Hard-Way/dp/0134123...[2] https://www.amazon.com/Exercises-Programmers-Challenges-Deve...[3] https://www.amazon.com/Ruby-Way-Programming-Addison-Wesley-P...[4] https://vanillajsacademy.com/[5] https://www.greatcodeclub.com/
 
reply



  
 
Zhyl 3 hours ago  
             | parent | next [–] 

Specifically ""Automate the Boring Stuff with Python"" is my favourite Al Sweigart book.
 
reply



  
 
michaelcampbell 2 hours ago  
             | root | parent | next [–] 

I'm not much of a Python guy, but concur with this; I enjoyed this book and did get something out of it.
 
reply



  
 
lazyant 3 hours ago  
             | parent | prev | next [–] 

> - A few python books from No Starch Press (notably those authored by Al Sweigart)Yes for example https://nostarch.com/inventwithpython and https://nostarch.com/big-book-small-python-projects
 
reply



  
 
jypepin 1 hour ago  
             | parent | prev | next [–] 

I learnt ruby with Zed Shaw’s Learn Ruby the Hard Way. Was a really great experience!
 
reply



  
 
darreninthenet 4 hours ago  
             | parent | prev | next [–] 

Is Shaw's first Python book any good?
 
reply



  
 
danpalmer 3 hours ago  
             | root | parent | next [–] 

Shaw has had some strong opinions that have gone significantly against the mainstream viewpoint in Python. Whether he's right or not doesn't really matter - as an educator that is targeting the mainstream it's fairly important to stay on that and not introduce personal bias around these things. Unfortunately his criticism was often not very constructive.
 
reply



  
 
cameron_b 2 hours ago  
             | root | parent | next [–] 

despite my other comment here I do agree with you on this, especially remembering his comments on test tools, nose for example
 
reply



  
 
danpalmer 1 hour ago  
             | root | parent | next [–] 

Yeah, he pushed back hard against the Python 3 migration, by picking on a small number of trade-offs that he personally didn't like and using that to conclude that the entire endeavour was doomed. The Python 3 migration wasn't flawless by any means, but it was better than he made out.I know his ""criticism"" also held back many from moving their libraries to Python 3 by sowing doubt in the community, and ultimately may have slowed down adoption overall.
 
reply



  
 
kennyfrc 4 hours ago  
             | root | parent | prev | next [–] 

If you’re a pure beginner with zero programming experience, yes. If you’ve programmed a few scripts and have done a flask web app, his first book might be too easy.
 
reply



  
 
cameron_b 3 hours ago  
             | root | parent | prev | next [–] 

I found his Learn Python3 The Hard Way to be a great series of exercises if you're starting out or starting again. You won't find -everything- on any topic, but the exercises are complete enough to get you rolling and seed the sort of patterns that will make you successful in looking for more material.I had a little bit of Python(2) under my belt from general curiosity, and I've done some C++ in high school so I have some ""CS Theory""There is a ""more"" python version out now 
https://learncodethehardway.org/more-python-book/Amazon ( the ""newer"" version proposed is not, it's his python2 book )
https://www.amazon.com/Learn-More-Python-Hard-Way/dp/0134123...
 
reply



  
 
7speter 3 hours ago  
             | parent | prev | next [–] 

> and from reading open source code.Do you just search for projects in the language your using on github?
 
reply



  
 
kennyfrc 3 hours ago  
             | root | parent | next [–] 

Yes, and I would typically search for simple projects using keywords such as “micro”, “small”, “tiny”, or “pico”.I would then try to re-implement the “getting started” code in the readme from scratch, like a little programming puzzle. If I can’t figure it out, I‘ll add in some debugger breakpoints, inspect the stack trace to understand how it works, then code the needed methods / classes as I go.If you’re a ruby programmer, soveran’s work in github can be read in a day or two. I specifically like cuba (micro webframework) and mote (microtemplate).
 
reply



  
 
MarkusWandel 3 minutes ago  
             | prev | next [–] 

""Software Tools"" by Kernighan & Plauger, perhaps.The original was in Ratfor, which is a Fortran dialect.  That being similar syntax but more limited than C is not a bad intro language.
 
reply



  
 
culopatin 2 hours ago  
             | prev | next [–] 

My biggest gripe with all the project tutorials I’ve seen is that really quickly you get into a “ok I get what we’re doing” and then just copy the code they provide. At the end of the day you did zero mental workout because you didn’t have to figure out the code.
I hope to find one of these that guides you but doesn’t give you the code immediately, but has a code answer key at the end or something. I get it that it’s not easy to do, but there must be one out there that doesn’t spoon-feed you everything.
 
reply



  
 
ojl 1 hour ago  
             | parent | next [–] 

That’s why I like The Ray Tracer Challenge. He gives you test scenarios and a little bit of pseudo code (sometimes). The actual implementation has to be done by the reader.http://raytracerchallenge.com/
 
reply



  
 
the_gastropod 1 hour ago  
             | root | parent | next [–] 

+1 This is one of my favorite programming books. It's a very rewarding-feeling experience to build something so capable so quickly.
 
reply



  
 
cardanome 1 hour ago  
             | parent | prev | next [–] 

One thing that helps me a lot when learning new languages is to NOT copy and paste but actually re-type the code.You will be forced to read more carefully and, more importantly, you will make typing mistakes. Hunting down your own mistake will teach you the subtleties of the syntax as well as further increase your understanding of the code.For more advanced programmers, there is also the challenge of translating the code into another language entirely, which requires a actual semantic understanding of the code.Also once the project is done, adding small additional bonus features will further deepen your understanding.
 
reply



  
 
culopatin 23 minutes ago  
             | root | parent | next [–] 

I re type it but I don’t necessarily learn how to come up with that code when I’m off the leash. It’s useful when learning new syntax for sure though.
 
reply



  
 
mgomez 1 hour ago  
             | parent | prev | next [–] 

To force myself “to figure out the code,” I typically do the tutorial in a different language. I wanted to get familiar with Java last year, but all the books/tutorials were boring to me. What I ended up doing was going through a book that showed how to write a backend in Go (which was also unfamiliar to me at the time) while simultaneously porting it to Java (using Vert.x). Probably sounds inefficient tackling two languages at once, but it was the only way I was able to stay engaged with the material. Of course I ended up with something unidiomatic in terms of Java, but the project works and I now have motivation for a proper refactor to keep the momentum going.
 
reply



  
 
thr0wnawaytod4y 1 hour ago  
             | parent | prev | next [–] 

For this reason you need to find _your own_ projects, solve your own problems (which are actually pretty common, but you have to search or figure out a solution)That's what I tell everyone
 
reply



  
 
travisjungroth 1 hour ago  
             | root | parent | next [–] 

The issue I’ve found with that in my students is they then have to play the role of product manager in addition to software engineer. Those choices add a lot of mental overhead. There’s a balance between motivation and pressure for most students.I think cloning is more the way to go. Pick some existing project/product and copy the essential components. Less choices, doesn’t have to end up getting used.
 
reply



  
 
intelVISA 1 hour ago  
             | parent | prev | next [–] 

Thinking of building something along those lines i.e. you can peek ahead if you really get stuck.Proj-based learning is really fun to both create and consume imo.
 
reply



  
 
culopatin 1 hour ago  
             | root | parent | next [–] 

If it helps you in any way, what I try to do is to use a tutorial book to build my own app. For example a Flask tutorial that shows how to build a blog app, but I’m building a library. I still learn about the Flask way and I learn some building blocks but I have to figure out the equivalent for what I want to do. I often get stuck though.So maybe showing the building blocks and letting you use them for something. You could perhaps show 2 or 3 suggested projects that are built with the building blocks. You show the generic version of the blocks, and then you figure out how to tweak them to make one of the 2 or 3 suggested projects.Sort of buying the legos and then showing what you could do with them. But building it is up to you.
 
reply



  
 
dielll 1 hour ago  
             | parent | prev | next [–] 

Datacamp does what you want
 
reply



  
 
__warlord__ 4 hours ago  
             | prev | next [–] 

Not a book, but check [Build your own X](https://github.com/codecrafters-io/build-your-own-x), a compilation of well-written, step-by-step guides for re-creating our favorite technologies from scratch.
 
reply



  
 
deckard1 51 minutes ago  
             | parent | next [–] 

Excellent resource. Never seen that before.In a similar vein, there is The Architecture of Open Source Applications which is great:https://aosabook.org/en/index.html
 
reply



  
 
inxode 2 hours ago  
             | parent | prev | next [–] 

It was better when Daniel maintained it.
 
reply



  
 
kriro 3 hours ago  
             | prev | next [–] 

""Hands on Rust"" teaches Rust by building a rogue like game step by step. Before that there's a chapter where you rebuild Flappy Bird from scratch that teaches the ""basics"" before diving into more advanced concepts in the rogue like. I liked the approach and recommend the book but it's fast paced and expects quite a bit from the reader (it's excellent if you have some programming experience already but probably daunting as a true first book imo).
 
reply



  
 
myers 3 hours ago  
             | parent | next [–] 

I second this one.  This was a great book.There are some follow ups to the final project I likeCompile to WASM so you can run on the web: <https://hands-on-rust.com/2021/11/06/run-your-rust-games-in-...>Port to the Bevy ECS: <https://saveriomiroddi.github.io/learn_bevy_ecs_by_ripping_o...>
 
reply



  
 
kriro 2 hours ago  
             | root | parent | next [–] 

Oh cool I wasn't aware of these. Thanks for sharing, both look very interesting. I like the idea of reusing projects to teach different tech :)
 
reply



  
 
modernerd 3 hours ago  
             | prev | next [–] 

For Swift:https://www.apple.com/swift/playgrounds/ (I've had more success introducing programming with this than with any of the links below; it's a very compelling intro for those who already own an iPad/Mac, and the core concepts are generalisable to other languages/environments even if it's specific to Apple's APIs and hardware.)~~~For Python:https://automatetheboringstuff.comhttps://nostarch.com/big-book-small-python-projects~~~For JS:https://eloquentjavascript.net (project-based and for beginners)https://javascript30.com (not for total beginners or self-study, would need a friend/tutor)
 
reply



  
 
bpesquet 1 hour ago  
             | parent | next [–] 

I beg to differ: for all its qualities, Eloquent JavaScript is not a very beginner-friendly book. It goes quite deep into many JS intricaties and contains a lot of challenging tasks for people just discovering programming.More beginner-focused alternatives are https://www.freecodecamp.org and https://thejsway.net.Disclaimer: I've been teaching programming professionally for 15+ years and wrote the latter.
 
reply



  
 
Sn0wCoder 52 minutes ago  
             | root | parent | next [–] 

I have recommended to beginners before and most of them thanked me.  I skimmed the page for your book but did not see the link to the free version.  I am on mobile so maybe just missed it.  Says read free here, but here is not a link.  Clicking the book takes you to Amazon.
 
reply



  
 
Sn0wCoder 51 minutes ago  
             | root | parent | next [–] 

Never-mind the website is the book lol
 
reply



  
 
Sn0wCoder 2 hours ago  
             | parent | prev | next [–] 

I second eloquent JavaScript.  Been doing this book since v1 and recommend to just about anyone that asks for recommendations.  I use typescript daily and have been hoping that this book gets updated for TS although it would be hard since it uses lots of JS tricks that may or may not be best practices in TS.  Technically just renaming the files from .js to .TS would work the linter would go crazy
 
reply



  
 
samvher 1 hour ago  
             | prev | next [–] 

Software Foundations is a series of interactive books where you work through proofs in Coq, gradually increasing in complexity: https://softwarefoundations.cis.upenn.edu/This is a bit on the edge of the domain that you're asking about (not really for beginners, and proofs are perhaps a somewhat niche type of programming) but I learned a lot from this and many people don't seem to know it, so I think it belongs in the list.
 
reply



  
 
rcardo11 26 minutes ago  
             | prev | next [–] 

This is the best programming book I have ever picked https://www.amazon.com/-/es/Noam-Nisan/dp/0262539802/ref=sr_...
 
reply



  
 
WoodenChair 49 minutes ago  
             | prev | next [–] 

For intermediate programmer, that's what we do in the Classic Computer Science Problems series (https://classicproblems.com). We combine learning programming with learning computer science problem solving techniques.
 
reply



  
 
jcon321 2 hours ago  
             | prev | next [–] 

No joke, ""C for Dummies"" - it's more teaching you CS101 concepts, less about ""C"". Tons of examples to go through and the attitude of the author is very helpful compared to a lot of other styles out there.I was failing my computer science classes first semester. Towards the end of the semester I sat down in the library for a few days and just went page by page typing every example and compiling it. Aced every IT class I had the next few years. I attribute that book to making everything ""click"" for me.Once you understand the beginner concepts that book shows, then you as an individual would know where to look to gain improvement.
 
reply



  
 
rahuldan 4 hours ago  
             | prev | next [–] 

There is this excellent Github repo for this: https://github.com/codecrafters-io/build-your-own-xIt has a collection of blogs for building various small projects to learn different languages.
 
reply



  
 
asicsp 4 hours ago  
             | parent | next [–] 

Similar resources:* Projectbook https://projectbook.code.brettchalupa.com/* Project Based Learning https://github.com/practical-tutorials/project-based-learnin...
 
reply



  
 
CraigJPerry 2 hours ago  
             | prev | next [–] 

HTDPv2 https://htdp.org/ - it's about learning to program rather than teaching a specific language.I thought the little projects you build along the way struck a good balance of interest, e.g. you're building a snake game by section 2 (but crucially you've already been exposed to how to think about data so you're not just dropped into the deep end with some boilerplate to fiddle with).Anyway, i could write tons of praise for this book but it's convinced me to ditch python for teaching newbies and i LOVED python for getting newbies started real quick with projects for years.
 
reply



  
 
agumonkey 22 minutes ago  
             | parent | next [–] 

Interesting, I rarely see it mentioned. How do people react to the mindset it teaches ? I really like that it's near language-less programming book, really more about and/or :)
 
reply



  
 
dns_snek 1 hour ago  
             | parent | prev | next [–] 

That sound like an interesting approach and I'd love to hear more about it!One striking difference compared to most ""beginner languages"" is the Lisp-like syntax. I think focusing on problem solving rather than learning syntax is a great goal, but does this particular choice of a language make it harder for your ""students"" to continue learning in another language?I'm assuming they would switch to a more popular language at some point (with syntax that resembles C), so I'm curious what that transition is like for beginners.
 
reply



  
 
newsoul 2 hours ago  
             | parent | prev | next [–] 

I wouldn't mind if you write some more about the book.
 
reply



  
 
weird_user 1 hour ago  
             | prev | next [–] 

For beginners, Daniel Holden's Build Your Own Lisp[1] is excellent material for learning C. It's a very succinct book.For intermediate programmers, Build Your Own Redis[2] is a WIP book I am currently writing.[1] https://buildyourownlisp.com/[2] https://build-your-own.org/
 
reply



  
 
patchorang 2 hours ago  
             | prev | next [–] 

Not a book and webdev focused, but I've been doing https://fullstackopen.com/en/ and it's been great.The chapters are structured so you are building 2 projects at once. One where you follow along with the chapter material with example code and explanations. And a second in the exercises.I really like it because it introduces the concepts in the context of a project, but you can't get stuck because the code is there too. Then you need to really apply it in the exercises on your own project, without the example code.
 
reply



  
 
ajoseps 3 hours ago  
             | prev | next [–] 

This book is still in progress but I've gone through some of the chapters and have enjoyed it. Rust from the Ground Up: https://leanpub.com/rust-from-the-ground-upI was looking for a book that had offline projects I can work on while on flights, and this book focuses on rebuilding linux utilities using rust. The other nice part is that you get a better understanding of linux internals.I believe the author is also responsive on the rust subreddit.
 
reply



  
 
mprovost 1 hour ago  
             | parent | next [–] 

Thanks for the kind words! I'm desperately polishing the next chapter for publication...
 
reply



  
 
ajoseps 1 hour ago  
             | root | parent | next [–] 

great to see you on HN as well! I haven't revisited the book in a while but I really liked the chapters I did work on. I have another flight coming up soon so maybe I'll try another chapter :)
 
reply



  
 
eatonphil 2 hours ago  
             | prev | next [–] 

Sort of like that, I've been slowly working on a rosetta code for implementations of small problems. Rosettacode.org allows you to use standard library functions so many (but not all) of their examples are useless when you actually want to see how to code something from hand in the language.So my rosetta code only allows you to implement the main part of the thing yourself. But it's not appropriate for beginners. It's more appropriate for teaching a new language to someone who is already an intermediate programmer.https://tinyprograms.org/
 
reply



  
 
hiAndrewQuinn 2 hours ago  
             | parent | next [–] 

God, I hope you just found Rosetta.code off limits. That's a fantastic name
 
reply



  
 
tjpnz 3 hours ago  
             | prev | next [–] 

MUD Game Programming. Not sure how easy obtaining a copy would be in 2023 but it has two projects which I recall being a lot of fun. I never did build a successful MUD but did learn a fair amount about network programming.https://www.goodreads.com/en/book/show/927128.Mud_Game_Progr...
 
reply



  
 
dartharva 3 hours ago  
             | prev | next [–] 

I don't know why jtimiclat's comment recommending Al Sweigart's books got downvoted and faded out, they are indeed quite good for beginners wanting to get their hands dirty with programming instead of cramming it in an academic way.Not to mention they're all freely available and give excellent value for both time and money: https://inventwithpython.com/
 
reply



  
 
autodev1 40 minutes ago  
             | prev | next [–] 

- Pick a language- Find a recent book (< 2 yrs old) on LibGen.is- OR, a Udemy.com video series (note: never pay more than $12 or so-- the promo rate.  If you see a higher price, just create a new email & user account (or reset cookies?) and you'll see the promo price again)- Visit: Roadmap.sh to get a sense of a learning roadmap what knowledge to build.- Use a search engine to answer questions, such as ""free learning resource for learning <XYZ thing>""
 
reply



  
 
tsm 4 hours ago  
             | prev | next [–] 

Software Design by Example (https://www.routledge.com/Software-Design-by-Example-A-Tool-...)Available free online at https://third-bit.com/sdxjs/
 
reply



  
 
forrestbrazeal 3 hours ago  
             | prev | next [–] 

My Cloud Resume Challenge project [0] and book [1] uses a set of small, stackable mini-projects to introduce beginners to many of the pragmatic skills used in cloud software engineering.[0] https://cloudresumechallenge.dev/docs/the-challenge/
[1] https://cloudresumechallenge.dev/book/
 
reply



  
 
Taylor_OD 26 minutes ago  
             | prev | next [–] 

Most udemy/MOOC courses are structured this way. I tend to just look for the highest rated ones on whatever language I'm interested in and make sure it says build projects or something similar.
 
reply



  
 
hlship 1 hour ago  
             | prev | next [–] 

Not quite a beginner book, but you should check out https://www.amazon.com/Mazes-Programmers-Twisty-Little-Passa....
 
reply



  
 
larve 3 hours ago  
             | prev | next [–] 

Peter norvig’s paradigms of artificial intelligence programming, despite its age, is a delight.https://norvig.github.io/paip-lisp/#/
 
reply



  
 
newsoul 3 hours ago  
             | parent | next [–] 

But is that suitable for a beginner?
 
reply



  
 
larve 2 hours ago  
             | root | parent | next [–] 

It depends on what you mean by beginner, but I would say for someone who has been futzing with a couple of js boot camps or building some php websites, it’s what takes you from hacking to software development. Completely new to programming, maybe, if someone has a background in maths or physics and has a sense of “rigor” in thinking.But, of all the books I worked through, this is the one I remember just having tons of fun because the examples are so cool and the code so… clear. I think norvigs programming course in python (which I only skimmed) could be a great modern equivalent.
 
reply



  
 
nesarkvechnep 3 hours ago  
             | root | parent | prev | next [–] 

Yes, it is. It’s not about machine learning, neural networks but the more approachable AI of the 80s.
 
reply



  
 
globalise83 3 hours ago  
             | root | parent | next [–] 

Haven't tried his, but can GPT3 write code that results in 1980s style AI?
 
reply



  
 
nemoniac 4 hours ago  
             | prev | next [–] 

Nand2tetris https://www.nand2tetris.org/
 
reply



  
 
tmtvl 3 hours ago  
             | prev | next [–] 

Common Lisp: A Gentle Introduction to Symbolic Computation(https://www.cs.cmu.edu/~dst/LispBook/) has you build a number of small applications, like a substitution decipher application, a plotting function, and more, which it calls ""keyboard exercises"".
 
reply



  
 
theshrike79 4 hours ago  
             | prev | next [–] 

If we're talking a complete beginner with maybe only ""hello world"" level skills I recommend the Lego BOOST set: https://www.lego.com/en-fi/product/boost-creative-toolbox-17...It teaches you step by step the basics of programming (loops, function calls etc) using a scratch-like programming language.
 
reply



  
 
oidar 59 minutes ago  
             | parent | next [–] 

Any alternative to this? I think lego stopped making it.
 
reply



  
 
RomanPushkin 1 hour ago  
             | prev | next [–] 

Shameless plug:FREE https://leanpub.com/rubyisforfun - Ruby Is For Fun, I made the book so you do a lot of exercise. There is around 80 exercises in total.Rewritten as a course:https://www.educative.io/courses/handbook-ruby-developers
 
reply



  
 
fedeb95 2 hours ago  
             | prev | next [–] 

Parenthesis: keep in mind that while learning by doing might be a good way to begin, even if I wouldn't be so sure as wether one ""should"" or ""could"" start along such a path, at some point you'll be stuck without studying the ""boring"" stuff you've delayed. True programming is realising the boring stuff weren't so boring in the first place
 
reply



  
 
phowat 3 hours ago  
             | prev | next [–] 

http://landoflisp.com/Was a really fun read.
 
reply



  
 
rg111 3 hours ago  
             | parent | next [–] 

A similar great book is Realm of Racket.
 
reply



  
 
tasuki 2 hours ago  
             | prev | next [–] 

I'm very much a fan of a project-based approach. However, I wouldn't recommend any book for that. I'd ask the apprentice what they'd like to achieve, and try to reduce scope as much as possible.There's a world of difference in motivation when building something you want to exist, versus building a project because a book told you so.
 
reply



  
 
fellowniusmonk 2 hours ago  
             | prev | next [–] 

It's not quite the same as what you are looking for, but I've known multiple people who were stuck in their attemp to learn to code before using railstutorial, I view it and the book ""sql: visual quickstart guide"" as two very well designed intros that manage to avoid having sections with random jumps in difficulty and terminology usage that can leave true beginners behind.
 
reply



  
 
elias94 3 hours ago  
             | prev | next [–] 

I would suggest Practical Common Lisphttps://gigamonkeys.com/book/You will create a MP3 database, a web server, a spam filter, a HTML generator. Really practical!
 
reply



  
 
throwawaybnb123 4 hours ago  
             | prev | next [–] 

Codecademy has projects page: https://www.codecademy.com/projects
 
reply



  
 
dan-g 1 hour ago  
             | prev | next [–] 

https://beautifulracket.com/ by Matthew Butterick - “An Introduction to Language-Oriented Programming Using Racket”
 
reply



  
 
jonjacky 2 hours ago  
             | prev | next [–] 

The famous Structure and Interpretation of Computer Programs (SICP) does this, in the Scheme language.
The whole book is online here, with the table of contents right at the front:https://web.mit.edu/6.001/6.037/sicp.pdf
 
reply



  
 
bosch_mind 2 hours ago  
             | parent | next [–] 

There’s a JS edition too
 
reply



  
 
olkyts 3 hours ago  
             | prev | next [–] 

Learning Rust, I use these 2:
1. Command-Line Rust: A Project-Based Primer for Writing Rust CLIs
2. Zero To Production In Rust (it's actually one project)
 
reply



  
 
velmu 2 hours ago  
             | prev | next [–] 

The Symfony Fast Track book takes this approach: https://symfony.com/doc/6.2/the-fast-track/en/index.html
 
reply



  
 
unrequited 2 hours ago  
             | prev | next [–] 

https://shop.jcoglan.com/building-git/This is an excellent resource.
 
reply



  
 
pixelmonkey 4 hours ago  
             | prev | next [–] 

Not a book, but CS50x is a freely available course with high quality videos and lecture notes. The lecture exercises are programming projects of sorts. It teaches programming from scratch with C and then Python. Plus, since a lot of students take it, there are lots of online resources with tips, like the CS50 subreddit.https://cs50.harvard.edu/x/2023/
 
reply



  
 
cehrlich 3 hours ago  
             | parent | next [–] 

CS50x and CS50web are fantastic. web is a bit outdated but it doesn't matter because the projects are so good.
 
reply



  
 
LastTrain 3 hours ago  
             | prev | next [–] 

When I was young, I went about it the opposite way. I thought of some small project and researched how to implement it, then moved on to something bigger. Everybody is different of course, but I felt like I learned more having to figure out how to go about implementing each myself.
 
reply



  
 
azhenley 2 hours ago  
             | prev | next [–] 

Check out my blog series, ""Challenging projects every programmer should try"". Who knows, maybe I will turn it into a book that shows how to implement each project :)https://austinhenley.com/blog/challengingprojects.html
 
reply



  
 
cinntaile 1 hour ago  
             | parent | next [–] 

I like your list. Maybe you could break the projects down into smaller subproblems (you sort of do that already) but I personally don't like seeing the implementation. It gives you a false sense of understanding imo.
 
reply



  
 
allie1 1 hour ago  
             | prev | next [–] 

Manning.com also has LiveProjects which are good for Machine Learning projects
 
reply



  
 
jtmiclat 4 hours ago  
             | prev | next [–] 

I found Al Sweigart's books nice for beginners.Highlights 
https://automatetheboringstuff.com
https://inventwithpython.com/bigbookpython
 
reply



  
 
evandale 2 hours ago  
             | prev | next [–] 

How to Design Programs: https://htdp.org/2022-8-7/Book/index.htmlIt's lots of small problems that seem like projects and I've found it fun to expand on the examples on your own beyond what the book asks.
 
reply



  
 
eu 2 hours ago  
             | prev | next [–] 

http://tinypythonprojects.com/
 
reply



  
 
gen_greyface 4 hours ago  
             | prev | next [–] 

check thishttps://news.ycombinator.com/item?id=22299180
Ask HN: What are some books where the reader learns by building projects?
 
reply



  
 
skizm 3 hours ago  
             | prev | next [–] 

While on the topic, how about any books or courses that teach building a 3D game engine from scratch? I'm language agnostic, so any language is fine.
 
reply



  
 
boredemployee 3 hours ago  
             | prev | next [–] 

>> one should start with small projects to build something real rather than learning rules and syntax of the language only.Yes, but don't overlook the great learning that is gained, at the beginning of any learning, by studying from many different sources (aka good books). It seems that something magical happens when you do this: the confrontation of ideas from different sources can make you better absorb ideas and resolve any doubts that you didn't understand before.
 
reply



  
 
icco 1 hour ago  
             | prev | next [–] 

Pretty much all of the books at newline do this: https://www.newline.co/
 
reply



  
 
kdmitry 2 hours ago  
             | prev | next [–] 

I have done courses on udemy like this before for learning new languages and tech stacks. There are plenty of courses on there that you can usually get for $20 that will teach you by building a bunch of small apps or services that build up in complexity.
 
reply



  
 
werber 4 hours ago  
             | prev | next [–] 

https://eloquentjavascript.net/ is nice and project based, it builds up from 0
 
reply



  
 
saperyton 4 hours ago  
             | prev | next [–] 

The Advanced Beginner blog post series by Robert Heaton is great.
 
reply



  
 
asicsp 4 hours ago  
             | parent | next [–] 

Link: https://robertheaton.com/2018/12/08/programming-projects-for...
 
reply



  
 
anurags 2 hours ago  
             | prev | next [–] 

https://github.com/practical-tutorials/project-based-learnin...
 
reply



  
 
robaye 2 hours ago  
             | prev | next [–] 

C Programming: A Modern Approach by K. N. King. I’ve never seen another book that contains as many exercises and projects as Kings book does.
 
reply



  
 
HellDunkel 2 hours ago  
             | prev | next [–] 

The Rust Programming Language by Steve Klabnik and Carol Nichols. I don’t think it’s a language for beginners though.
 
reply



  
 
devd00d 3 hours ago  
             | prev | next [–] 

Not quite programming but certainly covers the ""small projects"" part. This is what taught me the basics of Unreal Engine: https://www.youtube.com/watch?v=k-zMkzmduqI
 
reply



  
 
bartvk 3 hours ago  
             | prev | next [–] 

Learn Swift in 100 days. It start with the basics using Swift Playgrounds. But as soon as possible, it starts with building apps.https://www.hackingwithswift.com/100
 
reply



  
 
basicallydan 4 hours ago  
             | prev | next [–] 

If they don't exist, this is a good idea for anybody looking for a project to write a programming book
 
reply



  
 
cameron_b 2 hours ago  
             | parent | next [–] 

There are many, but the field is still wide open. Anyone with a particular domain experience should find a market.There are lots of starting from scratch guides, and few focused guides.for subject in [""Finance"", ""Contact Management"", ""Service Desk Automation"", ""Web Apps from scratch"", ""Web Apps on someone else's API"", ""Baseball Metrics""]:
   for language in [""R"", ""Go"", ""Python3"", ""Rust"", ""Lisp""]:
      print(f""Examples for {subject} in {language} would make a good book."")edit: no idea how to smash indent-formatting into MD-for-HN
 
reply



  
 
Nzen 41 minutes ago  
             | root | parent | next [–] 

Prefix each line with two spaces, per the formatdoc page [0][0] https://news.ycombinator.com/formatdoc  for subject in [
   ""Finance"",
   ""Contact Management"",
   ""Service Desk Automation"",
   ""Web Apps from scratch"",
   ""Web Apps on someone else's API"",
   ""Baseball Metrics""
  ] : for language in [
   ""R"",
   ""Go"",
   ""Python3"",
   ""Rust"",
   ""Lisp""
  ] : print(f""Examples for {subject} in {language} would make a good book."")
 
reply



  
 
cameron_b 0 minutes ago  
             | root | parent | next [–] 

Thanks!
 



  
 
feliixh 2 hours ago  
             | prev | next [–] 

Automate the boring stuff with Python
 
reply



  
 
cratermoon 55 minutes ago  
             | prev | next [–] 

Practical Common Lisp by Peter Seibel
 
reply



  
 
poszlem 4 hours ago  
             | prev | next [–] 

I can recommend ""Programming: Principles and Practice Using C++"", although the fact that it uses C++ as a first language can be a downside to some.
 
reply



  
 
ThomPete 1 hour ago  
             | prev | next [–] 

ChatGPT is probably much better at doing that for you. You can start with anything you want and explore as deep as you want.
 
reply



  
 
rg111 3 hours ago  
             | prev | next [–] 

I recently finished the book that teaches programming by developing games using DragonRubyGameToolkit. Really loved this book._____Python Crash Course by Eric Matthes has a section dedicated to projects.I really liked this book.This is what taught me Python.I knew C before.
 
reply



  
 
volodarik_lemon 2 hours ago  
             | prev | next [–] 

All Swift by Sundell series are pretty good.
 
reply



  
 
kgwxd 3 hours ago  
             | prev | next [–] 

Programming Games for Atari 2600 https://forums.atariage.com/topic/339819-upcoming-book-on-at...
 
reply



  
 
hgsgm 3 hours ago  
             | prev [–] 

checkio.com for increasing challenges in Python or JS.But also pretty much every book does that.
 
reply







Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
Search:  


"
https://news.ycombinator.com/rss,UK lawmakers vote to jail tech execs who fail to protect kids online,https://arstechnica.com/tech-policy/2023/01/uk-lawmakers-vote-to-jail-tech-execs-who-fail-to-protect-kids-online/,Comments,"






      Protect kids or face prosecution    —

UK lawmakers vote to jail tech execs who fail to protect kids online
If passed, UK online safety bill could jail tech leaders for up to two years.


Ashley Belanger
    -  Jan 17, 2023 4:16 pm UTC

 




Enlargeilkercelik | E+ 


reader comments
70
 with 0 posters participating


Share this story

Share on Facebook
Share on Twitter
Share on Reddit




The United Kingdom wants to become the safest place for children to grow up online. Many UK lawmakers have argued that the only way to guarantee that future is to criminalize tech leaders whose platforms knowingly fail to protect children. Today, the UK House of Commons reached a deal to appease those lawmakers, Reuters reports, with Prime Minister Rishi Sunak’s government agreeing to modify the Online Safety Bill to ensure its passage. It now appears that tech company executives found to be ""deliberately"" exposing children to harmful content could soon risk steep fines and jail time of up to two years.
The agreement was reached during the safety bill's remaining stages before a vote in the House of Commons. Next, it will move on to review by the House of Lords, where the BBC reports it will “face a lengthy journey.” Sunak says he will revise the bill to include new terms before it reaches the House of Lords, where lawmakers will have additional opportunities to revise the wording.
Reports say that tech executives responsible for platforms hosting user-generated content would only be liable if they fail to take “proportionate measures” to prevent exposing children to harmful content, such as materials featuring child sexual abuse, child abuse, eating disorders, and self-harm. Some measures that tech companies can take to avoid jail time and fines of up to 10 percent of a company's global revenue include adding age verification, providing parental controls, and policing content. 
Advertisement 


If passed, the Online Safety Bill would make managers liable for holding tech companies to their own community guidelines, including content and age restrictions. If a breach of online safety duties is discovered, UK media regulator Ofcom would be responsible for prosecuting tech leaders who fail to respond to enforcement notices. Anyone found to be acting in good faith to police content and protect kids reportedly won’t be prosecuted.
Ars could not immediately reach any major tech company for comment on the House of Commons deal, but Reuters reported that executives based in the United States have been closely monitoring updates to the Online Safety Bill.
UK Culture Secretary Michelle Donelan said in a statement that this amendment would prevent senior managers at tech companies from ignoring otherwise enforceable requirements of the Online Safety Bill, giving Ofcom “additional teeth to deliver change and ensure that people are held to account if they fail to properly protect children.”
Last month, Donelan wrote a letter to parents explaining why she was advocating for criminal penalties for any tech leaders who “consent or connive” to skirt Online Safety Bill requirements.
“The onus for keeping young people safe online will sit squarely on the tech companies’ shoulders,” Donelan wrote. “You or your child will not have to change any settings or apply any filters to shield them from harmful content. Social media companies and their executives in Silicon Valley will have to build these protections into their platforms—and if they fail in their responsibilities, they will face severe legal consequences.”














reader comments
70
 with 0 posters participating


Share this story

Share on Facebook
Share on Twitter
Share on Reddit







Ashley Belanger
      Ashley Belanger is the senior tech policy reporter at Ars Technica, writing news and feature stories on tech policy and innovation. She is based in Chicago.    

Email ashley.belanger@arstechnica.com
//
Twitter @ashleynbelanger








Advertisement 





















Channel Ars Technica




← Previous story Next story →




Related Stories









Today on Ars







"
https://news.ycombinator.com/rss,The FBI Identified a Tor User,https://www.schneier.com/blog/archives/2023/01/the-fbi-identified-a-tor-user.html,Comments,"





The FBI Identified a Tor User - Schneier on Security






































							Schneier on Security						






						Menu						

Blog
Newsletter
Books
Essays
News
Talks
Academic
About Me
 



Search

Powered by DuckDuckGo





Blog

Essays

Whole site

Subscribe



 



HomeBlog 


The FBI Identified a Tor User
No details, though:
According to the complaint against him, Al-Azhari allegedly visited a dark web site that hosts “unofficial propaganda and photographs related to ISIS” multiple times on May 14, 2019. In virtue of being a dark web site—­that is, one hosted on the Tor anonymity network—­it should have been difficult for the site owner’s or a third party to determine the real IP address of any of the site’s visitors.
Yet, that’s exactly what the FBI did. It found Al-Azhari allegedly visited the site from an IP address associated with Al-Azhari’s grandmother’s house in Riverside, California. The FBI also found what specific pages Al-Azhari visited, including a section on donating Bitcoin; another focused on military operations conducted by ISIS fighters in Iraq, Syria, and Nigeria; and another page that provided links to material from ISIS’s media arm. Without the FBI deploying some form of surveillance technique, or Al-Azhari using another method to visit the site which exposed their IP address, this should not have been possible.
There are lots of ways to de-anonymize Tor users. Someone at the NSA gave a presentation on this ten years ago. (I wrote about it for the Guardian in 2013, an essay that reads so dated in light of what we’ve learned since then.) It’s unlikely that the FBI uses the same sorts of broad surveillance techniques that the NSA does, but it’s certainly possible that the NSA did the surveillance and passed the information to the FBI.

Tags: dark web, de-anonymization, FBI, hacking, NSA, privacy, surveillance, Tor 

Posted on January 17, 2023 at 7:02 AM			•
			10 Comments 



Comments



thorvold •

					
						January 17, 2023 8:27 AM					

The filing mentions that it is referencing a purported Top Secret document “Exhibit 2” from the timeframe of 2013.  Based on that info, I am assuming this is a document purportedly from the Edward Snowden leak.  The current policy of the government is that a classified document that is leaked is still classified until officially de-classified at a later date. Public access != Unclassified.  The government is not going to acknowledge that the document is indeed classified in an open context because that would then confirm that the information contained in the document is likely true.  Potentially the “fact of” information that the lawyer obtained in that document and then references in his motion may also be classified.
This would make the motion a derivatively classified document based on the inclusion of classified information in it.  If the government managed to convince the judge that the information was still classified, then that would show the need required to seal the motion, without actually stating in open writing that the document was indeed true.






Will •

					
						January 17, 2023 9:29 AM					

The gist of the article is that the US government could have compromised the website, or the website may have been a honeypot, or they may have ways of unmasking TOR traffic generally.
But isn’t it more likely that they compromised the machine he used to access the dark web instead?






Winter •

					
						January 17, 2023 9:44 AM					

A known way to re-identify an IP address over Tor is when the user enables javascript support. If you do so, it is advised to use the browser in a VM with the IP address shielded. This is especially true when the user does not use the Tor browser, but accesses Tor using SOCKS5 on a regular browser.
If the FBI already had access to the dark web site, it could install Javascript code to get at the IP address.
Another, fairly unlikely way, is to look for searches on public fora in the open for certain websites just before the access.
A real killer would be asking for translating the offending page in open Google Translate just after you accessed it via Tor. Google can be fickle when used over Tor, it generally blocks access from Tor.






Clive Robinson •

					
						January 17, 2023 10:46 AM					

@ Bruce, ALL,
Re : The cost of catching a tiddler.
“There are lots of ways to de-anonymize Tor users.”
Yes there are[1] but non of them are “resource inexpensive”, thus as with angling,
“You only throw the bait where you’ve a good idea there is a fish that will bite.”
Thus I’d be more interested in what the suspect allegedly did to first attract attention to themselves.
After that we know it’s more likely to be “Methods” rather than “sources”.
It’s knowing if the first flagging up event was just technical or involved human agency. If the latter wether it was an error by the suspect or somebody else “provided confidential information”.
1, If technical, We all have a problem.
2, If Suspect error, Some others have a problem.
3, If Confidential source, Similar others have a problem.
The last does not overly concern me on the “If you can’t do the time…” principle I avoid doing that sort of “crime” thing.
The second I suspect is actuall quite probable on the “Johnny can’t encrypt” principle. This unfortunately is a major failing of most encryption systems going all the way to BC times.
If it’s the first then I’m very concerned because that has the implication that there is a fault in the standards, protocols or algorithms, which is likely to effect a great number of other systems not just Tor.
Hopefully we get to find out, and find out fairly soon.
[1] When you think about it at a fundemental level there are two issues,
1.1, All traffic is point to point.
1.2, All traffic is bidirectional.
Together these guarenty that all such connections are tracable, with enough resources to gather the needed information.
Due to Tors fixation with “low latency” this makes “tracking in the time domain” relatively painless and no amount of encryption no matter how clever can hide the time domain information. Nor can encryption hide the data flow domain information, all it can hide and often not well at all is the “traffic content”.






TRX •

					
						January 17, 2023 11:22 AM					

This all assumes it wasn’t something so simple as a piece of malware running that passed all his keystrokes on to an FBI host.






Winter •

					
						January 17, 2023 11:27 AM					

Another discussion of the OP at Hacker NEws:
‘https://news.ycombinator.com/item?id=34412080
Scroll down:

  I wouldn’t get so excited about this. There have been tons of javascript exploits to leak IP addresses in the past, it’s more likely that than the FBI running thousands of servers.







Winter •

					
						January 17, 2023 11:42 AM					

To counter my own arguments, it seems it is not that easy anymore to leak your IP address over Javascript.
See this discussion on Reddit:
‘https://www.reddit.com/r/TOR/comments/om5aiv/what_exactly_is_the_risk_of_running_javascript/
In the grey old days, long ago, it used to be parallel calls of web addresses in plain TCP/IP traffic inside complicated pages. But the Tor browser seems to block all know ways to do that.
Best guess to me, beside that they did not break Tor, but only use that as a cover for what they really did, is that the FBI used a zero-day in the browser of the accused. Or that he logged into a honeypot site and his credentials gave him away.






JonKnowsNothing •

					
						January 17, 2023 12:19 PM					

@All
re: It’s unlikely that the FBI uses the same sorts of broad surveillance techniques that the NSA does, but it’s certainly possible that the NSA did the surveillance and passed the information to the FBI. 
The high probability is that the FISA Courts (1) granted warrants for the overseas surveillance, which is the province of the NSA.
All such photos and documents as contained on that site are watermarked by USA LEAs (and maybe other LEAs).  This is similar to the watermarks on other “illegal materials”.  MITM insertion and interception, allows the LEAs to collect the images and replace any intercepted requests for download or browser pre-fetch (2) with a watermarked version (aka Honey Pot).  There are teams of LEAs that are dedicated to finding and tagging all such illicit images, as in moderation decisions, there are always more to discover, watermark and/or block.
Since the end user is inside the USA, that requires a special FISC Warrant, which is a boomerang warrant.  Normally the NSA stops at USA borders, but with a boomerang warrant they can follow the pipe directly inside the USA.  In theory the NSA doesn’t do this often but in practice it is done a lot. The NSA Collect It All strategy is that the collection is not “viewed by humans” and is sorted by computer, so it remains within their legal boundaries to collect.  If they are going to use any of this already collected material and if the target is inside the USA, they need a boomerang warrant to access and analyze the USA end of the pipe.  They do not need any authorization to analyze the other end of the pipe outside of the USA.
After 9/11, some aspects of this revealed the scope of investigations inside the USA and in foreign countries. The FBI, Dept of Defense (Dod), the CIA and NSA all have presence in the field.  It was a bit of a surprise when it was disclosed that the DoD was doing interrogations of renditioned/kidnapped US Citizens in foreign countries.
afaik All the agencies continue to work outside of the USA.
===
1) Foreign Intelligence Surveillance Act (FISA)
The Act created the Foreign Intelligence Surveillance Court (FISC) to oversee requests for surveillance warrants by federal law enforcement and intelligence agencies.
2) Browsers do a pre-fetech for images. All you need to do is access a page and the browser presumes you are going to look at all the images.  They pre-fetch an image to reduce the display render time as you look at the page.  The browser pings the source image host for their “counters”.  This process is done as if you visited and viewed the item even if you closed the window without ever seeing the displayed image.






AlexT •

					
						January 17, 2023 12:50 PM					

I don’t know much about the specifics of this case but is he “only” charged for visiting this site or was the de-anonymization the basis for probable cause for a search warrant that tuned up further evidence ?
If the former I guess they will have to somehow come clean about how they did it (even in a semi restricted setting): they  can’t just say “we know he did it because we tell you so”.
If the latter parallel construction comes to mind.
Certainly a story to follow.






SpaceLifeForm •

					
						January 17, 2023 1:03 PM					

Re: NSA passed info to FBI
Yes, that would be a top choice given EO 12333.
Related to this I believe:
‘https://slate.com/news-and-politics/2023/01/biden-cybersecurity-inglis-neuberger.html
A new policy will empower U.S. agencies to hack into the networks of criminals and foreign governments, among other changes.






			Subscribe to comments on this entry		


Leave a comment Cancel replyLoginName 
Email 
URL: 
 Remember personal info?


		Fill in the blank: the name of this blog is Schneier on ___________ (required):	



Comments:





Allowed HTML
	<a href=""URL""> • <em> <cite> <i> • <strong> <b> • <sub> <sup> • <ul> <ol> <li> • <blockquote> <pre>
	Markdown Extra syntax via https://michelf.ca/projects/php-markdown/extra/




 

Δ 

← Hacked Cellebrite and MSAB Software Released 
Sidebar photo of Bruce Schneier by Joe MacInnis.



About Bruce SchneierI am a public-interest technologist, working at the intersection of security, technology, and people. I've been writing about security issues on my blog since 2004, and in my monthly newsletter since 1998. I'm a fellow and lecturer at Harvard's Kennedy School, a board member of EFF, and the Chief of Security Architecture at Inrupt, Inc. This personal website expresses the opinions of none of those organizations.
Related Entries

Identifying People Using Cell Phone Location DataHacking the JFK Airport Taxi Dispatch SystemUkraine Intercepting Russian Soldiers' Cell Phone CallsQatar SpywareLarge-Scale Collection of Cell Phone Data at US BordersSignal Phone Numbers Exposed in Twilio Hack

Featured Essays

The Value of EncryptionData Is a Toxic Asset, So Why Not Throw It Out?How the NSA Threatens National SecurityTerrorists May Use Google Earth, But Fear Is No Reason to Ban ItIn Praise of Security TheaterRefuse to be TerrorizedThe Eternal Value of PrivacyTerrorists Don't Do Movie Plots 
More EssaysBlog Archives

Archive by Month100 Latest Comments
Blog Tags3d printers9/11A Hacker's MindAaron Swartzacademicacademic papersaccountabilityACLUactivismAdobeadvanced persistent threatsadwareAESAfghanistanair marshalsair travelairgapsal QaedaalarmsalgorithmsalibisAmazonAndroidanonymityAnonymousantivirusApacheAppleApplied Cryptographyartificial intelligenceMore TagsLatest BookMore Books




 




Blog
Newsletter
Books
Essays
News
Talks
Academic
About Me
 

















"
https://news.ycombinator.com/rss,"It's time to put cancer warning labels on alcohol, experts say",https://www.cbc.ca/news/health/alcohol-cancer-risk-warning-1.6715769,Comments,"HealthIt's time to put cancer warning labels on alcohol, experts sayThe pressure on the government to put cancer warning labels on alcohol containers is growing, as experts say the majority of Canadians don't know the risks that come with consuming even moderate amounts.Social SharingNo amount of alcohol is safe, says new report from Canadian Centre on Substance Use and AddictionIoanna Roumeliotis & Brenda Witmer  · CBC News  · Posted: Jan 17, 2023 6:00 AM EST | Last Updated: 3 hours agoA person walks past shelves of bottles of alcohol on display at an LCBO in Ottawa in this 2020 file photo. The Canadian Centre on Substance Use and Addiction, which released its final report today, points out that no amount of alcohol is safe and that consuming any more than two drinks a week is risky.    (Adrian Wyld/The Canadian Press) commentsThe pressure on the government to put cancer warning labels on alcohol containers is growing, as experts say the majority of Canadians don't know the risks that come with consuming even moderate amounts.The latest catalyst is Canada's new Guidance on Alcohol and Health, which updates the 2011 Low Risk Drinking Guidelines. The Canadian Centre on Substance Use and Addiction (CCSA), which released its final report today, points out that no amount of alcohol is safe and that consuming any more than two drinks a week is risky. It's a drastic shift from previous guidance, which recommended no more than 15 drinks for men and 10 drinks for women per week to reduce long-term health risks. The CCSA says the new advice reflects thousands of studies in the last decade that link even small amounts of alcohol to several types of cancer.Have a question or something to say? Email: ask@cbc.ca or join us live in the comments now.The new recommendations lay out a continuum of risk. Three-to-six drinks a week increases the risk of developing certain cancers, including colorectal and breast cancer, and more than seven drinks a week also increases your risk of heart disease and stroke. The danger goes up with every additional drink.Record number of people died from alcohol and drug use during the pandemic: StatsCanAlcohol should have cancer warning labels, say doctors and researchers pushing to raise awareness of risk""The last time we did the guidelines, it was in 2011,"" said Catherine Paradis, the interim associate director, research, for the CCSA, who co-chaired the scientific expert panel that came up with the new guidance. ""In 10 years there's definitely been significant improvements in our understanding of mortality and morbidity associated with alcohol use. We have a much better understanding of the link between alcohol and cancer.""According to the report, many Canadians are already in risky drinking territory, with 17 per cent of Canadians consuming three-to-six drinks a week, while 40 per cent drink more than six drinks a week.Paradis says the panel spent the last two years combing through nearly 6,000 peer-reviewed studies, including research that now confirms alcohol use as a risk factor for an increasing number of diseases including at least seven types of cancers. But despite the evidence, most Canadians are unaware or overlook the risk, says Paradis, and many still believe there are health benefits to drinking, though she says the most recent studies show that's not true. Catherine Paradis, the interim associate director, research, for the CCSA, who co-chaired the scientific expert panel that came up with the new guidance on alcohol, in pictured in Ottawa in October, 2022. (Ousama Farag/CBC)'People need to be able to count their drinks'Based on its findings, the CCSA is now calling for health warning labels that include the cancer risk on alcohol containers, and labels that inform people of how many standard drinks are in every container.""Standard drink labels are necessary because people need to be able to count their drinks,"" said Paradis.""Labels about the health risk will provide people with that rationale as to why they should follow the guidance.""The CCSA's call for health warning labels, recently echoed by the World Health Organization, is based on research led by Canada. Erin Hobin, a senior scientist with Public Health Ontario, ran one of the only real-world experiments of cancer warning labels on alcohol in Yukon in 2017. The labels were placed on alcohol containers in two government-owned liquor stores for a month.""What we learned from that study was that the cancer warning grabbed consumer attention,"" said Hobin. ""They read the cancer warning very closely. They thought about that message. They talked to their neighbours and their friends about that message, so there was real deep processing of that message."" People not only talked about the warnings, Hobin said — they drank less, too.Erin Hobin, a senior scientist with Public Health Ontario, ran one of the only real-world experiments of cancer warning labels on alcohol in Yukon in 2017.  She's pictured in Ottawa in October, 2022.                               (Ousama Farag/CBC)""Exposing people to cancer warnings on alcohol containers actually is associated with a reduction, a seven per cent reduction in per capita alcohol use compared to sites that were not exposed to the alcohol warning labels,"" Hobin said, adding the study found increased awareness led to more consumer support for other alcohol policies like higher pricing.The scientists behind the Yukon cancer label study say it was cut short because the alcohol industry intervened and the Yukon government couldn't afford a potential legal battle. CBC's, The National reached out to the industry to ask where they stand on cancer warning labels now.Spirits Canada, Wine Growers Canada and Beer Canada responded with statements focusing on drinking responsibly and in moderation. Wine Growers Canada added it doesn't believe health warning labels are ""the best way to effectively educate consumers on the responsible consumption of alcohol."" But Beer Canada says it remains ""open to labelling suggestions that would be demonstrably helpful to consumers… to reduce harmful drinking.""A legal duty to inform consumersBut experts say health warning labels should already be on alcohol containers, because the industry actually has a legal duty to clearly inform consumers of any risks — especially when those risks are not well known.""They are not just critical, they are required under the law,"" said Jacob Shelley, the director of the health ethics, law and policy lab at Western University. Shelley, who has worked on alcohol policy, says the obligation for manufacturers to inform consumers of any risks associated with their products is higher when a product is ingested.Jacob Shelley, the director of the health ethics, law and policy lab at Western University, says the alcohol industry actually has a legal duty to clearly inform consumers of any risks. (Jacob Shelley)""It's very frustrating, because there is a legal responsibility that's not very difficult, and that the courts have actually identified, that when the product is ingested or consumed, that the duty to warn is actually increased because it poses an increased risk.""Shelley says there's a conflict on the alcohol industry's side, which makes billions in profits every year, to provide cancer-warning labels. ""There's money to be made by increasing consumption,"" said Shelley.Shelley says the normalization of alcohol in society may be contributing to a lack of political will to mandate health risk labeling, but he says government action is needed.More than 6 drinks a week leads to higher health risks, new report suggests — especially for womenAlcoholic beverages need labels with calorie counts, Manitoba group says ""We have governments regulating all sorts of products to ensure they're safe, from baby cribs to cars, right? And so the government really ought to be more involved in requiring these types of labels, and can justify that requirement by saying that this is an obligation that manufacturers already have.""CBC's The National reached out to Health Canada, which partly funded the CCSA's new drinking guidance. In an email, the agency said it recognizes ""alcohol use presents a significant public health and safety issue that affects individuals and communities across Canada,"" adding, ""we look forward to receiving the updated guidance developed by the Canadian Centre on Substance Use and Addiction and reviewing any recommendations.""Public support may be growing. According to the Canadian Cancer Society, a survey they led in February 2022 found that eight out of 10 Canadians support adding warning labels or health messaging on alcohol containers.WATCH | Breaking down the latest information on alcohol:Is your drinking risky? Why there’s a big push for warning labels6 hours agoDuration 9:57There’s overwhelming evidence that alcohol causes cancer, and yet most people are unaware of the risks that come with drinking even a small amount. CBC’s Ioanna Roumeliotis breaks down the latest information and the growing push for mandatory warning labels.'I just didn't know'As efforts to educate Canadians about the risk grow, the political pressure is heating up, too.""The thing that stands out most to me is the amount of people who have said, you know, I just didn't know, I didn't know that alcohol was a Class 1 carcinogen,"" said Lisa Marie Barron, a New Democrat MP from Nanaimo, B.C. ""If they don't have that information, how can they best make the decisions that fit them? I had somebody tell me, you know what, I might second guess that second drink.""Barron forwarded a motion in the House of Commons last June calling for a national warning label strategy. Drinking has been glamorized, she says, but her past work in addictions exposed her to how harmful the effects can be. Ottawa, not the industry, should dictate what Canadians know, Barron said. Canadian Cancer Society's 'Dry Feb' campaign urges Canadians to stop drinking, lower their cancer riskWhy some women are pushing back against alcohol and the wine-to-unwind culture""Right now it's left to the industry to decide what Canadians should or shouldn't know on the bottles,"" Barron said, adding her bill could help move the labelling conversation forward. ""I'm trying to turn that around and say it's federal responsibility to ensure that Canadians have this information, and here's one tool for us to be able to get that moving forward.""And the pressure is heating up on another political front too. Senator Patrick Brazeau, a non-affiliated senator from Quebec, introduced a bill in the senate last fall proposing an amendment to the Food and Drug Act to require what he calls ""honest labelling."" Brazeau says his own experience with addiction was a devastating lesson.""I know that alcohol causes a lot of hurt,"" he said, ""and this is just my way of trying to give back.""Senator Patrick Brazeau is pictured in Ottawa in October 2022. He introduced a bill in the senate last fall proposing an amendment to the Food and Drug Act to require what he calls 'honest labelling.'                                (Ousama Farag/CBC)It's up to Ottawa to lead the charge for all Canadians, Brazeau said.""If they're serious about following science, well here's the science. Alcohol causes at least seven types of cancer and now it's up to the federal government to be bold. To take a strong stance and have the moral courage to say exactly that.""Brazeau says no matter how much opposition he faces, he's in it for the long-haul — whether it takes months, years or even a new government.""I know that the industry is very powerful and they have a lot of resources,"" he said.""But you know this is not a fight against the industry, it's a fight against cancer and this is a fight that I'm willing to take on.""CBC's Journalistic Standards and Practices|About CBC NewsCorrections and clarifications|Submit a news tip|Report errorCommentsTo encourage thoughtful and respectful conversations, first and last names will appear with each submission to CBC/Radio-Canada's online communities (except in children and youth-oriented communities). Pseudonyms will no longer be permitted.By submitting a comment, you accept that CBC has the right to reproduce and publish that comment in whole or in part, in any manner CBC chooses. Please note that CBC does not endorse the opinions expressed in comments. Comments on this story are moderated according to our Submission Guidelines. Comments are welcome while open. We reserve the right to close comments at any time.Become a CBC MemberJoin the conversation  Create accountAlready have an account?"
https://news.ycombinator.com/rss,Glitching a Microcontroller to Unlock the Bootloader,https://grazfather.github.io/posts/2019-12-08-glitcher/,Comments,"

Glitching the Olimex LPC-P1343























            Grazfather
        



                now
            

                posts
            

                tags
            

                about
            

                RSS
            


        
Hugo Theme Diary by Rise

Ported from Makito's Journal. 


©
	
	2019 Grazfather
	

    



- CATALOG -



									Background
								



									Setup
								



									Design
								



									Testing
								




									Simulation
								




									Troubles
								



									Running it in real life
								




									Modifying the board
								



									Determining the supply voltage
								



									The moment of truth
								



									Conclusions
								







                keyboard_arrow_up
            



                brightness_4
            

                brightness_7
            






                    now
                

                    posts
                

                    tags
                

                    about
                

                    RSS
                


- CATALOG -



									Background
								



									Setup
								



									Design
								



									Testing
								




									Simulation
								




									Troubles
								



									Running it in real life
								




									Modifying the board
								



									Determining the supply voltage
								



									The moment of truth
								



									Conclusions
								















                menu
            


            Grazfather
        


                brightness_4
            

                brightness_7
            





Grazfather








                    Glitching the Olimex LPC-P1343
                    
                    

                            2019-12-08 12:00
                        
label
re
                                 
                            
                                pwn
                                 
                            
                                electronics
                                 
                            
                                fpga
                                 
                            
                        
                        
                    




Back in the summer I was lucky enough to finagle my boss into letting me take Dmitry Nedospasov (@nedos)’s hardware hacking training. In it I cut my teeth on using an FPGA to interface with target hardware. After implementing a UART we implemented a module that could parse part of Apple’s OneWire, used to negotiate power exchange, among other things, with your iPhone over the lightning cable. Our ‘final project’ was to build a UART-controllable glitcher, using it to try to glitch a development board.
While we got it working, it was with a bit of hand-holding from Dmitry, including setting up the scope, the power supply etc. In an effort to concretize the knowledge, I decided when I got home that I wanted to do it again on my own.
If you haven’t noticed, hobbyist FPGAs have flooded the market. In 2015 Lattice’s iCE40 series FPGA had its bitstream format reverse engineered, spawning an explosion of open source tooling for synthesizing, place and route, and simulation. Altera’s (Intel’s) and Xilinx’s bit stream format have not been reverse engineered, and so you are stuck using their tools if you decide to develop on those boards. I’ve played around with a few boards, and the iCEBreaker is my recent favourite. The people on their Discord are super helpful, the toolchain is excellent, and the board itself is great for the price.
Porting it over wasn’t too much work, the only real difference was that the FPGA we used in training was a Digilent Arty, which has a 100MHz clock, while the iCEBreaker’s has a 12MHz. This requires we change anything that’s counting cycles to account for the 8.33x slower clock, and we lose some granularity in anything we want to count (since each clock cycle has a longer duration). This is also an opportunity to generalize some of the code to not make as many assumptions about the FPGA it’s running on. Because I was rusty, I chose to try to ‘blindly’ re-write some of the modules, instead of using the code Dmitry has on his github.

Background
I can’t really give a better background to this than Dmitry does in his blog post about it. In short, when the target board boots up the bootROM reads the flash, and depending on the value it reads from address 0x2FC, and the state of a few pins, it determines whether the UART goes to a sort of shell, and whether you can use this shell to read out the flash. This is intended so that you can develop your firmware and debug in the bootloader, but then flash a version that sets this value when your firmware is production ready, hopefully preventing the end user from dumping it from the flash. The vulnerability here is that it’s a 4 byte value, and only a specific value (0x12345678) will lock the bootloader in the expected way. That means that if any of the 32 bits read here are read incorrectly, the bootROM will consider the device unlocked. This is opposed to, for example, requiring a specific value to unlock the bootloader, and having the other 4 billion values lock it.
If we can get the CPU to misread the flash at the very moment it happens to be reading that value, then we can have it jump to the bootloader in the unlocked state. It’s as easy as that! (Famous last words)
The idea here is that we will use the FPGA as a tool that goes between my host machine and the target board. We can communicate with it using UART, and certain special bytes are interpreted as commands for the FPGA, while other values are simply passed through to the target board (to talk to its bootloader). The values sent back from the target board are simply passed directly through to the host machine. The FPGA supports configuring the delay between resetting the target board and pulsing a ‘glitch voltage’, and how long that glitch pulse lasts. It also supports sending multiple pulses, and of course can reset the target board and activate the glitch.
We use an FPGA here instead of a microcontroller for a few reasons:

First, because we can configure things at the clock level, we can have very specific timing (1s/12000000 = 83.3ns precision).
Second, for the same reason, we don’t have to worry about jitter: With a raspberry PI we’d worry about the OS scheduling other process and such, contributing to inconsistency between runs. Even with an Arduino or other microcontroller, with no operating system, we’d have to worry about interrupts messing up the timing.
Third, I wanted to get more practice writing Verilog and using an FPGA.

Setup
For the toolchain, I mostly took everything from WTFpga, which is a beginner’s lab that uses the iCEBreaker board. It uses Yosys, nextpnr, and a few tools from project icestorm. These are all open source tools that you can invoke from the command line, you don’t need a GUI (so I don’t need to run Vivado in a VM) and the time to build is much, much faster than the few minutes it takes to get a synthesis to fail with obscure errors in Vivado. This really sped me up because I am not disciplined and instead of inspecting my code for errors ahead of time, I tend to compile, patch, and iterate until it builds.
For debugging I used PulseView (part of sigrok) when debugging real signals, and GTKwave to look at my simulated waveforms.
For hardware, I obviously used the iCEBreaker, as well as the Olimex target board. My bench top supply is a DC50V5A, a cheap but handy configurable buck converter I got on Ali express. While I have a Saleae, I prefer Sigrok, and at the speeds I was running things at, a cheap 24MHz logic analyzer was enough. In the end I had some troubles debugging something using that alone and borrowed an oscilloscope, but in hindsight it wasn’t necessary, just nice to have.
Design
Again, I am just porting over Dmitry’s design, so here’s the block drawing stolen from his blog:

The tl;dr:

The cmd module intercepts everything the host computer sends over UART. Based on the first byte it either interprets it as a command for the FPGA, or passes it through to the target.
The resetter simply holds the reset line down long enough for the target to reset (instead of one cycle).
The delay module starts counting on reset and waits the configured number of cycles before sending its own signal.
The trigger module waits for the delay to finish and then tells the pulse module to send a pulse.
The pulse module is a lot like the delay module, except that it uses a different config, and its output is connected to the power multiplexer.

This is all controlled by a python script that talks to the UART, first configuring the FPGA, then activating a glitch, and then communicating with the target. It determines whether it can read out the flag, and if it can’t, it adjusts the delay and pulse width configs and tries again.
The FPGA has two inputs: The UART from the host, and the UART from the target board. It has 4 outputs: The UART to the host, the UART to the target board, a reset line to reset the target, and a vout that is used to control the analogue multiplexer, to quickly drop the voltage powering the target board.
Testing
While you can certainly test on a real FPGA, it’s very difficult to see what’s going on inside the FPGA. You can blink LEDs, or, if you have the hardware, use seven segments displays to output whatever relevant value. I found this very handy and ended up buying a second just so that I could tell both my pulse width and delay count at a glance.
That said, even with the rather quick tools, flashing and debugging with a logic analyzer is a lot slower than ideal. I suggest setting up a good test bench and run simulations.
Simulation
I used Icarus Verilog to simulate the various modules, and GTKWave to look at the waveforms it generates. The basic idea is that you write extra verilog that simulates the inputs to your top module, and then verify that behaviour of the internal signals are as expected. As opposed to running on real hardware, it’s easy to introspect any internal value any any point in time. You can also write test benches for any individual module, ensure that each part is behaving as expected before combining them together.
Here we see my simulation of the whole glitcher, I send a few configurations, and then some commands that should be passed through to the target.

And here is what happens when the glitch command is sent. We see the reset line go down, then there is a delay based on the delay configuration we previously set, and the vout line goes low for a period of time determined by the pulse width configuration.

While a simulation is good, the real test is when you see it work on real hardware, which I was able to see here with my logic analyzer:

Troubles
The Olimex board actually runs faster than my FPGA, and in training our FPGA was more than 8 times faster. In practice our successful glitch had a very short pulse, in the tens of cycles (at 100MHz). With a 12MHz clock (I thought) I was in trouble. There is significantly less granularity in pulse widths, and we risk the ideal pulse width between somewhere between a n cycles and (n+1) cycles.
I tried to fix this by using a PLL which allows you to generate a clock that is faster than the input clock by some multiple. Again, icestorm to the rescue here, I was able to use icepll to generate most of the code needed to generate a 48MHz clock from a 12MHz input.
I kept most modules on the main clock, but fed my new fast_clk to the pulse module. I adjusted my testbench to generate the faster clock and was able to verify that I could generate shorter pulses (with four times the granularity). I actually found a bug in my pulse module here: Since my pulse module was running on a clock four times faster than the rest of the system, including the module that enables the pulse, with short width values my pulse was ending before the enable signal was unset. This was causing the pulse module to immediately start a second pulse. I fixed this by adding an extra state to the module that made it wait until after the enable signal was unset before returning to the waiting state.
Unfortunately, when I ran this on real hardware, I found that my pulse width seemed to be consistently the same value. I was unable to debug this and decided to see if I could get the glitch to work without the finer granularity (spoiler: I could).
Running it in real life
Once I had my simulation looking good, and I was able to see that the behaviour in the real world looked like what I was expecting, it was time to actually glitch the board.
Modifying the board
To have the CPU misread the lock value we want the voltage to drop at precisely the time when it’s reading the appropriate address from flash. Because there are decoupling capacitors on the board, which would smooth out any abrupt voltage change and make this a lot more difficult, I had to remove those capacitors. I also cut traces between the board’s voltage regulator and the VCC and VCCIO, so that it’s powered exclusively from the output of my analogue multiplexer. This is all well documented in the third part of Dmitry’s blog series.
Determining the supply voltage
Because we don’t live in a frictionless vacuum where infinitely fast changes in voltage are possible, even without decoupling capacitors the CPU won’t see an instantaneous change in voltage when we toggle vout. Because of this, we want to minimize the time the voltage change happens, and so we run the board at the lowest voltage we can find where the board behaves normally.
For me this was 2.30V, which I determined by running a loop where I continuously reset the board and then tried to communicate with the bootloader. I adjusted the voltage on my benchtop supply while this was running until it was just barely high enough to reliably read “Synchronized” after sending the “?” bootloader command. The glitch voltage for my setup was simply 0V, but this was mostly due to me only having a single channel power supply. You might be able to more reliably reproduce the glitch if instead of glitching between X and 0 you glitch between X and Y, but if I got it to work with 0V, you can, too.
The moment of truth
I hooked up the reset line of my FPGA to the reset on the board, hooked up vout to the control pin of the multiplexer, tied Vglitch to ground, and then tied Vcc to the 2.3V I determined as the minimum stable voltage.

A friend from work let me borrow his oscilloscope, so I was able to see in real time the reset signal followed by the vout signal pulsing and the corresponding voltage drop in Vcc.

The yellow line is the reset signal, which is held low for 5uS, and then vout, the cleaner purple signal, is toggled delay cycles later, for pulse width cycles. The cyan signal is the actual voltage seen, which we can see is ‘dirtier’ than the purely logic-level vout signal.
Given enough time you can basically brute force any combination of delay and width, but knowing that the board boots in under 100uS and knowing that with pulses too wide we never get a stable system I played with delays between 60 and 1200 cycles (5uS to 100uS) and pulses between 1 and 25 cycles.
I then ran my script, which brute forced through the range of delay and pulse width values until I was treated to a nice dump of the flash I was not supposed to read! You can see in the video below that the width of the pulse get wider until it reaches its max, at which point the delay is incremented and the width values are all tried again.


And with a certain width and delay (15 and 680 in my case) our script dumps out the flash!

If we look at the lock value at 0x2fc, we see that it’s 0x12345678 as we expect:

We don’t see a glitched out value because this dump is after we’ve glitched the bootloader, with the voltage back at its stable value. At this point in time the bootloader has already (incorrectly) determined that the bootloader is unlocked, and so all subsequent reads succeed. We don’t know exactly what happened during the glitch: The read could have misread the value from flash, the comparator could have mis-behaved, the conditional flag could have not been set, the jump instruction could have been skipped, or countless other weird things. What we do know, however, is that we got what we want: The bootloader running stably in a state where it thinks the flash isn’t locked down.
Of course in real life things didn’t go this smoothly, while testing it out I found a bunch of bugs, including how if my delay was too short it would send a pulse while the reset was still low and my pulse width counter had an off-by-one in counting its cycles, among other things, so don’t worry if you try this and have issues as well: Isolate the issue, make sure your simulation works, compare to real life, and debug.
Conclusions
I found this to be a really fun personal project. I find repetition key to remembering things, and so rewriting modules I had written months ago, relearning Verilog syntax, and hitting the same issues while debugging (hopefully) helped me to remember it for next time.
On the FPGA side, the open source tool chain I used for the FPGA stuff was very easy to use, and being able to quickly build both simulations and the real bitstream helped me iterate a lot more quickly. It was well worth my time to get my Makefile running nicely.
On the glitching side, I found that it really demystified the whole thing for me. If I can whip up something that is apparently precise enough to (sort of) reliably glitch a target board running in the dozens of MHz, I must be doing something right.
I wouldn’t have been able to do this without a lot of help from Dmitry. Even now months after taking his training he was willing to help me with whatever dumb questions I have. Seriously consider taking his training. I had Dmitry look at this post before publishing and he offered a discount code! So if you sign up use the code grazfather for 5% off!
Since I was simultaneously trying to figure out glitching stuff while also re-learning Verilog and trying to do it in a new ecosystem, I needed and received lot of help from the nice people on the 1bitsquared discord, so big thanks to them! If you’re looking for your own FPGA board, I can’t recommend the iCEBreaker enough!
My code is available on github, which includes the verilog to configure the FPGA, as well as the python script used to orchestrate everything. It’s heavily based on nedos’s own version.

Last modified on 2019-12-08




			NextNo newer posts.
                

			PreviousMicrocorruption Hollywood
                







Hugo Theme Diary by Rise

Ported from Makito's Journal. 


©
	
	2019 Grazfather
	






"
https://news.ycombinator.com/rss,Faster virtual machines: Speeding up programming language execution,https://mort.coffee/home/fast-interpreters/,Comments,"



Faster virtual machines: Speeding up programming language execution - Mort's Ramblings










home
	







projects
	







blurbs
	










Faster virtual machines: Speeding up programming language execution





Date: 2023-01-15 
Git: https://gitlab.com/mort96/blog/blob/published/content/00000-home/00015-fast-interpreters.md
In this post, I hope to explore how interpreters are often implemented,
what a ""virtual machine"" means in this context, and how to make them faster.

Note: This post will contain a lot of C source code.
Most of it is fairly simple C which should be easy to follow,
but some familiarity with the C language is suggested.

What is a (virtual) machine?
For our purposes, a ""machine"" is anything which can read some sequence of instructions
(""code"") and act upon them.
A Turing machine
reads instructions from the cells of a tape and changes its state accordingly.
Your CPU is a machine which reads instructions in the form of binary data representing x86
or ARM machine code and modifies its state accordingly.
A LISP machine
reads instructions in the form of LISP code and modifies its state accordingly.
Your computer's CPU is a physical machine, with all the logic required to read and execute
its native machine code implemented as circuitry in hardware.
But we can also implement a ""machine"" to read and execute instructions in software.
A software implementation of a machine is what we call a virtual machine.
QEMU is an example of a project which implements common CPU instruction
sets in software, so we can take native machine code for ARM64 and run it in a
virtual ARM64 machine regardless of what architecture our physical CPU implements.
But we don't have to limit ourselves to virtual machines which emulate real CPU architectures.
In the world of programming languages, a ""virtual machine"" is usually used to mean something which
takes some language-specific code and executes it.
What is bytecode?
Many programming languages are separated into roughly two parts:
the front-end, which parses your textual source code and emits some form of machine-readable code,
and the virtual machine, which executes the instructions in this machine-readable code.
This machine-readable code that's inteneded to be executed by a virtual machine is usually called
""bytecode"".
You're probably familiar with this from Java, where the Java compiler produces .class files
containing Java bytecode, and the Java Virtual Machine (JVM) executes these .class files.
(You may be more familiar with .jar files, which are essentially zip files with a bunch of
.class files.)
Python is also an example of a programming language with these two parts.
The only difference between Python's approach and Java's approach is that the Python compiler
and the Python virtual machine are part of the same executable, and you're not meant to distribute
the Python bytecode. But Python also generates bytecode files; the __pycache__ directories and
.pyc files Python generates contains Python bytecode. This lets Python avoid compiling your
source code to bytecode every time you run a Python script, speeding up startup times.
So how does this ""bytecode"" look like? Well, it usually has a concept of an ""operation""
(represented by some numeric ""op-code"") and ""operands"" (some fixed numeric argument which somehow
modifies the behavior of the instruction).
But other than that, it varies wildly between languages.

Note: Sometimes ""bytecode"" is used interchangeably with any form of code intended to be
executed by a virtual machine.
Other times, it's used to mean specifically code where an instruction is always encoded
using exactly one byte for an ""op-code"".

Our own bytecode
In this post, we will invent our own bytecode with these characteristics:

Each operation is a 1-byte ""op-code"", sometimes followed by a 4-byte operand that's interpreted
as a 32-bit signed integer (little endian).
The machine has a stack, where each value on the stack is a 32-bit signed integer.
In the machine's model of the stack, stackptr[0] represents the value at the top of the stack,
stackptr[1] the one before that, etc.

This is the set of instructions our bytecode language will have:
00000000: CONSTANT c:
Push 'c' onto the stack.
> push(c);

00000001: ADD:
Pop two values from the stack, push their
sum onto the stack.
> b = pop();
> a = pop();
> push(a + b);

00000010: PRINT:
Pop a value from the stack and print it.
> print(pop());

00000011: INPUT:
Read a value from some external input,
and push it onto the stack.
> push(input())

00000100: DISCARD:
Pop a value from the stack and discard it.
> pop();

00000101: GET offset:
Find the value at the 'offset' from the
top of the stack and push it onto the stack.
> val = stackptr[offset];
> push(val);

0000110: SET offset:
Pop a value from the stack, replace the value
at the 'offset' with the popped value.
> val = pop();
> stackptr[offset] = val;

00000110: CMP:
Compare two values on the stack, push -1 if
the first is smaller than the second, 1 if the
first is bigger than the second, and 0 otherwise.
> b = pop();
> a = pop();
> if (a > b) push(1);
> else if (a < b) push(-1);
> else push(0);

00000111: JGT offset:
Pop the stack, jump relative to the given 'offset'
if the popped value is positive.
> val = pop();
> if (val > 0) instrptr += offset;

00001000: HALT:
Stop execution


I'm sure you can imagine expanding this instruction set with more instructions.
Maybe a SUB instruction, maybe more jump instructions, maybe more I/O.
If you want, you can play along with this post and expand my code
to implement your own custom instructions!

Throughout this blog post, I will be using an example program which multiplies two numbers together.
Here's the program in pseudocode:
A = input()
B = input()

Accumulator = 0
do {
	Accumulator = Accumulator + A
	B = B - 1
} while (B > 0)

print(Accumulator)

(This program assumes B is greater than 0 for simplicity.)
Here's that program implemented in our bytecode language:

INPUT // A = input()
INPUT // B = input()

CONSTANT 0 // Accumulator = 0

// Loop body:

// Accumulator + A
GET 0
GET 3
ADD
// Accumulator = <result>
SET 0

// B - 1
GET 1
CONSTANT -1
ADD
// B = <result>
SET 1

// B CMP 0
GET 1
CONSTANT 0
CMP
// Jump to start of loop body if <result> > 0
// We get the value -43 by counting the bytes from
// the first instruction in the loop body.
// Operations are 1 byte, operands are 4 bytes.
JGT -43

// Accumulator
GET 0
// print(<result>)
PRINT

HALT


Note: If you're viewing this in a browser with JavaScript enabled,
the above code should be interactive!
Press the Step or Run buttons to execute it.
The bar on the right represents the stack.
The yellow box indicates the current stack pointer,
a blinking green box means a value is being read,
a blinking red box means a value is being written.
The blue rectangle in the code area shows the instruction pointer.
You can also edit the code; try your hand at writing your own program!
Here's a link which takes you directly to the interactive virtual machine.

You should take some moments to convince yourself that the bytecode truly reflects the pseudocode.
Maybe you can even imagine how you could write a compiler which takes a syntax tree reflecting
the source code and produces bytecode?
(Hint: Every expression and sub-expression leaves exactly one thing on the stack.)
Implementing a bytecode interpreter
A bytecode interpreter can be basically just a loop with a switch statement.
Here's my shot at implementing one in C for the bytecode language we invented:
#include <stdio.h>
#include <stdint.h>
#include <stdlib.h>

enum op {
	OP_CONSTANT, OP_ADD, OP_PRINT, OP_INPUT, OP_DISCARD,
	OP_GET, OP_SET, OP_CMP, OP_JGT, OP_HALT,
};

void interpret(unsigned char *bytecode, int32_t *input) {
	// Create a ""stack"" of 128 integers,
	// and a ""stack pointer"" which always points to the first free stack slot.
	// That means the value at the top of the stack is always 'stackptr[-1]'.
	int32_t stack[128];
	int32_t *stackptr = stack;

	// Create an instruction pointer which keeps track of where in the bytecode we are.
	unsigned char *instrptr = bytecode;

	// Some utility macros, to pop a value from the stack, push a value to the stack,
	// peek into the stack at an offset, and interpret the next 4 bytes as a 32-bit
	// signed integer to read an instruction's operand.
	#define POP() (*(--stackptr))
	#define PUSH(val) (*(stackptr++) = (val))
	#define STACK(offset) (*(stackptr - 1 - offset))
	#define OPERAND() ( \
		((int32_t)instrptr[1] << 0) | \
		((int32_t)instrptr[2] << 8) | \
		((int32_t)instrptr[3] << 16) | \
		((int32_t)instrptr[4] << 24))

	int32_t a, b;

	// This is where we just run one instruction at a time, using a switch statement
	// to figure out what to do in response to each op-code.
	while (1) {
		enum op op = (enum op)*instrptr;
		switch (op) {
		case OP_CONSTANT:
			PUSH(OPERAND());
			// We move past 5 bytes, 1 for the op-code, 4 for the 32-bit operand
			instrptr += 5; break;
		case OP_ADD:
			b = POP();
			a = POP();
			PUSH(a + b);
			// This instruction doesn't have an operand, so we move only 1 byte
			instrptr += 1; break;
		case OP_PRINT:
			a = POP();
			printf(""%i\n"", (int)a);
			instrptr += 1; break;
		case OP_INPUT:
			PUSH(*(input++));
			instrptr += 1; break;
		case OP_DISCARD:
			POP();
			instrptr += 1; break;
		case OP_GET:
			a = STACK(OPERAND());
			PUSH(a);
			instrptr += 5; break;
		case OP_SET:
			a = POP();
			STACK(OPERAND()) = a;
			instrptr += 5; break;
		case OP_CMP:
			b = POP();
			a = POP();
			if (a > b) PUSH(1);
			else if (a < b) PUSH(-1);
			else PUSH(0);
			instrptr += 1; break;
		case OP_JGT:
			a = POP();
			if (a > 0) instrptr += OPERAND();
			else instrptr += 5;
			break;
		case OP_HALT:
			return;
		}
	}
}

That's it. That's a complete virtual machine for our little bytecode language.
Let's give it a spin! Here's a main function which exercises it:
int main(int argc, char **argv) {
	unsigned char program[] = {
		OP_INPUT, OP_INPUT,
		OP_CONSTANT, 0, 0, 0, 0,

		OP_GET, 0, 0, 0, 0,
		OP_GET, 3, 0, 0, 0,
		OP_ADD,
		OP_SET, 0, 0, 0, 0,

		OP_GET, 1, 0, 0, 0,
		OP_CONSTANT, 0xff, 0xff, 0xff, 0xff, // -1 32-bit little-endian (two's complement)
		OP_ADD,
		OP_SET, 1, 0, 0, 0,

		OP_GET, 1, 0, 0, 0,
		OP_CONSTANT, 0, 0, 0, 0,
		OP_CMP,
		OP_JGT, 0xd5, 0xff, 0xff, 0xff, // -43 in 32-bit little-endian (two's complement)

		OP_GET, 0, 0, 0, 0,
		OP_PRINT,

		OP_HALT,
	};
	int32_t input[] = {atoi(argv[1]), atoi(argv[2])};
	interpret(program, input);
}


Note: We use two's complement
to represent negative numbers, because that's what the CPU does.
A 32-bit number can represent the numbers between 0 and 4'294'967'295.
Two's complement is a convention where the numbers between 0 and 2'147'483'647
are treated normally, and the numbers between 2'147'483'648 and 4'294'967'295
represent the numbers between -2'147'483'648 and -1.
Little-endian just means that order of the bytes are ""swapped"" compared to what you'd expect.
For example, to express the number 35799 (10001011'11010111 in binary) as 2 bytes in
little-endian, we put the last 8 bits first and the first 8 bits last:
unsigned char bytes[] = {0b11010111, 0b10001011}.
It's a bit counter-intuitive, but it's how most CPU architectures these days represent numbers
larger than one byte.

When I compile and run the full C program with the inputs 3 and 5, it prints 15. Success!
If I instead ask it to calculate 1 * 100'000'000,
my laptop (Apple M1 Pro, Apple Clang 14.0.0 with -O3) runs the program in 1.4 seconds.
My desktop (AMD R9 5950x, GCC 12.2.0 with -O3) runs the same program in 1.1 seconds.
The loop contains 12 instructions, and there are 6 instructions outside of the loop,
so a complete run executes 100'000'000*12+6=1'200'000'006 instructions.
That means my laptop runs 856 million bytecode instructions per second (""IPS"") on average,
and my desktop runs 1.1 billion instructions per second.



Clang + Apple M1 Pro
GCC + AMD R9 5950x


Time
IPS
Time
IPS



Basic bytecode interpreter
1'401ms856M1'096ms1'095M




Note: The actual benchmarked code defines the program variable in a separate
translation unit from the main function and interpret function,
and link-time optimization is disabled.
This prevents the compiler from optimizing based on the knowledge of the bytecode program.

Not bad, but can we do better?
Managing our own jump table
Looking at Godbolt, the assembly generated for our
loop + switch is roughly like this:
loop:
	jmp jmp_table[*instrptr]

jmp_table:
	.quad case_op_constant
	.quad case_op_add
	.quad case_op_print
	.quad case_op_discard
	.quad case_op_get
	.quad case_op_set
	.quad case_op_cmp
	.quad case_op_jgt
	.quad case_op_halt

case_op_constant:
	; (code...)
	add instrptr, 5
	jmp loop

case_op_add:
	; (code...)
	add instrptr, 1
	jmp loop

; etc


Note: This isn't real x86 or ARM assembly, but it gives an idea of what's going on
without getting into the weeds of assembly syntax.

We can see that the compiler generated a jump table; a table of memory addresses to jump to.
At the beginning of each iteration of the loop, it looks up the target address in the jump table
based on the opcode at the instruction pointer, then jumps to it.
And at the end of executing each switch case, it jumps back to the beginning of the loop.
This is fine, but it's a bit unnecessary to jump to the start of the loop just to immediately
jump again based on the next op-code. We could just replace the jmp loop with
jmp jmp_table[*instrptr] like this:
	jmp jmp_table[*instrptr]

jmp_table:
	.quad case_op_constant
	.quad case_op_add
	.quad case_op_print
	.quad case_op_discard
	.quad case_op_get
	.quad case_op_set
	.quad case_op_cmp
	.quad case_op_jgt
	.quad case_op_halt

case_op_constant:
	; code
	add instrptr, 5
	jmp jmp_table[*instrptr]

case_op_add:
	; code
	add instrptr, 1
	jmp jmp_table[*instrptr]

; etc

This has the advantage of using one less instruction per iteration, but that's negligible;
completely predictable jumps such as our jmp loop are essentially free.
However, there's a much bigger advantage: the CPU can exploit the inherent predictability of
our bytecode instruction stream to improve its branch prediction.
For example, a CMP instruction is usually going to be followed
by the JGE instruction, so the CPU can start
speculatively executing the JGE instruction
before it's even done executing the CMP instruction.
(At least that's what I believe is happeneing; figuring out why something is as fast or slow
as it is, at an instruction-by-instruction level, is incredibly difficult on modern CPUs.)
Sadly, standard C doesn't let us express this style of jump table.
But GNU C does! With
GNU's Labels as Values extension,
we can create our own jump table and indirect goto:
#include <stdio.h>
#include <stdint.h>
#include <stdlib.h>

enum op {
	OP_CONSTANT, OP_ADD, OP_PRINT, OP_INPUT, OP_DISCARD,
	OP_GET, OP_SET, OP_CMP, OP_JGT, OP_HALT,
};

void interpret(unsigned char *bytecode, int32_t *input) {
	int32_t stack[128];
	int32_t *stackptr = stack;
	unsigned char *instrptr = bytecode;

	#define POP() (*(--stackptr))
	#define PUSH(val) (*(stackptr++) = (val))
	#define STACK(offset) (*(stackptr - 1 - offset))
	#define OPERAND() ( \
		((int32_t)instrptr[1] << 0) | \
		((int32_t)instrptr[2] << 8) | \
		((int32_t)instrptr[3] << 16) | \
		((int32_t)instrptr[4] << 24))

	// Note: This jump table must be synchronized with the 'enum op',
	// so that `jmptable[op]` represents the label with the code for the instruction 'op'
	void *jmptable[] = {
		&&case_constant, &&case_add, &&case_print, &&case_input, &&case_discard,
		&&case_get, &&case_set, &&case_cmp, &&case_jgt, &&case_halt,
	};

	int32_t a, b;
	goto *jmptable[*instrptr];

case_constant:
	PUSH(OPERAND());
	instrptr += 5; goto *jmptable[*instrptr];
case_add:
	b = POP();
	a = POP();
	PUSH(a + b);
	instrptr += 1; goto *jmptable[*instrptr];
case_print:
	a = POP();
	printf(""%i\n"", (int)a);
	instrptr += 1; goto *jmptable[*instrptr];
case_input:
	PUSH(*(input++));
	instrptr += 1; goto *jmptable[*instrptr];
case_discard:
	POP();
	instrptr += 1; goto *jmptable[*instrptr];
case_get:
	a = STACK(OPERAND());
	PUSH(a);
	instrptr += 5; goto *jmptable[*instrptr];
case_set:
	a = POP();
	STACK(OPERAND()) = a;
	instrptr += 5; goto *jmptable[*instrptr];
case_cmp:
	b = POP();
	a = POP();
	if (a > b) PUSH(1);
	else if (a < b) PUSH(-1);
	else PUSH(0);
	instrptr += 1; goto *jmptable[*instrptr];
case_jgt:
	a = POP();
	if (a > 0) instrptr += OPERAND();
	else instrptr += 5;
	goto *jmptable[*instrptr];
case_halt:
	return;
}

With this interpreter loop, my laptop calculates 1 * 100'000'000 in 898ms,
while my desktop does it in 1 second.
It's interesting that Clang + M1 is significantly slower than GCC + AMD with the basic interpreter
but significantly faster for this custom jump table approach.
At least it's a speed-up in both cases.



Clang + Apple M1 Pro
GCC + AMD R9 5950x


Time
IPS
Time
IPS



Basic bytecode interpreter
1'401ms856M1'096ms1'095M


Custom jump table
898ms1'336M1'011ms1'187M



Getting rid of the switch entirely with tail calls
Both of the implementations so far have essentially been of the form, ""Look at the current instruction,
and decide what code to run with some kind of jump table"". But we don't actually need that.
Instead of doing the jump table look-up every time, we could do the look-up once for every instruction
before starting execution.
Instead of an array of op codes, we could have an array of pointers to some machine code.
The easiest and most standard way to do this would be to have each instruction as its own function,
and let that function tail-call the next function. Here's an implementation of that:
#include <stdio.h>
#include <stdint.h>
#include <stdlib.h>

union instr {
	void (*fn)(union instr *instrs, int32_t *stackptr, int32_t *input);
	int32_t operand;
};

#define POP() (*(--stackptr))
#define PUSH(val) (*(stackptr++) = (val))
#define STACK(offset) (*(stackptr - 1 - offset))
#define OPERAND() (instrs[1].operand)

static void op_constant(union instr *instrs, int32_t *stackptr, int32_t *input) {
	PUSH(OPERAND());
	instrs[2].fn(&instrs[2], stackptr, input);
}

static void op_add(union instr *instrs, int32_t *stackptr, int32_t *input) {
	int32_t b = POP();
	int32_t a = POP();
	PUSH(a + b);
	instrs[1].fn(&instrs[1], stackptr, input);
}

static void op_print(union instr *instrs, int32_t *stackptr, int32_t *input) {
	int32_t a = POP();
	printf(""%i\n"", (int)a);
	instrs[1].fn(&instrs[1], stackptr, input);
}

static void op_input(union instr *instrs, int32_t *stackptr, int32_t *input) {
	PUSH(*(input++));
	instrs[1].fn(&instrs[1], stackptr, input);
}

static void op_discard(union instr *instrs, int32_t *stackptr, int32_t *input) {
	POP();
	instrs[1].fn(&instrs[1], stackptr, input);
}

static void op_get(union instr *instrs, int32_t *stackptr, int32_t *input) {
	int32_t a = STACK(OPERAND());
	PUSH(a);
	instrs[2].fn(&instrs[2], stackptr, input);
}

static void op_set(union instr *instrs, int32_t *stackptr, int32_t *input) {
	int32_t a = POP();
	STACK(OPERAND()) = a;
	instrs[2].fn(&instrs[2], stackptr, input);
}

static void op_cmp(union instr *instrs, int32_t *stackptr, int32_t *input) {
	int32_t b = POP();
	int32_t a = POP();
	if (a > b) PUSH(1);
	else if (a < b) PUSH(-1);
	else PUSH(0);
	instrs[1].fn(&instrs[1], stackptr, input);
}

static void op_jgt(union instr *instrs, int32_t *stackptr, int32_t *input) {
	int32_t a = POP();
	if (a > 0) instrs += instrs[1].operand;
	else instrs += 2;
	instrs[0].fn(&instrs[0], stackptr, input);
}

static void op_halt(union instr *instrs, int32_t *stackptr, int32_t *input) {
	return;
}

This time, we can't just feed our interpreter an array of bytes as the bytecode,
since there isn't really ""an interpreter"", there's just a collection of functions.
We can manually create a program containing function pointers like this:
int main(int argc, char **argv) {
	union instr program[] = {
		{.fn = op_input}, {.fn = op_input},

		{.fn = op_constant}, {.operand = 0},

		{.fn = op_get}, {.operand = 0},
		{.fn = op_get}, {.operand = 3},
		{.fn = op_add},
		{.fn = op_set}, {.operand = 0},

		{.fn = op_get}, {.operand = 1},
		{.fn = op_constant}, {.operand = -1},
		{.fn = op_add},
		{.fn = op_set}, {.operand = 1},

		{.fn = op_get}, {.operand = 1},
		{.fn = op_constant}, {.operand = 0},
		{.fn = op_cmp},
		{.fn = op_jgt}, {.operand = -19},

		{.fn = op_get}, {.operand = 0},
		{.fn = op_print},

		{.fn = op_halt},
	};

	int32_t input[] = {atoi(argv[1]), atoi(argv[2])};
	int32_t stack[128];
	program[0].fn(program, stack, input);
}

And that works.
In a real use-case, you would probably want to have some code to automatically generate
such an array of union instr based on bytecode, but we'll ignore that for now.
With this approach, my laptop calculates 1 * 100'000'000 in 841ms,
while my desktop does it in only 553ms.
It's not a huge improvement for the Clang + M1 case, but it's almost twice as fast with GCC + AMD!
And compared to the previous approach, it's written in completely standard ISO C99,
with the caveat that the compiler must perform tail call elimination.
(Most compilers will do this at higher optimization levels, and most compilers
let us specify per-function optimization levels with pragmas, so that's not a big issue in practice.)



Clang + Apple M1 Pro
GCC + AMD R9 5950x


Time
IPS
Time
IPS



Basic bytecode interpreter
1'401ms856M1'096ms1'095M


Custom jump table
898ms1'336M1'011ms1'187M


Tail calls
841ms1'427M553ms2'171M




Note: The timings from the benchmark includes the time it takes to convert the bytecode
into this function pointer array form.

Final step: A compiler
All approaches so far have relied on finding ever faster ways to select which source code snippet
to run next.
As it turns out, the fastest way to do that is to simply put the right source code snippets
after each other!
If we have the following bytecode:
CONSTANT 5
INPUT
ADD
PRINT

We can just generate C source code to do what we want:
PUSH(5);

PUSH(INPUT());

b = POP();
a = POP();
PUSH(a + b);

printf(""%i\n"", (int)POP());

We can then either shell out to GCC/Clang, or link with libclang to compile the generated C code.
This also lets us take advantage of those projects's excellent optimizers.

Note: At this point, we don't have a ""virtual machine"" anymore.

One challenge is how to deal with jumps.
The easiest solution from a code generation perspective is probably to wrap all the code
in a switch statement in a loop:
int32_t index = 0;
while (1) {
	switch (index) {
	case 0:
		PUSH(5);

	case 5:
		PUSH(INPUT());

	case 6:
		a = POP();
		b = POP();
		PUSH(a + b);

	case 7:
		printf(""%i\n"", (int)POP());
	}
}

With this approach, a jump to instruction N becomes index = N; break;.

Note: Remember that in C, switch statement cases fall through to the next case
unless you explicitly jump to the end with a break.
So once the code for instruction 5 is done, we just fall through to instruction 6.

Here's my implementation:
#include <stdio.h>
#include <stdint.h>
#include <stdlib.h>

enum op {
	OP_CONSTANT, OP_ADD, OP_PRINT, OP_INPUT, OP_DISCARD,
	OP_GET, OP_SET, OP_CMP, OP_JGT, OP_HALT,
};

void write_operand(unsigned char *i32le, FILE *out) {
	fprintf(out, ""    operand = %i;\n"",
		(int)i32le[0] | (int)i32le[1] << 8 | (int)i32le[2] << 16 | (int)i32le[3] << 24);
}

void compile(unsigned char *bytecode, size_t size, FILE *out) {
	fputs(
		""#include <stdio.h>\n""
		""#include <stdint.h>\n""
		""#include <stdlib.h>\n""
		""\n""
		""int main(int argc, char **argv) {\n""
		""  int32_t stack[128];\n""
		""  int32_t *stackptr = stack;\n""
		""  char **inputptr = &argv[1];\n""
		""\n""
		""#define POP() (*(--stackptr))\n""
		""#define PUSH(val) (*(stackptr++) = (val))\n""
		""#define STACK(offset) (*(stackptr - 1 - offset))\n""
		""\n""
		""  int32_t a, b, operand;\n""
		""  int32_t index = 0;\n""
		""  while (1) switch (index) {\n"",
		out);

	for (size_t i = 0; i < size;) {
		fprintf(out, ""  case %zi:\n"", i);

		enum op op = (enum op)bytecode[i];
		switch (op) {
		case OP_CONSTANT:
			write_operand(&bytecode[i + 1], out);
			fputs(""    PUSH(operand);\n"", out);
			i += 5; break;

		case OP_ADD:
			fputs(
				""    b = POP();\n""
				""    a = POP();\n""
				""    PUSH(a + b);\n"",
				out);
			i += 1; break;

		case OP_PRINT:
			fputs(
				""    a = POP();\n""
				""    printf(\""%i\\n\"", (int)a);\n"",
				out);
			i += 1; break;

		case OP_INPUT:
			fputs(""    PUSH(atoi(*(inputptr++)));\n"", out);
			i += 1; break;

		case OP_DISCARD:
			fputs(""    POP();\n"", out);
			i += 1; break;

		case OP_GET:
			write_operand(&bytecode[i + 1], out);
			fputs(
				""    a = STACK(operand);\n""
				""    PUSH(a);\n"",
				out);
			i += 5; break;

		case OP_SET:
			write_operand(&bytecode[i + 1], out);
			fputs(
				""    a = POP();\n""
				""    STACK(operand) = a;\n"",
				out);
			i += 5; break;

		case OP_CMP:
			fputs(
				""    b = POP();\n""
				""    a = POP();\n""
				""    if (a > b) PUSH(1);\n""
				""    else if (a < b) PUSH(-1);\n""
				""    else PUSH(0);\n"",
				out);
			i += 1; break;

		case OP_JGT:
			write_operand(&bytecode[i + 1], out);
			fprintf(out,
				""    a = POP();\n""
				""    if (a > 0) { index = %zi + operand; break; }\n"",
				i);
			i += 5; break;

		case OP_HALT:
			fputs(""    return 0;\n"", out);
			i += 1; break;
		}
	}

	fputs(
		""  }\n""
		""\n""
		""  abort(); // If we get here, there's a missing HALT\n""
		""}"",
		out);
}

If we run our compiler on the bytecode for our multiplication program, it outputs this C code:
#include <stdio.h>
#include <stdint.h>
#include <stdlib.h>

int main(int argc, char **argv) {
  int32_t stack[128];
  int32_t *stackptr = stack;
  char **inputptr = &argv[1];

  #define POP() (*(--stackptr))
  #define PUSH(val) (*(stackptr++) = (val))
  #define STACK(offset) (*(stackptr - 1 - offset))

  int32_t a, b, operand;
  int32_t index = 0;
  while (1) switch (index) {
  case 0:
    PUSH(atoi(*(inputptr++)));
  case 1:
    PUSH(atoi(*(inputptr++)));
  case 2:
    operand = 0;
    PUSH(operand);
  case 7:
    operand = 0;
    a = STACK(operand);
    PUSH(a);

  /* ... */

  case 49:
    b = POP();
    a = POP();
    if (a > b) PUSH(1);
    else if (a < b) PUSH(-1);
    else PUSH(0);
  case 50:
    operand = -43;
    a = POP();
    if (a > 0) { index = 50 + operand; break; }
  case 55:
    operand = 0;
    a = STACK(operand);
    PUSH(a);
  case 60:
    a = POP();
    printf(""%i\n"", (int)a);
  case 61:
    return 0;
  }

  abort(); // If we get here, there's a missing HALT
}

If we compile the generated C code with -O3, my laptop runs the 1 * 100'000'000 calculation in 204ms!
That's over 4 times faster than the fastest interpreter we've had so far.
That also means we're executing our 1'200'000'006 bytecode instructions at 5'882 million
instructions per second! Its CPU only runs at 3'220 million CPU clock cycles per second, meaning
it's spending significantly less than a clock cycle per bytecode instruction on average.
My desktop with GCC is doing even better, executing all the code in 47ms, which means a whopping
25.7 billion instructions per second!
Note that in this particular case, the compiler is able to see that some instructions always
happen after each other, which means it can optimize across bytecode instructions.
For example, the bytecode contains a sequence GET 1; CONSTANT -1; ADD;, which the compiler
is able to prove you won't ever jump into the middle of, so it optimizes out all the implied
stack manipulation code; it's optimized into a single sub instruction which subtracts the
constant 1 from one register and writes the result to another.
This is kind of an important point. The compiler can generate amazing code, if it can figure out
which instructions (i.e switch cases) are potential jump targets.
This is information you probably have access to in the source code,
so it's worth thinking about how you can design your bytecode such that GCC or Clang can figure it out
when looking at your compiler output.
One approach could be to add ""label"" bytecode instructions, and only permit jumping to such a label.
With our bytecode, the only jump instruction we have jumps to a known location, since the jump
offset is an immediate operand to the instruction.
If we added an instruction which reads the jump target from the stack instead,
we might quickly get into situations where GCC/Clang has lost track of which instructions
can be jump targets, and must therefore make sure not to optimize across instruction boundaries.
We can preventing the compiler from optimizing across instruction boundaries
by inserting this code after the case 61: (the code for the HALT instruction):
if (argc > 100) { PUSH(argc); index = argc % 61; break; }

With this modification, every single instruction might be a branch target,
so every instruction must make sense in its own right regardless of which instruction
was executed before or how the stack looks.
This time, the 1 * 100'000'000 calculation happens in 550ms on my laptop with Clang,
which is still not bad. It means we're executing 2'181 million bytecode instructions per second.
My desktop is doing even better, at 168ms.
At this point, I got curious about whether it's the CPU or the compiler making the difference,
so the next table contains all the benchmarks for both compilers on both systems.



Apple M1 Pro
AMD R9 5950x


GCC 12.1.0
Clang 14.0.0
GCC 12.2.0
Clang 15.0.2



Basic bytecode interpreter
1'902ms1'402ms
1'135ms2'347ms


Custom jump table
816ms897ms
1'023ms912ms


Tail calls
1'068ms843ms
557ms645ms


Compiler (pessimized)
342ms548ms
172ms302ms


Compiler
71ms205ms
52ms161ms



I have no intelligent commentary on those numbers. They're all over the place.
In the basic interpreter case for example, GCC is much faster than Clang on the AMD CPU,
but Clang is much faster than GCC on the Apple CPU.
It's the opposite in the custom jump table case, where GCC is much master than Clang on the Apple CPU,
but Clang is much faster than GCC on the AMD CPU.
The overall pattern we've been looking at holds though, for the most part:
for any given CPU + compiler combination, every implementation I've introduced is faster than
the one before it.
The big exception is the tail call version, where the binary compiled by GCC performs horribly on
the Apple CPU (even though it performs excellently on the AMD CPU!).
If anything though, this mess of numbers indicates the value of knowing about all the different
possible approaches and choosing the right one for the situation.
Which takes us to...
Bringing it all together
We have 4 different implementations of the same bytecode , all with different advantages
and drawbacks.
And even though every instruction does the same thing in every implementation,
we have written 4 separate implementations of every instruction.
That seems unnecessary. After all, we know that ADD, in every implementation,
will do some variant of this:
b = POP();
a = POP();
PUSH(a + b);
GO_TO_NEXT_INSTRUCTION();

What exactly it means to POP or to PUSH or to go to the next instruction
might depend on the implementation,
but the core functionality is the same for all of them.
We can utilize that regularity to specify the instructions only once
in a way that's re-usable across implementations using so-called
X macros.
We create a file instructions.x which contains code to define all our instructions:
X(CONSTANT, 1, {
	PUSH(OPERAND());
	NEXT();
})

X(ADD, 0, {
	b = POP();
	a = POP();
	PUSH(a + b);
	NEXT();
})

// etc...

Let's say we want to create an instructions.h which contains an enum op with all the operation
types and a const char *op_names[] which maps enum values to strings.
We can implement that by doing something like this:
#ifndef INSTRUCTIONS_H
#define INSTRUCTIONS_H

enum op {
#define X(name, has_operand, code...) OP_ ## name,
#include ""instructions.x""
#undef X
};

static const char *op_names[] = {
#define X(name, has_operand, code...) [OP_ ## name] = ""OP_"" #name,
#include ""instructions.x""
#undef X
};

#endif

This code might look a bit confusing at first glance, but it makes sense:
we have generic descriptions of instructions in the instructions.x file,
and then we define a macro called X to extract information from those descriptions.
It's basically a weird preprocessor-based application of the
visitor pattern.
In the above example, we use the instruction definitions twice: once to define the enum op,
and once to define the const char *op_names[].
If we run the code through the preprocessor, we get something rouhly like this:
enum op {
OP_CONSTANT,
OP_ADD,
};

const char *op_names[] = {
[OP_CONSTANT] = ""OP_CONSTANT"",
[OP_ADD] = ""OP_ADD"",
};

Now let's say we want to write a function which executes an instruction.
We could write that function like this:
void execute(enum op op) {
	switch (op) {
#define X(name, has_operand, code...) case OP_ ## name: code break;
#include ""instructions.x""
#undef X
	}
}

Which expands to:
void execute(enum op op) {
	switch (op)
	case OP_CONSTANT:
		{
			PUSH(OPERAND());
			NEXT();
		} break;
	case OP_ADD:
		{
			b = POP();
			a = POP();
			PUSH(a + b);
			NEXT();
		} break;
	}
}


Note: We use a variadic argument for the code block because the C preprocessor has
annoying splitting rules. Code such as X(FOO, 1, {int32_t a, b;}) would call the macro
X with 4 arguments: FOO, 1, {int32_t a, and b;}.
Using a variadic argument ""fixes"" this, because when we expand code in the macro body,
the preprocessor will insert a comma between the arguments.
You can read about more stupid preprocessor hacks here:
https://mort.coffee/home/obscure-c-features/

This is starting to look reasonable, but it doesn't quite work.
We haven't defined those PUSH/OPERAND/NEXT/POP macros, nor the a and b variables.
We need to be a bit more rigorous about what exactly is expected by the instruction,
and what's expected by the environment which the instruction's code is expanded into.
So let's design a sort of ""contract"" between the instruction and the execution environment.
The environment must:

Provide a POP() macro which pops the stack and evaluates to the result.
Provide a PUSH(val) macro which push the value to the stack.
Provide a STACK(offset) macro which evaluates to an
lvalue
for the stack value at offset.
Provide an OPERAND() macro which evaluates to the current instruction's operand as a int32_t.
Provide an INPUT() macro which reads external input and evaluates to the result.
Provide a PRINT(val) macro which outputs the value somehow (such as by printing to stdout).
Provide a GOTO_RELATIVE(offset) macro which jumps to currentInstruction + offset
Provide a NEXT() macro which goes to the next instruction
Provide a HALT() macro which halts execution.
Provide the variables int32_t a and int32_t b as general-purpose variables.
(This turns out to significantly speed up execution in some cases compared to
defining the variables locally within the scope.)

As for the instruction:

It must call X(name, has_operand, code...) with an identifier for name, a 0 or 1 for
has_operand, and a brace-enclosed code block for code....
The code block may only invoke OPERAND() if it has set has_operand to 1.
The code block must only contain standard C code and calls to the macros we defined earlier.
The code block must not try to directly access any other variables which may exist
in the context in which it is expanded.
The code block can assume that the following C headers are included: <stdio.h>, <stdlib.h>,
<stdint.h>.
The code must not change the stack pointer and dereference it in the same expression
(essentially, no PUSH(STACK(1)), since there's no
sequence point between the dereference and
the increment).

With this, we can re-implement our basic bytecode interpreter:
#include ""instructions.h""

#include <stdio.h>
#include <stdlib.h>
#include <stdint.h>

void interpret(unsigned char *bytecode, int32_t *input) {
	int32_t stack[128];
	int32_t *stackptr = stack;
	unsigned char *instrptr = bytecode;

	int instrsize; // Will be initialized later

	#define POP() (*(--stackptr))
	#define PUSH(val) (*(stackptr++) = (val))
	#define STACK(offset) (*(stackptr - 1 - offset))
	#define OPERAND() ( \
		((int32_t)instrptr[1] << 0) | \
		((int32_t)instrptr[2] << 8) | \
		((int32_t)instrptr[3] << 16) | \
		((int32_t)instrptr[4] << 24))
	#define INPUT() (*(input++))
	#define PRINT(val) (printf(""%i\n"", (int)(val)))
	#define GOTO_RELATIVE(offset) (instrptr += (offset))
	#define NEXT() (instrptr += instrsize)
	#define HALT() return

	int32_t a, b;
	while (1) {
		switch ((enum op)*instrptr) {
#define X(name, has_operand, code...) \
		case OP_ ## name: \
			instrsize = has_operand ? 5 : 1; \
			code \
			break;
#include ""instructions.x""
#undef X
		}
	}
}

And that's it! That's our whole generic basic bytecode interpreter, defined using the
instruction definitions in instructions.x.
And any time we add more bytecode instructions to instructions.x,
the instructions are automatically added to the enum op and const char *op_names[] in
instructions.h, and they're automatically supported by this new basic interpreter.
I won't deny that this style of code is a bit harder to follow than straight C code.
However, I've seen VM with their own custom domain-specific languages and code generators
to define instructions, and I find that much harder to follow than this preprocessor-based approach.
Even though the C preprocessor is flawed in many ways, it has the huge advantage that C programmers
already understand how it works for the most part, and they're used to following code which
uses macros and includes.
With decent comments in strategic places, I don't think this sort of ""abuse"" of the C preprocessor
is wholly unreasonable.
Your mileage may differ though, and my threshold for ""too much preprocessor magic""
might be set too high.
For completeness, let's amend instructions.x with all the instructions in the bytecode language
I defined at the start of this post:
X(CONSTANT, 1, {
	PUSH(OPERAND());
	NEXT();
})

X(ADD, 0, {
	b = POP();
	a = POP();
	PUSH(a + b);
	NEXT();
})

X(PRINT, 0, {
	PRINT(POP());
	NEXT();
})

X(INPUT, 0, {
	PUSH(INPUT());
	NEXT();
})

X(DISCARD, 0, {
	(void)POP();
	NEXT();
})

X(GET, 1, {
	a = STACK(OPERAND());
	PUSH(a);
	NEXT();
})

X(SET, 1, {
	a = POP();
	STACK(OPERAND()) = a;
	NEXT();
})

X(CMP, 0, {
	b = POP();
	a = POP();
	if (a > b) PUSH(1);
	else if (a < b) PUSH(-1);
	else PUSH(0);
	NEXT();
})

X(JGT, 1, {
	a = POP();
	if (a > 0) { GOTO_RELATIVE(OPERAND()); }
	else { NEXT(); }
})

X(HALT, 0, {
	HALT();
})

Implementing the custom jump table variant and the tail-call variant using this X-macro system
is left as an exercise to the reader.
However, just to show that it's possible, here's the compiler variant implemented generically:
#include ""instructions.h""

#include <stdio.h>
#include <stdint.h>
#include <stdlib.h>

void compile(unsigned char *bytecode, size_t size, FILE *out) {
	fputs(
		""#include <stdio.h>\n""
		""#include <stdint.h>\n""
		""#include <stdlib.h>\n""
		""\n""
		""int main(int argc, char **argv) {\n""
		""  int32_t stack[128];\n""
		""  int32_t *stackptr = stack;\n""
		""  char **inputptr = &argv[1];\n""
		""\n""
		""#define POP() (*(--stackptr))\n""
		""#define PUSH(val) (*(stackptr++) = (val))\n""
		""#define STACK(offset) (*(stackptr - 1 - offset))\n""
		""#define OPERAND() operand\n""
		""#define INPUT() (atoi(*(inputptr++)))\n""
		""#define PRINT(val) printf(\""%i\\n\"", (int)(val))\n""
		""#define GOTO_RELATIVE(offset) index += offset; break\n""
		""#define NEXT()\n""
		""#define HALT() return 0\n""
		""\n""
		""  int32_t a, b, operand;\n""
		""  int32_t index = 0;\n""
		""  while (1) switch (index) {\n"",
		out);

	for (size_t i = 0; i < size;) {
		fprintf(out, ""  case %zi:\n"", i);

		enum op op = (enum op)bytecode[i];
		switch (op) {
#define X(name, has_operand, code...) \
		case OP_ ## name: \
			fprintf(out, ""    index = %zi;\n"", i); \
			i += 1; \
			if (has_operand) { \
				fprintf(out, ""    operand = %i;\n"", (int)( \
					((int32_t)bytecode[i + 0] << 0) | ((int32_t)bytecode[i + 1] << 8) | \
					((int32_t)bytecode[i + 2] << 16) | ((int32_t)bytecode[i + 3] << 24))); \
				i += 4; \
			} \
			fputs(""    "" #code ""\n"", out); \
			break;
#include ""instructions.x""
#undef X
		}
	}

	fputs(
		""  }\n""
		""\n""
		""  abort(); // If we get here, there's a missing HALT\n""
		""}"",
		out);
}

A word on real-world performance
I thought I should mention that the techniques described in this post won't magically make any
interpreted language much faster.
The main source of the performance differences we have explored here is due to the overhead
involved in selecting which instruction to execute next; the code which runs between
the instructions.
By reducing this overhead, we're able to make our simple bytecode execute blazing fast.
But that's really only because all our instructions are extremely simple.
In the case of something like Python, each instruction might be much more complex to execute.
The BINARY_ADD operation, for example, pops two values from the stack, adds them together,
and pushes the result onto the stack, much like how our bytecode's ADD operation does.
However, our ADD operation knows that the two popped values are 32-bit signed integers.
In Python, the popped values may be strings, they may be arrays, they may be numbers, they may be
objects with a custom __add__ method, etc.
This means that the time it takes to actually execute instructions in Python will dominate
to the point that speeding up instruction dispatch is likely insignificant.
Optimizing highly dynamic languages like Python kind of requires some form of
tracing JIT
to stamp out specialized functions which make assumptions about what types their arguments are,
which is outside the scope of this post.
But that doesn't mean the speed-up I have shown here is unrealistic.
If you're making a language with static types, you can have dedicated fast instructions
for adding i32s, adding doubles, etc.
And at that point, the optimizations shown in this post will give drastic speed-ups.
Further reading

I watched this video about a year ago ago:
Cheaply writing a fast interpreter - Neil Mitchell.
I can't directly cite anything specific,
but some ideas such as converting the instruction stream to an array of function pointers
comes from that talk.
Here's a discussion on how to do the custom jump table optimization in Zig:
https://github.com/ziglang/zig/issues/8220

That thread links to this paper, which is also relevant:
http://www.cs.toronto.edu/~matz/dissertation/matzDissertation-latex2html/node6.html




So those are my thoughts on speeding up virtual machine execution.
If you want, you may check out my programming languages
Gilia and osyris.
Neither makes use of any of the techniques discussed in this post,
but playing with Gilia's VM is what got me started down this path of exploring different techniques.
If I ever get around to implementing these ideas into Gilia's VM,
I'll add a link to the relevant parts of the source code here.




		Read More
	






"
https://news.ycombinator.com/rss,Muse: Text-to-Image Generation via Masked Generative Transformers,https://muse-model.github.io/,Comments,"

















Muse: Text-To-Image Generation via Masked Generative Transformers











A birthday cake with the word ""Muse"" written on it.





A fireplace where the flames spell ""Muse"".




Muse: Text-To-Image Generation via Masked Generative Transformers

Huiwen Chang*, Han Zhang*, Jarred Barber†, AJ Maschinot†, José Lezama, Lu Jiang,  Ming-Hsuan Yang, Kevin Murphy,  William T. Freeman,  Michael Rubinstein†, Yuanzhen Li†, Dilip Krishnan† 
*Equal contribution. †Core contribution.




Muse is a fast, state-of-the-art text-to-image generation and editing model.




        We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space:  given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens.  Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality, etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06.  The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model:  inpainting, outpainting, and mask-free editing.
        
Research Paper




Text-to-image Generation
Our model generates high-quality images from text prompts fast (1.3s for 512x512 resolution or 0.5s for 256x256 resolution on TPUv4).






An abstract, flowery painting





.







A shallow focus photography of a lamb in a wine glass.













A cake made of macarons in a heart shape.





A cake made of macarons in a heart shape.







A tilt shift macro shot of a tiny Christmas town





A cake made of macarons in a heart shape.







An abstract, flowery painting





.







A shallow focus photography of a lamb in a wine glass.













.





.







A tilt shift macro shot of a tiny Christmas town





A cake made of macarons in a heart shape.





Zero-shot, Mask-free editing
Our model gives us zero-shot, mask-free editing for free by iteratively resampling image tokens conditioned on a text prompt.
    




Original

""A cat wearing a tie.""
""A dog.""
""A pig.""
""A shiba inu.""
""A rabbit.""
""A raccoon.""
""A tiger.""
""An owl.""








Mask-free editing controls multiple objects in an image using only a text prompt.
      


A croissant next to a latte with a flower latte art.


Original





Edited





A bottle of Pinot Grigio next to a glass of white wine and a cork.


Original





Edited





Zero-shot Inpainting/Outpainting
Our model gives us mask-based editing (inpainting/outpainting) for free: mask-based editing is equivalent to generation.




Original (with mask)
""New York in the background""
""Paris in the background""
""San Francisco in the background""








Hot air balloons.


Original (+mask)





Edited





Beautiful fall foliage; the gazebo is on a lake


Original (+mask)





Edited





Model details




Muse training pipeline. We feed low resolution and high resolution images into two independent VQGAN tokenizer networks. These tokens are then masked, and low resolution (""base"") and high resolution (""superres"") transformers are trained to predict masked tokens, conditioned on unmasked tokens and T5 text embeddings.




Contributions

We present a state-of-the-art model for text-to-image generation which achieves excellent FID and CLIP scores, which quantitatively measure image generation quality, diversity and alignment with text prompts.
Our model is significantly faster than other comparable models due to the use of a compressed, discrete latent space and parallel decoding.
Our architecture enables out-of-the-box zero-shot capabilities such as inpainting, outpainting, and mask-free image editing.




Muse improves inference time over other models.

*Stable Diffusion times represent the best reported inference times and were not measured internally. The configuration used to achieve FID of 12.63 uses 5x more diffusion steps than what was used for benchmarking, roughly equivalent to 18.5s sampling time. 
            


Model
 Resolution
 InferenceTime (↓)





Stable Diffusion 1.4*
512x512
3.7s


Parti-3B
256x256
6.4s


Imagen
256x256
9.1s


Imagen
1024x1024
13.3s


Muse-3B
256x256
0.5s


Muse-3B
512x512
1.3s







FID (quality/diversity) and CLIP (text-image alignment) scores measured on zero-shot COCO.

Model
FID (↓)
 CLIP (↑)


Stable Diffusion 1.4
12.63
-


Parti-3B
8.10
-


Imagen
7.27
0.27


Muse-3B
7.88
0.32





Special Thanks
We thank William Chan, Chitwan Saharia, and Mohammad Norouzi for providing us training datasets, various evaluation codes, website templates and generous suggestions. Jay Yagnik, Rahul Sukthankar, Tom Duerig and David Salesin provided enthusiastic support of this project for which we are grateful. We thank Victor Gomes and Erica Moreira for infrastructure support, Jing Yu Koh and Jason Baldridge for dataset, model and evaluation discussions and feedback on the paper, Mike Krainin for model speedup discussions, JD Velasquez for discussions and insights, Sarah Laszlo, Kathy Meier-Hellstern, and Rachel Stigler for assisting us with the publication process, Andrew Bunner, Jordi Pont-Tuset, and Shai Noy for help on internal demos, David Fleet, Saurabh Saxena, Jiahui Yu, and Jason Baldridge for sharing Imagen and Parti speed metrics.






"
https://news.ycombinator.com/rss,Show HN: Cosh – concatenative command-line shell,https://github.com/tomhrr/cosh,Comments,"








tomhrr

/

cosh

Public




 

Notifications



 

Fork
    1




 


          Star
 40
  









        Concatenative command-line shell
      
License





     BSD-3-Clause license
    






40
          stars
 



1
          fork
 



 


          Star

  





 

Notifications












Code







Issues
2






Pull requests
0






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







tomhrr/cosh









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





1
tag







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




tomhrr

[main] Fix test issue (2).




        …
      




        52a05c2
      

Jan 17, 2023





[main] Fix test issue (2).


52a05c2



Git stats







289

                      commits
                    







Files

Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github/workflows


 


 









bin


 


 









doc


 


 









lib


 


 









src


 


 









test-data


 


 









test-misc


 


 









tests


 


 









.gitignore


 


 









Cargo.toml


 


 









LICENCE


 


 









Makefile


 


 









README.md


 


 




    View code
 



















cosh
Why?
Install
Dependencies
Supported systems
Building
Running
Documentation
Licence





README.md




cosh


cosh is a concatenative command-line shell.
Why?
Basic shell operations like ls, ps, stat, and so on are
implemented as functions that return first-class values, as opposed to
relying on executables that return text streams.  This makes working
with the results simpler:


Find files matching a path, and search them for data:

sh:     find . -iname '*test*' -print0 | xargs -0 grep data
cosh: lsr; [test m] grep; [f<; [data m] grep] map



Find the total size of all files in the current directory:

sh:     ls | xargs stat -c %s | awk '{s+=$1} END {print s}' -
cosh: ls; [is-dir; not] grep; [stat; size get] map; sum



A small set of versatile primitives means that less needs to be
remembered when compared with typical shells (see e.g. the various
flags for cut(1)), though some commands may be longer as a result:


Get the second and third columns from each row of a CSV file:

sh:     cut -d, -f2,3 test-data/csv
cosh: test-data/csv f<; [chomp; , split; (1 2) get] map



Sort files by modification time:

sh:     ls -tr
cosh: ls; [[stat; mtime get] 2 apply; <=>] sortp



Arithmetical operators and XML/JSON/CSV encoding/decoding functions
reduce the number of times that it becomes necessary to use a more
full-featured programming language or a third-party executable:


Increment floating-point numbers in file:

sh:     sed 's/$/+10/' nums | bc
cosh: nums f<; [chomp; 10 +] map;



Get the first value from the ""zxcv"" array member of a JSON file:

sh:     jq .zxcv[0] test-data/json2
cosh: test-data/json2 f<; from-json; zxcv get; 0 get



It also integrates with external executable calls, where that is
necessary:

Print certificate data:

bash: for i in `find . -iname '*.pem'`; do openssl x509 -in $i -text -noout; done
cosh: lsr; [pem$ m] grep; [{openssl x509 -in {} -text -noout}] map;



Install
Dependencies

Rust

Supported systems
This has been tested on Linux (Debian 12), but should work on any
Linux/macOS/BSD system where Rust can be built.
Building
make
make test
sudo make install

Apart from the core cosh executable, this will also install a
compiled library of core functions (rt.chc).
Running
user@host:/$ cosh
/$ hello println;
hello

Documentation
Documentation
Licence
See LICENCE.









About

      Concatenative command-line shell
    
Resources





      Readme
 
License





     BSD-3-Clause license
    



Stars





40
    stars

Watchers





2
    watching

Forks





1
    fork







    Releases





1
tags







    Packages 0


        No packages published 





Languages












Rust
97.7%







xBase
2.2%







Other
0.1%











"
https://news.ycombinator.com/rss,Ask HN: Has anyone worked at the US National Labs before?,https://news.ycombinator.com/item?id=34414527,Comments,"

Ask HN: Has anyone worked at the US National Labs before? | Hacker News

Hacker News
new | past | comments | ask | show | jobs | submit 
login




 Ask HN: Has anyone worked at the US National Labs before?
54 points by science4sail 1 hour ago  | hide | past | favorite | 62 comments 

I have spent the last 10 years working for FAANG companies, but nowadays I find their performance-review and promotion obsessed cultures to be really draining. Worse, those negative feelings seem to be leaking into my personal life and slowly alienating friends and family.Therefore, I've been pondering a change of pace. The classic HN answer is of course ""create/join a startup"", but I've also been looking at areas more adjacent to scientific research.One option that has come up is the US Department of Energy's national laboratory network[0]. From what I understand, the pay is 33-50% of FAANG, but they do seem to have interesting projects (e.g. the nuclear fusion facility that was recently in the news).Has anyone here worked at one of them before? What is/was the day-to-day like?[0] https://www.energy.gov/national-laboratories 
 
  
 
indigochill 1 minute ago  
             | next [–] 

I haven't, but outside the DoE (but still in national lab land) my brother worked at APL and I interviewed there (after hearing all the praise he had for it). I loved interviewing with them. Everyone, even when they severely outclassed my own education level (I have a bachelor's in journalism and am a self-taught software engineer, and I was talking to a couple of PhDs with many years of experience), were super personable, humble, and passionate about their area of expertise. They made me feel like I was already part of their team even when I was just interviewing (they even went out of their way to make international interviewing work since I live abroad). If they'd made me an offer, I almost certainly would have taken it. Great people and by far the best interviewing experience I've ever had (though in retrospect, interviewing for a position requiring clearance while living abroad was probably an uphill battle).As others have said, the work was highly self-directed. As for the need for software engineers, it was definitely there according to my brother. The scientists he worked with were capable in their field, but they needed someone capable of translating their models into something that would execute on a computer. I don't know what exactly he worked on, of course, but he's an ML specialist and was pretty interested in CUDA programming around that time, so maybe that's a clue what kind of skills he was applying.Anyway, maybe something to check out similar to the DoE network.
 
reply



  
 
floren 1 hour ago  
             | prev | next [–] 

I worked for Sandia. Pay is pretty good by almost any standards except FAANG. The glory days where every staff member got a real office with a real door are over (shared offices are the norm) but it's still a pretty decent work environment.Things don't move fast, as another commenter said. In my area of work, projects tended to last 1-3 years and you'd be on several projects at any given time. In general, it is ICs rather than managers who run the projects. Your manager might say ""Bob over in 9876 has a neat project that could use somebody like you, send him an email if you're interested"".You have to acknowledge that the core mission of the DOE National Labs is nuclear weapons. You might not ever come in contact with the mission, but it is there. They have strong HPC programs--because HPC as we know it is basically driven by the need to simulate nuclear weapons. Some people have moral objections to this, and that's fine!I thought it was a good place to work, all in all.Edit: I'd like to stress that probably the biggest advantage of the labs is the opportunity for self-directed work. If you can convince somebody (external sponsors, internal R&D funding committees) to give you money, you can work on just about anything. If you can't get funding of your own, you are still more or less able to choose what you work on.Your work environment will depend highly on which group you're in. Some groups look like a university department without the students: you work in the unclassified area, you publish papers, you can even open-source software (with some effort). Other people spend their whole day in a windowless SCIF working on very sensitive stuff which they can never, ever discuss outside of a SCIF -- but while their public visibility is nil, their impact is arguably greater.
 
reply



  
 
kbarros 55 minutes ago  
             | parent | next [–] 

I'm a computational physicist at Los Alamos and would echo these sentiments.Note that there are two main types of DOE labs: NNSA (Sandia, Los Alamos, Livermore) and Office of Science (Brookhaven, Berkeley, Oak Ridge, Argonne, ...). Although the former is more focused on ""nuclear weapons stockpile stewardship"", there is still much basic science at all DOE labs, especially where computer science meets physics and other domain sciences.Perhaps relevant to HN, I would mention the Applied Computer Science group at Los Alamos, which is in hiring mode (https://www.lanl.gov/org/ddste/aldsc/computer-computational-...). Besides supporting computational physicists in code development efforts, this group does a variety of researchy things like designing programming model, doing compiler development, building ML models, especially with an eye towards large scale scientific computing. The pay at a DOE lab is less than FAANG (PhD student interns might be around $80k/yr and starting staff scientists maybe $130k/yr), but the tradeoff for some people would be the research-flavor of the work, and the flexibility. Many of the LANL codes being developed are open source, for example. Other DOE labs have similar computer science divisions. For example, Oak Ridge, Argonne, and Berkeley all have ""leadership computing"" facilities.
 
reply



  
 
screwturner68 23 minutes ago  
             | parent | prev | next [–] 

I've worked at a few DOE sites albeit as a consultant not an FTE, Fermi, Los Alamos and a couple others.  You are correct that the work is interesting and a like academic atmosphere, it was the academic atmosphere part that I found off putting.  Where I worked there was very much a hierarchy and if you didn't have a PHD your opinion didn't matter much, you just did what you were told.  Having a couple of decades of experience and being brought in to spin them up on their system and being talked down to on a daily basis like I was a Sophomore in college was really annoying -that said I'm a consultant so I get paid to be annoyed by the people who hire me.  Aside from that I liked working there, the tech was cool, as far as the moral issues I don't ask nor do I want to know about what I'm working on -I don't have a need to know.
 
reply



  
 
MadVikingGod 19 minutes ago  
             | parent | prev | next [–] 

I worked, as a civilian, for part of the R&D arm of the Navy, but it has a very similar feel to what you have posted.  I would 100% agree with everything you have said, except the pay.One thing I will add is getting tools and resources to do the work you want to do can be a challenge.  There can be times when getting through the process to buy a $75 multimeter can be more difficult then the $16,000 signal generator.
 
reply



  
 
rcpt 39 minutes ago  
             | parent | prev | next [–] 

My friends who worked there say that the job was great but living in a remote science outpost made dating unbelievably difficult
 
reply



  
 
screwturner68 17 minutes ago  
             | root | parent | next [–] 

If you are talking about Los Alamos you can commute in from Santa Fe, it wasn't bad, about 45 minuets each way.  In my opinion it's kind of expensive for being in the middle of nowhere, you don't get much bang for your buck house-wise.  Social wise I couldn't imagine living in LA for an extended period of time if I were single.
 
reply



  
 
kbarros 22 minutes ago  
             | root | parent | prev | next [–] 

The town of Los Alamos is beautiful, but it's small. It can be a great place if you already a have family that enjoys outdoors activities. Many people prefer to commute from Santa Fe (45 min drive with mountain views, negligible traffic). Certain groups allow a flexible hybrid home/office working mode.https://www.usnews.com/news/healthiest-communities/articles/...
 
reply



  
 
neltnerb 15 minutes ago  
             | root | parent | next [–] 

I was only a guest scientist at LBL, but LBL is practically in Berkeley.But yeah, generally I assume this is true, though you may just find that you need a car and that if you have one it's not so bad. For instance, NREL (where I was also briefly a guest scientist) is in an incredibly gorgeous area near Golden which isn't super small but everything is still very spread out and it'd take a while to walk anywhere for sure.I guess it also depends on your definition of ""middle of nowhere"" I suppose. Golden is hardly Oakland, but I am pretty sure I could find people to date as long as I included Denver... and if you have a car in Colorado you will find that Denver is considered close to a lot of things you might not at first consider it close to. It's only an hour drive from Boulder (where I lived and drove to NREL as needed).
 
reply



  
 
dqpb 5 minutes ago  
             | parent | prev | next [–] 

Is there a lot of work left in nuclear weapons?
 
reply



  
 
fryz 3 minutes ago  
             | prev | next [–] 

Haven't seen anyone mention a non-DOE lab, so figured I'd weigh in.I interned twice with MIT Lincoln Labs, which among other things, helped build and deploy Radar for WWII which turned into building/managing the technology for Air-Traffic Control, and then turned towards space.They are primarily a DOD-associated research lab (even located on an US Air Force Base), and so most of the projects have some military-oriented mission. Their mission is entrepreneurial-minded (which I found cool), in that they do the ""basic research"" and prototyping to prove viability and then the DOD turns over the project to a contractor to make feasible.While I was there I worked in their GeoIntelligence and Natural Language groups, doing research which I'd ultimately come to understand as being relevant for Project Maven (year 1) and PRISM (year 2). While I'm sure as an intern my contributions weren't directly related to or otherwise leveraged for these programs, in hindsight it was clear that this was the bigger picture that the work was contributing to. Take from this what you will.Most of the anecdotes that I've read through in the comments mirrors my experience. However, one thing I see missing was how opportunity was ""metered"" out. Each group I was in was organized like a research lab and the level of your academic progression limited (or opened) your ability to get access to specific projects/work. Their pay scale was also dictated based on this as well. So if you have a BS, your ability to ""move up"", doesn't exist, but it does if you have PhD.Ultimately, I was given an offer to work there, but ended up taking a SWE position in the Bay Area because I wasn't interested in continuing my education and felt like my ability to have a career progression at MITLL would have necessitated that.
 
reply



  
 
dlivingston 1 hour ago  
             | prev | next [–] 

I worked at LANL [0] for five years and enjoyed the hell out of it. I was a Research Technologist, which is basically an R&D Software Engineer.You will find a research group within a division to work for. For example, mine was the Computational Earth Science group (since renamed) within the Earth & Environmental Sciences division.You will be working with a handful (3+) of research scientists as their supporting engineer. On some projects, you may be doing machine learning work in Julia. On others, you may be coding a fluid dynamics simulation in FORTRAN or C++. On others, you 
might be doing data analytics in Python. It's highly, highly variable, depending strongly on the PIs you're working with, and can change as frequently or infrequently as you wish (within reason).Ultimately, I did the reverse: went from a DOE lab into a FAANG company. My reasons are particular to me, but if you're at all interested in a slower paced, more varied and collaborative environment, you can't do much better than working for the labs.For context, at LANL, I was making ~$100,000 / yr with 3 YOE (circa 2019). This is in northern New Mexico, with such a low cost of living that this amount of money goes about as far as $150k+ up in the Boston area (where I am now).[0]: https://www.lanl.gov/
 
reply



  
 
floren 1 hour ago  
             | parent | next [–] 

LANL is also in an absurdly beautiful place, and Los Alamos is a very pleasant little town.
 
reply



  
 
ianai 37 minutes ago  
             | parent | prev | next [–] 

It’s much less affordable than it used to be. Lots of transplants seem to be driving prices up.
 
reply



  
 
ouid 1 hour ago  
             | parent | prev | next [–] 

>low cost of livingExcept for the nightly drive to Santa Fe.
 
reply



  
 
nukenuke 8 minutes ago  
             | prev | next [–] 

I worked at LBNL as a research scientist and spun a startup out of there. It was a great place to work in some respects, lots of really smart people, awesome brainstorming, seeing Nobel prize winners around. But if you like getting things done quickly it’s quite a challenge due the the bureaucracy (ex ordering simple things could take an extra couple weeks going through lab purchasing). I once described it to a friend who worked at a FAANG company and they said “Oh so it’s like working at a big company but without the advantages of a big company”.Working at our startup almost feels like working at the lab (ie we have scientists and are doing hard tech), but we can also move fast and don’t have the bureaucracy. So maybe consider working at a hard tech startup with a heavy science base!
 
reply



  
 
q845712 36 minutes ago  
             | prev | next [–] 

I interned at Sandia around '04-'05. That was a little bit before the current ""FAANG"" thing we have now (IIRC amazon was turning itself from a bookstore into an e-commerce startup, and google was still not being evil) but certainly it was already less glamorous than getting a Microsoft or even Apple internship.My recollection is that the whole experience depended on which group you were in, and mine was fortunately very chill. Smart, friendly people who arrived and left more or less at the same time every day. Lots of matrixing and loaning of people from different orgs -- I had the feeling that if I were making a career there I would wind up slowly drifting around between projects.The biggest surprise to naive me-in-my-early-20s was that ""Department of Energy"" is a euphemism for ""Department of Nukes."" Nuclear stockpile stewardship was a large portion of the activity there, and so a lot of your colleagues will be people who are at least vaguely comfortable with that.There was a ton of ""basic research"" too -- some high-energy group had a daily experiment that would deliver a ""whomp"" of a shockwave around 3:15pm most afternoons, there was a room temp. fusion group, lots of interest in assisted driving cars and unmanned aerial vehicles...  you just had to appreciate that all the first applications of all this tech was going to be military.Also the security clearances.... the joke was that the ""L"" clearance stood for ""Lavatory pass"" because in our building until you got one, you needed a line-of-sight escort at all times, even in the bathroom. Even for the ""L"" the process was quite onerous, and I understood that the 'Q' clearance held by nearly all full-time staff was even more burdensome. I heard stories of people waiting for their clearance getting stuck in rooms with nothing to work on. One person in my group basically got sent offsite to some ""think tank"" or something for several months while he waited for his clearance - I only met him once the whole summer, at a conference.
 
reply



  
 
madcaptenor 21 minutes ago  
             | parent | next [–] 

The biggest surprise to naive me-in-my-early-20s was that ""Department of Energy"" is a euphemism for ""Department of Nukes.""It also surprised Rick Perry when he got to be Secretary of Energy (he thought it had to do with oil)
 
reply



  
 
throwawy_gfdh 6 minutes ago  
             | prev | next [–] 

Throwaway since I'm going to say some negative things.Overall I agree with almost all the positive things people say in other comments - the national labs have a lot of very smart and kind people, there are interesting things (or at least interesting ideas), and if you're at home at a university, you'll find a lot of kindred spirits.But I made the opposite move you're considering and quit my national lab job and moved to FAANG. Why? Because I wasn't a scientific superstar with a clear vision, and IMO my field (applied math, computational science, etc.) seemed to asking (by which I mean funding) people to do software engineering without many engineers and at the same time still be academics: scientists and mathematicians who spend a lot of time writing grants and trying to publish papers, etc. This made me feel like a liar, writing the grants, and a hack, writing the code. Not to mention that like all academic-type and ""interesting"" jobs, you are supposed to be happy with the idea that you're going to expend a lot of your free time and energy, and perhaps not be paid quite as much as you would if you weren't pursuing your (supposed) passion.Industry is for sure a whole other pile of bullshit, but don't assume that the performance review and promotion stuff is any more draining than the things you will have to do to get funding (you'll likely either be spending a lot of time writing proposals, or you'll be funded by weapons money). Don't make trying to get away from money and politics your reason for moving, though there are plenty of good reasons. Good luck!
 
reply



  
 
tsbischof 1 hour ago  
             | prev | next [–] 

I worked at LBNL, at the Molecular Foundry. The day to day was a mix of typical nanoscience work (chemical synthesis, electron microscopy, etc) and work in support of the user facility. In my case that involved consulting on projects involving our users (design of high-throughput screens, teaching spectroscopy, etc), setting up and maintaining instrumentation, and developing workflows for our chemical synthesis robots.I liked the work and really enjoyed getting to be a consultant on many projects. Turnover is massive among the researchers because there are few permanent positions, and most groups are heavy on postdocs since graduate students tended to be primarily on campus (UC Berkeley).If pay is a concern, look closely for the open databases of salaries. At LBNL there is the ""book of tears"" at the library under the cafeteria, listing every employee and their salary. The exact amount you get varies wildly with the department: prior to unionization in 2016, the range was from 20k to 125k annual salary for postdocs. I hear they raised the floor to NIH levels at least, but I assume they did not make NERSC take a paycut.
 
reply



  
 
seanlane 32 minutes ago  
             | prev | next [–] 

It's been a minute since I was an intern there, but haven't seen this one mentioned yet. I spent a summer at Pacific Northwest National Laboratory (PNNL, https://pnnl.gov) and really enjoyed my time. It seemed like compensation was pretty good, especially considering the cost of living in the area. There were a number of times when we needed some help from a subject matter expert, and we could go down the hall or to another building and speak with someone who recently published on the topic. There was a lot of interesting work going on, from national security issues to storing nuclear waste, etc.The campus there was also different than many other national lab campuses in that it's an open campus and doesn't have the military entrances that others have. It felt like the culture was much more laid back than the FAANG and other corporate cultures that OP mentioned, but perhaps more bureaucratic as well. Again, I was an intern, so didn't have much visibility into that aspect. Overall, definitely a positive experience and I could see myself there if things lined up right.
 
reply



  
 
chemeril 28 minutes ago  
             | prev | next [–] 

Did some time at LANL as an R&D engineer in the non-global security skunky areas, though wound up leaving for reasons not pertaining to the work. Participated in several projects involving Sandia and LLNL.Pros:
 - Pay was excellent, especially for the area
 - Incredibly beautiful country
 - Very interesting work
 - Infinite well of taxpayer dollars for equipment and materials
 - The best job security one can find
 - Crippling bureaucracy enforced a remarkably safe work environmentCons:
 - Crippling bureaucracy made it difficult to move quickly and hit tight deadlines
 - Internal politics (intra-lab and inter-lab) often adversely affected decision making and program success
 - Living in a company town
 - An inability to remove demonstrably problematic employees
 - A Q clearance limits certain extracurricular activitiesPersonal experiences with LANL were all over the place and highly, highly dependent on which group one works with. I was very lucky to get in with a group of wonderful people and immediate management that firewalled most adverse developments from higher up the food chain. This is not a common experience but organizational mobility is relatively free, so you can move to work and groups that are attractive.Worth noting for those coming from private industry: the national labs are institutions first and foremost, not businesses. Organizationally and operationally they exist in a very different mindset and within very different value systems than FAANG-like orgs. The adjustment can be a bit jarring.My work at LANL will likely be the most interesting and most fulfilling work I'll have done: every day was an adventure into the unknown. The work/life balance was also excellent. If you're a naturally curious person and have an inclination for basic science I'd recommend taking a look at the labs. If you have specific questions feel free to drop them here!
 
reply



  
 
uberman 1 hour ago  
             | prev | next [–] 

Just a heads up that most of the interesting work will require a ""current DOE security clearance"".  Many positions at places like LLL or any DOE lab really are going to require the more intense Q clearance.Sometimes prior clearance can be negotiated and a well qualified candidate who is likely to clear might be accepted and placed in a holding pen until they clear but I'm not sure what the backlog is now or even if they do that anymore. At the very least you will almost certainly need to be a US citizen proper.
 
reply



  
 
floren 1 hour ago  
             | parent | next [–] 

When I worked at Sandia, most of us were hired without any clearance but with the expectation that we would get one soon.It is not difficult to get a Q clearance, just annoying. You have to fill out a massive document listing everywhere you've lived in the last 10 years (a real hassle for a recent grad) and give all sorts of info about people you know. They will drive out and interview people.There was plenty of work that did not require a clearance, but so much of the sites are cleared-only that it just makes your life easier to have it.edit: oh yeah good point made in the dead comment below, if you've smoked weed in the last 7 (? something like that) years you're gonna have to tell them. Even if it was legal in the state where you did it. I've heard it's not a deal-killer these days, but they want to know and you will have to stop using it. There are not a ton of ways to lose your job at a National Lab but failing a drug test is one. Do not toy around with it.
 
reply



  
 
oppanoppen 54 minutes ago  
             | root | parent | next [–] 

> It is not difficult to get a Q clearance, just annoying.Will be if: you do drugs (including pot), have debt problems, have dual citizenship esp. if you have made use of it in some way.> You have to fill out a massive documentSF-86. https://www.opm.gov/forms/pdf_fill/sf86.pdf
 
reply



  
 
justinzollars 48 minutes ago  
             | parent | prev | next [–] 

Is this something you can apply for prior to getting a job?
 
reply



  
 
hakkoru 38 minutes ago  
             | root | parent | next [–] 

No, a company or organization must sponsor your clearance. You cannot start the process on your own.
 
reply



  
 
wpasc 1 hour ago  
             | prev | next [–] 

I'm not sure my response qualifies, but I see no other responses yet so:I interned in high school at Brookhaven National Lab working on a team that analyzed STAR (Solenoidal Tracker at RHIC) data from RHIC (Relativistic Heavy Ion Collider). I didn't contribute all that much as a high school intern but the program director said at the end that he liked the high school program because he wanted to help funnel and bring people back to help build up the labs.My experience was that everyone there was extremely smart, but all post-doc and top scientists in their field (the team I worked on was looking for Anti-Alpha particles from gold-gold particle collisions that also helped create Quark Gluon Plasma). So I'm not sure their relative need for regular software engineers.In terms of bureaucracy, you're still working for the government. The scientists all complained about the layers of government bureaucracy but were mostly okay with it. High-tier science moves at a pretty slow pace; coming from a tech background you might not be used to the slow pace around the actual physical construction of some of these devices, let alone the fund-seeking, approvals, testing, runs, and data collection. and 33-50% is a hopeful estimate. Let's say one is a 500k a year senior/staff SWE at FAANG. at a similar level of experience, one's pay would be very lucky to break 150k.So fascinating science, layers of bureaucracy, slow moving stuff, PhD's in their fields, and reduced pay. Again I was only a high school intern, but I spoke with the scientists about their experiences so take my recollection with massive salt. I walked away from the summer fascinated by the work and I had a love of physics at the time; but I also left (this was 2010 IIRC?) watching the world of tech explode at a massive pace and thought that I didn't like physics enough ( I had spent my junior/senior year of high school doing a capstone project on theoretical physics and having taken a lot of physics classes). When I went to college the next year, I tried a few engineering courses, and switched to CS. I'm glad I made the switch.
 
reply



  
 
AlotOfReading 1 hour ago  
             | parent | next [–] 

There's a pretty decent need for software engineers at national labs and they occasionally do some cool stuff. If you've used ZFS on Linux for example, you've used something produced by LLNL. They do some pretty massive software projects and have a huge number of software testers. The ones I've interacted with were pretty exceptional.
 
reply



  
 
mdmglr 7 minutes ago  
             | prev | next [–] 

Speaking from my experience. All comments here are my own.- Exceptional work/life balance that you will not find anywhere else.- I started at 100k fresh out of masters program, at 5 YOE I was 150k. A little more today.- The labs operate like a variety of small businesses. This is because there are many projects and funding sources from a variety of customers.- From what I hear some labs are super relaxed. Like Los Alamos. The working environment is unlike any other.-  It is typical for teams to be in the same building but have no idea about each other. Overlap and rework is common, but is improving.- Performance reviews depends on the lab and whatever review process du jour HR wants. Where I am you are in competition with your peers. Limited bonus money. So you’ll need to go above and beyond your peers to get it.- day to day is: you work on one or more projects that last anywhere from weeks to years and report to that projects principle investigator. The PI will interface with the customer and get funding.- for software development we need to go through strict security processes that dictate what libraries and dev tools we can use. We use self hosted versions of popular tools like Mattermost and Gitlab. No cloud. GovCloud is typically too expensive for most customers unless your working on very well funded projects.- Managers are hands off and mostly there to ensure corporate compliance activities get done. E.g training, timesheet, perf reviews,etc.- High level of autonomy. So your expected to be knowledgeable in your area, able to learn quickly and able to work with and network with others to deliver results quickly to customers. For example, you might be tasked with implementing an algo a staff scientist came up with in C++ for an ARM board. It doesn’t matter if you haven’t done that before. Your expected to learn C++, get a demo out, and maybe you can pull in some colleagues you previously worked with who are experts to help.- There is a political structure in place and reputation is important. While it may not be as intense as FAANG, and underperforming is likely not to get you fired, if you consistently underperform folks will remember and your reputation will be permanently ruined. Which will result in not being picked for more desirable projects. And likely shunned. I’ve seen it happen a few times.The lab was going to be a brief stint on my way to FAANG but will likely turn into my career.
 
reply



  
 
rqtwteye 7 minutes ago  
             | prev | next [–] 

I live in NM and meet quite a few people who work at Sandia Labs. I am sure it's not perfect but they seem pretty content. It's definitely not a high stress place. If anything, it's too slow and relaxed for some people.
 
reply



  
 
xvedejas 1 hour ago  
             | prev | next [–] 

I've known several people who worked at LANL, and they reported you might sit around for six months or more waiting on bureaucratic approval for your project. They all agreed: very slow moving, lots of red tape, otherwise fine.
 
reply



  
 
jvanderbot 22 minutes ago  
             | prev | next [–] 

I spent 7 years at JPL, which is run mostly like a national lab, based on this comment [https://news.ycombinator.com/item?id=34414812]Is it a change of pace? Yes, but not in workload. You can easily work yourself to death if you let yourself. And politics happens everywhere -- usually in the form of missing the good projects. But the promotion frenzy is minimized, there's a sense of greater purpose in all the projects that is impossible to replicate anywhere else, and the technologists run everything. Managers help connect, but they don't typically determine your day to day priorities. In that sense, you can continuously shop around for good projects and teams without any formal change of position.
 
reply



  
 
dekhn 1 hour ago  
             | prev | next [–] 

I worked at LBNL (in Berkeley) and it was great.  It's like academia but with no teaching responsibilities.  Yes, the pay was lower and the expectations were roughly the same as MAAA (Microsoft-Amazon-Apple-Alphabet) but you didn't get fired if you didn't meet expectations.
 
reply



  
 
aksjdglkjlk 1 hour ago  
             | parent | next [–] 

LBL is a special place. It's still managed by UC without the involvement of any other orgs (aside from the DoE). It's basically just another UC campus, but research oriented and federally funded. The other labs can be quite different, managed by LLCs that involve a few universities and some private companies.Things might have improved now that Bechtel is out of the picture, but for many years LBL was hands down the best lab to work for, purely because of the management situation.
 
reply



  
 
dekhn 38 minutes ago  
             | root | parent | next [–] 

I wonder if the same thing can be said about Argonne (run by U of Chicago)?My manager at LBNL worked there because he was kicked out of one of the other labs (run by Batelle or Bechtel or one of those) for refusing to take a drug test.  They said he'd fit in well at LBL- and he did.
 
reply



  
 
mrpf1ster 1 hour ago  
             | prev | next [–] 

I worked at Argonne National Lab for two years as a web developer for a few projects (https://afleet.es.anl.gov/home/, https://energyjustice.egs.anl.gov/).The people I worked with were super smart in their fields, but were pretty bad at writing code / handling data outside of Excel so they usually hired interns to help with code-related stuff. Some of the divisions had full-time software developer teams, but I was the only software developer in mine.The pace was extremely relaxed, deadlines were not tight at all.I worked remotely, but came in to the office a few times a month. The campus is beautiful, as it is right inside a nature preserve. Everyone there is doing scientific work, so it feels like a real scientific think tank atmosphere and I loved it.
 
reply



  
 
hprotagonist 1 hour ago  
             | prev | next [–] 

Your experience will vary strongly depending on which PI you work for.Expect a very different work culture.  If academia is cozy to you, you'll fit in fine.
 
reply



  
 
dopeboy 1 hour ago  
             | prev | next [–] 

I have a friend who works at Sandia. I remember him saying the pay is lower and there is way more work life balance. There is less of a sink or swim attitude around perf there. I got the sense you can come in, do your thing, and be out by 4 or 5 - everyday.
 
reply



  
 
dehrmann 52 minutes ago  
             | parent | next [–] 

My dad works at a national lab. The other side of this is that underperformers stick around longer, and working with them can be frustrating.
 
reply



  
 
JackFr 24 minutes ago  
             | prev | next [–] 

I worked at National Center for Biotechnology Information at the National Library of Medicine at the National Institutes of Health.  It was a nice entry level job, pay was OK, environment was fantastic (low stress, extremely collegial), but without an MD or a Phd (many had both) my path there was relatively limited.One other thing, I was absolutely blown away by the number of outrageously brilliant people I got to work with.  Really some next level people.
 
reply



  
 
altintx 1 hour ago  
             | prev | next [–] 

I've personally done both Sandia (2004-05) and LANL (2006-10). I'd do LANL again, but Sandia was very political with dull work. Interesting ideas, but dull work. So much bikeshedding. So slow moving.I did workflow management systems, environmental controls in labs, and lightning prediction software.
 
reply



  
 
throwaway84592 0 minutes ago  
             | prev | next [–] 

I've worked at Sandia Labs as a software developer my entire career, so I can't compare to FAANG or SV in general. Obviously I like it, or I wouldn't have been here for 20 years.Their job classification system is such that you will want a Masters degree. I joined with a BS in CS, and until I got my MS, I was categorized as a technician and was paid the same as say someone who soldered and assembled electronics - just over half of what someone with masters in CS would get with simular rankings. I've heard it has improved since then, but there is still stong bias towards those with a masters. For many engineering jobs this makes sense, but it is out-of-touch for computer programmers and security researchers.Apart from that while the pay is less than SV, all the labs except LLNL are in parts of the country with much lower cost of living as well, so the pay is pretty darn good for the area IMO. Benifits are good, but not exceptional. Work-life balance is great. I've hand a handfull of month-or-two long crunches in a 20 year career where I had to work 60 hours a week. The rest of the time I work my normal 40 hour schedule and go home. They have standard, 9-80, and 4-10 schedules as options (which nearly all managers will approve). After years of having every other Friday off it would be hard for me to go back to a normal schedule.The actual work varies wildly with the project you are on. Nuclear Weapons work is extremely slow and process heavy as you might imagine, others are more nimble. Projects I've been on have varied from solo development writing software for the engineering next office over, to small agile teams on quarterly releases, to 5-year waterfall development cycles which a huge team. I've done everything from microcontroller software for sensor systems, realtime streaming data processing, desktop data analysis software, web tools for managing data stores, to pure algorithm research. And that is just a small sample of projects going not even touching supercomputer simulation or security work, that I have no experience with. I feel like I have had a great balance of interesting and stimulating technical work and necessary grunt work. Needless to say it is hard give a single ""this is what working at the labs is like"", and individual experiences will vary.There are some differences from industry that are independent of the project, particularly around security. It is not uncommon for software development to be done at the unclassified Official Use Only level, but (production and test) data to be at the classified level, which is done on separate networks (or stand-alone computers). Moving between the two environments can be a time sink. Getting approval to use third-party libraries on classified systems can be a very slow process (weeks at a minimum) depending on the network. If the generic security plans won't work for your project developing a custom one can take the better part of a year. There are many security processes for which I completely respect the purpose, but am flabbergasted at the inefficiency of the execution. Contributing patches back to open-source projects is painful enough that it is rarely done. There is some third-party software that is prohibited (like JetBrains due to connections with Russia), and cloud based tools (on the public internet) are obviously not allowed.  You need to be constantly mindful of what you type/say to maintain OPSEC and avoid leaking classified onto OUO systems, or leaking OUO to  friends and family.They are allowing WFH now, but most managers for most jobs will want you start on-site to help acclimate to the security culture, and to live in town to be able to come on-site to work in the classified environments when needed. You will need to apply for a security clearance once hired, and some projects are better than others at finding meaningful work for you to do while waiting for the security clearance to be granted.As far as ethics go, on one hand you won't be asked to write dark-pattern advertising-driven manipulative spyware. On the other, most work will be related to defense applications directly or indirectly. There are some projects related strictly to energy generation and power-grid security and the like, but they are the exception. The best way to advance your career in the Labs is to move around between departments every several years, so you will be limited in your options to do that if you have reservations about defense work.
 
reply



  
 
ArtWomb 41 minutes ago  
             | prev | next [–] 

Fascinating thread. Love reading the oral histories. I heard some tales of covered up nuclear accidents happening perilously close to Manhattan at Brookhaven National Lab back in the day, and its clear from the fusion results, government science is still the largest research outlay. If you are comfortable with payscale, lack of advance, essentially academic role in natsec context, then its a dream job! I feel like the only people I know anymore who have multi-decade careers for the same entity (DoE/PPPL) are gov scientists ;)
 
reply



  
 
schoen 47 minutes ago  
             | prev | next [–] 

I was a summer intern at LBNL as a student. Beautiful views and very smart people (and very big computers).The cafeteria is way less fancy than tech company cafeterias. :-)As a few posts have pointed out, there are national labs that do only unclassified work (LBNL is one). So you don't have to get a clearance or be prohibited from accessing lots of places or conversations, and you don't have to work on weapons. You do still have to sign a loyalty oath as a state government employee (the lab being managed under contract by the University of California), something that became highly objectionable to me in retrospect.
 
reply



  
 
floren 45 minutes ago  
             | parent | next [–] 

> The cafeteria is way less fancy than tech company cafeterias. :-)The cafeteria at Sandia in Albuquerque always had extremely good posole, though.
 
reply



  
 
evanb 45 minutes ago  
             | prev | next [–] 

I was a postdoc at Lawrence Livermore National Lab (LLNL) from 2013-2016, working on computational particle physics (lattice QCD).  Pay was extremely high (for a postdoc) but not high by comparison to other bay-area employers.  What you get instead is reliable job security, and some sense of civil service (as most of the programs are federally funded).LQCD is kind of funny because it doesn't YET have anything practical to say about nuclear physics, which is what the lab cares about, but it will someday.  So I was pretty insulated from all the weapons+complex integration stuff; my work was 'pure research', which is not that common (though it is more common at the postdoc level, which the lab views as a way of recruiting talent).  But unless you can find your own funding (usually from a DOE grant), you're working on something that advances the lab's programs.  I can't bring myself to work on nuclear weapons, which is why I didn't stay [there's a lot more to LLNL than that, of course, but it's what my field funnels into, broadly speaking].The computational expertise for HPC is really unparalleled, especially at Livermore (and Oak Ridge, which I've only visited).  They're consistently pushing the envelope in terms of high-performance machines which can address scientific questions that require extremely tight coupling between computing resources, rather than a cluster, and they have a lot of experimental architectures and things like that.  LLNL publishes a lot of open-source software; if you've used a cluster in a scientific setting you might be most familiar with SLURM or spack.The day-to-day can be a bit surreal.  At the defense labs people with enormous machine guns thoroughly check your badge on the way in.  On your walk to the cafeteria you might pass a beach volleyball court that's inside the superblock [an extra-high-security area where they've got plutonium etc.], next to a machine gun turret.  Very few employers have teams that regularly win SWAT competitions.The food was fine.  No luxuries like free snacks or anything else I'd seen my tech-company friends enjoy.  No dogs allowed.  LLNL has a lot of employee organizations for sports, charities, exercise, etc.  Transportation around the LLNL site is via sporadic shuttles but more practically there's a bike share, which is just a bunch of bikes you can leave anywhere (on the sidewalk / by a building).
 
reply



  
 
boringg 49 minutes ago  
             | prev | next [–] 

Always had a grass is greener mentality of the Labs but i suspect there are certainly their own set of challenges to overcome.  Also i imagine if you are trying to level up competition is subtle but fierce at the higher levels given that its zero sum for the positions.  I imagine multi year to decade long political campaigns for top positions in the orgs.Staff levels probably pretty good if your motivation isn't to move upwards and focus on whatever your interests are.These are my speculations.
 
reply



  
 
myself248 19 minutes ago  
             | prev | next [–] 

Side question: How are these positions affected by things like government shutdowns and other Washington bullshit?
 
reply



  
 
2OEH8eoCRo0 1 hour ago  
             | prev | next [–] 

FYI- there is also the US Digital Service. My interviewer was a former SWE manager @ Microsoft who wanted to help people in a ""civic duty"" type of manner.https://usds.gov/
 
reply



  
 
jhart99 54 minutes ago  
             | parent | next [–] 

In addition to USDS(which has issues being under the White House), there is 18F and PIF at GSA which can also be good choices.
 
reply



  
 
jlturner 1 hour ago  
             | prev | next [–] 

My dad works in the Molecular Foundry division at LBNL / Lawrence Berkeley Labs (Dept of Energy and UC Berkeley), and loves it. He started there 40 years ago working in electron microscopy and oversaw the transition to digital imaging (you’d be surprised how much code they write). Good work life balance (he comes home for lunch everyday), a pension (rare these days!). His favorite part of the job is the revolving door of very smart people using/visiting the lab and getting to interact with so many ambitious (and not yet jaded) younger grad students from UC Berkeley.
 
reply



  
 
elil17 32 minutes ago  
             | prev | next [–] 

I have heard universally positive things from my friends working at NREL, Oak Ridge, and Argonne.You might also want to check out the US Digital Service, which might be more aligned with traditional SWE skills.
 
reply



  
 
robg 47 minutes ago  
             | prev | next [–] 

There’s a cool accelerator that is partnering with Oakridge and out of TN that might be a good middle ground:https://www.techstars.com/accelerators/industries-of-the-fut...
 
reply



  
 
kincl 4 minutes ago  
             | prev | next [–] 

Great thread! I noticed a bunch of the comments are from Sandia/LLNL/LANL all of which are mostly focused on the National Nuclear Security Administration side of the Department of Energy which is focused on the various aspects of maintaining the nuclear stockpile of the US.I worked at Oak Ridge National Laboratory in High Performance Computing and did not work with anything directly nuclear at all. The HPC efforts of the DOE are under the Office of Science (separate and at the same level as the NNSA) which is focused on more wider scientific impact and application than just nuclear. The Office of Science has a number of program offices that focus on all different kinds of science from basic energy sciences/physics to biological/environmental and scientific computing (where HPC is funded in DOE).I agree that the work/life balance is great and it is definitely a slower pace than what you would find in industry. The lab system is huge and there are plenty of opportunities but on the Office of Science side I like to break it down between what I think of as a research group and a user facility.Working in a research group is much like academia, they mostly require a PhD and from what I could tell performance is judged on publication output. These folks also write grant proposals that come from DOE program offices for funding their own research. In some cases I have seen these groups employ non-PhDs to be computational scientists and write code.The user facilities are long-running projects funded by the DOE at the labs to provide specific capabilities to researchers, sometimes just for DOE scientists but a number of them are open to scientific researchers all over the world. This is where I have the most experience where I worked at ORNL's National Center for Computational Sciences on the Oak Ridge Leadership Computing Facility (OLCF). These projects are generally well funded and have all kinds of interesting challenges to solve.  For example, the OLCF has consistently deployed the number one supercomputers on the Top500 list and it offers those computational resources to anyone through their allocation program INCITE which supports many different computational modeling and simulation experiments. Other examples of user facilities at ORNL are the Spallation Neutron Source and the High Flux Isotope Reactor.One thing I have noticed since moving from ORNL to industry is that the sense of shared purpose does not extend as far in the lab system as it does in company. What I mean is that with the small research group and with a user facility like the OLCF there is shared purpose with the people in those groups but it does not go much beyond that. A lab is generally made up of lots of different research groups and a few facilities but beyond the drive for ""Science!"" there is not a lot of shared purpose or collaboration at a macro level. The analogy I use is that a lab is a bunch of small dinghy boats that are all generally moving in a similar direction but a company is a single ship with a specific purpose driving it forward.Overall I loved my experience at ORNL, I learned so much working with so many smart people and made friends that I will have for life.
 
reply



  
 
scdlbx 1 hour ago  
             | prev | next [–] 

The projects are more interesting and feel like they are actually benefiting humanity, rather than making money for some random company. Though there can be a lot more bureaucracy and friction. Pay is certainly less, though sometimes the benefits can be better.There's also annoyances coming from political things, such as the budget not being done on time so no one gets paid or there are furlough days.
 
reply



  
 
secabeen 1 hour ago  
             | prev | next [–] 

The national labs are pretty good, as is Research Facilitation at most of the R1 research universities. You might find good leads through the Campus Research Computing Consortium.  We're still in the early days of developing these positions, but it is starting to snowgball.https://carcc.org/
 
reply



  
 
whalesalad 1 hour ago  
             | prev | next [–] 

Reminds me that I started re-reading The Cuckoo's Egg over the holiday break and need to keep up the habit.
 
reply



  
 
devoutsalsa 48 minutes ago  
             | prev | next [–] 

What are the qualifications to get a software job at a national lab?
 
reply



  
 
shagie 39 minutes ago  
             | parent | next [–] 

Likely depends on the lab and the project.For example, here's a software developer at Idaho National Lab - https://inl.taleo.net/careersection/inl_external/jobdetail.f... which is bachelors + 5 years professional.Same lab, Cybersecurity Researcher  https://inl.taleo.net/careersection/inl_external/jobdetail.f... which has bachelors + 0-2 years.Over at LLNL Embedded Software Developer  https://www.llnl.gov/join-our-team/careers/find-your-job/all... is ""just need a bachelors""while Software Developer https://www.llnl.gov/join-our-team/careers/find-your-job/all... asks for a masters.
 
reply



  
 
metalforever 27 minutes ago  
             | prev | next [–] 

I had a bad experience. I have top tech companies on my resume and they made the role look cool and I was trying to slow down a little . They instead put me into support . The codebase wasn’t in source control ; it was just a few scripts in any case . They didn’t have a sane deploy. The people that they had me working under had been there for decades but were not good . They talked down to me and did not give me tasks that aligned to my experience. I thought I would ruin my career if I stayed so I left. Much happier with a principal role where I’m at now .
 
reply



  
 
wedn3sday 25 minutes ago  
             | prev [–] 

I worked at LLNL for a little over 6 years, and recently took a job at a ""real"" software company. Other peoples comments here are completely valid so Im not going to rehash them all, but they all seem to be focusing on the positive so here's a few take aways from my time behind the curtain (taken with a grain of salt, there was a reason I left):1. Extremely high bureaucracy, very poor facilities management. Expect a terrible shared office in a temporary building with asbestos. Dont expect to be reimbursed for your travel expenses for like 3 months, and you're very likely going to have to travel for conferences (i.e. fly to DC to make your sponsors happy).2. The old joke around LLNL was, ""hey, you know how many people work here? About half."" Half the people you work with will be the smartest most productive (and friendly!) people you've ever met, the other half are also smart but have realized that their productivity has no bearing on their advancement and so have decided to not give a fuck. Since hiring is such a nightmare, its very hard to get fired. From a program managers perspective, they'd rather keep people around who massively underperform then fire them since getting a replacement could take years. The upside is that if you want to get paid to do nothing, this is an amazing place to work!3. You dont need a clearance to get hired, although it helps, you get to do the incredibly invasive FBI-agents-knocking-on-your-moms-door clearance process after starting, and then again every 5 years for the rest of your time there. The upside is that you get to make really fun Qanon jokes with all your coworkers. Be ready for random drug tests as this is a Federal facility and Cali's pot laws have no affect. Eat a gummy at a party? kiss your career goodbye.4. If you're cool working on nuclear weapons, you're set for life. If you dont want to operate the gas chamber at Auschwitz (how I see nuke people), then your funding will perpetually be in danger, and you will likely spend more time chasing grants than writing code. (Nearly) Everyone you work with will be totally OK working on a tool with the express purpose of killing 100MM people. I had friends/co-workers who couldnt talk to their spouces about what they did during the day. The nuke people are also very enthusiastically pro-america in a creepy way that always set my teeth on edge. All diversity/inclusivity programs here forcibly killed by the GOP, which leads me to:5. Politics affects everything. Government disfunction is annoying enough, but if there's ever a fight over the federal budget or a government lockdown expect it shutdown your work as well. If you work on something politically sensitive (hello climate program!) expect your funding to be on the chopping block. Real shit, when Trump came into office my entire program changed its name to exchange ""climate"" with ""earth system science"" to try to run under the radar.6. (LLNL Specific) Livermore is very expensive (about the same as the rest of the Bay, but still pretty expensive by anyones measure) with none of the things that make the Bay nice. No BART stop and massive traffic means more than 2 hour round trip to Berkeley and back. Your Bay friends are not going to come out to visit, and in terms of travel time Sacramento is closer than SF. Unlike the rest of the Bay, its temperature is not regulated by the ocean so it regularly hits 115 degrees for weeks on end during the summer.All that said, I still (mostly) enjoyed working there. The reason I left is because during covid I moved to SoCal and after the climate program got its funding reduced the HPC/Nuke people who wanted to hire me onto their team wanted me to come back into the office (there are certain terminals for accessing classified material that are fixed in place and cant move).
 
reply







Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
Search:  


"
https://news.ycombinator.com/rss,Ask HN: What interesting people are you following on Mastodon?,https://news.ycombinator.com/item?id=34413641,Comments,"

Ask HN: What interesting people are you following on Mastodon? | Hacker News

Hacker News
new | past | comments | ask | show | jobs | submit 
login




 Ask HN: What interesting people are you following on Mastodon?
111 points by null4bl3 2 hours ago  | hide | past | favorite | 36 comments 

So far I really like the fediverse, but I am really missing some cross server follow suggestions. I was hoping someone here had some good suggestions on who to follow? 
 
  
 
oldmapgallery 42 minutes ago  
             | next [–] 

Mastodon has been the social media experience we've been hoping to someday find. Much more content rich than twitter, and discussions that go somewhere interesting.
There are too many bright people on Mastodon, and it's wonderful... just a few...Rob Carlson    (author of ""Biology is Technology..."")
@rob_carlson@mastodon.worldGeorge Dyson          (great for history of science posts)
@gdyson@sciencemastodon.comRudy Rucker         (mathematician, author, artist) 
@rudytheelder@sfba.socialAlberto Cairo     (data visualization - infographics)
@albertocairo@mastodon.socialJames Gleick   (author of ""Chaos"", ""Genius"", etc.)
@JamesGleick@zirk.usAnne-Sophie Pereira De Sá  (data visualization)
@wonderveilleuze@vis.socialMaggie Koerth   (Senior science writer at @FiveThirtyEight )
@maggiek@sciencemastodon.comVicky Veritas  (Geologist, GISMS Earth science, kind person that gets people connected)
@vickyveritas@c.imthis is barely scratching the surface.
 
reply



  
 
beej71 1 hour ago  
             | prev | next [–] 

I know this is asking about people, but following hashtags are where it's at. And it's a great way to find people, besides.Here's mine, for example, a giveaway cross section of my interests...#linux
#unix
#weather
#bendoregon
#orwx
#railroad
#history
#caving
#caves
#seti
#train
#motorcycle
#bicycle
#openstreetmap
#osm
#amtrak
#railfan
#nasa
#usgs
#hacking
#computerscience
#math
#programmerhumor (some real groaners)
#astrophotography
#astronomy
#rustlang
#space
#science
#programmingAnd then mute people whose stuff you don't want to see.As for people, I follow my friends and:@openstreetmap@en.osm.town@royalastrosoc@astrodon.social@stephenfry@mastodonapp.uk@georgetakei@universeodon.com@archlinux@fedi.lynnesbian.space@nasa@mstdn.social (I think this is unofficial)@sdf@sdf.org
 
reply



  
 
safety1st 40 minutes ago  
             | parent | next [–] 

Tags are the way, the whole ethos of federation is about not being a sheep who needs to participate in some self-important global cult of personality to feel like they matter. Just follow your interests.
 
reply



  
 
josefresco 1 hour ago  
             | prev | next [–] 

Just wanted to share a tool (Chrome extension), that has made following people on Mastodon much easier.Mastodon View Profile:
https://chrome.google.com/webstore/detail/mastodon-profile-r...Basically you click to view someone's profile,  then click this extension's button in your toolbar and it will take you to YOUR instance, where you can follow that person with just one click (no copy+paste+search)
 
reply



  
 
lapcat 1 hour ago  
             | parent | next [–] 

From looking at the source on Github, it appears that this extension attempts to work with simple string rearrangement AFAICT. Unfortunately, this approach doesn't generalize, as I discovered myself while attempting to build a similar extension.It will work in many cases, but it will also fail in other cases. It's not universal.The only reliable method is to use the search field on your instance, or to directly query the Mastodon API.
 
reply



  
 
mg 59 minutes ago  
             | root | parent | next [–] 

This simple code works for me:    let your_instance=""masto.ai"";

    document.location=(
        ""https://""
        + your_instance
        + ""/authorize_interaction?uri=""
        + encodeURIComponent(document.location)
    )

Before you turn it into a bookmarklet, set your_instance to the Mastodon instance you use.You can use my bookmarklet editor to turn it into a bookmarklet:https://www.gibney.org/bookmarklet_editor
 
reply



  
 
josefresco 48 minutes ago  
             | root | parent | next [–] 

Your code works, but there's a slight delay in comparison to the extension. Thanks for sharing!
 
reply



  
 
lapcat 49 minutes ago  
             | root | parent | prev | next [–] 

This seems to work, thanks!
 
reply



  
 
josefresco 1 hour ago  
             | root | parent | prev | next [–] 

Good feedback.  I've used the button now around 60 times.  The times when it ""failed"" were typically related to a temporary outage of the instance, or rate limiting.  For example I used this thread to follow about a dozen new people, and eventually was blocked by ""rate limiting"".
 
reply



  
 
lapcat 1 hour ago  
             | root | parent | next [–] 

The issue I'm talking about isn't about rate limiting, it's about user name and server name mapping.Take one example below: https://fedi.simonwillison.net/@simonThis does not map to https://myserver/@simon@fedi.simonwillison.net but rather https://myserver/@simon@simonwillison.net
 
reply



  
 
weinzierl 1 hour ago  
             | parent | prev | next [–] 

Not a Chrome user, but thanks for creating this. I generally find Mastodon's UX better than its reputation, but following from the web is terrible.
 
reply



  
 
OscarCunningham 23 minutes ago  
             | prev | next [–] 

Here are some of the more notable mathematicians from Mathstodon.John Baez (https://mathstodon.xyz/@johncarlosbaez)Vi Hart (https://mathstodon.xyz/@vihart@mastodon.social)Greg Egan (https://mathstodon.xyz/@gregeganSF)Katie Steckles (https://mathstodon.xyz/@stecks)Marcus du Sautoy (https://mathstodon.xyz/@MarcusduSautoy)Eugenia Cheng (https://mathstodon.xyz/@DrEugeniaCheng)Terry Tao (https://mathstodon.xyz/@tao)Tim Gowers (https://mathstodon.xyz/@wtgowers)Matt Parker (https://mathstodon.xyz/@standupmaths)(Me) (https://mathstodon.xyz/@OscarCunningham)
 
reply



  
 
jakelazaroff 1 hour ago  
             | prev | next [–] 

Anil Dash https://me.dm/@anildashJulia Evans https://social.jvns.ca/@b0rkSimon Willison https://fedi.simonwillison.net/@simonSteve Ruiz https://mas.to/@steveruizokMolly White https://hachyderm.io/@molly0xfffMekka Okereke https://hachyderm.io/@mekkaokereke
 
reply



  
 
proactivesvcs 1 hour ago  
             | prev | next [–] 

The people who post to hashtags that I'm interested in, mostly. Try searching for your interests and you'll get cross-server results that should help you build up a network of people from other servers.
 
reply



  
 
ninethirty 1 hour ago  
             | prev | next [–] 

Kind of reminds me of the early days of the web, you have a feeling there's good websites out there, but there's no established mechanism to find them.
 
reply



  
 
etrautmann 1 hour ago  
             | parent | next [–] 

Yep, which is exactly what makes it exciting an awesome too. Lots of people experimenting and no centralization of dominant ideas. it’d be cool if mastadon stays more like the early web
 
reply



  
 
Daviey 1 hour ago  
             | parent | prev | next [–] 

Has this problem been fixed?
 
reply



  
 
sneak 1 minute ago  
             | root | parent | next [–] 

I started building a fediverse spider and search engine but the death threats and toxicity made me decide to work on other stuff.
 
reply



  
 
mcv 1 hour ago  
             | prev | next [–] 

Until recently, I most of my stream was politics, and I was missing the gamer crowd I used to hang with on Google+. Turns out they're all on dice.camp, and now that I found them, my stream is happy again.So I'm mostly following a lot of RPG designers, but a few non-RPG-related names are: Cory Doctorow (super interesting of course), Neil Gaiman, Stephen Fry (sometimes receives a lot of pointless responses, which makes it frustrating to engage) and George Takei (always receives a lot of pointless comments; I'd like to be able to hide other people's comments from his posts, because it's all noise).Of course not everybody I'm following is on Mastodon. I'm not even on Mastodon. The Fediverse is a lot bigger than that.
 
reply



  
 
barathr 1 hour ago  
             | prev | next [–] 

@futurebird@sauropods.win -- science, math, entomology
@librarianshipwreck@mastodon.social -- pithy critiques, cats
@nomdeb@mstdn.social -- history and other random things
@OkieSpaceQueen@scicomm.xyz -- science communications, conspiracy deconstruction
@tprophet@defcon.social -- writer at 2600
@kjhealy@mastodon.social -- pithy sociologist
@notjustbikes@notjustbikes.com -- urban planning
@enron@tilde.zone -- hacks and snark
@HelenBranswell@scicomm.xyz -- infectious disease reporting
@Jessicascott09@infosec.exchange -- misc infosec
@harshad@sharma.io -- farming in india
 
reply



  
 
ragebol 2 hours ago  
             | prev | next [–] 

@simonstalenhag@mastodon.art Great Sci-fi art@fasterthanlime@octodon.social Hacker@tlalexander@queer.party Working on open source farming robots@fchollet@sigmoid.social Deep learning stuff, creator of Keras
 
reply



  
 
TaylorAlexander 19 minutes ago  
             | parent | next [–] 

Hey that’s me! Thanks for the shout out.
 
reply



  
 
louismerlin 2 hours ago  
             | prev | next [–] 

https://mastodon.social/@Gargronhttps://infosec.exchange/@briankrebshttps://infosec.exchange/@troyhunthttps://cyberplace.social/@GossiTheDoghttps://infosec.exchange/@malwaretechhttps://infosec.exchange/@hacks4pancakeshttps://infosec.exchange/@tinkerhttps://merveilles.town/@neauoire
 
reply



  
 
ttrrooppeerr 1 hour ago  
             | prev | next [–] 

The, one and only, Ron Gilbert: https://mastodon.gamedev.place/@grumpygamer
 
reply



  
 
fattybob 1 hour ago  
             | prev | next [–] 

I feel that everyone should follow God on mastodon - quite refreshing!
 
reply



  
 
4ad 1 hour ago  
             | prev | next [–] 

Please don't just post a bunch of links! Try to give a short introduction or explanation on why that person is interesting or otherwise worthy of following.@breakingtaps@universeodon.com: electron microscopy images and hobbyist nano-scale maker and tinkerer. Has a youtube channel.@gregeganSF@mathstodon.xyz: the science fiction author, mostly posts about math.@danluu@mastodon.social: tech writer/blogger, his articles often make HN's top page.@johncarlosbaez@mathstodon.xyz: famous physicist, posts about math.@TechConnectify@mas.to: whimsical and eclectic youtuber, thorough explainer of things like dishwashers, toasters, and LEDs.@TomDuff@mastodon.social: inventor of Duff's device, posts about music and pen plots.@robpike@hachyderm.io: inventor of Plan 9 and Go.@aram@mastodon.sdf.org: don't follow this guy!Also mathstodon.xyz, math-oriented mastodon instance.
 
reply



  
 
INTPenis 2 hours ago  
             | prev | next [–] 

https://social.growyourown.services/@FediFollowshttps://mastodon.online/@streetartutopiahttps://spacelase.rs/@dekaminskihttps://infosec.exchange/@troyhunthttps://a.gup.pe/u/histodons # This is a group that boosts every post mentioning it. supposed to be history related but it can be abused of course.https://heath.social/@carlhttps://mstdn.social/@simonegiertzhttps://mastodon.social/@elonjethttps://mastodon.social/@Popehathttps://fosstodon.org/@PINE64https://hachyderm.io/@nova # Imho the best fediverse admin.https://infosec.exchange/@briankrebshttps://infosec.exchange/@cisacyberhttps://mastodon.social/@oatmealhttps://wandering.shop/@cstrosshttps://mastodon.world/@JeriLRyanhttps://bitbang.social/@NanoRaptorhttps://universeodon.com/@georgetakeihttps://njalla.social/@njallahttps://hachyderm.io/@molly0xfffhttps://atomicpoet.org/users/atomicpoethttps://social.data.coop/@cryptohagenhttps://mastodon.archive.org/@internetarchivehttps://mastodonapp.uk/@alicerobertshttps://mastodon.social/@neilhimselfhttps://mastodonapp.uk/@stephenfryhttps://mastodon.nu/@gretathunberghttps://m.brokep.com/@peterhttps://mstdn.social/@kathygriffinhttps://mastodonapp.uk/@zskhttps://social.coop/@dajbhttps://social.network.europa.eu/@EUSPA # Just a twitter relay so farhttps://bsd.network/@solenehttps://merveilles.town/@neauoirehttps://mastodon.ar.al/@aral
 
reply



  
 
randomcarbloke 2 hours ago  
             | prev | next [–] 

Terry Tao and Zack Weinersmith
 
reply



  
 
stjohnswarts 1 hour ago  
             | prev | next [–] 

Maybe just list your top 4 or 5 and a brief summary why you like them would be more helpful than just dropping a long list of people.
 
reply



  
 
weinzierl 1 hour ago  
             | prev | next [–] 

In no particular order within the respective sections.No BS, down to earth infosec folksFilippo Valsorda - https://abyssdomain.expert/@filippoMarcus Hutchins - https://infosec.exchange/@malwaretechTavis Ormandy - https://social.sdf.org/@tavisoRoyce Williams - https://mastodon.social/@tychotithonusThe Grugq - https://infosec.exchange/@thegrugqAlex Bloor - https://bloor.tw/@bloorAndrew Tierney - https://infosec.exchange/@cybergibbonsThe usual Rust crowdMara Bos - https://hachyderm.io/@MaraAaron Turon - https://hachyderm.io/@aturonLuca Palmieri - https://hachyderm.io/@algo_lucaTim McNamara - https://mastodon.nz/@timClicksRaph Levien - https://mastodon.online/@raphGame DevÓlafur Waage - https://mastodon.social/@olafurwGodot Engine - https://mastodon.gamedev.place/@godotengineCelebrities
(Stephen was one of the first people I followed on Twitter, so it is a tradition. The others are canaries.)Stephen Fry - https://mastodonapp.uk/@stephenfryKathy Griffin - https://mstdn.social/@kathygriffinGreta Thunberg - https://mastodon.nu/@gretathunbergHonorable mentionDan Luu - https://mastodon.social/@danluu
 
reply



  
 
tgv 1 hour ago  
             | prev [–] 

Yeah, let's repeat twitter, but with an even less controlled messaging system. That'll go well.
 
reply



  
 
INTPenis 1 hour ago  
             | parent | next [–] 

Clearly troll bait, but I just wanna say that no one is repeating twitter. This is decentralized. Even if corporations move in and create their own instances it will still remain decentralized. Even if it splinters into pockets of federation, it will still be more of a public domain than twitter ever was, being owned by one corporation.
 
reply



  
 
tgv 1 hour ago  
             | root | parent | next [–] 

With all the dangers of disinformation and ""echo chambers"", but a bit bigger. And what for? So more people can feel they have followers and spend time cultivating that?
 
reply



  
 
INTPenis 1 hour ago  
             | root | parent | next [–] 

You have to take the good with the bad. People seem to like being social online.I never understood it myself, was never active on anything but IRC. But ActivityPub seems truly magic when it comes to promoting communication across boundaries such as corporations and software stacks, so I jumped on it back in 2017 and loved seeing it blow up in 2022.There are clearly a lot of very nice, and very normal, people on there who just love the concept of microblogging. Who am I, or you, to stand in their way? And why should they depend on one corporation to communicate when they can take it into their own hands?It's really a very natural progression, just like we have many ISPs, phone companies, or anything else in society.
 
reply



  
 
4ad 1 hour ago  
             | root | parent | prev | next [–] 

Yes, let's all stay in the existing disinformation echo chambers.If you see disinformation and find yourself in an echo chamber on a platform where you can chose who you see and engage with, I am afraid it's not the platform's fault.
 
reply



  
 
karaterobot 1 hour ago  
             | parent | prev [–] 

People need to see this play out again in order to rule out the possibility that the problems Twitter had and caused were just due to that particular platform, in that particular scenario. If Mastodon (or any of the other alternatives) go down the same road, they'll begin to wonder if the cause may be something deeper. But, they won't arrive at that conclusion by abstraction, or else they would have already.Another possibility is it'll work out great.
 
reply







Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
Search:  


"
https://news.ycombinator.com/rss,Microsoft launches Azure OpenAI service with ChatGPT coming soon,https://www.theverge.com/2023/1/17/23558530/microsoft-azure-openai-chatgpt-service-launch,Comments,"Microsoft/Tech/Artificial IntelligenceMicrosoft launches Azure OpenAI service with ChatGPT coming soonMicrosoft launches Azure OpenAI service with ChatGPT coming soon / ChatGPT is coming to this Azure service soon, as businesses get to use new AI models in their own apps.By  Tom Warren / @tomwarren Jan 17, 2023, 10:38 AM UTC|Share this storyIf you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement. Illustration by Alex Castro / The VergeMicrosoft is rolling out its Azure OpenAI service this week, allowing businesses to integrate tools like DALL-E into their own cloud apps. Microsoft has been testing this Azure service for just over a year, and it will soon include access to ChatGPT, the conversational AI that made headlines last year.The Azure OpenAI service features a number of AI models made by OpenAI including GPT-3.5, Codex, and DALL-E, so businesses and developers can utilize these systems in their own apps and workloads. Microsoft essentially packages up GPT-3.5 with the scaling you’d expect from Azure and management and data handling additions. Developers could use Azure OpenAI to build apps that leverage AI for support tickets or for content matching to improve search results in online stores. Such models are already being used widely as tools to summarize documents and analyze text.Microsoft uses its own Azure OpenAI service to power GitHub Copilot, the $10 per month service that helps suggest lines of code to developers inside their code editor. Power BI also uses GPT-3 natural language models to generate formulae and expressions, and the upcoming Microsoft Designer app uses DALL-E 2 to generate art from text prompts.RelatedMicrosoft is giving businesses access to OpenAI’s powerful AI language model GPT-3Microsoft to challenge Google by integrating ChatGPT with Bing searchMicrosoft is looking at OpenAI’s GPT for Word, Outlook, and PowerPointMicrosoft rumored to invest $10 billion in OpenAI.The launch of Azure OpenAI comes just days after rumors of Microsoft looking to integrate ChatGPT and other OpenAI language AI models even further into its products and services. Microsoft is rumored to be preparing to challenge Google with ChatGPT integration into Bing search results, and the company is reportedly looking at integrating some language AI technology into its Word, PowerPoint, and Outlook apps.Speaking at the World Economic Forum on Tuesday, Microsoft CEO Satya Nadella was bullish on OpenAI integration. “Every product of Microsoft will have some of the same AI capabilities to completely transform the product,” said Nadella, at a Wall Street Journal panel. Tools like ChatGPT, Nadella argues, are needed to boost productivity. “We need something that truly changes the productivity curve so we can have real economic growth,” said Nadella.Microsoft purchased an exclusive license to the underlying technology behind GPT-3 in 2020 after investing $1 billion into OpenAI in 2019. It has built a deep relationship with OpenAI ever since, including plans to add an AI text-to-image model to Bing powered by OpenAI’s DALL-E 2. Microsoft is now reportedly eyeing a $10 billion bet on OpenAI that would see the software giant get 75 percent of profits and a 49 stake in OpenAI.CommentsMost PopularThe battle of the standards: why the US and UK can’t stop fighting the metric systemNew Pixel Fold leak lands in physical spaceNo, you can’t get a 16TB SSD for a hundred bucksExtremely HardcoreLaid-off Twitter workers must drop class-action severance lawsuit, judge saysVerge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily.Email (required)Sign upBy submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.From our sponsorAdvertiser Content From"
https://news.ycombinator.com/rss,What's Ahead for U.S. Propane Marketers in Winter 2022-23?,https://bpnews.com/supply/whats-ahead-propane-marketers-winter-2022-23,Comments,"












Supply experts outline the opportunities & challenges marketers will see as winter temps fluctuate





  Pat Thornton








Wednesday, January 4, 2023





































     As December 2022 began, propane inventories in the United States were in pretty good shape. With 90 MMbbl (million barrels), inventory levels were 18 MMbbl higher than a year ago. Should marketers feel comfortable at these levels? We have seen winters bring a depletion of 50 MMbbl during December and January. BPN asked several supply experts about the chances for a significant depletion this year and how various regions of the U.S. would be impacted if we saw a depletion of 50 MMbbl during the winter months.

Changes Can Happen as Exports, Colder Temps Ramp Up
  “The majority of exports will go out of Belvieu, which is where we would see the biggest impact to inventories in the U.S.,” said John Powell, senior vice president and head of Crestwood’s team of propane industry professionals. “As the U.S. has experienced a mild winter thus far, we continue to see inventories build in PADDs 1-3. There should be limited impact of additional exports in PADDs 1 and 2 as pricing in those areas will keep the product local if the domestic demand requires additional inventories.”

   “It looks like we’ll stay in good shape, but I think we’ll still have a winter,” said Jeff Thompson, supply consultant at Propane Resources. “Europe and Canada have been experiencing a cold winter, and it only takes a small shift in air flow to get to colder and colder-than-normal temperatures here. We can go through 26 million more barrels pretty fast. Once demand shows up, propane prices can quickly move up 23 cents per gallon.”

  







 


 “You’re looking at a roughly 20-week time period (December through mid-April). A 2 MMbbl draw per week on average would get you down to 50 MMbbl by the end of winter,” said Phil Farris, director of wholesale marketing at 3Eight Energy. “Certainly possible. If that kind of consistent winter demand and export activity happens, it would surely affect price and logistics (transportation snags, Hours of Service restrictions, etc.). But the difficulties would likely be toward the end of winter — the last lap of the race. Propane retailers are tough. They power through the last lap.”

   “Currently, U.S. exports are limited out of Marcus Hook and the Gulf Coast. Marcus Hook has limited export capacity due to facility and supply constraints, and the Gulf Coast is limited due to the 28-day backup at the Panama Canal for VLGs,” said D.D. Alexander, president of Global Gas Inc. and chairman of the National Propane Gas Association Propane Supply & Logistics Committee. “There may be additional Gulf Coast exports that choose to go the long route or traders can use smaller ships, but this takes more cargos and costs more freight.”

   Alexander noted, though, that if supplies somehow get pulled down by 50 MMbbl, the Northeast and Gulf Coast would be the hardest hit directly, and then Conway and the Northwest indirectly.

   “PADD 1 has no fast way to replace the inventory volume in the winter if it is drawn down suddenly due to increased exports (or weather). PADD 3 would be impacted due to the sheer volume of export capacity built over the last several years if exports pulled down supplies by 50 MMbbl. The U.S. is able to export large volumes due to the increased export facilities built in the Gulf. PADD 2 would be impacted if PADD 3 volumes were severely drawn down.”

   Alexander believes this would leave less barrels to move north to Conway to feed the Midwest and upper Midwest. “This is extremely critical this year due to the Medford Fractionator being down because of the fire.”

  






 Lastly, she mentioned the upper Northwest would be impacted if the exports increased out of the upper Northwest/Prince Rupert and the new propane dehydrogenation plant in Canada needed additional propane. “This would leave less propane available to move to PADDs 4 and 5.”

   “Given current economic conditions and the loss of strong demand from China, it’s unlikely we’ll see this kind of draw again this season,” said Anne Keller, managing director at Midstream Energy Group. “European storage is either maxed out, U.S. wood pellets are being brought in to support heating demand and/or rail shipments from Russia are displacing some U.S. product there.”

   Keller said the big inventory build in Canada despite the recent brutal cold spell in the western part of the country indicates slack demand from Asia.

   “We’re all waiting for a big cold snap to deplete stocks in the market areas, and production continues to head to the hubs in the meantime as gas volumes increase,” Keller said. “If we see a sustained interval of cold weather in the U.S. this year, the impact would likely be due to constraints on last mile deliveries due to road conditions more so than limited product.”

Factors That Could Lead to U.S. Supply Challenges
  BPN asked the experts what key factors could lead to supply challenges and what areas of the U.S. are already impacted.

  







 “If the possibility of a rail strike has indeed gone away, then I would say weather disruptions would be the main concern … and the effect that has on transportation,” Farris said. He hopes a COVID-19 outbreak (or some type of viral, flu-like outbreak) doesn’t happen, but it’s always a possibility. “That takes employees, drivers, etc. out of commission.”

   Powell agreed that if the railroad strike is behind us, the biggest challenge with an extended weather event will be truck transportation in the local markets.

   “While this issue could affect all areas, the areas with the highest concentration of demand — PADDs 1 and 2 — would be affected the most,"" he said.

   “If we have dramatically cold weather like a prolonged polar vortex, along with stronger exports, we could see our inventories deplete rapidly,” Alexander said. “Of course, it all depends on how long it lasts and how much the exports can be increased. Honestly, with the large inventories we have, this is hard to imagine, but it is definitely possible.”

   Alexander does not rule out more challenges with a work slowdown with the railroads. “I can’t imagine the rail unions are too happy about Congress forcing the deal down their throats and making them go back to work,” she said. “I think we just may see the rail become even more unreliable than in the past for some time to come. I think we have begun to see this in the Northeast. Also, trucking is still a huge issue across the U.S. If we have to truck gas farther, there just isn’t any extra capacity to handle the increased miles — nor is there additional capacity if we have severely cold weather for any length of time.”

  







 “So far, we’re not seeing much in terms of supply challenges related to product availability,” Keller said. “Refilling tanks at ski resorts in 8 feet of snow could be an issue. Although we’ve seen blizzard conditions in West Texas already this year, we’re currently in the middle of a December heat wave in Houston. And prices are much lower than this [past] summer when crude topped out at $60 over current levels.”

Is It Time to Buy More Gallons for This Winter & Beyond?
  BPN asked if the pullback in the market presented a buying opportunity for the retail propane marketplace.

   “I’m not sure we’ll get out of this winter unscathed,” Thompson said. “Too often in winter, we’re jerked one way and then the other. Too many are banking on this being over, but the squeeze usually comes in the first quarter. It just takes a few barrels in too few hands. Too many short positions can cause a January-February blowup. The Midwest is usually the tough spot. PADD 2 and Conway can often get nasty.”

   “Market pricing has been extremely volatile and more headline-driven recently,” Powell said. “Predicting pricing is extremely difficult, so managing your exposure should be a high priority as purchases and sales are made in the spot and forward markets.”

   “I think the consensus right now is that propane is oversold and under the radar, and everyone is ‘waiting’ for demand and exports to eat up the production increases,” Farris said. “If there are uncovered sales for Q1 2023, I would say yes … although a majority of retailers probably covered those sales through the summer and fall. Despite doing the ‘right thing’ and covering gallons at an acceptable margin, some will feel they guessed wrong or made a bad decision and ‘lost money.’ This sentiment will often take them out of the market for the next season (or longer) … for fear of making another ‘mistake,’ when in fact they haven’t made one … but may be getting ready to. I think historical price data would support buying for the next two to three years. Take the peaks and valleys out.”

   “I believe the upside risk is much higher than the downside risk,” Alexander said. “Having said that, I always caution marketers to be very careful if they are speculating. If they have accounts they are trying to shield from price volatility or they have customers who want to prebuy, then yes. They need to remember, however, that the price will fluctuate based on where our inventories are when we come out of winter.”

   “Wholesalers who weren’t able to take advantage of the so-called backwardated price curve this [past] summer to acquire winter barrels are now sitting on expensive inventory and are most likely not going to be buying again until they’ve run down their current stocks,” Keller said. “If there’s a year-end scramble for producers to clear the books, it might be a chance to top off supply for the 4-6 weeks of really cold weather we could still see this year. The truly bold and brave will add to that buy to start a lower cost layer of inventory for next year.”

   Keller said that despite bullish projections from the Energy Information Administration (EIA), U.S. oil production did not grow at the level the government was looking for to offset the sales from the SPR. “It looks unlikely that it will ramp strongly next year, either, for a wide variety of reasons, putting OPEC+ back in the driver’s seat in the world markets. Regardless of whether the government tells us so, a significant part of the global economy is in recession, which is reflected in an overall price decline in both energy and ag commodities.”

Crude Oil a Wild Card
  “I believe crude is the big wild card,” Alexander said. “Once oil stops being pulled from the strategic reserve, crude may very well be in short supply. Unless we can increase our crude production substantially, we may see crude really take off. There are many variables in trying to predict the price of crude, but it needs to be considered. Most of the time, when crude goes up, propane goes up. We have been somewhat disconnected to the price of crude recently due to our large inventories, but that can change very fast.”

The Pendulum Swings, but Customers Like Consistency
  “The old pendulum swings,” Farris observed. “To simplify, let’s say 100 MMbbl of propane inventory and a 50 cent per gallon price is one extreme, and 50 MMbbl and $2 per gallon represents the other extreme. We’ve been at or close to both of these neighborhoods in the past 10 years. The pendulum never stops; it just changes direction, looking for equilibrium, which it never finds for long. And nobody can predict that day when data, news and momentum change direction.

   “We have seen nearly a $1 per gallon (or 60%) decrease from March to early December. Time to buy? Maybe.”

   Farris shared a quote from John Maynard Keynes: “‘The market can stay irrational longer than you can stay solvent.’

   “A famous investment quote. Not that anyone is insolvent, but the point is things don’t happen when or just because they’re supposed to,” Farris said. “Successful propane retailers are not traders; they are service providers. And that shouldn’t change because of the price of propane. You buy to have control over your cost, and hopefully that means control over your selling price. Take the market peaks and valleys out of play — consistency in the customer’s eye is a big plus in the retail energy business.”

Structural Change Is the Best Way to Describe the Current Market
  J.D. Buss, president of Twin Feathers Advisors, made some observations regarding the current market.

   “While propane prices have dropped significantly in the past couple of months, the larger change has been in the structure of the forward price curve,” Buss said. “Instead of the propane curve resting in backwardation (current month prices higher than future months), prices are now in contango for the 2023-24 season.”

   Buss noted that summer values for Belvieu have been trending anywhere from 3 to 6 cents per gallon below middle of winter 2023-24 values. “Go back nine months and this scenario would have seemed unfathomable,” he said. “Russia’s invasion of Ukraine, the EU calling for an oil embargo and natural gas prices skyrocketing all seemed to point to endless upside for energy commodities. That bullish situation drove many markets, including propane, into a very bullish, backwardated price curve.”

   As the Beatles sang and the great biblical author stated, there’s a season for everything, Buss observed. “Inventory levels have escalated quickly over the last two months, but the more persistent theme has been growing production of natural gas liquids. These two facts have shifted the narrative from a bullish stance to a bearish and brought about a complete structural change in the propane price curve.

   “Such a strong change has reduced the broader concerns of supply challenges for this winter and mainly left the focus to logistical challenges,” Buss said. “Will the railroad union resolution truly work and keep cars rolling properly over the coming two months? Will a large weather event hinder the movement of transports or push retailers to further terminals? These ‘tried and true’ items will likely be the main challenges of this winter.

   “What about exports, though? Won’t they obliterate the healthy surplus of inventory sitting in the U.S. and force radical supply decisions?” he asked.

   He noted that according to the EIA’s weekly data, exports are already at record highs. “More volume could be loaded and exported, but three major items are slowing (or controlling) that possibility. First, spot freight values for VLGCs are near record highs and eating up the majority of the international price difference. Second, continued delays at the Panama Canal have forced vessels into longer journeys and helped support higher freight costs. Third, and most volatile, what will weather be like in Europe and Far East Asia in first quarter 2023? Northern Europe during the first part of December is cold, but some forecasts show weather moderating in January.

   “The final point to consider when looking at exports and the current inventory surplus: Both exports and domestic demand need to dramatically increase to pull down inventory levels,” Buss said. “An increase of 150,000 bbl/day in exports seems high but only accounts for an additional 9 MMbbl over two months. Both North American demand and exports need to ratchet higher to eat up the inventory surplus and cause additional shortage fears.”

   All of this brings us back to the beginning point: structural change, according to Buss. “The structural change in the forward price curve to contango tells the market that the bullish vibe from early in 2022 has faded. If this structural change continues, then we could be in for a very different summer in 2023 versus what was experienced [in 2022].”
 Pat Thornton is a 25-year veteran of the propane industry, with 20 years at Propane Resources and five years at Butane-Propane News. He has served on the PERC Safety & Training Advisory Committee and the Missouri PERC Board of Directors.
 













January 2023



The 2023 State of the Industry Issue










"
https://news.ycombinator.com/rss,Git Security Vulnerabilities Announced,https://github.blog/2023-01-17-git-security-vulnerabilities-announced-2/,Comments,"

Today, the Git project released new versions to address a pair of security vulnerabilities, CVE-2022-41903, and CVE-2022-23521, that affect versions 2.39 and older.
Git for Windows was also patched to address an additional, Windows-specific issue known as CVE-2022-41953.
The first two vulnerabilities affect Git’s commit formatting mechanism and .gitattributes parser, respectively. The former can be used to perform arbitrary heap writes, while the latter can be used for arbitrary reads, too. Both may result in arbitrary code execution, so users should upgrade immediately. Both were also found as part of an audit of the Git codebase conducted by X41. This audit was sponsored by the Open Source Technology Improvement Fund (OSTIF). Fixes were authored by engineers from the GitLab Security Research Team, as well as GitHub Engineers, and members of the git-security mailing list.
A complete copy of the report (along with a variety of issues that weren’t deemed to have security implications) is available here.
The Windows-specific issue involves a $PATH lookup including the current working directory, which can be leveraged to run arbitrary code when cloning repositories with Git GUI.
CVE-2022-41903
The first set of updates concern Git’s commit-formatting mechanism, used to display arbitrary information about commits, as in git log --format. When processing one of the padding operators (for example, %<(, %>(, etc.) an integer overflow can occur when a large offset is given).
This vulnerability can be triggered directly via git log --format. It may also be triggered indirectly via Git’s export-subst mechanism, which applies the formatting modifiers to selected files when using git archive.
This integer overflow can result in arbitrary heap reads and writes, which may result in remote code execution.
[source]
CVE-2022-23521
gitattributes are used to define unique attributes corresponding to paths in your repository. These attributes are defined by .gitattributes file(s) within your repository.
The parser used to read these files has multiple integer overflows, which can occur when parsing either a large number of patterns, a large number of attributes, or attributes with overly-long names.
These overflows may be triggered via a malicious .gitattributes file. However, Git automatically splits lines at 2KB when reading .gitattributes from a file, but not when parsing it from the index. Successfully exploiting this vulnerability depends on the location of the .gitattributes file in question.
Like the above, this integer overflow can result in arbitrary heap reads and writes, which may result in remote code execution.
[source]
CVE-2022-41953
After cloning a repository, Git GUI automatically applies some post-processing to the resulting checkout, including running a spell-checker, if one is available.
A Windows-specific vulnerability causes Git GUI to look for the spell-check in the worktree that was just checked out, which may result in running untrusted code.
[source]
Upgrade to the latest Git version
The most effective way to protect against these vulnerabilities is to upgrade to Git 2.39.1. If you can’t update immediately, reduce your risk by taking the following steps:

Avoid invoking the --format mechanism directly with the known operators, and avoid running git archive in untrusted repositories.
If you expose git archive via git daemon, consider disabling it if working with untrusted repositories by running git config --global daemon.uploadArch false.
Avoid using Git GUI on Windows when cloning untrusted repositories. 

In order to protect users against these attacks, GitHub has taken proactive steps. Specifically, we:

Scanned all repositories on GitHub.com to confirm that no evidence exists to conclude that GitHub was used as a vector to exploit any of these vulnerabilities.
Implemented mitigation steps to prevent GitHub.com from being used as an attack vector in CVE-2022-41903, and CVE-2022-23521.
Scheduled a GitHub Desktop release for later today, January 17, that prevents the exploitation of this vulnerability.
Scheduled updates to GitHub Codespaces and GitHub Actions to upgrade their versions of Git.
Scheduled updates to GitHub Enterprise Server1 with patched versions of Git.

Credit for CVE-2022-41903, and CVE-2022-23521 goes to Markus Vervier, and Eric Sesterhenn of X41 D-Sec, whose work was sponsored by OSTIF, as well as Joern Schneeweisz of GitLab. Fixes were written by Patrick Steinhardt of GitLab, with additional help from members of the Git security mailing list.
Credit for finding CVE-2022-41953 goes to 俞晨东.

Download Git 2.39.1

Notes




The updates will be present in GitHub Enterprise Server versions 3.3.19, 3.4.14, 3.5.11, 3.6.7, and 3.7.4. ↩






Tags: Git 

"
https://news.ycombinator.com/rss,EV batteries alone could satisfy short-term grid storage demand as early as 2030,https://www.nature.com/articles/s41467-022-35393-0,Comments,"



                        Electric vehicle batteries alone could satisfy short-term grid storage demand by as early as 2030
                    


Download PDF










Download PDF








Article

Open Access

Published: 17 January 2023

Electric vehicle batteries alone could satisfy short-term grid storage demand by as early as 2030
Chengjian Xu 
            ORCID: orcid.org/0000-0002-2512-58761, Paul Behrens 
            ORCID: orcid.org/0000-0002-2935-47991, Paul Gasper 
            ORCID: orcid.org/0000-0001-8834-94582, Kandler Smith 
            ORCID: orcid.org/0000-0001-7011-03772, Mingming Hu1, Arnold Tukker 
            ORCID: orcid.org/0000-0002-8229-29291,3 & …Bernhard Steubing 
            ORCID: orcid.org/0000-0002-1307-63761 Show authors

Nature Communications
volume 14, Article number: 119 (2023)
            Cite this article




33 Altmetric


Metrics details





Subjects

BatteriesEnvironmental sciencesRenewable energy




AbstractThe energy transition will require a rapid deployment of renewable energy (RE) and electric vehicles (EVs) where other transit modes are unavailable. EV batteries could complement RE generation by providing short-term grid services. However, estimating the market opportunity requires an understanding of many socio-technical parameters and constraints. We quantify the global EV battery capacity available for grid storage using an integrated model incorporating future EV battery deployment, battery degradation, and market participation. We include both in-use and end-of-vehicle-life use phases and find a technical capacity of 32–62 terawatt-hours by 2050. Low participation rates of 12%–43% are needed to provide short-term grid storage demand globally. Participation rates fall below 10% if half of EV batteries at end-of-vehicle-life are used as stationary storage. Short-term grid storage demand could be met as early as 2030 across most regions. Our estimates are generally conservative and offer a lower bound of future opportunities.



IntroductionElectrification and the rapid deployment of renewable energy (RE) generation are both critical for a low-carbon energy transition1,2. They also address many other environmental issues, including air pollution. However, the variability of critical RE technologies, wind and solar, combined with increasing electrification may present a challenge to grid stability and security of supply1,2. There are several supply-side options for addressing these concerns: energy storage, firm electricity generators (such as nuclear or geothermal generators), long-distance electricity transmission, over-building of RE (resulting in curtailment in periods of lower demand), and power-to-gas3 (in approximate ascending order of today’s estimated cost). Demand-side management is also vital in shifting and flattening peak demand4. Given rapid cost-declines, battery storage is one of the major options for energy storage and can be used in various grid-related applications to improve grid performance. Cost declines in batteries have been the major driver for electric vehicle (EV) cost reductions. Given that many batteries will be produced for light-duty transport these could offer a low-cost and materially-efficient approach for short-term electricity grid storage requirements5.EV batteries can be used while in the vehicle via vehicle-to-grid approaches, or after the end of vehicle life (EoL) (when they are removed and used separately to the chassis in stationary storage). “Smart” vehicle-to-grid charging can facilitate dynamic EV charging and load shifting grid services. EVs can also be used to store electricity and deliver it back to the grid at peak times6. These opportunities rely on standards and market arrangements that allow for dynamic energy-pricing and the ability of owners to benefit from the value to the grid. Value to the grid can include deferred or avoided capital expenditure on additional stationary storage, power electronic infrastructure, transmission build-out, and more6. When the remaining battery capacity drops to between 70-80% of the original capacity, batteries generally become unsuitable for use in EVs7. However, these batteries at vehicle EoL (hereafter termed retired batteries) may still have years of useful life in less demanding stationary energy storage applications and represent substantial value to the grid8.The utilisation of EV batteries could improve the flexibility of supply while reducing the capital costs and material-related emissions associated with additional storage and power-electronic infrastructure. However, the total grid storage capacity of EV batteries depends on different socioeconomic and technical factors such as business models, consumer behaviour (in driving and charging), battery degradation, and more9,10. Previous global-level studies, including those on vehicle-to-grid capacity2,11,12 and retired battery capacity12,13 are informative. However, they rarely consider several important factors that determine storage opportunity, such as non-linear, empirically-based battery degradation and neglect the impact of battery chemistry altogether;14,15,16 geographical and/or temporal temperature variance (which impacts battery degradation); and, driving intensity by vehicle type in different countries/regions (which constrains the total capacity available during the day). Additionally, consumer participation in the vehicle-to-grid market and utilisation of retired batteries in the second-use market impact the actual grid storage capacity10, both of which are important but rarely quantified.Here we link three models and databases to assess the global grid storage opportunity of EV batteries by 2050 for both vehicle-to-grid applications and EoL opportunities (see Fig. 1, Methods, and Supplementary Fig. 1). We cover the main EV battery markets (China, India, EU, and US) explicitly, and combine other markets in a Rest of the World region (RoW). We first use a dynamic battery stock model to estimate future battery demand as part of transport fleets per region (Supplementary Fig. 2). The model incorporates two EV fleet development scenarios based on the IEA’s (International Energy Agency), stated policy (STEP) and sustainable development (SD) scenarios. The STEP scenario incorporates existing EV policies only, while the SD scenario is compatible with the climate goals of the Paris agreement and sees a larger EV fleet. The scenarios include two battery chemistry sub-scenarios to capture different technological paths: one dominated by lithium nickel cobalt oxides (NCX, with an “X” denoting manganese or aluminum, i.e., NMC/NCA) and another dominated by lithium-ion phosphate or (LFP). Market shares of NCX and LFP batteries are assumed to reach 98% and 2% in the NCX path by 2050, respectively, and 40% and 60% in the LFP path (see Supplementary Fig. 3 for detailed market shares over time).Fig. 1: Model framework linking EV use model, battery degradation model, and dynamic battery stock models.See legend for use of colours. Square, white boxes indicate model outputs. Please see details for the model framework in the methods section. USDOE US Department of Energy, FASTSim Future Automotive Systems Technology Simulator, NREL National Renewable Energy Laboratory, IEA International Energy Agency, SoH State of Health.Full size imageThese estimates of future demand are linked to an EV driving and charging behavior model for small, mid, and large-size BEVs (battery electric vehicles) and PHEVs (plug-in hybrid electric vehicles) based on daily driving distance distributions for different regions (Supplementary Figs. 4–6). EV use behavior, battery chemistry, and temperature in each region are combined with the latest battery degradation data for NCX14,15,17 and LFP16 chemistries to account for region- and chemistry-specific battery degradation (Supplementary Fig. 7).We first analyze the technical capacity for short-term grid storage from vehicle-to-grid and second-use. We then analyze the impact of different factors on the real-world capacity. For example, we analyse in detail the impact of different rates of EV owner participation in vehicle-to-grid markets as well as the impact of different utilisation rates of retired EV batteries in stationary storage (see Fig. 1 and methods for further details). Finally, we compare the technical and real-world short-term storage capacities against scenarios for future storage requirements from the literature.We focus here on short-term energy storage since this accounts for the majority of the required storage capacity18 and EV batteries are not well suited for longer-term, seasonal storage due to self-discharging over time. Short-term energy storage demand is typically defined as a typical 4-hour storage system, referring to the ability of a storage system to operate at a capacity where the maximum power delivered from that storage over time can be maintained for 4 hours. For example, the 4-hour storage capacity of batteries that together deliver a maximum of 0.25 GW until depletion will be 1 gigawatt hour19 (GWh). The short-term storage capacity and power capacity are defined based on a typical 1-time equivalent full charging/discharge cycle per day (amounting to 4 hours of cumulative maximum discharge power per day). This 4-hour threshold is chosen as it is required by some jurisdictions such as the California Public Utilities Commission and New York Independent System Operator20, energy system analysts anticipate this threshold as the most important to markets21, and is often the length of time used in the literature22.We compare our results against storage requirements reported in the IRENA (International Renewable Energy Agency) Planned Energy and Transforming Energy Scenarios (with a warming of “likely 2.5 °C” and “well below 2 °C” in the second half of this century, respectively)2, along with two Storage Lab scenarios (Conservative and Optimistic)23. Both Storage Lab scenarios result in a warming of “well below 2 °C” by 2100, but differ in the role for grid storage please see Supplementary Table 1 for more). These scenarios report short-term grid storage demands of 3.4, 9, 8.8, and 19.2 terawatt hours (TWh) for the IRENA Planned Energy, IRENA Transforming Energy, Storage Lab Conservative, and Storage Lab Optimistic scenarios, respectively. When assuming a 4-hour storage period for this capacity, this results in power demand of 850-4800 GW, or, 2500 GW when assuming an average storage capacity demand of 10 TWh.ResultsTotal technical capacityWe define technical capacity as the total cumulative available EV battery capacity in use and in second use at a specific time while considering battery degradation and the capacity needed to meet driving demand. Globally, the SD scenario sees a total technical capacity twice that of the STEP scenario due to the larger fleet size (see Supplementary Fig. 8 and Note 1). Globally, the LFP scenario sees a slightly higher cumulative capacity than the NCX scenario, due to different battery market shares and the lower degradation of LFP across most countries/regions (see Supplementary Data 1 for a full comparison). Compared to the SD-NCX scenario, The SD-LFP scenario sees 2.6 TWh of higher technical capacity for China, EU, US, and RoW by 2050 compared to the SD-NCX and a 0.05 TWh lower technical capacity for India (see Supplementary Note 2). These capacity differences are small compared to the total technical capacity. As shown in Fig. 2, the SD-LFP scenario has a technical capacity 48% higher by 2030 and 91% higher than the STEP-NCX scenario by 2050 (3.8 TWh and 2.6 TWh in 2030 and 32 TWh and 62 TWh in 2050, respectively).Fig. 2: Total technical capacity for EV batteries and comparison to grid storage demand.a STEP-NCX scenario. b SD-NCX scenario. c STEP-LFP scenario. d SD-LFP scenario (see details in Supplementary Table 1). IRENA = International Renewable Energy Agency.Full size imageUnder all scenarios, cumulative vehicle-to-grid and second-use capacity will grow dramatically, by a factor of 13–16 between 2030 and 2050. Putting this cumulative technical capacity into perspective against future demand for grid storage we find that our estimated growth is expected to increase as fast or even faster than short-term grid storage capacity demand in several projections2,23 (Fig. 2). Technical vehicle-to-grid capacity or second-use capacity are each, on their own, sufficient to meet the short-term grid storage capacity demand of 3.4-19.2 TWh by 2050. This is also true on a regional basis where technical EV capacity meets regional grid storage capacity demand (see Supplementary Fig. 9).Vehicle-to-grid opportunities and limitationsExamining the vehicle-to-grid opportunity alone, we find that 21%-26% of the global theoretical battery capacity (i.e., on-board EV battery capacity of the entire EV fleet without considering battery degradation) could be available for vehicle-to-grid services by 2050 (Fig. 3a). The most important limiting factor is the battery capacity required to meet consumer driving demands24,25. Driving demand can limit the available capacity by 57%-63%. PHEVs, which make up around 11% of the theoretical capacity in 2050, are not considered for vehicle-to-grid as they have a low storage potential due to low capacities. On average, just 5% of the theoretical capacity is lost due to battery degradation by 2050. These losses vary between 7% in India and 4% in RoW due to differences in regional factors such as use conditions and temperature (for full regional results see Supplementary Fig. 10). Overall, taking these factors into account yields an estimated technical vehicle-to-grid capacity of 18-30 TWh by 2050 (see Fig. 3).Fig. 3: Global available vehicle-to-grid capacity in 2050.a Technical vehicle-to-grid capacity. Hatched bars indicate the capacity limits due to key factors and blue bars the technical vehicle-to-grid capacity. b Real-world vehicle-to-grid capacity as a function of participation rates. Results are shown for the STEP-NCX and the SD-NCX scenarios with a comparison to the range of storage demand computed by IRENA and Storage Lab models in 2050 (orange shading). Please see Supplementary Fig. 16 for global real-world vehicle-to-grid capacity under STEP-LFP and the SD-LFP scenarios and Supplementary Figs. 17–20 for regional real-world vehicle-to-grid capacity.Full size imageHowever, there are other factors that may limit real-world available storage capacity, primarily the vehicle-to-grid participation rate. Not all EV consumers will necessarily participate in the market and the participation rate is defined as the percentage of the technical vehicle to grid capacity connected to the grid, as shown in Fig. 3b. Participation rates of 38% and 20% are required to satisfy short-term storage demands of 10 TWh in 2050 (for STEP-NCX and SD-NCX scenarios, respectively). In practice, it is likely that EVs with high battery capacities and low degradation will be used for providing vehicle-to-grid services since these will provide the highest revenue for EV owners26 (the full battery capacity distributions by 2050 across countries/regions are available in Supplementary Figs. 11–15).Impacts of deploying second-use batteries in stationary storageOver time EV batteries degrade to the point they cannot be used to power vehicles27, generally when the battery’s relative State of Health (SoH) drops below 70%-80%7 (defined as actual capacity as percentage of original capacity). The relative SoH could fall even lower if a consumer is willing to accept relatively poor battery health and shorter ranges28. Given their economic, value, size, and end-of-life regulations, we assume all batteries will be collected29. This is reasonable given that today’s lead-acid batteries achieve a near 100% collection rate30 and modern EV batteries are of much higher economic value.Once collected, batteries are health tested to determine if they can be used in a less critical second-use application, or if they should be recycled31. Given the technical and economic feasibility of retired batteries for second-use32, we consider batteries with an SoH of 70% and higher only for second-use (a threshold often used in the literature32). Under this assumption, 74% of retired NCX batteries can be repurposed for second-use globally, while 26% goes to recycling by 2050. Regional differences can be significant due to the impact of temperature on NCX battery degradation (see Supplementary Fig. 21 and Supplementary Data 1). In contrast, nearly all LFP retired batteries can be repurposed.Business models are still developing, and repurposing is highly dependent on the technical specifications and market requirements of second-use applications33. Since battery disassembly is costly32, battery repurposing will likely happen on the pack level instead of modules and cell level. Repurposing will consist mainly of rebalancing and reconnecting the retired battery packs. There is no strong technical reason to model a capacity difference before and after the repurposing.Using these assumptions we find that 2.1–4.8 TWh of retired batteries are estimated to become available as annual technical second-use capacity globally in 2050, as shown in Fig. 4a. The cumulative technical second-use capacity is expected to reach 14.8–31.5 TWh by 2050 when assuming second-use batteries have a lifetime of 10-years34 (Fig. 4b). The actual second second-use lifespan is uncertain due to uncertainties surrounding the retired battery SoH, use conditions, among other factors. Another uncertainty is the further battery degradation during secondary use, which is difficult to model due to complicated degradation mechanisms of retired batteries35. Further research into degradation and second-use life span is required to improve estimates of technical second-use capacity. If the 10TWh global, short-term storage requirements are met with second-use batteries alone, then a 68% utilisation rate of retired batteries would be needed in the STEP-NCX scenario (14.8 TWh technical capacity) and utilisation rate of 32% in the SD-LFP scenario (31.5 TWh technical capacity).Fig. 4: Availability of second-use capacity globally in 2050.a Average annual additions and cumulative technical capacity of second-use batteries in 2050. Here capacity refers to the technically available capacity considering battery degradation but without considering battery second-use utilisation rate. b Impacts of second-use utilisation rate on cumulative actual second-use capacity and a comparison to storage demand in 2050 (orange shading). See Supplementary Figs. 22–25 for regional actual second-use capacity.Full size imageCombining vehicle-to-grid participation rate and second-use utilisation ratesThe global technical capacity for short-term grid storage of EV batteries grows rapidly in all scenarios. However, the real-world available capacity depends strongly on the vehicle-to-grid participation rate and the second-use utilisation rates. We show the real-world available capacity as a function of these rates in Fig. 5 (for the STEP-NCX scenario, please see Supplementary Figs. 26–28 for other scenarios). Participation and utilisation rates of 50% for vehicle-to-grid and second-use, results in a real-world capacity of 25–48 TWh by 2050, far higher than the short-term storage requirements estimated from the literature. Changes in vehicle-to-grid participation rates of 23–96%36,37 by 2050 could influence this real-world capacity by as much as -24% to +21%. When second-use utilisation rates vary from 10%-100%, the real-world capacity varies between -41% and 12%. Taken together, vehicle-to-grid participation rate and second-use utilisation rate could alter the real-world capacity in 2050 by -61% to +32%.Fig. 5: Total actual available capacity under various conditions in STEP-NCX scenario in 2050.Blue, white, and red colors depict minimum, average, and maximum values. See Supplementary Figs. 26–28 for other scenarios.Full size imageWe could see many different combinations of vehicle-to-grid and second-use to meet the short-term grid storage demands by 2050 (3.4–19.2 TWh). Without any second-use batteries in stationary storage, grids would require vehicle-to-grid participation rates of a modest 12–43%. If we assume that only half of second-use batteries are used on the grid (with others used off-grid, for other EV or storage purposes, etc.), the required participation rate of vehicle-to-grid drops to below 10%.The required market participation rates depend on EV fleet and battery chemistry scenarios but also are influenced by other factors, such as battery capacity per vehicle. To investigate the impact of our capacity assumptions we investigate a scenario where all BEVs are equipped with a smaller 33kWh battery (instead of 33, 66, and 100 kWh battery per vehicle for small, mid-size, and large BEVs globally, see methods for more details). Even in this extreme case, EV batteries can still meet global, short-term grid storage demand by 2050 with participation rates of 10%-40% in vehicle-to-grid and with half second-use batteries used as stationary storage (see Supplementary Table 4).DiscussionPrevious research has suggested that large EV fleets could exert additional stress on grid stability (e.g., if the majority of EVs are charged at grid peak time)38. Our findings reveal a different perspective that EV batteries could promote electricity grid stability via storage solutions from vehicle-to-grid and second-use applications. We estimate a total technical capacity of 32-62 TWh by 2050. This is significantly higher than the 3.4–19.2 TWh required by 2050 in IRENA and Storage lab scenarios.The real-world capacity depends on participation rates for vehicle-to-grid and utilisation rates for second-use of batteries. Participation rates may vary regionally depending on future market incentives and infrastructure, along with other factors39. The STEP-NCX scenario presented in Fig. 5 has the lowest technical capacity (32 TWh compared to 62 TWh in the SD-LFP scenario) which already easily meets requirements at participation rates of 40%–50% for vehicle-to-grid and with around half second-use batteries used as stationary storage. At a regional level, even lower participation rates may still contribute significantly to grid stability. Overall, EV batteries could meet short-term grid storage demand by as early as 2030 if we assume lower storage requirements from the literature and higher levels of participation and utilisation. By 2040–2050 storage demands are met across almost all scenarios and even low participation and utilisation rates.Harnessing this potential will have critical implications for the energy transition and policymakers should be cognizant of the opportunities. The participation rate of EV users in the vehicle-to-grid market is crucial and the government can play an important role in incentivization. This can include market-based efforts such as micro-payments for services to the grid, or regulations to require the connection of commercial fleets to the network while at depots. Further regulations will be required to ensure the required hardware and software solutions for EV integration. This may include smart controllers for consumers in order to facilitate easy market participation and communication of benefits to EV users39. Strong re-use regulations will also be necessary to ensure that batteries are recovered at EOL and easily integrated into the grid40. Finally, policymakers and researchers should aim to understand EV user behavior over time in order to tackle the key factors preventing EV users from participating in vehicle-to-grid (which may include concerns surrounding battery degradation).As we include a broader set of limitations for the total opportunity of EV storage our results are difficult to compare with other literature. Our estimated global EV fleet capacity in 2050 (68-144 TWh) is considerably higher than the estimate from IRENA (7.5-14 TWh)2. This is due to the IRENA’s very conservative scenarios on future EV fleet size and battery capacity per vehicle. The IRENA scenario also does not consider the availability of EV fleet capacity for grid services. While a different IEA estimate does not extend beyond 203012 it does highlight the importance of including battery degradation in analyses, which we include for our projection to 2050 (Fig. 4).We note several limitations in our approach that could be improved as data availability improves. For example, while we include battery degradation by using state-of-art data, future battery degradation is highly uncertain and depends on further technological breakthroughs both in battery chemistry such as Na-ion, Li-Air, and Li-Sulphur41 along with developments in battery management systems. Further, while we derived driving behaviour from empirical data, future changes in driving habits are uncertain and dependent on various factors such as EV-related infrastructure. Vehicle chargers increase in power output over time and 50 kW charging and above is already common across many countries42. Frequent fast charging could lead to faster degradation, especially in hotter/colder climates43. This challenge may be addressed by future technology improvements to battery materials44, electrode architectures, and optimized synergy of the cell/module/pack system design45. A further limitation is that we compare technical and real-world available vehicle-to-grid capacity with an average 4-hour storage requirement as provided in the scenarios by IRENA and Storage Lab. This omits potential differences in storage requirements at shorter time scales (seconds/minutes). Improved modelling and data can overcome this gap. It is however likely that the technical vehicle-to-grid capacity will be sufficient given low vehicle utilisation rates of just 5% for many regions46. Additionally, the development of smart charging infrastructure and grid digitization is likely to provide additional flexibility for matching electricity demand and supply47.A final limitation is that we assume that the rated capacity per vehicle remains the same in the future and that a small number of large BEVs might provide large actual vehicle-to-grid capacity (Fig. 3). These capacities may change further in the future due to policy incentives, vehicle design, consumer preferences, charging infrastructure, among other factors. Further, the transportation system could see radical and fundamental changes. A significant and rapid shift away from private car use to mass transit, a move to shared electric vehicles, autonomous driving, and the success of battery swap systems48 could all alter the available capacity by 2050.In this study, we build a model framework to combine the EV use model, battery degradation model, and dynamic battery stock model. The model framework combines datasets on the real-world daily driving distance (in the EV use model), battery degradation test datasets (in the battery degradation model), and future EV and battery market data (in the dynamic battery stock model). The framework allows a structured use of diverse data to build a consistent perspective on future battery capacity. Within this model framework, this study provides a more complete understanding of the energy storage capacity available from EV batteries over time in real-world conditions and use. Results reveal a substantial opportunity for EV battery storage to support the stability and flexibility of renewable energy transition, even under modest consumer participation rates. To harness this opportunity, regulations and innovative business models will be needed to incentivize participation.MethodsModel overviewWe develop an integrated model to quantify the future EV battery capacity available for grid storage, including both vehicle-to-grid and second-use (see Supplementary Fig. 1 for an overall schematic). The integrated model includes three sub-models:

(1)
A dynamic battery stock model27 to estimate total future EV battery stock and the retired batteries at vehicle EoL. This model considers EV fleet (i.e., battery stock) development and EV lifespan distribution (Supplementary Fig. 2), as well as future chemistry development (see Supplementary Fig. 3 for detailed battery market shares by chemistry).


(2)
An EV use model which includes behavioral factors such as EV driving cycle and charging behavior (changing power, time, and frequency), based on daily driving distance data for small/mid-size/large BEVs and PHEVs (Supplementary Figs. 4–6).


(3)
A battery degradation model based on the latest battery degradation test data, to estimate battery capacity fading over time under different EV use, battery chemistry, and temperature conditions (Supplementary Fig. 7).

Dynamic battery stock modelWe build on results and methods from the study27 where we built a global dynamic battery stock model to quantify the stock and flows of EV batteries. We model future EV fleet development (i.e., battery stock) until 2050. We determine the retired battery availability based on battery stock development and EV lifespan distribution (which is assumed to determine the time when EV batteries are retired). Battery degradation does affect the technical performance (such as driving distance capability) of EVs, thus influencing consumers’ choice of time when EVs come into EoL. Here, for model simplicity, we assume batteries will be retired only when EVs come into EoL. While for EV battery capacity, we use an average capacity of 33, 66, and 100 kWh for small/mid-size/large BEVs, and 21, 10, and 15 kWh for small/mid-size/large PHEVs.We use two EV fleet scenarios until 2030 from the IEA: the stated policies (STEP) scenario and the sustainable development (SD) scenario. We further extend these two scenarios to 2050 based on a review of EV projections until 2050. We use the EV fleet share across 5 main EV markets (China, India, EU, US, and RoW) from the IEA until 2030, and keep the EV fleet share by countries/regions in 2030-2050 the same as the year 2030 due to lack of reliable data after 2030 (see Supplementary Data 1 for EV fleet scenarios by countries/regions). Further, we include 56 cities in China, 9 cities in India, 32 cities in EU, 53 cities in US, and 9 cities in RoW. We compile future EV sales share among 159 cities globally in STEP scenario and SD scenario based on future EV fleet projections by counties/regions from the IEA25 and other data sources49,50 (see Supplementary Data 1).We consider battery market shares by chemistry based on the market share projections until 2030 from Avicenne Energy51 a specialist consulting firm, and potential trends until 2050 from battery technology roadmaps52,53,54 and commercial activities55,56. Current battery technology roadmaps issued by the US52, EU53, and China54 focus on the development of high-energy Lithium Nickel Cobalt Manganese Oxide (transition to low cobalt and high nickel content) and Lithium Nickel Cobalt Aluminum-based chemistries. NCM and NCA batteries will likely make up the majority of next-generation EV Lithium-ion batteries. Future battery chemistry is uncertain after 2030. Existing Lithium Iron Phosphate batteries could also dominate the EV market, as indicated by recent commercial activities55,56. LFP battery manufacturers intend to improve the specific energy of LFP batteries to compete with NCM batteries45. Large-scale deployments of LFP may help avoid potential material supply shortage and price spikes associated with NCM and NCA batteries27. To encompass these market uncertainties, two battery chemistry scenarios are developed, including an NCX scenario (with X representing Manganese or Aluminum), and an LFP scenario. The market shares of NCX and LFP are assumed to reach 98% and 2% in the NCX path by 2050, and 40% and 60% in the LFP path (see Supplementary Fig. 3 for detailed battery market shares by chemistry in two scenarios).EV use modelWe use the daily driving distance (DDD) of EVs based on data from Spritmonitor.de24, an online quality-controlled, crowd-sourced database containing detailed real-world information on distances traveled, fuel consumption, and corresponding costs. It is widely used in the literature, including for estimation of the environmental impacts of vehicles57 and the CO2 mitigation potential of EVs58. We build historical DDD distributions for small/mid-size/large BEVs/PHEVs models, and explore the EV driving behavior of each EV model based on the corresponding DDD distributions. Please see the DDD distributions of each EV model in Supplementary Data 1. Note DDDs less than 5 km are excluded.Further, we compile future DDD in different countries/regions (Supplementary Figs. 29–32) by assuming the future DDD is proportional to the future energy consumption per vehicle. The future energy consumption per vehicle in different countries/regions is estimated by the total EV fleet energy consumption divided by future EV fleet size in each country/region, which are both projected by the IEA25.By comparing various DDDs in multiples of EV range, we classify 5 DDD classes to formulate driving intensity and charging behavior. These 5 classes divided between 0% of the EV range to 200% of the EV range (i.e., a DDD twice the range of the EV) with intervals of 0–25%, 25–33%, 33–50%, 50–100%, 100–200%. We use the mean DDD of each class for calculations.We assume two commuting trips between home and working place per day on weekdays and two entertaining trips on weekends for all countries/regions. Each trip distance is half of DDD. According to the required trip distance, we compile the driving cycle of each trip (speed versus time) based on the standard US combined driving cycle (i.e., 55% city driving and 45% highway driving, see details in Supplementary Figs. 5 and 6, and Supplementary Note 1).Charging behavior may be affected by charging infrastructure, amongst others, on-board EV charger, consumer preferences. We assume an immediate and slow home charging at constant charging power to full charge for all EV sizes and types because home charging is the major charging way (see Supplementary Data 1). We assume the home charging power as 1.92, 6.6, 22, and 1.92 kW for small, mid-size, large BEV, and PHEV, respectively59. We assume that due to high costs and limited utility no consumers will install higher power charging infrastructure at home. We further anticipate the charging behaviors in terms of changing frequency by comparing the various DDDs in multiples of the EV range. As driving intensity increases, the higher charging frequency is assumed for 5 DDD classes (1x every four days, 1x every three days, 1x every two days, 1x each day, and 2x every day respectively). For example, if the DDD of mid-size BEV (with a 312 km EV range) increases from 75 km to 625 km, and the battery needs to be charged more frequently from 1 time per four days to 2 times per day.We calculate battery SoC under three EV states: driving, charging, and parked. For the battery SoC during driving, we use FASTSim model59, Future Automotive Systems Technology Simulator developed by National Renewable Energy Laboratory (NREL), to calculate EV battery SoC second-by-second. The model inputs include the EV driving cycle, EV configurations, and battery performance parameters (specific energy and battery capacity). We select one representative EV model from the FASTSim model59 for each EV size and type as EV configuration (Supplementary Table 2), and NCM622 as a representative chemistry for all EV types; because it was found that EV configurations and battery performance parameters (such as specific energy) had small effects on the resulting battery SoC simulations. For battery SoC during charging, we assume the battery SoC increases linearly under a constant charging power with a 90% charging efficiency60. If an EV is parked, the SoC of the battery is slowly decreasing due to losses caused by battery self-discharging. A typical self-discharging rate of 5% per month is assumed for lithium-ion battery61. Self-discharging occurs due to parasitic chemical reactions that consume active lithium and form electrochemically inactive species while lithium-ion batteries are at rest. These parasitic reactions both reduce the SoC of the cell, and also reduce the total amount of lithium available for cycling. The impact of self-discharge on the SoH of NCM and LFP batteries is captured in the battery degradation model we use. Note that for the sake of battery safety, a portion of battery capacity is unusable (15% for BEVs and 30% for PHEVs based on the BatPac model62), therefore we assume the usable SoC range as 5%-90% for BEV battery and 15%-85% for PHEV battery.The battery temperature depends on the heat generation from chemical reactions inside batteries, amongst others, ambient temperature and environment (such as solar power radiation), battery management system (air or liquid cooling system to control battery temperature). The temperature can also vary from cell to cell, module to module, and component to component in the battery pack. The modelling of battery temperature is complicated and out of scope of this study. Here we use city ambient temperature to represent battery temperature, which is then used to calculate battery degradation. The main justification for this simplification is that the degradation for most consumer vehicles is dominated by calendar aging effects, as light-duty vehicles are only driven for a relatively small fraction of time throughout their life. For higher vehicle utilisation, neglecting battery pack thermal management in the degradation model will generally result in worse battery lifetimes, leading to a conservative estimate of electric vehicle lifetime. As such our modelling suggests a conservative lower bound of the potential for EV batteries to supply short-term storage facilities. Here, we use monthly average temperature of total 159 cities to capture the effects of geographic and temporal temperature variance on battery degradation. The temperature data is collected from63,64,65,66, can be found in Supplementary Data 1.Battery degradation modelBattery degradation is crucially important for determining EV battery capacity both in use and for second-life applications, but there are still many open research questions surrounding the importance of EV driving habits, charging behavior, and battery chemistries on capacity development67. Degradation model approaches include physics-based degradation models68 as well as machine learning models69,70 though there is no agreed-upon best practice71. Here, to balance the complexity and accuracy of the battery degradation model, we develop a semiempirical battery degradation model based on method from16. The model considers both calendar life and cycle life aging (Eq. (1)), assuming a square-root dependence on time for calendar life (degradation rates depend on temperature and SoC, see Eq. (2)) and a linear dependence on energy throughput for cycle life (degradation rates depend on temperature, Depth-of-Discharge (DoD), and Current rate (Crate) see Eq. (3)).$$q=1-{q}_{{{{{{\rm{Loss}}}}}},{{{{{\rm{Calendar}}}}}}}-{q}_{{{{{{\rm{Loss}}}}}},{{{{{\rm{Cycling}}}}}}}$$
                    (1)
                $${q}_{{{{{{\rm{Loss}}}}}},{{{{{\rm{Calendar}}}}}}}={k}_{{Cal}}\cdot \exp \left(\frac{-{E}_{a}}{{{{{{\bf{R}}}}}}T}\left(\frac{1}{T}-\frac{1}{{{{{{{\bf{T}}}}}}}_{{{{{{\bf{ref}}}}}}}}\right)\right)\cdot \exp \left(\frac{\alpha {{{{{\bf{F}}}}}}}{{{{{{\bf{R}}}}}}}\left(\frac{{U}_{a}}{T}-\frac{{U}_{{a},{ref}}}{{{{{{{\bf{T}}}}}}}_{{{{{{\bf{ref}}}}}}}}\right)\right)\cdot \sqrt{t}$$
                    (2)
                $${q}_{{Loss},{Cycling}}={k}_{{Cyc}}\cdot (A\cdot {DOD}+B)\cdot (C\cdot {C}_{{rate}}+D)\cdot (G\cdot {(T-{{{{{{\bf{T}}}}}}}_{{{{{{\bf{ref}}}}}}})}^{2}+H)\cdot {EFC}$$
                    (3)
                where q is the relative battery degradation, qLoss, Calendar is the relative calendar life degradation, qLoss, Cycling is the relative cycling life degradation, T is temperature, t is time (unit: days), EFC is equivalent full cycles. Note R is the universal gas constant (8.3144598 J mol-1 K-1), Tref is the reference temperature (298.15 K), F is Faraday constant (96485 C mol-1), kCal (unit: days0.5), Ea (unit: J mol-1 K-1), and α (no unit) are fitting parameters for calendar life degradation, and kCyc (unit: EFC-1). A, B, C, D, G, and H (no units) are fitting parameters for cycling life degradation. The value of the anode-to-reference potential, Ua (unit: V), is calculated from the storage SoC using the Eqs. (4) and (5)72.$${U}_{a}({{x}}_{a})=	0.6379+0.5416\cdot \exp (-305.5309\cdot {x}_{a})+0.044\cdot \,\tanh \left(-\frac{{x}_{a}-0.1958}{0.1088}\right)\,\\ 	 -0.1978\cdot \,\tanh \left(\frac{{x}_{a}-1.0571}{0.0854}\right)-0.6875\cdot \,\tanh \left(\frac{{x}_{a}+0.0117}{0.0529}\right) \\ 	 -0.0175\cdot \,\tanh \left(\frac{{x}_{a}-0.5692}{0.0875}\right)$$
                    (4)
                where xa, which represents the lithiation fraction of the graphite, is a simple linear function of the SoC:73$${x}_{a}({SOC})\,=\,{x}_{{a},{0}}+{SOC}\cdot ({x}_{{a},{100}}-{{x}}_{{a},{0}})$$
                    (5)
                where xa, 0 is the lithiation fraction of the graphite at 0% SoC and xa, 100 is the lithiation fraction of the graphite at 100% SoC. xa, 0 equals to 0.0085, and xa, 100 equals to 0.78.To obtain these fitting parameters, we collect publicly available battery degradation data, including calendar life aging and cycle life aging, for NCM16 and LFP14,15,17 chemistry. These data sets represent state-of-the-art lifetime performance for each chemistry; the LFP cells shown reach between 5000 and 8000 equivalent full cycles before reaching 80% remaining capacity, 4000~5000 equivalent full cycles for NCM cells. This experimental data was then fit with the semiempirical model Eqs. (1), (2), and (3) using a non-linear least squares solver in MATLAB. The NCM model has no Crate dependence, due to lack of data in the aging data set, so the parameters C and D are simply set at 0 and 1. We first fit the calendar fade data with the time-dependent portion of the model (qLoss, Calendar, parameters kCal, Ea, and α); the parameter α is bounded between -1 and 1, with other parameters unbounded. The parameters for the cycling fade (A, B, C, and D) are optimized on the cycling aging data. For both LFP and NCM, the raw cycling fade data was processed prior to optimizing a model based on expert judgement. For LFP, only cells with linear fade trajectories and data for at least 5000 EFCs were used for model optimization. For NCM, only data after 200 EFC at T > 5 °C and data at q < 0.85 at T < 5 °C was used for the optimization of the NCM cycling model parameters. The optimized parameters for the LFP and NCM degradation models are shown in Supplementary Table 3. Fitting results are shown in Supplementary Fig. 33 and degradation rates are shown in Supplementary Fig. 34.Note that we assume NCA battery has the same degradation patterns as NCM battery due to a lack of state-of-the-art open-source data for NCA batteries. Besides cell chemistry, capacity degradation characteristics vary with cell design, manufacturing process, and proprietary additives67,74, which is out of scope of this study. We use cell degradation patterns to represent battery pack degradation without consideration of cell-to-cell and module-module differences.For simulation of the degradation under the EV driving loads (battery SoC evolution over time) and during dynamic temperature changes, the degradation model is reformulated to solve for the degradation occurring during consecutive timesteps15. We choose a timestep of 1 day for making SoH updates and update the SoC timeseries for each day by the current SoH. At each timestep, the temperature is the average temperature during the simulation month at city from different countries/regions. Average SoC, DoD, Crate, and the number of EFCs is extracted from the SoC timeseries. Average SoC refers to the time-averaged value of SoC. DoD is the difference between the maximum and minimum values of SoC. Crate is calculated using the absolute change of SoC per second, and then taking the average of all Crates greater than 0 during the entire timeseries. The number of EFCs is calculated by summing the changes to SoC over the timeseries. Dependence of the expected degradation rate on current SoH is incorporated by calculating a ‘virtual time’15. The virtual time is found by inverting the calendar degradation equation to solve for time:$${t}_{{virtual}}={\left({q}_{{Current}}/{k}_{{Cal}}\cdot \exp \left(\frac{-{E}_{a}}{{{{{{\bf{R}}}}}}T}\cdot \left(\frac{1}{T}-\frac{1}{{{{{{{\bf{T}}}}}}}_{{{{{{\bf{ref}}}}}}}}\right)\right)\cdot \exp \left(\frac{\alpha {{{{{\bf{F}}}}}}}{{{{{{\bf{R}}}}}}}\cdot \left(\frac{{U}_{a}}{T}-\frac{{U}_{{a},{ref}}}{{{{{{{\bf{T}}}}}}}_{{{{{{\bf{ref}}}}}}}}\right)\right)\right)}^{2}$$
                    (6)
                The degradation change ∆q during any given timestep Δt is then calculated by the following equation:$${\Delta }q=	\left({k}_{{{{{{\rm{Cal}}}}}}}\cdot \exp \left(\frac{-{E}_{a}}{{{{{{\bf{R}}}}}}T}\cdot \left(\frac{1}{T}-\frac{1}{{{{{{{\bf{T}}}}}}}_{{{{{{\bf{ref}}}}}}}}\right)\right)\cdot \exp \left(\frac{\alpha {{{{{\bf{F}}}}}}}{{{{{{\bf{R}}}}}}}\cdot \left(\frac{{U}_{a}}{T}-\frac{{U}_{{a},{ref}}}{{{{{{{\bf{T}}}}}}}_{{{{{{\bf{ref}}}}}}}}\right)\right)/2 \right. \\ 	 \left. \hskip 2pt \cdot \sqrt{{t}_{{virtual}}+{\Delta }t}\right)\cdot {\Delta }t+{k}_{{Cyc}}\cdot (A\cdot {DOD}+B)\cdot (C\cdot {C}_{{rate}}+D) \\ 	 \cdot (G\cdot {(T-{{{{{{\bf{T}}}}}}}_{{{{{{\bf{ref}}}}}}})}^{2}+H)\cdot {\Delta }EFC$$
                    (7)
                For cycling fade, the virtual EFC does not need to be calculated, as the degradation rate is constant with respect to the change of EFC during any given timestep. This reformulation of the degradation model captures the path-dependent degradation observed in real-world battery use. See Supplementary Note 2 for modelled battery degradation for NCM and LFP.Available capacity from EV batteriesVehicle EoL does not necessarily correspond to battery EoL. With technological improvements in battery reliability and durability, many batteries in EoL vehicles may still have years of useful life at the end of vehicle end of life. Vehicle battery EoL is usually as defined the time at which remaining battery capacity is between 70 and 80% of the original capacity7. We assume an EV lifespan distribution, used in our previous work27, to account for EoL of EV. In our modelling approach, the vehicle lifespan distribution determines when batteries are not used in EVs any more (i.e., retired batteries). Retired batteries may have quite different capacity under different use conditions. When vehicles reach EoL due to consumer choices or other issues before the battery pack reaches 70% relative capacity, retired batteries will still have over 70% relative SoH and are assumed to be used in a second-life application. When battery pack reaches 70% relative SoH before a vehicle reaches its EoL, we assume that batteries may be still be used in EVs for low distances-driving. Retired batteries from such vehicles will have lower than 70% relative SoH and are assumed to be recycled rather than for a second-use. We assume any battery with a relative SoH lower than 60% is recycled and removed from potential grid storage capacity75. However, even batteries with a relative SoH of 60–70% have a limited economic value and can have relatively high safety risks. (methods)32.We define technical vehicle-to-grid capacity as the availability of EV battery stock capacity for vehicle-to-grid application, considering the capacity reserved for EV driving, the capacity of PHEVs that will not participate in vehicle-to-grid due to low capacity, and capacity fade due to battery degradation. We further define the actual vehicle-to-grid capacity as the availability of technical vehicle-to-grid capacity for the grid under different consumer participation rates in the vehicle-to-grid business. Results focus on investigating under which participation rate can actual vehicle-to-grid capacity meet grid storage demand.The technical second-use capacity is defined as the retired batteries capacity that can be repurposed (i.e., retired batteries with over 70% relative SoH). We further investigate actual second-use capacity under different utilisation rates (i.e., not all retired batteries will be deployed in second-use). The results are intended to determine the required utilisation rate for the second-use battery to meet grid storage demand.We investigate the real-world capacity as a function of both vehicle-to-grid participation rate and second-use utilisation rates. We further analyze the market participation rates and utilisation rates that are required to meet short-term grid storage demand globally.Impact of battery capacity assumptionsThe model is highly influenced by the battery capacity per vehicle. Therefore, we conduct a sensitivity analysis of battery capacity per vehicle by assuming all BEVs are small BEVs equipped with a battery with a capacity of 33 kWh. This assumption is based on three arguments: first, small BEVs could provide most of the daily driving demand for consumers, even though they have a lower driving range than large BEVs equipped with a high-capacity battery. Second, the development of widespread EV charging infrastructure, including fast charging technology, could help to overcome the range anxiety of small BEV owners. Third, the increasing use of small BEVs would reduce demand for batteries and materials, along with lowering embodied GHG emissions of those batteries.


Data availability
The datasets, including EV fleet size by country, EV sales share by cities, and battery chemistry share, are all deposited in an Excel file (https://doi.org/10.6084/m9.figshare.21542472.v1). These raw data are used for the dynamic battery stock model for quantifying future battery flows. Please see the dynamic battery stock model from this link (https://doi.org/10.6084/m9.figshare.13042001.v4). City ambient temperature and its effects on battery degradation are also deposited in the Excel file, while the code for estimating battery degradation, which is under privacy and license, is available upon reasonable request.
ReferencesWorld Energy Outlook 2020 (IEA, 2020). https://www.iea.org/reports/world-energy-outlook-2020Global Renewables Outlook: Energy transformation 2050 (International Renewable Energy Agency, 2020). https://www.irena.org/publications/2020/Apr/Global-Renewables-Outlook-2020Lund, P. D., Lindgren, J., Mikkola, J. & Salpakari, J. Review of energy system flexibility measures to enable high levels of variable renewable electricity. Renew. Sust. Energ. Rev. 45, 785–807 (2015).Article 
    
                    Google Scholar 
                Palensky, P. & Dietrich, D. Demand Side Management: Demand Response, Intelligent Energy Systems, and Smart Loads. IEEE Trans. Ind. Inform. 7, 381–388 (2011).Article 
    
                    Google Scholar 
                Brown, T., Schlachtberger, D., Kies, A., Schramm, S. & Greiner, M. Synergies of sector coupling and transmission reinforcement in a cost-optimised, highly renewable European energy system. Energy 160, 720–739 (2018).Article 
    
                    Google Scholar 
                Guille, C. & Gross, G. A conceptual framework for the vehicle-to-grid (V2G) implementation. Energy Policy 37, 4379–4390 (2009).Article 
    
                    Google Scholar 
                Identifying and Overcoming Critical Barriers to Widespread Second Use of PEV Batteries (National Renewable Energy Lab, 2015). https://www.osti.gov/biblio/1171780Haram, M. H. S. M. et al. Feasibility of utilising second life EV batteries: Applications, lifespan, economics, environmental impact, assessment, and challenges. Alex. Eng. J. 60, 4517–4536 (2021).Article 
    
                    Google Scholar 
                Sovacool, B. K., Axsen, J. & Kempton, W. The Future Promise of Vehicle-to-Grid (V2G) Integration: A Sociotechnical Review and Research Agenda. Annu. Rev. Environ. Resour. 42, 377–406 (2017).Article 
    
                    Google Scholar 
                Sovacool, B. K., Noel, L., Axsen, J. & Kempton, W. The neglected social dimensions to a vehicle-to-grid (V2G) transition: a critical and systematic review. Environ. Res. Lett. 13, 013001 (2018).Article 
    ADS 
    
                    Google Scholar 
                Ralon, P., Taylor, M., Ilas, A., Diaz-Bone, H. & Kairies, K. Electricity storage and renewables: Costs and markets to 2030. International Renewable Energy Agency: Abu Dhabi, UAE (2017).Global EV Outlook 2020 (IEA, 2020). https://www.iea.org/reports/global-ev-outlook-2020Second-life EV batteries: The newest value pool in energy storage (McKinsey & Company, 2019). https://www.mckinsey.com/industries/automotive-and-assembly/our-insights/second-life-ev-batteries-the-newest-value-pool-in-energy-storageBaghdadi, I., Briat, O., Delétage, J.-Y., Gyan, P. & Vinassa, J.-M. Lithium battery aging model based on Dakin’s degradation approach. J. Power Sources 325, 273–285 (2016).Article 
    ADS 
    CAS 
    
                    Google Scholar 
                Naumann, M., Spingler, F. B. & Jossen, A. Analysis and modeling of cycle aging of a commercial LiFePO4/graphite cell. J. Power Sources 451, 227666 (2020).Article 
    CAS 
    
                    Google Scholar 
                Smith, K. et al. Life prediction model for grid-connected Li-ion battery energy storage system. in 2017 American Control Conference (ACC) 4062-4068.Naumann, M., Schimpe, M., Keil, P., Hesse, H. C. & Jossen, A. Analysis and modeling of calendar aging of a commercial LiFePO4/graphite cell. J. Energy Storage 17, 153–169 (2018).Article 
    
                    Google Scholar 
                Guerra, O. J. Beyond short-duration energy storage. Nat. Energy 6, 460–461 (2021).Article 
    ADS 
    
                    Google Scholar 
                Energy Storage Grand Challenge: Energy Storage Market Report (U.S. Department of Energy, 2020). https://www.energy.gov/sites/prod/files/2020/12/f81/Energy%20Storage%20Market%20Report%202020_0.pdf.The Potential for Battery Energy Storage to Provide Peaking Capacity in the United States (NREL, 2019). https://www.nrel.gov/docs/fy19osti/74184.pdfStorage Futures Study: Economic Potential of Diurnal Storage in the U.S. Power Sector (National Renewable Energy Laboratory, 2021). https://www.nrel.gov/docs/fy21osti/77449.pdf.Energy Storage Technology and Cost Characterization Report. https://www.osti.gov/servlets/purl/1573487 (2019).Electric Insights Quarterly (Drax, 2019). https://www.drax.com/wp-content/uploads/2019/12/191202_Drax_Q3_Report.pdf.MPG and Cost Calculator and Tracker (Spritmonitor, 2020). https://www.spritmonitor.de/en/.Global EV Data Explorer (IEA, 2021). https://www.iea.org/articles/global-ev-data-explorer.Thompson, A. W. Economic implications of lithium ion battery degradation for Vehicle-to-Grid (V2X) services. J. Power Sources 396, 691–709 (2018).Article 
    ADS 
    CAS 
    
                    Google Scholar 
                Xu, C. et al. Future material demand for automotive lithium-based batteries. Commun. Mater. 1, 99 (2020).Article 
    
                    Google Scholar 
                Saxena, S., Le Floch, C., MacDonald, J. & Moura, S. Quantifying EV battery end-of-life through analysis of travel needs with vehicle powertrain models. J. Power Sources 282, 265–276 (2015).Article 
    ADS 
    CAS 
    
                    Google Scholar 
                Harper, G. et al. Recycling lithium-ion batteries from electric vehicles. Nature 575, 75–86 (2019).Article 
    ADS 
    CAS 
    
                    Google Scholar 
                Gaines, L. The future of automotive lithium-ion battery recycling: Charting a sustainable course. Sustain. Mater. Technol. 1-2, 2–7 (2014).
                    Google Scholar 
                DeRousseau, M., Gully, B., Taylor, C., Apelian, D. & Wang, Y. Repurposing Used Electric Car Batteries: A Review of Options. JOM 69, 1575–1582 (2017).Article 
    
                    Google Scholar 
                Neubauer, J., Pesaran, A., Williams, B., Ferry, M. & Eyer, J. Techno-Economic Analysis of PEV Battery Second Use: Repurposed-Battery Selling Price and Commercial and Industrial End-User Value. (Sponsor Org.: USDOE Office of Energy Efficiency and Renewable Energy (EERE), Vehicle Technologies Office (EE-3V)).Reinhardt, R., Christodoulou, I. & Gassó-Domingo, S. & Amante García, B. Towards sustainable business models for electric vehicle battery second use: A critical review. J. Environ. Manag. 245, 432–446 (2019).Article 
    
                    Google Scholar 
                Casals, L. C., Amante García, B. & Canal, C. Second life batteries lifespan: Rest of useful life and environmental analysis. J. Environ. Manag. 232, 354–363 (2019).Article 
    
                    Google Scholar 
                Podias, A. et al. Sustainability Assessment of Second Use Applications of Automotive Batteries: Ageing of Li-Ion Battery Cells in Automotive and Grid-Scale Applications. World Electr. Veh. J. 9 (2018).The Present & Future Of Vehicle-To-Grid Technology (CleanTechnica, 2020). https://cleantechnica.com/2020/09/05/the-present-future-of-vehicle-to-grid-technology/Zonneveld, J. A. Increasing participation in V2G through contract elements: Examining the preferences of Dutch EV users regarding V2G contracts using a stated choice experiment. (2019).Anwar, M. B. et al. Assessing the value of electric vehicle managed charging: a review of methodologies and results. Energy Environ. Sci. (2022).A Vision for a Sustainable Battery Value Chain in 2030 (2019). https://www.globalbattery.org/media/publications/WEF_A_Vision_for_a_Sustainable_Battery_Value_Chain_in_2030_Report.pdf.Bai, Y. et al. Energy and environmental aspects in recycling lithium-ion batteries: Concept of Battery Identity Global Passport. Mater. Today 41, 304–315 (2020).Article 
    CAS 
    
                    Google Scholar 
                Sapunkov, O., Pande, V., Khetan, A., Choomwattana, C. & Viswanathan, V. Quantifying the promise of ‘beyond’ Li–ion batteries. Transl. Mater. Res. 2, 045002 (2015).Article 
    
                    Google Scholar 
                Srdic, S. & Lukic, S. Toward Extreme Fast Charging: Challenges and Opportunities in Directly Connecting to Medium-Voltage Line. IEEE Electrif. Mag. 7, 22–31 (2019).Article 
    
                    Google Scholar 
                What can 6,000 electric vehicles tell us about EV battery health? (GEOTAB, 2020). https://www.geotab.com/blog/ev-battery-health/Park, Y.-U. et al. A New High-Energy Cathode for a Na-Ion Battery with Ultrahigh Stability. J. Am. Chem. Soc. 135, 13870–13878 (2013).Article 
    CAS 
    
                    Google Scholar 
                Yang, X.-G., Liu, T. & Wang, C.-Y. Thermally modulated lithium iron phosphate batteries for mass-market electric vehicles. Nat. Energy 6, 176–185 (2021).Article 
    ADS 
    CAS 
    
                    Google Scholar 
                Alam, M. et al. Real-Time Smart Parking Systems Integration in Distributed ITS for Smart Cities. J. Adv. Transp. 2018, 1485652 (2018).
                    Google Scholar 
                Giordano, V. & Fulli, G. A business case for Smart Grid technologies: A systemic perspective. Energy Policy 40, 252–259 (2012).
                    Google Scholar 
                Zheng, Y. et al. Electric Vehicle Battery Charging/Swap Stations in Distribution Systems: Comparison Study and Optimal Planning. IEEE Trans. Power Syst. 29, 221–229 (2014).Article 
    ADS 
    
                    Google Scholar 
                Electric vehicle capitals: Cities aim for all-electric mobility (The International Council on Clean Transportation, 2020). https://theicct.org/publications/electric-vehicle-capitals-update-sept2020Electric Vehicle Industry in India: Why Foreign Investors Should Pay Attention (India Briefing, 2021). https://www.india-briefing.com/news/electric-vehicle-industry-in-india-why-foreign-investors-should-pay-attention-21872.html/The Rechargeable Battery Market and Main Trends 2018-2030 (Avicenne Energy, 2019). https://www.bpifrance.fr/content/download/76854/831358/file/02%20-%20Presentation%20Avicenne%20-%20Christophe%20Pillot%20-%2028%20Mai%202019.pdfElectrochemical energy storage technical team roadmap (USDRIVE, 2017). https://www.energy.gov/sites/prod/files/2017/11/f39/EESTT%20roadmap%202017-10-16%20Final.pdfInventing the sustainable batteries of the future (BATTERY 2030+, 2020). https://battery2030.eu/digitalAssets/816/c_816048-l_1-k_roadmap-27-march.pdfChen, K., Zhao, F., Hao, H. & Liu, Z. Selection of lithium-ion battery technologies for electric vehicles under China’s new energy vehicle credit regulation. Energy Procedia 158, 3038–3044 (2019).Article 
    
                    Google Scholar 
                Tesla wins China approval to build Model 3 vehicles with LFP batteries: ministry - Reuters (Reuters, 2020).LFP chemistry is emerging as the future of batteries (Clean Future, 2020).Plötz, P., Funke, S. Á. & Jochem, P. Empirical Fuel Consumption and CO2 Emissions of Plug-In Hybrid Electric Vehicles. J. Ind. Ecol. 22, 773–784 (2018).Article 
    
                    Google Scholar 
                Plötz, P., Funke, S. A., Jochem, P. & Wietschel, M. CO2 Mitigation Potential of Plug-in Hybrid Electric Vehicles larger than expected. Sci. Rep. 7, 16493 (2017).Article 
    ADS 
    
                    Google Scholar 
                Brooker, A. et al. FASTSim: A Model to Estimate Vehicle Efficiency, Cost and Performance. SAE Technical Paper (2015).Mitsubishi i-Miev charging cost and time calculator (EVcompare.io, 2009). https://evcompare.io/cars/mitsubishi/mitsubishi_i-miev/charging/What does Elevated Self-discharge Do? (Battery University, 2011). https://batteryuniversity.com/learn/article/elevating_self_dischargeBatPaC: Battery Manufacturing Cost Estimation (Argonne National Laboratory, 2021). https://www.anl.gov/partnerships/batpac-battery-manufacturing-cost-estimationClimate Data Online: Dataset Discovery (National Oceanic and Atmospheric Administration, 2021). https://www.ncdc.noaa.gov/cdo-web/datasets.CLIMATE DATA FOR CITIES WORLDWIDE (CLIMATE-DATA.ORG, 2021). https://en.climate-data.org/Climates for travelers. Information on the climates in the world to plan a trip (Climates to travel, 2021). https://www.climatestotravel.com/Climate Zone Finder (Weather and Climate, 2021). https://tcktcktck.org/Uddin, K., Dubarry, M. & Glick, M. B. The viability of vehicle-to-grid operations from a battery technology and policy perspective. Energy Policy 113, 342–347 (2018).Article 
    
                    Google Scholar 
                Safari, M., Morcrette, M., Teyssot, A. & Delacourt, C. Multimodal Physics-Based Aging Model for Life Prediction of Li-Ion Batteries. J. Electrochem. Soc. 156, A145 (2009).Article 
    CAS 
    
                    Google Scholar 
                Zhang, Y. et al. Identifying degradation patterns of lithium ion batteries from impedance spectroscopy using machine learning. Nat. Commun. 11, 1706 (2020).Article 
    ADS 
    CAS 
    
                    Google Scholar 
                Severson, K. A. et al. Data-driven prediction of battery cycle life before capacity degradation. Nat. Energy 4, 383–391 (2019).Article 
    ADS 
    
                    Google Scholar 
                Edge, J. S. et al. Lithium ion battery degradation: what you need to know. Phys. Chem. Chem. Phys. 23, 8200–8221 (2021).Article 
    CAS 
    
                    Google Scholar 
                Safari, M. & Delacourt, C. Modeling of a Commercial Graphite/LiFePO4 Cell. J. Electrochem. Soc. 158, A562 (2011).Article 
    CAS 
    
                    Google Scholar 
                Schimpe, M. et al. Comprehensive Modeling of Temperature-Dependent Degradation Mechanisms in Lithium Iron Phosphate Batteries. J. Electrochem. Soc. 165, A181–A193 (2018).Article 
    CAS 
    
                    Google Scholar 
                Peterson, S. B., Apt, J. & Whitacre, J. F. Lithium-ion battery cell degradation resulting from realistic vehicle and vehicle-to-grid utilization. J. Power Sources 195, 2385–2392 (2010).Article 
    ADS 
    CAS 
    
                    Google Scholar 
                Martinez-Laserna, E. et al. Technical Viability of Battery Second Life: A Study From the Ageing Perspective. IEEE Trans. Ind. Appl. 54, 2703–2713 (2018).Article 
    CAS 
    
                    Google Scholar 
                Download referencesAcknowledgementsC.X. acknowledges the China scholarship for financial support as well as Haixiang Lin and Gjalt Huppes for their useful recommendations to the study. P.G. and K.S. are supported by the National Renewable Energy Laboratory which is operated by Alliance for Sustainable Energy, LLC, for the U.S. Department of Energy under Contract No. DE-AC36-08GO28308, and acknowledge support from the Assistant Secretary for Energy Efficiency and Renewable Energy, Office of Vehicle Technologies of the U.S. Department of Energy. The views expressed in the article do not necessarily represent the views of the DOE or the U.S. Government. The U.S. Government retains and the publisher, by accepting the article for publication, acknowledges that the U.S. Government retains a nonexclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of this work, or allow others to do so, for U.S. Government purposes.Author informationAuthors and AffiliationsInstitute of Environmental Sciences (CML), Leiden University, 2300, RA Leiden, The NetherlandsChengjian Xu, Paul Behrens, Mingming Hu, Arnold Tukker & Bernhard SteubingNational Renewable Energy Lab, 15013, Denver West Parkway, Golden, CO, USAPaul Gasper & Kandler SmithNetherlands Organisation for Applied Scientific Research TNO, 2595 DA, Den Haag, NetherlandsArnold TukkerAuthorsChengjian XuView author publicationsYou can also search for this author in
                        PubMed Google ScholarPaul BehrensView author publicationsYou can also search for this author in
                        PubMed Google ScholarPaul GasperView author publicationsYou can also search for this author in
                        PubMed Google ScholarKandler SmithView author publicationsYou can also search for this author in
                        PubMed Google ScholarMingming HuView author publicationsYou can also search for this author in
                        PubMed Google ScholarArnold TukkerView author publicationsYou can also search for this author in
                        PubMed Google ScholarBernhard SteubingView author publicationsYou can also search for this author in
                        PubMed Google ScholarContributionsC.X. designed and conducted the research with valuable inputs from B.S., P.B., A.T., M. H., as well as P. G. and K. S. C.X. wrote the manuscript with the help of P.B., A.T., B.S., P. G., and other authors. P.B., A.T., and B.S. contribute significantly to the structure of research results and scientific writing of this research. P. G. and K. S. developed the battery degradation model, and further provided technical inputs on how to integrate the degradation model into the analysis of the results.Corresponding authorCorrespondence to
                Chengjian Xu.Ethics declarations
Competing interests
The authors declare no competing interests.
Peer review
Peer review information
Nature Communications thanks I-Yun Lisa Hsieh, Rahmat Khezri and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Peer reviewer reports are available.
Additional informationPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Supplementary informationSupplementary InformationPeer Review FileDescription of Additional Supplementary FilesSupplementary Data 1Rights and permissions
Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.
Reprints and PermissionsAbout this articleCite this articleXu, C., Behrens, P., Gasper, P. et al. Electric vehicle batteries alone could satisfy short-term grid storage demand by as early as 2030.
                    Nat Commun 14, 119 (2023). https://doi.org/10.1038/s41467-022-35393-0Download citationReceived: 07 April 2022Accepted: 30 November 2022Published: 17 January 2023DOI: https://doi.org/10.1038/s41467-022-35393-0Share this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        

CommentsBy submitting a comment you agree to abide by our Terms and Community Guidelines. If you find something abusive or that does not comply with our terms or guidelines please flag it as inappropriate.







"
https://news.ycombinator.com/rss,Stadia Bluetooth Mode,https://stadia.google.com/controller/,Comments,"Play wirelessly with BluetoothSwitch your Stadia Controller to Bluetooth mode to keep gaming wirelessly on your favorite devices and services after Stadia shuts downSwitch to Bluetooth modearrow_right_altSwitching to Bluetooth mode downloads a software updateSystem requirements: Chrome 108 or newerImportant things to knowtoggle_onSwitching is permanentOnce you switch your controller to Bluetooth mode, you can’t change it back to use Wi-Fi on Stadia. You can still play wired with USB in Bluetooth mode.Switch to Bluetooth modearrow_right_altcalendar_todayAvailable until December 31, 2023You can switch to Bluetooth mode, check the controller mode, and check for Bluetooth updates until Dec 31, 2023.Go to mode selection pagearrow_right_altFind the answers you needHow do I pair my controller using Bluetooth?expand_moreexpand_lessAfter switching your controller into Bluetooth mode, press and hold the “Y + Stadia” buttons for 2 seconds to enter pairing mode. The status light will flash orange.

Then, go to the device you want to play on and pair your controller in the settings. Once paired and connected, the status light on the controller will turn solid white.

Congrats! You’re ready to start playing. The next time you turn on the controller, it should automatically connect to the last paired device.Learn how to use Bluetooth modeWhat devices are supported?expand_moreexpand_lessNot all Bluetooth devices are the same, so compatibility will vary. The Stadia Controller uses Bluetooth Low Energy connections, so some features, such as pass-through audio, aren’t possible wirelessly.

We’ve verified that the Stadia Controller works for gameplay with the list of supported devices. It hasn’t been tested with all Bluetooth device types, so it might not work with others.List of supported devicesIs audio supported in Bluetooth mode?expand_moreexpand_lessWhen using the controller wirelessly in Bluetooth mode, the 3.5mm port and USB port won’t be able to be used for headphones.

When using the controller wired with USB, you’ll be able to plug headphones into the controller’s 3.5mm port.Can I remap the Assistant and Capture buttons?expand_moreexpand_lessYes. By default, the Google Assistant and Capture buttons won’t do anything in Bluetooth mode until remapped. You can remap the buttons using button mapping services.How do I verify my controller’s mode?expand_moreexpand_lessYou can verify whether your controller is in Bluetooth mode by checking the controller mode. Doing this also checks for any updates to Bluetooth mode.Check controller modeI can’t complete a step. What should I do?expand_moreexpand_lessMake sure your controller is charged for at least 30 minutes, and that you’re using a USB data cable that can transfer data. The cable that came with your controller will work.Go to Bluetooth mode Help CenterMy controller isn’t working. What should I do?expand_moreexpand_lessMake sure your controller is charged for at least 30 minutes. Unplug your controller and hold the Stadia button for 10 seconds — this should reset the device to be playable again.

If this still doesn’t work, factory reset the controller by holding the Google Assistant and Capture buttons for 6 seconds — the controller should vibrate and the status light should flash.Go to Bluetooth mode Help CenterSomething else is wrong. Who do I contact?expand_moreexpand_lessUnfortunately, customer support is not available for Bluetooth mode. If you have any questions, please go to the help center article.Go to Bluetooth mode Help CenterThe Bluetooth® word mark and logos are registered trademarks owned by Bluetooth SIG, Inc. and any use of such marks by Google is under license. Other trademarks and trade names are those of their respective owners."
https://news.ycombinator.com/rss,"Microsoft to lay off 11,000 employees",https://www.reuters.com/technology/microsoft-cut-thousands-jobs-sky-news-2023-01-17/,Comments,"Technology2 minute readJanuary 17, 20238:20 PM UTCLast Updated  agoMicrosoft to cut thousands of jobs across divisions - reportsReuters[1/2] A Microsoft logo is seen in Los Angeles, California U.S. November 7, 2017. REUTERS/Lucy Nicholson12Jan 17 (Reuters) - Microsoft Corp (MSFT.O) plans to cut thousands of jobs with some roles expected to be eliminated in human resources and engineering divisions, according to media reports on Tuesday.The expected layoffs would be the latest in the U.S. technology sector, where companies including Amazon.com Inc (AMZN.O) and Meta Platforms Inc (META.O) have announced retrenchment exercises in response to slowing demand and a worsening global economic outlook.Microsoft's move could indicate that the tech sector may continue to shed jobs.""From a big picture perspective, another pending round of layoffs at Microsoft suggests the environment is not improving, and likely continues to worsen,"" Morningstar analyst Dan Romanoff said.U.K broadcaster Sky News reported, citing sources, that Microsoft plans to cut about 5% of its workforce, or about 11,000 roles.The company plans to cut jobs in a number of engineering divisions on Wednesday, Bloomberg News reported, according to a person familiar with the matter, while Insider reported that Microsoft could cut recruiting staff by as much as one-third.The cuts will be significantly larger than other rounds in the past year, the Bloomberg report said.Microsoft declined to comment on the reports.The company had 221,000 full-time employees, including 122,000 in the United States and 99,000 internationally, as of June 30, according to filings.Microsoft is under pressure to maintain growth rates at its cloud unit Azure, after several quarters of downturn in the personal computer market hurt Windows and devices sales.It had said in July last year that a small number of roles had been eliminated. In October, news site Axios reported that Microsoft had laid off under 1,000 employees across several divisions.Shares of Microsoft, which is set to report quarterly results on Jan. 24, were marginally higher in late afternoon trading.Reporting by Yuvraj Malik in Bengaluru; Editing by Maju Samuel and Sriraj KalluvilaOur Standards: The Thomson Reuters Trust Principles.Read NextTechnologycategoryTesla video promoting self-driving was staged, engineer testifies, article with image5:46 PM UTCTechnologycategoryApple rolls out Macbooks with new M2 chips in rare January launch, article with image7:05 PM UTCTechnologycategoryMicrosoft to expand ChatGPT access as OpenAI investment rumors swirl, article with image4:18 AM UTCTechnologycategoryDavos 2023: CEOs buzz about ChatGPT-style AI at World Economic Forum, article with image8:32 AM UTC"
https://news.ycombinator.com/rss,A new CT scan to detect and cure the commonest cause of high blood pressure,https://www.qmul.ac.uk/media/news/2023/smd/ten-minute-scan-enables-detection-and-cure-of-the-commonest-cause-of-high-blood-pressure.html,Comments,"

Breadcrumb

Queen Mary University of LondonNewsNews stories2023SMD






            Ten-minute scan enables detection and cure of the commonest cause of high blood pressure
        


Doctors at Queen Mary University of London and Barts Hospital, and Cambridge University Hospital, have led research using a new type of CT scan to light up tiny nodules in a hormone gland and cure high blood pressure by their removal. The nodules are discovered in one-in-twenty people with high blood pressure.






Published on: 
16 January 2023








Tweet













  Nurse taking patient's blood pressure. Credit: iStock.com

Published today in Nature Medicine, the research solves a 60-year problem of how to detect the hormone producing nodules without a difficult catheter study that is available in only a handful of hospitals, and often fails. The research also found that, when combined with a urine test, the scan detects a group of patients who come off all their blood pressure medicines after treatment.
128 people participated in the study of a new scan after doctors found that their Hypertension (high blood pressure) was caused by a steroid hormone, aldosterone. The scan found that in two thirds of patients with elevated aldosterone secretion, this is coming from a benign nodule in just one of the adrenal glands, which can then be safely removed. The scan uses a very short-acting dose of metomidate, a radioactive dye that sticks only to the aldosterone-producing nodule. The scan was as accurate as the old catheter test, but quick, painless and technically successful in every patient. Until now, the catheter test was unable to predict which patients would be completely cured of hypertension by surgical removal of the gland. By contrast, the combination of a ‘hot nodule’ on the scan and urine steroid test detected 18 of the 24 patients who achieved a normal blood pressure off all their drugs.
The research, conducted on patients at Barts Hospital, Cambridge University Hospital, and Guy’s and St Thomas’s, and Universities of Glasgow and Birmingham, was funded by the National Institute for Health and Care Research (NIHR) and Medical Research Council (MRC) partnership, Barts Charity, and the British Heart Foundation.
Professor Morris Brown, co-senior author of the study and Professor of Endocrine Hypertension at Queen Mary University of London, said: “These aldosterone-producing nodules are very small and easily overlooked on a regular CT scan. When they glow for a few minutes after our injection, they are revealed as the obvious cause of Hypertension, which can often then be cured. Until now, 99% are never diagnosed because of the difficulty and unavailability of tests. Hopefully this is about to change.”
Professor William Drake, co-senior author of the study from St Bartholomew’s Hospital and Professor of Clinical Endocrinology at Queen Mary University of London, said: “This study was the result of years of hard work and collaboration between centres across the UK. Much of the ‘on the ground’ energy and drive came from the talented research fellows who, in addition to doing this innovative work, gave selflessly of their time and energy during the national pandemic emergency. The future of research in this area is in very safe hands.”
Minister of State for Health Will Quince said: “Around a third of adults in the UK have high blood pressure, increasing the risk of heart attack and stroke. That’s why this technological breakthrough, made possible through government-backed funding, is hugely encouraging.
“This revolutionary CT new scan has the ability to save lives by identifying nodules on the bladder which cause high blood pressure so they can be removed - curing the condition so people can live healthier, happier lives.
“We are establishing the UK as a life sciences superpower -  investing £790 million to fund research into new treatments, diagnostics and medical technology to improve patients’ lives and bolster the economy.”
In most people with Hypertension (high blood pressure), the cause is unknown, and the condition requires life-long treatment by drugs. Previous research by the group at Queen Mary University discovered that in 5-10% of people with Hypertension the cause is a gene mutation in the adrenal glands, which results in excessive amounts of the steroid hormone, aldosterone, being produced. Aldosterone causes salt to be retained in the body, driving up the blood pressure. Patients with excessive aldosterone levels in the blood are resistant to treatment with the commonly used drugs for Hypertension, and at increased risk of heart attacks and strokes.
A recently completed extension to the study also evaluated a longer-lasting form of metomidate, which can be sent to any hospital in the UK with a PET CT scanner.
More information
Research paper: Wu et al. [11C]metomidate PET-CT versus adrenal vein sampling for diagnosing surgically curable primary aldosteronism: a prospective, within-patient trial.




Related items






 News story: 
  
   
Researchers identify protein that helps skin cancer spread throughout the body

  

9 January 2023






 News story: 
  
   
Expanding radiation research at Queen Mary University of London

  

5 January 2023






 News story: 
  
   
Queen Mary announces Strategic Collaboration Agreement with Envisagenics and Cancer Research Horizons

  

13 December 2022




For media information, contact:
Press Officeemail: press@qmul.ac.uk



"
https://news.ycombinator.com/rss,Show HN: Plus – Self Updating Screenshots,https://www.plusdocs.com/,Comments,"Always-up-to-date screenshots of any website. No complicated integrations.Build the big picture, together.All of your team’s data where you need it. No complicated integrations.Sign up for freeAlways-up-to-date screenshots of any website. No complicated integrations.All your team’s data, wherever you need it.Build dashboards, connect apps, and share data without sharing logins. Zero technical setup.Get startedWatch a 2 minute demoHow it worksCapture and share the latest data from any app or website with Live Snapshots.Try it yourselfAlways freshSnapshots always reflect the latest data from your tools.Version historyEvery version of every Snapshot is at your fingertips.Embed anywhereEmbed live data in tools like Notion, Slack, Coda, Confluence, and more.Snapshots in seconds1Install the extensionOur Chrome extension allows you to capture Snapshots.2Take a SnapshotIt’s like taking a screenshot, just click and drag to capture.3Use it anywhereEmbed directly in your favorite tools, or use our app to organize a library of Snapshots.Build it with PlusEmbedded Snapshots stay up to dateNo-code dashboards for anythingBuild a dashboard with data from any app in 60 seconds, no code required.Learn moreStart conversations with dataAutomated metrics reportingPush data from any app to Slack on a schedule.Learn moreBuild team data literacyShared data libraryShare data with everyone on your team without sharing logins.Learn moreWorks where you doSnapshots embed easily in the tools you use every day.Stop wasting time moving data between tools. Plus integrates with modern business software to seamlessly connect everything.What’s more: our web app works great on mobile, so you can always check your Snapshots from your phone.More about integrationsSlackShare Snapshots with your team as easily as sending a link.Google SlidesEmbed live Snapshots directly in Google Slides.NotionEmbed Snapshots directly in Notion.ConfluenceEmbed Snapshots directly in Confluence.CodaEmbed Snapshots directly in Coda.CanvaEmbed Snapshots directly in Canva.And more!Plus works automatically in any tool with embed support, and we’re hard at work on bespoke integrations for tools that don’t.Used by teams that make great productsThis is the easiest way to build dashboards I have ever seen — no training or extra tools required. This will save my team hours of work each week.Tony HuangCEO, Possible FinancePlus makes it easier for us to get value out of our tools, rather than struggling to aggregate data.Mahesh GuruswamyCTO, KajabiPlus is the perfect solution for our weekly executive meetings, where we review data from a dozen different tools.Shane KovalskyCEO, Mystery"
https://news.ycombinator.com/rss,What’s Going on in the World of Extensions,https://blog.mozilla.org/en/products/firefox/extensions-addons/heres-whats-going-on-in-the-world-of-extensions/,Comments,"



 
                    Extensions + Addons        


Here’s what’s going on in the world of extensions



January 17, 2023





Credit: Nick Velazquez
About one-third of Firefox users have installed an add-on before – whether it’s an extension to add powerful and customizable features or a visual theme to personalize the web browsing experience. But if you’re unfamiliar, add-ons are sort of like apps for your browser. They can add all kinds of features to Firefox to make browsing faster, safer or just more fun.
The past year introduced some exciting new changes to the extensions world. The majority of these changes are foundational and take place in the deeply technical back-end of the system, typically out of sight of most Firefox users. However, if you pride yourself on hanging out in popular cybersecurity hubs, reading the latest tech news or developing your own extensions then you might have caught wind of some of these changes yourself.
If you’re not in the loop about the new changes in extensions, let us break it down for you!
Several years ago, Google proposed Manifest V3 (aka a number of intrinsic changes to the Chrome extension framework). Many of these changes would introduce incompatibilities between Firefox and Chromium-based browsers. This means developers would need to support two very different versions of their extensions if they wanted them available for both Firefox and Chromium-based browser users – a heavy burden for most developers that could result in some extensions only being available for one browser.
We believe that Firefox users benefit most when they have access to the broadest selection of useful extensions and features available, thus we’ve always placed long-term bets on cross-browser compatibility and a standards-driven feature for extensions.
With that, we agreed to introduce Manifest V3 support for add-ons, maintaining a high level of compatibility to support cross-browser development. However, there are some critical areas — like security and privacy — where our principles call for a different course of action. In a few targeted areas we decided to depart from Chrome’s implementation and incorporate our own distinctively Mozilla elements. Thus Firefox’s version of Manifest V3 will provide cross-browser extension interoperability, along with uniquely improved privacy and security safeguards, and enhanced compatibility for mobile extensions.
If ads give you the ick, then one distinction we’ve made around ad blockers has been especially crucial to privacy-lovers everywhere.
Content blockers are super important to privacy-minded Firefox users and tend to be the most popular type of browser extension. They not only prevent ick-inducing ads from following you around the internet, but they also make browsing faster and more seamless.
So we weren’t surprised to hear that Chrome users were concerned after learning that several of the internet’s most popular ad blockers, like uBlock Origin, would lose some of their privacy-preserving functionality on Google’s web browser, resulting from the changes Manifest V3 brings to Chrome’s extensions platform – changes that strengthen other facets of security, while unfortunately limiting the capabilities of certain types of privacy extensions.
But rest assured that in spite of these changes to Chrome’s new extensions architecture, Firefox’s implementation of Manifest V3 ensures users can access the most effective privacy tools available like uBlock Origin and other content-blocking and privacy-preserving extensions.
The new extensions button on Firefox gives users control
Adopting Manifest V3 also paved the way for a handy new addition to your Firefox browser toolbar: the extensions button. This gives users the ability to inspect and control which extensions have permission to access specific websites you visit.
The majority of extensions need access to user data on websites in order to work, which allows extensions to offer powerful features and cater to a variety of user needs. Regrettably, this level of site access can be misused and jeopardize user privacy. The extensions button essentially provides users with an opt-in capability and choice that didn’t exist before.

The panel shows the user’s installed and enabled extensions and their current permissions. Users are free to grant ongoing access to a website or to make that decision per visit and can remove, report, and manage extensions and their permissions directly from the toolbar. 
And if you’re not seeing those controls for a beloved extension of yours, it’s most likely because it’s not yet available in its Manifest V3 version. Don’t fret! Changes take time.
We love choice, especially when tied to enhancing user privacy and security – a double-win!
At Mozilla, we’re all about protecting your privacy and security – all while offering add-ons and features that enhance performance and functionality so you can experience the very best of the web. If interested, you can find more information about the extensions button at support.mozilla.org
And if you’re a longtime Chrome user, don’t sweat it! Exploring a safer and more private alternative doesn’t have to be challenging. We can help you make the switch from Chrome to Firefox as your desktop browser in five simple steps. And don’t worry, you can bring along your bookmarks, saved passwords and even browsing history with you!

Interested in exploring thousands of free add-ons created by independent developers from all over the world? Please visit addons.mozilla.org to explore Firefox-recommended add-ons.













"
https://news.ycombinator.com/rss,Common Lisp and Music Composition,https://ldbeth.sdf.org/articles/cm.html,Comments,"


Table of Contents

1 Introduction
2 Modulate Modulation
3 Polyphony? No problem
4 General composition structure

4.1 Multi-track
length alignment
4.2 Snap to the
scale


5 Quick music theory

5.1 Major
keys







1 Introduction

Once upon a time, there were many music composition software
packages, mostly written in Common LISP. To name a few I know,
Common Music, Symbolic Composer (not related to the Symbolics Lisp
Machine). Common Music is still available in source code but I
doubt if anyone is still able to setup that. Symbolic Composer just
vaporized decades ago, which some archived programs scattered over
the Internet. The package is also a 32bit Intel program so newer
macOS versions would not support it and it is license protected
anyway. Yet another is Patchwork, a more graphic oriented composing
program, and I'd be surprise if it is able to survive from bit
rot.
The one I actually uses is Opusmodus, similar in the spirit of
Symbolic Composer. Opusmodus has release 3.0 based on LispWorks on
Dec 2022 which has Apple M1 support, and the Windows port is on the
way.



2 Modulate Modulation

Before introducing all the fancy terms about algorithm
composition, let's first see a brief example that generates melody
from modulation of sin wave.

  |(setq pitch
  |      (filter-repeat 1 (quarter-tone-closest
  |                        (vector-to-pitch
  |                         '(g3 g5)
5 |                         (add-sine-waves
  |                            12 120 9 4.2 :phase 7
  |                            :modulation (gen-sine 120 1 0.4 :phase 60))
  |                         :quantize 1/2))))








Figure 2.1 Visualized
waveform.


Then add some classic euclidean rhythm. Since this is a
randomized process, a :seed argument
is provided to produce consistent result.

  |(setq rythm
  |      (euclidean-rhythm (gen-repeat 18 8) 1/2 8 1/16 :seed 9941))









The audio element is not
supported.



Figure 2.2 The generated
piece.





3 Polyphony? No problem

Sometimes it is just desirable to have some polyphonic1 part playing. This
time, we could add some variation by changing velocity a bit. The
basic idea is still sine wave modulation.

   |(setq rythm
   |      (quantize
   |       (add-sine-waves 17 120 9 0.3
   |                       :phase 44
 5 |                       :modulation (gen-sine 120 3 0.4 :phase 60))
   |       '(1 2 3 4) :type :ratio))
   |(setq pitch
   |      (chord-inner-remove
   |       '(5 1)
10 |       (harmonic-progression
   |      (filter-repeat 2 (mapcar #'floor
   |              (add-sine-waves 12 120 9 1
   |                              :phase 9
   |                              :modulation
15 |                              (gen-sine 120 1 0.3 :phase 3))))
   |      '(c major))
   |       :seed 564))
   | 
   |(setq score (make-omn :pitch pitch :length rythm
20 |                      :velocity '(f mf p mp)))









The audio element is not
supported.



Figure 3.1 Generated
polyphony chords.





4 General composition structure



4.1 Multi-track length alignment

To start composing music it is important to first decide how
many sections one would like to have. Which that settled one can
use get-span to acquire the length
information, then use length-span to
fill other materials like percussion generated by gen-euclidean-omn into the desirable length.


✎︎


Note


Instead of specify a very long count number to gen-euclidean-omn, which would cause a long time
to compute, length-span takes much
less time to since it just repeats materials.







4.2 Snap to the scale

Now we have some method to generate wave forms and quantize them
to twelve tones. But just playing these pitches won't make you feel
right. The thing is, you have to decide an interesting scale and
map the generated notes to it.
To do that, we have tonality-map.

   |(setq pitch
   |      (quarter-tone-closest
   |       (vector-to-pitch
   |        '(g3 g5)
 5 |        (add-sine-waves
   |         12 120 9 4.2 :phase 7
   |         :modulation (gen-sine 120 1 0.4 :phase 60))
   |        :quantize 1/2))))
   | 
10 |(filter-repeat 3 (tonality-map '(c4e4a4f4e5) pitch))









The audio element is not
supported.



Figure 4.2.1 Generated score






5 Quick music theory



5.1 Major keys

Major keys are of the '(0 2 4 5 7 9 11
12) interval2. The key
signatures are hard to remember, but one trick is for sharp keys
the name is one half step higher than last sharp; for flat keys the
name is the second to last flat. The reason why it works is
demonstrated below. The name is just from the first note in each
scale.








The audio element is not
supported.



Figure 5.1.1 A list of major keys


This is produced by OMN generated by this lisp loop:

  |(loop for i from 0 to 11
  |  collect (append '(q)
  |                  (integer-to-pitch (x+b '(0 2 4 5 7 9 11 12) i))))



"
https://news.ycombinator.com/rss,Pegasus: A Spy in Your Pocket Threatens the End of Privacy,https://www.tatteredcover.com/book/9781250858696,Comments,"
















Pegasus: How a Spy in Your Pocket Threatens the End of Privacy, Dignity, and Democracy (Hardcover) | Tattered Cover Book Store






















































Skip to main content

































My AccountCart 


0

















About UsAbout UsLocations & HoursHistory of Tattered CoverBest Cellars MenuIn The NewsPrivacy PolicyTerms & Conditions/FAQ'sEmployment OpportunitiesRefund PolicyEventsEvents CalendarShopBest Books of 2022partner shops @ tcAlliance Française de DenverDenver Central MarketNeustadt JAAMM SeriesPen & PodiumPrentiss InstituteRemergTARRADigital Gift CodeNew ReleasesBestsellersFiction BestsellersKids & YA BestsellersNon-Fiction BestsellersIndie BestsellersStaff PicksKidsBabies & ToddlersEarly ReadersInformational Early ReadersInformational Teens & TweensKids & YA BestsellersMiddle GradePicture BooksTeensSigned First Editions ClubAbout Signed First Editions ClubFAQ'sSign-Up for The ClubYoung AdultBrowse Collections#BookTokSigned EditionsGifts, Games, and Fun StuffGamesArt Supplies & Coloring BooksBook Accessories NotebooksPuzzles Stationary & NotecardsToysIndieNext ListAwardsColorado Book AwardsReading the WestNobel Prize Winner Annie ErnauxBooks of The MonthOutside The LinesLibro.fm Audio BooksTattered Cover GearDebuts to DevourProgramsFriends of Tattered CoverBook ClubsDenver7 Book ClubBlog PostsColorado Author ProgramStorytimeServicesUsed Books at the Tattered CoverHue-Man Experience at Tattered CoverHue-Man Experience Monthly Book Recommendations2022 Hue-Man Experience Book Recommendations2021 Hue-Man Experience Book Recommendations2023 Hue-Man Experience Book RecommendationsHue-Man Experience at Tattered CoverBusiness & Education Book SalesHonors English Book Lists 2021-2022Off-site Author EventsSchool BookfairsMountain Vista HS Reading ListCrest Academy Virtual BookfairGraland Book FairWyatt Academy WishlistsContact Us 





























Search form



Advanced Search



 






































Pegasus: How a Spy in Your Pocket Threatens the End of Privacy, Dignity, and Democracy (Hardcover) 





 


Pegasus: How a Spy in Your Pocket Threatens the End of Privacy, Dignity, and Democracy (Hardcover)
By Laurent Richard, Sandrine Rigaud, Rachel Maddow (Introduction by)


					    $29.99



Available to Order - Usually Arrives in Our Store in 4 to 7 Days

Add to Wish List














Description
About the Author
Details
Reviews & Media



Featuring an introduction by Rachel Maddow, Pegasus: How a Spy in Our Pocket Threatens the End of Privacy, Dignity, and Democracy is the behind-the-scenes story of one of the most sophisticated and invasive surveillance weapons ever created, used by governments around the world.Pegasus is widely regarded as the most effective and sought-after cyber-surveillance system on the market. The system’s creator, the NSO Group, a private corporation headquartered in Israel, is not shy about proclaiming its ability to thwart terrorists and criminals. “Thousands of people in Europe owe their lives to hundreds of our company employees,” NSO’s cofounder declared in 2019. This bold assertion may be true, at least in part, but it’s by no means the whole story.NSO’s Pegasus system has not been limited to catching bad guys. It’s also been used to spy on hundreds, and maybe thousands, of innocent people around the world: heads of state, diplomats, human rights defenders, political opponents, and journalists.This spyware is as insidious as it is invasive, capable of infecting a private cell phone without alerting the owner, and of doing its work in the background, in silence, virtually undetectable. Pegasus can track a person’s daily movement in real time, gain control of the device’s microphones and cameras at will, and capture all videos, photos, emails, texts, and passwords—encrypted or not. This data can be exfiltrated, stored on outside servers, and then leveraged to blackmail, intimidate, and silence the victims. Its full reach is not yet known. “If they’ve found a way to hack one iPhone,” says Edward Snowden, “they’ve found a way to hack all iPhones.”Pegasus is a look inside the monthslong worldwide investigation, triggered by a single spectacular leak of data, and a look at how an international consortium of reporters and editors revealed that cyber intrusion and cyber surveillance are happening with exponentially increasing frequency across the globe, at a scale that astounds.Meticulously reported and masterfully written, Pegasus shines a light on the lives that have been turned upside down by this unprecedented threat and exposes the chilling new ways authoritarian regimes are eroding key pillars of democracy: privacy, freedom of the press, and freedom of speech. 



Laurent Richard is a Paris-based award-winning documentary filmmaker who was named the 2018 European Journalist of the Year at the Prix Europa in Berlin. He is the founder of Forbidden Stories, a network of investigative journalists devoted continuing the unfinished work of murdered reporters to ensure the work they died for is not buried with them.For more than twenty years Laurent Richard has been conducting major stories for television. He is the author of numerous investigations into the lies of the tobacco industry, the excesses of the financial sector, and the clandestine actions of Mossad and the CIA.Since its creation, Forbidden Stories has received numerous awards, including a prestigious European Press Prize, two George Polk Awards, and a RSF Impact Prize for the Pegasus Project, published in 2021.Sandrine Rigaud is a French investigative journalist. As editor of Forbidden Stories since 2019, she coordinated the award-winning Pegasus Project and the Cartel Project, an international investigation of assassinated Mexican journalists. Before joining Forbidden Stories, she directed feature-length documentaries for French television. She has reported from Tanzania, Uzbekistan, Lebanon, Qatar, and Bangladesh.				




Categories

Political Science / Intelligence & Espionage
Political Science / Privacy & Surveillance
Social Science / Privacy & Surveillance



Product Details
ISBN: 9781250858696
ISBN-10: 1250858690
Publisher: Henry Holt and Co.
Publication Date: January 17th, 2023
Pages: 336
Language: English





“Pegasus is an alarming and urgent book—an engrossing thriller about cybersurveillance software so sly and powerful that it can take over your cell phone without your knowledge. This is terrifying stuff. Richard and Rigaud reveal how authoritarian regimes can use Pegasus software to spy on dissidents, human rights activists, journalists—and virtually anyone with a mobile phone.”—David Zucchino, Pulitzer Prize-winning author of Wilmington’s Lie “Paced like a thriller, Pegasus reveals a manifested dystopia where repressive governments purchase digital bolt-cutters to break into the phones of their critics and adversaries. But it also details the power of investigative journalists to expose a 21st-century arms market whose wares are aimed at civil society.” —Spencer Ackerman, Pulitzer Prize-winning journalist and author of Reign of Terror“The story of how investigative journalists exposed the frightening abuse of software that can infect your phone…It makes for absorbing reading…A celebration of journalism and hacking being used to unmask the bad guys.”—The Guardian “Paced like a thriller, this is an exposé of invasive malware, and a cautionary tale.”—The Economist












 


























BEST SELLERS
STAFF PICKS
KIDS BOOKS
YOUNG ADULT BOOKS
AUDIOBOOKS



 














GIFT CARDS
GEAR
GAMES
BOOK CLUBS
STORYTIME



 














LOCAL AUTHOR
BLOG
HUE-MAN EXPERIENCE
OUTSIDE SALES
CONTACT US



  














 





 




    
























  



  

















"
https://news.ycombinator.com/rss,What Is Our Universe Expanding Into?,https://nautil.us/what-is-our-universe-expanding-into-258168/,Comments,"
403 Forbidden

403 Forbidden
nginx


"
https://news.ycombinator.com/rss,Losing Taiwan Means Losing Japan,https://scholars-stage.org/losing-taiwan-means-losing-japan/,Comments,"


Losing Taiwan Means Losing Japan 
26 February, 2020Tags:Japan, Military Affairs, The Middle Kingdom 








Image Source



The United States could bounce back from the fall of Taiwan to Communist rule. It would have far more dire consequences for Japan. Consider this post a short, informal primer on why this is so. 
Ian Easton explains the PLA’s view:
The Course Book on the Taiwan Strait’s Military Geography is a restricted-access PLA manual, used to teach senior officer seminars in Beijing… This source [informs] readers that Taiwan is a chokepoint of great utility for blockading Japan. The Taiwan Strait, it notes, is a Japanese maritime lifeline that runs from Europe and the Middle East, and based on PLA studies, Japan receives 90 percent of its oil imports, 99 percent of its mineral resources, and 100 percent of its nuclear fuel needs from ships that travel across these sea lanes. In total, 500 million tons of Japanese imports pass by Taiwanese waters each year, with 80 percent of all Japan’s container ships traveling right through the Strait, the equivalent of one Japanese cargo ship every ten minutes. Consequently, these waters will, “directly affect Japan’s life or death, its survival or demise.” PLA intentions and plans for a conquered Taiwan are made plain in another internal document, The Japanese Air Self Defense Force, a handbook studied by mid-career officers at the PLA Air Force Command College in Beijing. The stated purpose of the text is to help Chinese pilots and staff officers understand the strengths and weaknesses of their Japanese adversaries. Buried amidst hundreds of pages of detailed maps, target coordinates, organizational charts, weapons data, and jet fighter images are the following lines:
As soon as Taiwan is reunified with Mainland China, Japan’s maritime lines of communication will fall completely within the striking ranges of China’s fighters and bombers…Our analysis shows that, by using blockades, if we can reduce Japan’s raw imports by 15-20%, it will be a heavy blow to Japan’s economy. After imports have been reduced by 30%, Japan’s economic activity and war-making potential will be basically destroyed. After imports have been reduced by 50%, even if they use rationing to limit consumption, Japan’s national economy and war-making potential will collapse entirely…blockades can cause sea shipments to decrease and can even create a famine within the Japanese islands. [1]
The first PLA document Easton quotes here has the statistics slightly wrong: the larger part of Japan’s energy imports travel to the south of Taiwan through the Bashi channel, in the Luzon strait.[2] To get a sense for what those shipping lanes look like, here is a map of A.P Moeller-Maersk, Mediterranean Shipping Co. and CGM SA’s Japan bound shipping routes:






Image Source



The Luzon Strait, you will notice, also runs directly adjacent to Taiwan. Chinese control of Taiwan would—in event of conflict—force Japanese shipping out of the South China Sea entirely. This in itself is not a death blow: at some cost, sea traffic that now passes through Malacca and runs adjacent to Taiwan could be rerouted through the Sunda Strait and up the east coast of Mindanao. I am sure someone in Japan must have calculated the likely economic costs of rerouting Japan-bound traffic this way (or in a more extreme circumstance, replacing Middle Eastern energy supplies with North American ones) but I have not yet seen any actual numbers. But given alternate sea lane possibilities, I doubt clearing Japanese shipping out of Taiwanese waters entirely would be enough to threaten Japan with “famine.”

But the problem posed by Chinese control of Taiwan is not really limited to the shipping that passes through the Taiwan and Luzon Straits. Navalists like to talk about what they call the “First Island Chain,” a group of islands that keeps the PLA Navy and PLA Air Force hemmed into the East and South China Seas. These islands include the Philippines Archipelago, Taiwan and the Pescadores, the Japanese Archipelago, and the Ryukyu Islands, which are Japanese territory. Here is a map of that last group:







Image Source



 In times of peace there is little to stop Chinese naval and air forces from crossing out into the Pacific as they wish, but in times of war things will be different. Over the last few years the Japanese have been quietly stocking these islands with anti-ship and anti-air missile units; were war imminent these deployments would grow. It is very difficult to imagine a significant number of Chinese commerce raiders slipping out to prey on Japanese shipping outside the Taiwan Strait as long as they have to slip between hostile Japanese and Taiwanese island bastions.  It is very easy to imagine this if the Taiwanese side of the equation is no longer hostile to Chinese forces.

This is true for several reasons. One of the more interesting ones involves submarines. Look again at the image at the top of this post; that is a seafloor depth map of the West Pacific. You will notice that the water east of the first island chain is much deeper than the water west of it. This has very practical implications for submarine warfare. The prime reason the Chinese built their most important submarine base in Sanya is because it allows the submarines harbored there to slip into the deeper waters of the South China Sea where detection is far more difficult. For China, this is the cornerstone to a credible seaborne nuclear “second strike.” If the Chinese had direct access to the western Pacific—the kind of access possession of Taiwan would give them—their nuclear armed submarines could roam freely across the globe. It would also make detecting and tracking submarines tasked with commerce raiding far more difficult.

The loss of Taiwan would also put to question Japan’s ability to hold and defend the Ryukyu islands altogether. Yonaguni, at the tail end of the Ryukyu chain, is less than 70 miles away from Taiwan’s east coast. That is almost one fourth the distance between the island and the Chinese coast (approx. 250 miles), and one fifth the distance between the island and Okinawa (330 miles). Okinawa itself is closer to Taiwan’s north coast (approx. 370 miles) than it is to the Japanese Archipelago proper (approx. 480). If Taiwan were in hostile hands, Japan would be fatally vulnerable to an island hopping campaign that would rob it of the ability to control its near sea lanes.[3]

Taiwan is the keystone of China’s naval containment. Lose Taiwan, and Japan loses the ability to keep the PLA Navy hemmed up against their own coast line. Lose Taiwan, and Japan loses control of its most important supply lanes. Lose Taiwan, and Japan loses the extended island chain defense system that protects its home waters. 

Japanese naval leaders understand this. They always have. It is why the Imperial Japanese Navy insisted upon Taiwan’s annexation in 1895, and it is why Taiwan contingencies have been an important part of the Self Defense Force’s thinking since the 1950s.[4] They understand—even if most Japanese civilians do not—that the loss of Taiwan would give the Chinese incredible leverage over Japan. 

There are some who believe that America could retreat from the defense of Taiwan while keeping the rest of its alliance system in the Far East intact. This is a fantasy. An argument to retreat from Taiwan is an argument to fatally undermine the defense of Japan. In truth, it is an argument to retreat from East Asia. That argument can be made, but I would prefer to see it made openly. 

As a final note: while preparing this post I came across a map of all of the currently existing submarine cables that run next to Taiwan. You will notice the great number that run through the Luzon Strait:


A decade ago an underseas earthquake in the strait knocked out the internet in Taiwan, Japan, South Korea, and Eastern China. I will admit that I do not know how easy it would be to isolate the cables headed towards Japan and knock them out of service, but I am interested in finding out. If you work in that industry or have expertise in underseas infrastructure, please sound off in the comments!
—————————————————————————————
If you found this analysis of Taiwanese and Japanese military affairs of  interest, you might also like the posts “Taiwan Can Win a War With China,” “Why Taiwanese Leaders Put Political Symbolism Above Military Power,” “Taiwan Will Be Defended by the Bullet or Not At All,” and “At What Point is Defending Japan No Longer Worth It.” To get updates on new posts published at the Scholar’s Stage, you can join the Scholar’s Stage mailing list, follow my twitter feed, or support my writing through Patreon. Your support makes this blog possible.
————————————————————————————–
[1]  Ian Easton, The China Invasion Threat: Taiwan’s Defense and American Strategy in East Asia (Washington DC: Project 2049 Institute, 2017), 27-28. [2]Euen Graham, Japan’s Sea Lane Security: A Matter of Life and Death? (Nissan Institute/Routledge Japanese Studies, 2006), pp.23-26. [3] The Senkaku Islands are also easier to assault from Taiwan than anywhere else. However, I tend to agree with Todd Hall’s assessment that the Senkaku island dispute is more about the symbolics of honor than strict military utility. See Hall, “Why the Senkaku/Dioyu Islands Are Like a Toothpaste Tube,” War on the Rocks (4 September 2019).  [3]  E. I Chen, “Japan’s Decision to Annex Taiwan: A Study of Ito-Mutsu Diplomacy, 1894-95,” The Journal of Asian Studies (1977), vol 37, iss. 1, pp. 65-67; Graham, Japan’s Sea Lane Security, passim.
Share this:Click to share on Twitter (Opens in new window)Click to share on Facebook (Opens in new window)Click to share on LinkedIn (Opens in new window)Click to share on Pinterest (Opens in new window)Click to share on Telegram (Opens in new window)Click to share on WhatsApp (Opens in new window) 


Post navigation
Previous Post Previous post: A Note on the Romney VoteNext Post Next post: The New England Colonies: A History Decided by Culture, or by Ecology?



Leave a Comment Cancel reply Comment Name * 
Email * 
 Save my name and email in this browser for the next time I comment.
 Notify me of follow-up comments by email. Notify me of new posts by email. 

Δ 

			12 Comments		





Anonymous 



							26 February, 2020 at 11:58 am						




Easy to find the cables and knock them out of service.
The cables come into a 'landing point' which will also be a major hub of the Taiwan internet/phone network. It is likely that the cables will all come into a single landing point.
However there are lots of cables directly between the USA and Japan so the impact of taking out the cables would not necessarily be severe.
Access to Asian internet resources would be slower because rather than routing directly it would go via the USA and then Australia to Asia or Europe-Africa to Asia. Access to US resources would be slowed a bit because of the extra traffic on the link.
A lot of internet resources are also available locally because they will be hosted in the AWS data centres in Japan.
Ultimately I think taking the cables offline would be significant symbolically and cause some short term disruption but have minimal real impact.


Reply 




Jacques René Giguère 



							27 February, 2020 at 1:30 pm						




Even worse is the Philippines falling into Chinese hands. Which is what Duterte is now allowing.


Reply 




Kartturi 



							29 February, 2020 at 9:55 am						




Some links:
https://www.scmp.com/news/china/society/article/2130058/surveillance-under-sea-how-china-listening-near-guam
https://www.scmp.com/news/china/diplomacy-defence/article/2064672/china-developing-manned-submersible-capable-reaching
https://en.wikipedia.org/wiki/Turbidity_current#Examples_of_turbidity_currents


Reply 




Anonymous 



							02 March, 2020 at 10:27 am						




While I agree that many have forgotten or simply choose to ignore Taiwan's geographical strategic importance to the U.S. in the Asia-Pacific region, this is hardly a new viewpoint nor one ignored by many in the military and intelligence fields who focus on this region. William Sebold once recorded General MacArthur's 1949 brief to the State Department on Taiwan (Formosa), an analysis that is just as true today as it was then:
""[MacArthur] said that if Formosa went to the Chinese Communists our whole defensive position in the Far East was definitely lost; that it could only result in eventually putting our defensive line back to the west coast of the continental United States. . . He pointed out that Formosa was astride the lines of communications between Okinawa and the Philippines, that it outflanked our position on Okinawa and, in the hands of the Chinese Communists, broke through the island wall which we must have along the Asiatic 'littorals' in order to maintain in a strategic sense a defensive line in the western Pacific.""
Source: David M, Finkelstein, Washington's Taiwan Dillemma, 1949-1950, p. 225


Reply 




russell1200 



							02 March, 2020 at 1:27 pm						




There is no doubt that Taiwan is a strategic location.  But it was bypassed in WW2 by the Allies and Okinawa was taken. So it is not the only location that can block Japan's supply lines.  In fact, there are so many locations you could use to block those supply lines, it is not clear why Taiwan should be considered a necessary (as opposed to sufficient) condition to blocking them.
In addition, although it is not particularly well known, the weapon that totally destroyed Japanese commerce was not submarine torpedoes, but the late war magnetic mine campaign referred to as Operation Starvation.
The Chinese have shown a lot of interest in mine warfare as an equalizer to deal with the larger United States Navy.  Although rocket assisted mines increase the depth that mines are useful at, those very same shallow waters are the fertile area for modern sea mines that sit on the ocean floor and rise up when the appropriate ""signature"" activates them.
Given that modern submarines, even diesel-electrics, are very expensive the Chinese would likely make heavy use of sea mines to blockade Japan. The alternative of an active blockade off the coast against a Japanese Navy that specializes (not surprisingly) anit-submarine warfare would not be a particularly palatable strategy.
Since the Japanese no longer have a navy that can defeat the Chinese one-on-one, they have no good strategy for dealing with a Chinese military threat. Taiwan does cork a bottle. But it also forces a very forward deployment to maintain control of it.  It is very similar to the situation with the Philippines at the start of WW2.  The US planned to concede their loss (after hopefully slowing down the Japanese more than they did), with the US defeating the Japanese in more neutral waters and retaking them.  An all-in forward defense was not considered feasible.


Reply 




russell1200 



							07 May, 2020 at 9:44 am						




terminals –
https://www.epd.gov.hk/eia/register/report/eiareport/eia_1252006/html/eiareport/Part1/Images/Figure3.5.jpg 
In following, Page 4 (labeled page 6) shows the Japanese sources of LNG.  As you can see, unlike oil, LNG comes from a number of different places.  The Middle East is not nearly so critical. With the Russians being one of the main suppliers, the Chinese might have a hard time cutting that source off.
https://www.tokyo-gas.co.jp/IR/english/library/pdf/anual/19e02.pdf
Japan is one of the more vulnerable countries to a blockade. They need to import all sorts of stuff.  And to pay for that stuff, they need to export a lot as well.  So any sort of blockade is going to hurt them alot.


Reply 




Anonymous 



							03 March, 2021 at 6:03 pm						




Taking out the cables or even the stations would be fairly easy (considering how frequently accidental cuts happen). But you don’t need to take them all out. You simply need to take out the newer ones which will drop available capacity below critical levels. Combine that with area denial to repair ships (there aren’t that many) and you’ll effectively isolate Japan. Thankfully for them the Japanese internet is fairly insular but Tbps of capacity will still deeply impact local properties in strange ways. US based web properties will no doubt be hardest hit first as it will be very difficult to maintain in country resources.


Reply 




Anonymous 



							08 March, 2021 at 10:44 pm						




My biggest fear is the collusion of the US and China to spite on Japan. The US did it with Vietnam in the 1973 and 1979 in the South China Sea. The US specifically told the South Vietnam not to reclaim the Paracel Islands. In 1979, The US sponsored Khmer Rouge and China to attack Vietnam which China invaded a few more Vietnamese islands in Spratley Islands. Today, China has become more powerful as the US regretted the mistakes from Nixon era. 
The US desperately tried to mend ties with Vietnam against China but it's largely fail as Vietnam is still highly suspiscious of Americans. Additionally, China and Vietnam ruling Communists entered a secret agreement in the Post-Soviet era where both Parties will put aside national differences to manage crises and manipulate the world for the glory of Communism – Chinese and Vietnamese Communists never abandoned the Cold War. In the past, China ceded back the control of Gulf of Tonkin back to Vietnam, and South China Sea can be a similar story. The US will probably agree to allow Vietnamese control of all ""islands"" across the SCS as long as China isn't. China probably agrees not on American behalf but on the intra-Party conscience where both China and Vietnam must permanently resolve territorial disputes for bigger goals of spreading Communism – Chinese artificial islands are largely vulnerable and costly to maintain, so it is not a good long-term investment. Giving those islands to Vietnam will gain the popular support from Vietnamese people as well as the VCP for the upcoming China-Russia dream of new ""WARSAW PACT"" – to become a superpower, China needs friends. China and the US will focus the rest of 21st century on gaining friends in the ASEAN, and East Asia will become irrelevant. Vietnam has become the de facto leader of ASEAN since the Covid-19 Pandemic.


Reply 


				Pingback: China Is Not Russia; Taiwan Is Not Ukraine – The Diplomat - NEWS TV GLOBAL 



				Pingback: 台海危機與烏克蘭危機，不應直接比較？ - *CUP 





Jay Francis 



							09 September, 2022 at 3:32 am						




I have to question the logic of this on several grounds.
Firstly, the Chinese don’t have to take Taiwan to block the trade routes going past it: they can do that with ballistic anti ship missiles from the mainland.
Secondly, if the Chinese did try to starve Japan out, the Japanese would go nuclear. They’ve already developed ballistic missiles even if they call them launch vehicles, and the engineering challenges of nuclear weapons are fairly trivial for them. (They’re widely suspected of having nuclear weapons in kit form.)
And, as North Korea will tell you, you don’t try to starve our nuclear powers.


Reply 



Jay Francis 



							09 September, 2022 at 3:35 am						




In fact if anything Taiwan’s strategic position relative to Japan acts as a restraining influence on China – because the conquest of Taiwan would lead to Japan assembling nuclear forces and the Chinese really, really don’t want that.


Reply 



"
https://news.ycombinator.com/rss,Show HN: A tool for motion-capturing 3D characters using a VR headset,https://diegomacario.github.io/Hands-In-The-Web/public/index.html,Comments,"









Downloading...



        Woops! Your browser doesn't support WebGL 2.0 - Please try using Chrome or Firefox.
    








"
https://news.ycombinator.com/rss,Patterns (YC S21) is hiring,https://www.patterns.app/,Comments,"











Patterns | Build next-gen AI systems | Patterns





Skip to main contentMarketplacePricingBlogDocsSign upSearchScale next-gen cloud appsRun and deploy web apps, task queues, massively parallel compute jobs, machine learning models, GPUs, and much more with a single unified framework for code and infrastructure.Start BuildingExplore Patterns Accelerate development with powerful building blocksPatterns provides powerful data abstractions called nodes that are automatically orchestrated by a reactive graph. Build data systems 10x faster. Focus on developing code specific to the problem you’re solving, not messy implementation details.About Node TypesPYScript with PythonYour browser does not support the video tag.SQLBuild pipelines with SQLYour browser does not support the video tag.Store data in TablesYour browser does not support the video tag.Setup endpoints with WebhooksYour browser does not support the video tag.Execution with a reactive GraphYour browser does not support the video tag.Visualize with ChartsYour browser does not support the video tag.Document with MarkdownYour browser does not support the video tag.Develop anywhere Patterns apps are fully defined by git-backable code and files. Develop locally with our local devkit or in the UI, version control with confidence.Your browser does not support the video tag.Check out our Development KitProduction-grade infrastructure Patterns abstracts away micro-management of compute and storage and provides easy to configure system controls so that you can focus on shipping apps that create business value.GraphsAbstract away orchestration messiness and build production grade pipelines with our functional reactive execution protocol.CodeBuild sophisticated pipelines in languages you already know - Python and SQL. Version control with GitHub, and release with CI/CD.Compute & StorageRun your apps on any infrastructure. Turn up processing with any CPU/GPU and store data in the best database for the job at hand.  Leverage open-source components Components encapsulate a sub-graph of nodes that contain commonly reused data workflows such as integrations to third-party APIs for ETL, Reverse ELT, enrichment APIs, ML models, and can even contain logic for entire data pipelines.  Clone and contribute to our open-source marketplace of components.About ComponentsVersion ControlReceive updated code from the Component developer with zero-effort. No need to pull git and redeploy a container.Clone via the marketplaceClone Components directly from the marketplace into your App for immediate use.Build and share your ownDon't see a connector you need? Use our devkit complete with helper methods helper methods and a testing framework to develop a robust API integration yourself.App examples Apps are composable, defined by code, and easily cloned across accounts. Browse select apps below, explore more from our marketplace, build your own to share through the marketplace.App MarketplaceEng Advice GPT3 Slack Botslackopen_aibotsSet up this app in a few minutes and have a new engineering voice in the room.             This app utilizes Slack and OpenAI to bring a chatbot experience to any Slack Channel.             Clone and customize for your own use case or use in a production environment.             The OpenAI component in this App allows you to construct prompts to fit your desired completions.Crypto Wallet Tracker with Web3.pyweb3.pycryptoethThis App uses web3.py to query the Ethereum blockchain and extract data for a set of relevant wallets. Paramaterize this app with wallet addresses and set up a monitoring system to track wallets for changes.Lead Enrichment, Scoring, and Routingpeople data labssalesrevopsThis app powers sales, marketing, and growth teams with fast and accruate data about their leads.             Clone this app, customize it, and integrate with your marketing website and CRM to fully automate how you score, route, and prioritize your leads. rate with your existing data tools or directly with your business systems.             This is an example of a simple configuration of lead scoring using a webhook as input and Slack channel as output. This template can be configured differently to ingest data from a database, CRM, or any API that contains your lead data. Additionally, you can also configure Patterns to export enriched and scored leads to your CRM, email marketing tool, or back to the database of your choice.Activity Schema Analyticscdpproduct analyticsIngest data from any source and model into an activity schema - an event based data structure that             simplifies customer data analytics. This app has pre-built data pipelines to calculate leveraging an activity schema such as --             customer cohort retained revenue, active revenue, and cohort retention percentage.Twitter Sentiment and Stock Trading ApptwitterintrinioIngest data from Twitter and Intrinio and conduct sentiment analysis with textblob.py.            Get current stock market data and analyze price changes alongside sentiment. Eventually turn into a stock trading bot!  Hacker News Analysis and Notificationshacker newsslackIngest data from the Hacker News API, do an analysis on the most common domains, chart a plot of the most recent and top stories. Build a monitoring app that will check for keywords in posts and send messages to Slack when found.Scalable and Secure Patterns is cloud native and can be used as a fully-managed product, alternatively bring-your-own database and use alongside Modern Data Stack products.SOC2 Type II CertificationRest assured your data is in secure hands, we’ve obtained a SOC2 Type II certification that employs best practices in data security, management, and infrastructure. Loved by buildersThe ability to use code when we need it and UI when we don’t makes it quick and easy to create data workflows in patterns. We’re excited to partner with patterns to make sophisticated data use cases more accessible to our customers.Marion NammackDirector of Product Management at BrazeWhen I came across Patterns it promised to 10x our data science. A month later we’ve integrated all major adtech platforms and are using Patterns for production pipelines, that’s more like a 100x.Philippe AssefCo-founder @ BilyBlendJet offers free worldwide shipping using a variety of carriers. 
 Patterns was able to find and ingest all of the data from our mishmash of carriers, make it uniform, and visualize it in a heat map of the world. We’re now setting up automated alerts on the order level to provide proactive customer support when we see a package is lost or late. This is a huge win for our customers and our customer service team. 
 This is game changing software, which essentially lets you solve a problem you might have. I am extremely impressed, and you will be, too!Ryan PamplinCEO at BlendjetWe were able to prototype and rapidly iterate parts of our data pipeline in a fraction of the time it would have taken to build out and orchestrate elsewhere. Felt like one person could 10x themselves and get a data-driven backend up and running in an afternoon. Being able to visualize and manipulate the pipeline with straightforward but powerful base components rewrote our own product roadmap and makes many formerly far off features implementable today.Allen RomanoCo-founder and Chief Product Officer at LogoiOn Patterns we integrated, built data pipelines, and dashboards for all of our ads + marketing data. What we thought would take us a month, we shipped in a week. Abstracting away orchestration and data storage really did accelerate our development process.Casey DonahueFounder at OptiwattPatterns makes building pipelines easy. It enables the speed and flexibility to iterate and get immediate customer feedback.Ryan WallaceSoutions Architect at Lemay.aiAboutDocsBlogCareersGitHubTwitterLinkedInTerms of ServicePrivacy PolicyContactCopyright © 2023 Patterns Data Systems, Inc.



"
https://news.ycombinator.com/rss,The Quest for a Dumber Phone,https://every.to/cybernaut/the-quest-for-a-dumber-phone,Comments,"


The Quest for a Dumber Phone - Cybernaut - Every






































Subscribe





≡


About
Founders‘ Letter
Publications
Collections

Contact Us
Become a Sponsor
Login











Cybernaut




          The Quest for a Dumber Phone
        
Digital minimalists are using simple talk-and-text devices to spend less time online, curb distraction, and navigate internet addiction

by Fadeke Adegbuyi
January 17, 2023
♥ 16





Listen







Graphic by Olamide Rowland





Sponsor Every

Every is the premier place for 65,000+ founders, operators, and investors to read about business, AI, and personal development.
We're currently accepting sponsors for Q1! If you're interested in reaching our audience, learn more:

Learn More


Want to hide ads? Become a subscriber

When Jose Briones immigrated to the United States in 2010 at age 15, he was met with a technological shock. In Nicaragua, where he was born, smartphones had yet to become widespread. They were expensive luxury items or reserved for emergencies, with minutes purchased in advance to make phone calls. His childhood was largely spent offline. In America, he got his first smartphone—a Huawei device with unlimited talk, text, and data—and relished the freedom to quickly look up soccer scores, listen to music on Pandora, and access information in seconds. But as he moved from adolescence to adulthood, his enthusiasm for technology turned to obligation, then oppression. Throughout his years in college and in the working world, he felt tethered to his devices and stuck to a screen. Distraction plagued his waking hours, resulting in a painful period of procrastination while obtaining his Master’s degree. In 2019, he decided he needed a break—the notifications, the constant texting, the binging of TV shows and social media and news—and made a change. Today, Briones has relinquished his smartphone outside of working hours, and is an advocate and evangelist for digital minimalism.  He’s not alone. Briones is part of a growing movement of people who believe we benefit as individuals and a collective by unplugging from internet-enabled technology. “People are tired of feeling a lack of control over their lives. It physically makes a difference in our lives when we are tethered, in our eyesight and the way we feel,” says Briones. “Especially knowledge workers, you're in front of a computer all day. You're sitting down. There's no movement [or] physical engagement with anything.”A 2021 survey from Pew Research Center found that 31% of U.S. adults report they are online “almost constantly.” U.S. teenagers age 13-17, similarly, feel a stronger pull to be online—in another 2022 survey from Pew Research Center, nearly half of teens say they use the internet “almost constantly,” 36% say they spend “too much” time on social media, with 54% of teens admitting it would be hard to give up social media. Excessive internet use has led to a slew of proposed solutions—digital detoxes and disconnection retreats, 24-hour tech relief in the form of a (somewhat dubious) “digital Shabbat,” and 12-step programs for internet and technology addiction. Boot camps intended to rehabilitate the internet-addicted have been around since the aughts.The “why” behind excessive internet use—from social isolation to addictive algorithms—varies depending on who you ask. But some see a clear culprit for our distraction-prone era: the smartphone. This is the case for over 14,000 Reddit users in r/dumbphones, a subreddit in the top 5% of communities on the platform. Briones, a dumbphone user himself, is one of the community’s moderators and also runs the website Dumbphone Finder. He suggests that making a transition to a dumphone—simple talk-and-text devices—is challenging, but ultimately worthwhile. “Learning to live an ‘inconvenient life’ is difficult,” says Briones. “In the beginning, there's a lot of friction, but it's rewarding if you adapt it to your lifestyle.”Many of the community's members are keen to take on the challenge, giving up smartphones they feel are a gateway to distraction and addiction. The community defines dumbphones as devices that “lack the advanced technology of smartphones” and note that they lack “distracting feature-rich apps like YouTube, Instagram, or Facebook.” Some opt for flip phones with tiny screens and buttons instead of touch screens, like the Nokia 3310, while others explore a new breed of minimalist devices like the Light Phone II. The aim of the community is clear: “the quest towards a simpler life.” Swapping smartphones for ‘dumbphones’Long gone are the days of mainstream talk-and-text devices and paying for minutes. For nearly 20 years, the phone industry has been on a mission to fit the most advanced computer into pocket-sized machinery. The last Apple event in September 2022 saw the release of the iPhone 14 Plus, touted as its “most innovative lineup yet” with features like Always-On display (keep your home screen dimmed but accessible while locked), Cinematic 4K24 (automatic focal point switching while shooting video), and a Photonic Engine (a technology to enhance low-light photos)—not to mention 1.8 million apps at your fingertips. But those on r/dumbphones are seeking out another kind of experience entirely: a phone that does next to nothing. According to the subreddit’s analytics, r/dumbphones saw 408,587 page views (14.1% of which were unique) and gained 971 members in November 2022 alone. Some members are digital minimalists, some are privacy enthusiasts, some enjoy the aesthetic of a minimalist phone, and some are “self-diagnosed internet addict[s].” All are on the quest for a quieter phone experience free from the pull of algorithmic feeds. Browse through the conversations and you’ll find users sharing their experiences with phones like the Alcatel Go Flip or the Nokia 8110. Others ask for advice on maintaining a dumbphone for life but a smartphone for work—needing the advanced security features like two-factor authentication and work apps like a calendar to perform their roles on the latter. The ethos of the community isn’t anti-tech, but tech-critical. The smartphone is viewed as a rabbit hole leading towards hours of scrolling; some discuss the “constant IV stream of social media” as their motivation for making the switch. For years we’ve had options to make the smartphone less distracting: switch to grayscale, turn off notifications, use an app blocker. But these solutions are meant to affect your habits, not your lifestyle. For the r/dumbphones community, a simpler life requires a simpler phone. Disconnection isn’t within reach if you still have unmitigated access to a bright phone with dark UX patterns that aim to keep us hooked. When Ashton Womack, an online creator and the entrepreneur behind the stationary brand Virgo and Paper, watched the 2020 Netflix documentary The Social Dilemma, she resolved to change her relationship to technology. A business owner, she had felt compelled to use Instagram as a growth tool, to connect to customers and promote her work. It took around six months for her to decide on switching to a dumbphone—a simple Nokia 225 flip phone. She reflected on the year-long experiment in a series of YouTube videos, including a Q&A on her year without a smartphone and a discussion on lessons learned.  It wasn’t until after the experiment that Womack realized how attached to her phone she had been. Before a Nokia flip phone replaced her iPhone, she was spending countless hours online, losing presence in the world outside her device. Her ability to get from place to place was limited, because she had relied on the GPS on her smartphone for directions. “I didn't think at that time that I had a smartphone addiction, or that I was spending too much time on my phone. But after going without it for a year, I realized maybe I actually really do have a dependency on it,” said Womack to me over Zoom. “I was spending a lot of time on it back then. But at the time, it seemed normal, because everyone else is doing the same exact thing.” Missing the ability to take high-quality photographs, Womack has since returned to using a smartphone, but her relationship with technology has permanently shifted. She has just eight apps on her homescreen, and now considers herself a digital minimalist. Rather than scrolling on her phone, she spends time offline in nature and with friends and family, sparking a creative boon as an artist that she’s funneled into her stationary business. Perhaps most important was Womack’s realization that she didn’t need a smartphone, or an Instagram presence, for her business to thrive. Free from all the time she’d spent trying to guess what the algorithm would think of her posts, she could focus on the fundamentals of business—a great product and impeccable customer service. “Going into this experiment, I thought that you had to have a smartphone to survive in today's world, especially as a business owner,” says Womack. “I just thought having a smartphone is essential, and there's no way to live without it. And I've proven that that's not the case.”The realization that we don’t need our smartphones as much as we think is the “eureka” moment shared again and again in the r/dumbphones community. “I feel free because I am not tethered to my screen [for] 50% of my day,” says one user. “You don't realize how your smartphone can really overwhelm you until it's gone,” notes another. But the forum isn’t simply a space to romanticize the early 2000s. Wade through and you’ll find an exploration of the trade-offs necessary to living in a smartphone-powered world without one. Most dumbphones lack GPS, so users need to plan ahead, looking up directions at the start of their day and being intentional about planning out their route. Most also lack a camera, which means no photos, but also the inability to use QR codes to enter concerts or access restaurant menus. They’re incompatible with modern apps, leaving users without access to rideshare and food delivery with Uber, podcasts on Spotify, or international calls with friends and family on WhatsApp. Most have individual texting, but no group texting capabilities to trade memes and memories with friends and family.Not everyone is convinced that dumbphones are the answer to our compulsive smartphone use. Nir Eyal documented his own unsuccessful experiment with giving up his smartphone for an “old-school phone” without email, Instagram, or Twitter in his latest book, Indistractable: How to Control Your Attention and Choose Your Life. Given his own experience—and research about drastic anti-tech measures—he doesn’t view dumbphones as a long-term solution for most people, partially because they’re giving up too much. “I don't think that for most people, that's a great solution. It's also giving up so much,” says Eyal over email. “It's giving up GPS, it's giving up audiobooks, it's giving up a lot of things that could be great.”Rather than external distraction triggers—smartphones, laptops, email—he suggests turning towards internal triggers instead. “If turning to a reduced-feature phone is a good idea for you, and you don't want those features, and they're not according to your values, great,” says Eyal. “[But] thinking that just getting rid of technology is the solution is false—it's a mirage.” Briones has experienced his own adjustments using a dumbphone, and notes that unplugging from technology is beyond the concept of mere “willpower.” For him, leaving smartphones behind has meant asking for paper menus at restaurants with QR codes, printing tickets for public transit and concerts, and carrying around index cards with appointments in place of a digital calendar. When members share their frustrations on r/dumbphones, the stories of lost convenience are repetitive. Indeed, the phones are just a little too “dumb” to work with the expectations of a modern lifestyle. Briones says dumbphone users need to dig deep to move past inconvenience and get the true rewards of life without a smartphone. Inevitably, some of the dumbphone-curious will eventually open their laptops to order the next “most innovative” phone and re-enter Apple’s universe. Others will find the benefits outweigh the setbacks—that the setbacks are actually the point. “Learn to live with the inconvenience—learn to even be an inconvenience,” he says. “It takes a lot of time for the adjustment to be made. I'm still making the adjustments for my life. But I enjoy seeing where it’s taking me.”The anatomy of a dumbphone After working in product design and development in the mobile phone industry for over a decade, Kai Tang was looking for something new. “I quit the job of designing and making smartphones for Motorola, Nokia, [and] Blackberry,” says Tang. “Primarily, because I just don't see the point of making another smartphone every other month because of a slightly faster chip set or a slightly better camera.” His search led him to a Google incubator for designers where he met his co-founder, Joe Hollier. Both men longed to make a product where they could make money without monetizing customers’ time, attention, and data—a rejection of the attention economy. In February 2015, they went on to found Light, the creators of the popular “premium, minimal” phone Light Phone II. The device has a clear value proposition to digital minimalists everywhere: “designed to be used as little as possible.” The phone is as small as a deck of cards with a simple electronic paper screen. It comes in black and white and has limited features including texts, calls, alarms, and the ability to set up a digital hotspot. You can customize the phone to add native tools like a calculator, music player, podcast tool, notes, and directions. Tang notes that tens of thousands of customers have purchased a Light Phone II.Watch the bevy of video reviews of the Light Phone II online and you’ll also find people who want more. They want a calendar. Apps like Spotify and Uber. A camera. Balancing these demands with the stated goal of the phone—minimalism—is a design challenge that the founders have embraced, guided by a handful of key product development tenants that they are unwilling to compromise. “I have three principles,” says Tang. “One, there will never be advertisement in any of the tools that we create. Number two, we will never have infinite feeds. Nothing for you to swipe, nothing for you to browse. Number three, every action that a user takes has a clear ending. We're never going to have emails because that's a black hole. We have no media, no fees, no games, just utility.” Not all dumbphones are built the same. Briones, who has a Light Phone II, recently reflected on three years of using the device in a video on his YouTube channel. He’s also tested and reviewed the growing number of dumbphones on the market. His Dumbphone Awards 2022 video assesses popular options—like the popular Jelly 2E, which is essentially a small version of a smartphone with all the apps you’re accustomed to; or the CAT S22 Flip, a flip phone with throwback buttons instead of a touch screen that also allows for the use of apps from the Android Play store. Taken together, these devices are proof that not everyone wants a mini-computer in their pocket.What they do want is something the manufacturers and users are still learning. Much of the rise of dumbphones isn’t about having a phone with the least features, but instead what such an arrangement enables. Tang and his team at Light have yet to complete a study into screen time on the Light Phone II. But they have completed surveys and frequently speak to users who tell them that their time on mobile has plummeted. Tang puts this reduction at about an average 90-95% less screen time as compared to a smartphone. But his team is also focused on some intangible benefits. “So many users have been telling us that they feel less anxious, they feel less stressed, they sleep better,” says Tang. “They feel like they have a better relationship with [their] families and loved ones because they are mostly without a smartphone.”Though there is little data to study at this point, anecdotally users are finding that even some time away from their smartphones can have long-lasting positive effects. In 2022, according to a story in the Wall Street Journal, the administration at a Massachusetts boarding school noticed its academic community eroding. Students exhibited antisocial behavior and treated the internet as their preferred world, while teachers acted as “gadget police,” punishing distracted pupils. After a physical altercation between students was broadcast online, the school banned smartphones and ordered Light Phones for teachers and students for “essential communication.” Though there was initial pushback—both from students and parents—the experiment went surprisingly well. Teachers reported that students were more engaged in class, and students reported building deeper friendships and having less attachment to their smartphones once they were returned.  After returning to her minimally-enabled smartphone, Ashton Womack noticed the same lasting benefits of her time away. “I honestly feel like [during] my time without a smartphone, I felt more connected to people,” she says. “I really treasured and focused on those connections, because they weren't just the surface-level relationships of social media. They were real, deep relationships.” With the extra time that Briones now has, detached from his devices, he’s been inspired by the 80,000 hours project that asks people to consider the impact they can have on the world through the totality of hours they will spend on their careers. For him, this has meant a budding passion for reforming public transit.  “I don't think digital minimalism is just about getting rid of the devices, but reorienting your life towards something that you actually want to do. I want to make a good impact in society,” he says. “Now that you have time, now that you have regained time, what are you going to do with it?”These stories reveal more about what digital minimalists really want—time, focus, and connection. Can a society bent on controlling how you spend your time, where you focus your energy, and how you connect tolerate the growing numbers of this movement?The rise of the digital minimalism movementIt’s a running joke on the internet that we can’t log off, no matter how desperately we want to. The discourse is “cursed.” The algorithm is “polarizing.” The platforms (and people) are “toxic.” And yet, our eyes remain fixed on the screen. Proponents of digital minimalism don’t find the joke that funny at all. In 2019, author Eyal published his book Indistractable, about our current “crisis of distraction” and how to find focus in the attention economy—which was itself a follow-up to his bestselling book Hooked that outlined the habit-forming mechanisms and hidden psychology that companies use to keep us online. The same year, Cal Newport released Digital Minimalism (coining the term in the process), a call to unplug from technology—the distractions of social media, the call of email, the pull of our phones—to live more meaningful lives. Two months later, Jenny Odell released How to Do Nothing: Resisting the Attention Economy, a rejection of the cult of capitalist productivity and a call to deepen our attention to physical place. A year later, Tristan Harris starred in The Social Dilemma, bringing his work with the Center for Humane Technology to a mainstream audience in the Netflix documentary. They all pull at a similar string, but their theses and solutions vary. Ask Eyal, and our distractions begin from within, called forth by instant gratification triggers that we can resolve through tactics like planning ahead, finding purposeful things to do, and creating personal “pacts.” Ask Newport, and the online tools we use “have a way of cultivating behavioral addictions” that we can circumvent by stepping away from online activity for 30 days and reconfiguring our relationship to technology thereafter. Ask Odell, and it’s the power of commercial social media and its incentive to “keep us in a profitable state of anxiety, envy, and distraction” that we must resist by engaging meaningfully with our environments. Ask Harris, and it’s persuasive technology that modifies behavior which has turned the internet into a Las Vegas slot machine that needs to be reformed through ethical design. Regardless of “why,” the “what” is that we’re in the midst of a modern problem. But as nearly as long as the internet has existed, there have been discussions about “internet addiction.” As far back as 1996, research started on the concept of internet addiction. Dr. Kimberly Young is widely cited as the first researcher to complete an empirical study on the condition and mentions side effects including “academic, social, and occupational impairment.” Research on the topic has also been conducted across the world, in China, Japan, and South Korea in particular. China was the first country to diagnose “online addiction disorder” in 2008, and as early as 1997, internet addiction camps sprung up in the country (after reports of death and electroshock therapy, a ban on these facilities was proposed). Today there are screen time limits for minors under the age of 18. A 2000 piece said of internet addiction, “it’s no joke,” citing “compulsive email checking” and “denying that you spend too much time online when people confront you.” 
Paul Millerd@p_millerd

internet addiction - it's no joke (from 2000) 

November 26th 2022, 10:15pm EST
2 Likes
 Today, while much academic literature exists about the phenomenon, “internet addiction” as a disorder—and its offshoots—remains in dispute amongst researchers. In the book Behavioral Addictions: Conceptual, Clinical, Assessment, and Treatment Approaches, individual chapters explore the literature and evidence for addiction to social media, gaming, smartphones, and the internet. On smartphone addiction—or “problematic smartphone use” and “smart-phone use disorder”—researchers note that while the true prevalence of these disorders is unclear, “excessive smartphone use has been found to be correlated with depression and anxiety, as well as loss of productivity at work and school.”Proponents and practitioners of digital minimalism are aiming to circumvent these ills of technology use. They’re satisfied being ignorant of the daily discourse, opting out of all-day group chats, and finding ways to cultivate connections without the internet. The desire to unplug has made way for an entire market, selling focus and technological detachment. R/dumbphone and the rise of low-tech phones are part of a wider phenomenon that includes the use of site blockers, lock boxes for your phone, and other online movements like no surf. These movements—and these markets—are small. You’re unlikely to meet someone who has a dumbphone, and the market for “feature phones” (old-school tech that doesn’t have the capability of smartphones) is projected to continue declining. But it’s probable you know someone—or are someone—who wants to spend less time on their phone, disconnecting in a world full of distraction. Regardless of our concerns about what the Internet is doing to our brains, the expansion of the online world into our lives (and minds and bodies) shows no signs of stopping. The quest to develop the so-called metaverse presses on, and new apps are always competing for our collective attention. It will be up to individuals whether they stay engaged in the online world, find a balance through moderate use, or decide to opt out entirely. Despite adding new enthusiasts, it’s more likely than not that the digital minimalism movement will remain small—as common as those who opt for vinyl records over Spotify playlists or wall planners instead of Google calendars. But for the interest of those it piques, they’ll live in a radically different world than the rest of us—one where they’re spending a lot more time looking up.Disclosure: Every co-founder and CEO Dan Shipper is an investor in Light.




What did you think of this post?

Amazing
Good
Meh
Bad





Send Privately

      Your feedback has been saved anonymously. If you want it to be attributed to you, login or sign up.
    



Like this?Become a subscriber.
Subscribe →
Or, learn more.




Read this next:








Cybernaut


Blind Ambition
Big tech’s highest-paid workers get real on an anonymous professional social network

♥ 61

          Apr 13, 2022
          by Fadeke Adegbuyi











Cybernaut


The Blurred Lines of Parasocial Relationships
The disappearing divide between “followers” and “friends”

♥ 44

          Jul 14, 2021
          by Fadeke Adegbuyi











Cybernaut


On Twitch, You Can Never Log Off
The demands of daily livestreaming are driving creators to rethink the benefits of Twitch fame

♥ 30

          Dec 6, 2022
          by Fadeke Adegbuyi







Comments







Post









Post



    You need to login before you can comment.
    Don't have an account? Sign up!





✕
Thanks for reading Every!
Sign up for our daily email featuring the most interesting thinking (and thinkers) in tech.
Subscribe
Already a subscriber? Login




Contact Us ·
            Become a Sponsor ·
            Search ·
            Terms

©2023 Every Media, Inc





"
https://news.ycombinator.com/rss,Show HN: Product Hunt Launch Dashboard,https://plusdocs.notion.site/Public-Beta-Launch-Dashboard-c5a62f9a22c245b888ee6e71061fdce2,Comments,"Notion – The all-in-one workspace for your notes, tasks, wikis, and databases."
https://news.ycombinator.com/rss,Republicans to pass bill in House to end telework for federal employees,https://www.wusa9.com/article/news/local/dc/show-up-act-federal-workers-telework-bill/65-11d0e826-21c4-4a2e-96ba-1741637411d2,Comments,"















DC
This bill could mean the end of telework for federal workers
The Stopping Home Office Work’s Unproductive Problems (SHOW UP) Act is aimed at bringing workers back to the office.















More Videos












Next up in 5
Example video title will go here for this video

























































Author: Evan Koslof
                

Published: 8:34 AM EST January 16, 2023
                

Updated: 10:20 AM EST January 16, 2023
                








 



WASHINGTON — A new bill could have major impacts on federal workers in the D.C. area. If the bill passes, it could mean the end of working from home for many federal employees. 

At just six pages long, The Stopping Home Office Work’s Unproductive Problems (SHOW UP) Act could bring the changes. The effort to bring federal workers back to the office is being led by Republican Rep. James Comer from Kentucky. He is the new Chairman of the House Committee on Oversight and Accountability.  







The bill would require that within 30 days of passing, every agency returns to pre-pandemic arrangements. That means a lot of federal workers would be coming back. The bill would also require federal agencies to complete and submit to Congress studies outlining how pandemic-era telework impacted their performance.  

Comer denied WUSA9's request for an interview on the subject, but issued a statement. 

""President Biden's unnecessary expansion of telework crippled the ability of departments and agencies to fulfill their responsibilities created cumbersome backlogs,"" the statement reads in part.  

The bill is already being blasted by the American Federation of Government Employees, which represents 700,000 federal and D.C. governmental workers.  

The national president, Everett Kelley, wrote in part, ""At a time when agencies across government are struggling to hire and retain a new generation of government employees – Congress should be focused on improving pay, benefits, and career development opportunities. Instead we see message bills like the SHOW UP Act that denigrate the federal workforce and undermine recruitment and retention…""   

All this is a national backdrop to a local issue. 

In her inaugural address, D.C. Mayor Muriel Bowser called for 100,000 new residents in the downtown area. She called on the federal government, which leases a third of the District's office space, to turn some of that over to residents. 

""That's a bold goal, but the fact is, no matter what we do, it won't be fast enough without the help of the White House..."" Bowser said. ""We need decisive action by the White House.""  

We reached out to Bowser's office to ask what they thought of the SHOW UP Act. They told us they were still reviewing it and could not offer a comment. 

It is important to remember that Democrats control the Senate, so without Democratic buy-in, the bill will go nowhere.  







Rep. Comer also submitted a letter to the Administrator for the General Service Administration, accusing the leader of spending most of his time ""working in a location other than Washington, D.C."" The letter claims they heard about this from ""whistleblower reports,"" and requested the administrator's calendar since he took office.  




     

Related Articles


Mayor Bowser hopes to convert empty offices into new apartments in downtown DC


Mayor Bowser reveals DC comeback plan


 
WUSA9 is now on Roku and Amazon Fire TVs. Download the apps today for live newscasts and video on demand. 

Download the WUSA9 app to get breaking news, weather and important stories at your fingertips. 

Sign up for the Get Up DC newsletter: Your forecast. Your commute. Your news.Sign up for the Capitol Breach email newsletter, delivering the latest breaking news and a roundup of the investigation into the Capitol Riots on January 6, 2021. 


 





































































 


















Before You Leave, Check This Out





































 






















"
https://news.ycombinator.com/rss,New make --shuffle mode,https://trofi.github.io/posts/238-new-make-shuffle-mode.html,Comments,"




trofi's blog: New make --shuffle mode









/
/archive
/atom
/rss


New make --shuffle mode
TL;DR:
I implemented new --shuffle option for GNU make to simulate
non-deterministic build order in parallel makefiles.
UPDATE: --shuffle mode was released as part of GNU make 4.4 and
can be used as:

make --shuffle ... from the command line.
GNUMAKEFLAGS=--shuffle via environment variables for wider scale
builds.

Options Summary
has more details.
It already found bugs in 30+ packages
like gcc, vim, ghc, subversion, strace, ispell and others.
Background
About 11 years ago I was a year old Gentoo dev who just started getting
downstream bug reports on mysterious ghc build failures like
https://bugs.gentoo.org/326347.
The symptoms were seemingly simple: some file was inaccessible while
it was being written to, or executed.
Years later I mastered the intricacies of ghc’s build system on how
to debug it effectively. But at that time I did not really know what to
do. My main working machine was a Core 2 duo HP laptop which could not do
more than -j2. And even that required a bit of swap for ghc’s
linking stage. Throwing more parallelism was not really an option to
trigger such bugs.
Makefile target ordering
Makefiles are fundamentally simple: it’s a graph of dependencies
with a sequence of shell commands attached to a node. There are numerous
caveats, but they should not break this model too much.
In theory you can topologically sort the graph and execute the
dependencies in various conforming orders and expect the same result.
Modulo missing dependencies in the graph.
In practice GNU make happens to traverse the graph in very specific
topological order: it maintains syntactic order as much as possible.
Here is an example Makefile:
# cat Makefile
all: a b c

b: b1 b2 b3
a: a1 a2 a3
c: c1 c2 c3

a b c a1 a2 a3 b1 b2 b3 c1 c2 c3:; @echo $@ && sleep 1
(I added sleep 1 to make it more visible when next goal schedules).
Here is the sequential execution by GNU make:
$ make
a1
a2
a3
a
b1
b2
b3
b
c1
c2
c3
c
The seen order is exactly all’s prerequisites left-to-right
recursively.
Adding parallelism does not change the order too much: make still
traverses prerequisites in the same order and starts as many targets
with satisfied dependencies as possible.
Parallel example:
$ make -j4
a1
a2
a3
b1

b2
b3
c1
c2

c3
a
b

c
I added newlines where 1-second pauses visibly happen.
Note that in this example a1 does not depend on
c2. But c2 practically always starts execution after a1
finishes.
The “only” way to run a1 and c2 in parallel is to run
make with at least -j8. Which is a lot.
Or do something with the system that stalls task execution for
indefinite amount of time (like, adding various nice levels
or put system under high memory or CPU pressure).
Very occasionally already stressed system naturally gets into
unusual task execution order. You get the one-off failure and
struggle to repeat it ever again. Which makes it very hard to
test the fix unless you know where exactly to put the sleep
command to make it more reproducible.
An old idea
Even then it was clear that CPU count per device will only increase
and it will be increasingly painful to work with sequentially built
projects :) Bugs will come back again and again on you the more cores
you throw at the Makefiles.
I had a silly idea back then (post in russian):
what if we arbitrarily reorder the prerequisites in Makefiles? Or
maybe even trace spawned processes to know for sure what files targets
access? That might allow us to weed out most of the parallel bugs with
some sort of stress test on a low-core machine.
Fast forward 11 years I attempted to enable build parallelism by default
in nixpkgs.
A few packages still had some issues.
I recalled the idea and tried to implement target random shuffle within
GNU make!
Better reproducer: make --shuffle
Initial idea was very simple: pick target order at Makefile
parse time and reshuffle the lists randomly. To pick an example
above one of the example shuffles would be:
$ cat Makefile
all: c b a

b: b2 b1 b3
a: a3 a2 a1
c: c1 c2 c3

a b c a1 a2 a3 b1 b2 b3 c1 c2 c3:; @echo $@ && sleep 1
I wrote the proof of concent and
proposed
it to GNU make community.
The example run of patched make shows less determinism now:
$ ~/dev/git/make/make --shuffle -j4
c2
c3
c1
a2

a1
b2
a3
b3

b1

c
b
a
Paul did not seem to object too much to the idea and pointed out
that implementation will break more complex Makefiles as there
is a simple way to refer to individual prerequisites by number.
To pick Paul’s example:
%.o : %.c
	$(CC) $(CFLAGS) -c -o $@ $<

foo.o: foo.c foo.h bar.h baz.h

#

foo%: arg%1 arg%2 arg%3 arg%4
	bld $< $(word 3,$^) $(word 2,$^) $(word 4,$^)
In both cases syntactic reshuffling breaks the build rules
by passing wrong file name.
To fix it I came up with a way to store two orders at the same time:
syntactic and shuffled and posted patch as
https://lists.gnu.org/archive/html/bug-make/2022-02/msg00042.html.
Running make --shuffle on real projects
While I was waiting for the feedback I ran the build tests against
nixpkgs packages.
First, I almost instantly got build failures on the projects that
already explicitly disable parallel builds in nixpkgs to avoid known
failures: groff, source-highlight, portaudio, slang, gnu-efi,
bind, pth, libomxil, dhcp, directfb, doxygen, gpm, judy
and a few others. That was a good sign.
A bit later I started getting failures I did not encounter before in
ghc(!), gcc(!!), automake(!!!), pulseaudio,
libcanberra, many ocaml and some perl packages.
All the failures looked genuine missing dependencies. For example
gcc’s libgfortran is missing a libquadmath build dependency.
It is natural not to encounter it in real world as libquadmath is
usually built along with other small runtimes way before g++ or
gfortran is ready.
Fun fact: while running the build I stumbled on a GNU make bug
not related to my change:
https://lists.gnu.org/archive/html/bug-make/2022-02/msg00037.html.
The following snippet tricks GNU make to loop for a while until
it crashes with argument list exhaustion (or inode exhaustion in
/tmp):
$ printf 'all:\n\techo $(CC)' | ./make -sf -
<hung>
This bug is not present in any releases yet. And hopefully will not be.
I’d like to land the --shuffle change upstream in some form before
sending bug reports and trivial fixes to upstream projects.
How you can test it
If you are keen to try this shuffling mode on your make-based
projects (be it manually written, automake-based or cmake-based)
here is a rough instruction to do it:

install GNU make 4.4
use it as make --shuffle <your-typical-make-arguments>
against your project (or set it via GNUMAKEFLAGS=--shuffle
environment variable)
check if the build succeeds, run it a few times

Both sequential and parallel modes should work fine. I suggest trying
both. The shuffling overhead should be negligible.
How do failures look like
When build fails it reports the shuffling mode and seed used. Let’s try
it on a concrete cramfsswap example:
$ git clone https://github.com/julijane/cramfsswap.git
$ cd cramfsswap

$ ~/dev/git/make/make clean && ~/dev/git/make/make
rm -f cramfsswap
gcc -Wall -g -O -o cramfsswap -lz cramfsswap.c
strip cramfsswap

$ ~/dev/git/make/make clean && ~/dev/git/make/make
rm -f cramfsswap
strip cramfsswap
strip: 'cramfsswap': No such file
make: *** [Makefile:10: strip] Error 1 --shuffle=1645603370
Here we see a successful run and a failed run. Failed run reports
specific seed that might trigger the failure: --shuffle=1645603370.
We can use this seed explicitly:
$ ~/dev/git/make/make --shuffle=1645603370
strip cramfsswap
strip: 'cramfsswap': No such file
make: *** [Makefile:10: strip] Error 1 --shuffle=1645603370
$ ~/dev/git/make/make --shuffle=1645603370
strip cramfsswap
strip: 'cramfsswap': No such file
make: *** [Makefile:10: strip] Error 1 --shuffle=1645603370
$ ~/dev/git/make/make --shuffle=1645603370
strip cramfsswap
strip: 'cramfsswap': No such file
make: *** [Makefile:10: strip] Error 1 --shuffle=1645603370
Note how ordering is preserved across the runs with fixed seed.
Parting words
Implementing the shuffling idea took a weekend. I should have tried
it earlier :) The result instantly found existing and new missing
dependencies in a small subset of real projects. Some of these failures
are very hard to trigger otherwise.
UPDATE: --shuffle was upstreamed and released as part of
GNU make 4.4 release (see announcement).
Have fun!

    Posted on February 23, 2022 by trofi. Email,
    pull requests or comments
    are welcome!



            powered by hakyll



"
https://news.ycombinator.com/rss,Ask HN: Books that teach programming by building a series of small projects?,https://news.ycombinator.com/item?id=34412069,Comments,"

Ask HN: Books that teach programming by building a series of small projects? | Hacker News

Hacker News
new | past | comments | ask | show | jobs | submit 
login




 Ask HN: Books that teach programming by building a series of small projects?
342 points by newsoul 7 hours ago  | hide | past | favorite | 165 comments 

It is common knowledge that when first learning programming, one should start with small projects to build something real rather than learning rules and syntax of the language only.Which are some of the best books that take a project based approach in teaching programming to a beginner? 
 
  
 
azemetre 4 hours ago  
             | next [–] 

While not for beginners, if you'd like to learn rust I recently finished ""Command line Rust"" [1].It was my first introduction to rust and the book was quite enjoyable. It starts off with teaching you the very basics of a command line (what it means to exit, true, or false, etc) and each chapter has you recreate a popular command line tool (like grep, cal, tail, wc) while introducing a new rust concept.The book also does TDD, test driven design, by first teaching you how to create these tests then in subsequent chapters having the tests prewritten for you.It's definitely worth a look, the author has a great writing style as well that isn't as monotonous as most programming books I've read.[1] https://www.oreilly.com/library/view/command-line-rust/97810...
 
reply



  
 
dbalatero 3 hours ago  
             | parent | next [–] 

Just to clarify, when you say ""not for beginners"" do you mean beginners to Rust or programming in general?edit: Based on your ""It was my first introduction to rust"" comment I think maybe you meant programming beginners?
 
reply



  
 
azemetre 2 hours ago  
             | root | parent | next [–] 

Sorry, I meant that you need some programming experience. It will not teach the basics of if loops, making structs, etc; it's a good introduction to rust, prior programming experience required.
 
reply



  
 
lifewallet_dev 3 hours ago  
             | parent | prev | next [–] 

Thank you! I needed this one, how fun, I did  this when learning Haskell, now I can do it with Rust.
 
reply



  
 
adolph 4 hours ago  
             | parent | prev | next [–] 

Came here to mention just that book. It is excellent.
 
reply



  
 
twawaaay 6 hours ago  
             | prev | next [–] 

My favourite is ""Practical Common Lisp"" by Peter Seibel (https://gigamonkeys.com/book/)Not only is the book free to read (although I would suggest to pay for it if you like it!) The code to parse binary files actually ""inspired"" my design of an actual production application which was very flexible, succinct and, most importantly,  so fast I had to spend a lot of effort convincing people the numbers are actually true.It taught me some important lessons about how you can achieve performance with Lisp languages and the real reasons for the power of macros. Not too shabby for the first book on Lisp I red!
 
reply



  
 
peterhil 1 hour ago  
             | parent | next [–] 

I was going to suggest this. It's cool that you build a file based database (IIRC) and query language in the third chapter!
 
reply



  
 
halkony 4 hours ago  
             | parent | prev | next [–] 

I've been learning lisp this past week. I think I get the main idea of ""code is data and data is code"", but I struggle to understand how it's more powerful than, say, making a function factory in Python.
 
reply



  
 
majormunky 3 hours ago  
             | root | parent | next [–] 

I'm also learning lisp, mainly as a part of learning emacs, so I may not be super accurate here.  In python, when you have a reference to a function, its a bit opaque, you can tell the thing is a function, but I don't think you can really go into the function object, and alter it lets say.  You may be able to using the AST or something, but its not a ""first-class"" thing you can do.In lisp (and code is data, data is code), the function is also a list, so you would be able to use all the normal programming tools to iterate over the list, add new items to the list, etc, which can change how the function runs.
 
reply



  
 
altruios 2 hours ago  
             | root | parent | next [–] 

Such a concept could use an example to solidify that point.If I have a function Foo that adds two numbers together.
How is that represented in lisp (as a list you say?)?
How would one alter that function/lisp-list: to multiply numbers instead?
 
reply



  
 
majormunky 2 hours ago  
             | root | parent | next [–] 

I've not really encountered any situations where I've needed to do anything like this yet (still learning), but, I found this example, where there's an existing function, and someone alters the body of the function to create a new function.  My guess is that there's various ways of doing this type of stuff:  https://stackoverflow.com/a/1220198Edit: a macro example, which I think may be the more common route of doing this type of stuff?  https://lisp-journey.gitlab.io/blog/common-lisp-macros-by-ex...
 
reply



  
 
kfoley 3 hours ago  
             | root | parent | prev | next [–] 

I used to have the same issue understanding the benefit.  One simple example that helped it clicked for me was considering the `when` or `unless` macro.They're just very simple macros based around the `if` special form.  Because they're macros, they can just treat the body as data, i.e. it's not executed as part of argument resolution.If you wanted to make a `when` function in Python, the ""body"" would have to be a callable like a lambda or named function.I know this probably isn't the most compelling use case but I found it to be a really simple way to understand some of the benefits of macros and how they could be used elsewhere.
 
reply



  
 
cess11 3 hours ago  
             | root | parent | prev | next [–] 

Not sure what you mean by ""function factory"", could you elaborate? Isn't it metaclasses and decorators that are used for metaprogramming in Python?In my opinion the 'everything is an expression' is the really tasty part of the lisps, which goes well with quoting.
 
reply



  
 
twawaaay 2 hours ago  
             | root | parent | prev | next [–] 

There is a number of reasons that make Lisp macros so much more powerful than functions in any other language.Consider you want to implement printf that is compatible with C printf.As the function implementor, you want something like printf(""%d"", 5+2, anExpensiveFunctionThatProducesAString()) do the right thing and print ""7"" on the terminal, but you get many more options and mechanisms you can use when implementing it in Lisp vs other languages.Every other language:* every time printf is called, all of the arguments have already been evaluated. There is no way to avoid calling the expensive function when the format indicates the result is not needed.* every time the function runs you have to parse the format string. You could try and be smart and cache parsing results but this will quickly make it very expensive and complex and might cause problems when format strings are generated dynamically.* your function cannot make use of the fact that both the format string and the argument never changeLisp macro:* you can decide to only evaluate arguments when they are needed,* you can notice that the format string is static and output code IN PLACE of the macro based on this information. This means you have the option to make parsing happen only at compile time if it is possible.* you can analyse the arguments further and decide to output code that does princ('7') IN PLACE of the macro. The resulting code will have no operations to parse or analyse anything and will always execute quickly and efficiently.In general this is the way I think about Lisp macros vs functions in other languages:""Lisp macros allow you to run logic at compile time that can create code dynamically that does exactly what you need it to do, no more, no less, based on macro arguments and/or unevaluated macro arguments and/or the environment.""Lisp macros are ultimate in building abstractions. Where in other languages you have to contend with the language always ""sticking out"" of your abstractions -- there is only so much you can do to hide the underlying language, in Common Lisp you can always hide the language completely. Implementing your abstraction is taking a data structure (can be for example an XML file) and running an arbitrary logic to generate another data structure (a function) that is then outputted in place of the macro. If the XML describes your binary file structure, the macro can take those descriptions and generate perfect parsing code that does exactly what is needed without any extra stack frames, conditionals, syntax artifacts, etc.
 
reply



  
 
agumonkey 3 hours ago  
             | root | parent | prev | next [–] 

maybe a function factory is just an OO pattern for return closures, it's difficult to not perceive things from 'now', but i'll be frank i 1) never liked DP 2) couldn't spend more than 5 minutes reading about them as it seemed more verbose and redundant
 
reply



  
 
kennyfrc 7 hours ago  
             | prev | next [–] 

There’s quite a few:- Zed Shaw’s Learn More Python the Hard Way[1]- Brian Hogan’s Exercises for Programmers (best for beginners or for learning a new language)[2]- Hal Fulton’s The Ruby Way[3]- Chris Ferdinandi’s Vanilla JS Academy[4]- Marc-Andre Cournoyer’s Great Code Club (it’s old, and the community doesn’t exist anymore, but i think he still maintains it)[5]- A few python books from No Starch Press (notably those authored by Al Sweigart)I learned the most as a beginner  from Zed Shaw’s work, and from reading open source code.Once you’re done with the initial “learn from tutorials” phase, there’s no better resource than reading open source code.[1] https://www.amazon.com/Learn-More-Python-Hard-Way/dp/0134123...[2] https://www.amazon.com/Exercises-Programmers-Challenges-Deve...[3] https://www.amazon.com/Ruby-Way-Programming-Addison-Wesley-P...[4] https://vanillajsacademy.com/[5] https://www.greatcodeclub.com/
 
reply



  
 
Zhyl 6 hours ago  
             | parent | next [–] 

Specifically ""Automate the Boring Stuff with Python"" is my favourite Al Sweigart book.
 
reply



  
 
downrightmike 34 minutes ago  
             | root | parent | next [–] 

Best explanation of Regex that I have ever seen
 
reply



  
 
michaelcampbell 5 hours ago  
             | root | parent | prev | next [–] 

I'm not much of a Python guy, but concur with this; I enjoyed this book and did get something out of it.
 
reply



  
 
lazyant 6 hours ago  
             | parent | prev | next [–] 

> - A few python books from No Starch Press (notably those authored by Al Sweigart)Yes for example https://nostarch.com/inventwithpython and https://nostarch.com/big-book-small-python-projects
 
reply



  
 
jypepin 4 hours ago  
             | parent | prev | next [–] 

I learnt ruby with Zed Shaw’s Learn Ruby the Hard Way. Was a really great experience!
 
reply



  
 
darreninthenet 7 hours ago  
             | parent | prev | next [–] 

Is Shaw's first Python book any good?
 
reply



  
 
danpalmer 6 hours ago  
             | root | parent | next [–] 

Shaw has had some strong opinions that have gone significantly against the mainstream viewpoint in Python. Whether he's right or not doesn't really matter - as an educator that is targeting the mainstream it's fairly important to stay on that and not introduce personal bias around these things. Unfortunately his criticism was often not very constructive.
 
reply



  
 
cameron_b 5 hours ago  
             | root | parent | next [–] 

despite my other comment here I do agree with you on this, especially remembering his comments on test tools, nose for example
 
reply



  
 
danpalmer 4 hours ago  
             | root | parent | next [–] 

Yeah, he pushed back hard against the Python 3 migration, by picking on a small number of trade-offs that he personally didn't like and using that to conclude that the entire endeavour was doomed. The Python 3 migration wasn't flawless by any means, but it was better than he made out.I know his ""criticism"" also held back many from moving their libraries to Python 3 by sowing doubt in the community, and ultimately may have slowed down adoption overall.
 
reply



  
 
cameron_b 6 hours ago  
             | root | parent | prev | next [–] 

I found his Learn Python3 The Hard Way to be a great series of exercises if you're starting out or starting again. You won't find -everything- on any topic, but the exercises are complete enough to get you rolling and seed the sort of patterns that will make you successful in looking for more material.I had a little bit of Python(2) under my belt from general curiosity, and I've done some C++ in high school so I have some ""CS Theory""There is a ""more"" python version out now 
https://learncodethehardway.org/more-python-book/Amazon ( the ""newer"" version proposed is not, it's his python2 book )
https://www.amazon.com/Learn-More-Python-Hard-Way/dp/0134123...
 
reply



  
 
kennyfrc 7 hours ago  
             | root | parent | prev | next [–] 

If you’re a pure beginner with zero programming experience, yes. If you’ve programmed a few scripts and have done a flask web app, his first book might be too easy.
 
reply



  
 
mayankkaizen 2 hours ago  
             | root | parent | prev | next [–] 

If you ignore his strong opinions, it can be good. Although he was (is?) very much hated back in the day (when Python 2 was still mainstream), many many noobs did benefitted from his book. So I'd say give it a try.
 
reply



  
 
7speter 6 hours ago  
             | parent | prev | next [–] 

> and from reading open source code.Do you just search for projects in the language your using on github?
 
reply



  
 
kennyfrc 6 hours ago  
             | root | parent | next [–] 

Yes, and I would typically search for simple projects using keywords such as “micro”, “small”, “tiny”, or “pico”.I would then try to re-implement the “getting started” code in the readme from scratch, like a little programming puzzle. If I can’t figure it out, I‘ll add in some debugger breakpoints, inspect the stack trace to understand how it works, then code the needed methods / classes as I go.If you’re a ruby programmer, soveran’s work in github can be read in a day or two. I specifically like cuba (micro webframework) and mote (microtemplate).
 
reply



  
 
FranklinMaillot 57 minutes ago  
             | prev | next [–] 

Not a book but The Coding Train on Youtube is probably one of the best introduction to programming. It's fun to watch, Daniel Shiffman is a fantastic teacher and he doesn't take himself too seriously.But what sets this course apart from any other are the coding challenges. Following the thought process of a programmer in real time, watching him making mistakes and hunting for bugs is invaluable.He uses p5js, a javascript library for graphics. The lessons are geared towards generative art but cover a broad range of topics including machine learning.https://www.youtube.com/@TheCodingTrain
 
reply



  
 
nerpderp82 40 minutes ago  
             | parent | next [–] 

Coding Train is soooo good, it is like if Bob Ross had a baby with Donald Knuth.
 
reply



  
 
__warlord__ 7 hours ago  
             | prev | next [–] 

Not a book, but check [Build your own X](https://github.com/codecrafters-io/build-your-own-x), a compilation of well-written, step-by-step guides for re-creating our favorite technologies from scratch.
 
reply



  
 
deckard1 3 hours ago  
             | parent | next [–] 

Excellent resource. Never seen that before.In a similar vein, there is The Architecture of Open Source Applications which is great:https://aosabook.org/en/index.html
 
reply



  
 
inxode 4 hours ago  
             | parent | prev | next [–] 

It was better when Daniel maintained it.
 
reply



  
 
intelVISA 1 hour ago  
             | root | parent | next [–] 

Can't believe he sold out..
 
reply



  
 
culopatin 5 hours ago  
             | prev | next [–] 

My biggest gripe with all the project tutorials I’ve seen is that really quickly you get into a “ok I get what we’re doing” and then just copy the code they provide. At the end of the day you did zero mental workout because you didn’t have to figure out the code.
I hope to find one of these that guides you but doesn’t give you the code immediately, but has a code answer key at the end or something. I get it that it’s not easy to do, but there must be one out there that doesn’t spoon-feed you everything.
 
reply



  
 
cardanome 4 hours ago  
             | parent | next [–] 

One thing that helps me a lot when learning new languages is to NOT copy and paste but actually re-type the code.You will be forced to read more carefully and, more importantly, you will make typing mistakes. Hunting down your own mistake will teach you the subtleties of the syntax as well as further increase your understanding of the code.For more advanced programmers, there is also the challenge of translating the code into another language entirely, which requires a actual semantic understanding of the code.Also once the project is done, adding small additional bonus features will further deepen your understanding.
 
reply



  
 
culopatin 3 hours ago  
             | root | parent | next [–] 

I re type it but I don’t necessarily learn how to come up with that code when I’m off the leash. It’s useful when learning new syntax for sure though.
 
reply



  
 
gre 1 hour ago  
             | root | parent | next [–] 

You can try to retype it from memory too. Or write down a list of the crux functions (like a word bank) and then figure it out again.
 
reply



  
 
grugagag 2 hours ago  
             | root | parent | prev | next [–] 

It’s a stepping stone. Doing that will make you a little bit less uncomforable when you get out of your depth, that until you hit another stepping stone and feel even more comforable.
 
reply



  
 
ojl 4 hours ago  
             | parent | prev | next [–] 

That’s why I like The Ray Tracer Challenge. He gives you test scenarios and a little bit of pseudo code (sometimes). The actual implementation has to be done by the reader.http://raytracerchallenge.com/
 
reply



  
 
aeontech 1 hour ago  
             | root | parent | next [–] 

That's really cool, probably deserves its own submission for discussion!
 
reply



  
 
the_gastropod 4 hours ago  
             | root | parent | prev | next [–] 

+1 This is one of my favorite programming books. It's a very rewarding-feeling experience to build something so capable so quickly.
 
reply



  
 
mgomez 4 hours ago  
             | parent | prev | next [–] 

To force myself “to figure out the code,” I typically do the tutorial in a different language. I wanted to get familiar with Java last year, but all the books/tutorials were boring to me. What I ended up doing was going through a book that showed how to write a backend in Go (which was also unfamiliar to me at the time) while simultaneously porting it to Java (using Vert.x). Probably sounds inefficient tackling two languages at once, but it was the only way I was able to stay engaged with the material. Of course I ended up with something unidiomatic in terms of Java, but the project works and I now have motivation for a proper refactor to keep the momentum going.
 
reply



  
 
dgb23 1 hour ago  
             | root | parent | next [–] 

I have done the same thing! Read Crafting Interpreters but wrote the first half in Clojure instead of Java.I think it's a good method for experienced programmers, who are self-taught (like me) or just want to brush up some skills and knowledge.Yes, you stay engaged, but there is also something to be said about porting code between different languages. The differences, strengths, weaknesses etc. stand out more, similarities become more clear and things that seem different, but are analogous emerge.
 
reply



  
 
usmannk 1 hour ago  
             | root | parent | prev | next [–] 

I was looking for golang in this thread and this is the only reference I found. Would you mind linking the Go book you reference? Thanks
 
reply



  
 
mgomez 4 minutes ago  
             | root | parent | next [–] 

Sure thing:https://lets-go-further.alexedwards.netThat book is a follow-up to an easier book [1] that I also bought, but ended up skipping since I was only interested in writing JSON APIs.[1] https://lets-go.alexedwards.net
 
reply



  
 
thr0wnawaytod4y 4 hours ago  
             | parent | prev | next [–] 

For this reason you need to find _your own_ projects, solve your own problems (which are actually pretty common, but you have to search or figure out a solution)That's what I tell everyone
 
reply



  
 
travisjungroth 4 hours ago  
             | root | parent | next [–] 

The issue I’ve found with that in my students is they then have to play the role of product manager in addition to software engineer. Those choices add a lot of mental overhead. There’s a balance between motivation and pressure for most students.I think cloning is more the way to go. Pick some existing project/product and copy the essential components. Less choices, doesn’t have to end up getting used.
 
reply



  
 
intelVISA 4 hours ago  
             | parent | prev | next [–] 

Thinking of building something along those lines i.e. you can peek ahead if you really get stuck.Proj-based learning is really fun to both create and consume imo.
 
reply



  
 
culopatin 4 hours ago  
             | root | parent | next [–] 

If it helps you in any way, what I try to do is to use a tutorial book to build my own app. For example a Flask tutorial that shows how to build a blog app, but I’m building a library. I still learn about the Flask way and I learn some building blocks but I have to figure out the equivalent for what I want to do. I often get stuck though.So maybe showing the building blocks and letting you use them for something. You could perhaps show 2 or 3 suggested projects that are built with the building blocks. You show the generic version of the blocks, and then you figure out how to tweak them to make one of the 2 or 3 suggested projects.Sort of buying the legos and then showing what you could do with them. But building it is up to you.
 
reply



  
 
intelVISA 1 hour ago  
             | root | parent | next [–] 

Good idea, originally I would just build something that already exists from scratch e.g. a UNIX-like OS complete with 99 footguns.Now I've learned better I try to channel my learning energies into extending my portable, reusable library that solves common problems I can iterate, improve and port into future projects.
 
reply



  
 
dielll 4 hours ago  
             | parent | prev | next [–] 

Datacamp does what you want
 
reply



  
 
kriro 6 hours ago  
             | prev | next [–] 

""Hands on Rust"" teaches Rust by building a rogue like game step by step. Before that there's a chapter where you rebuild Flappy Bird from scratch that teaches the ""basics"" before diving into more advanced concepts in the rogue like. I liked the approach and recommend the book but it's fast paced and expects quite a bit from the reader (it's excellent if you have some programming experience already but probably daunting as a true first book imo).
 
reply



  
 
myers 6 hours ago  
             | parent | next [–] 

I second this one.  This was a great book.There are some follow ups to the final project I likeCompile to WASM so you can run on the web: <https://hands-on-rust.com/2021/11/06/run-your-rust-games-in-...>Port to the Bevy ECS: <https://saveriomiroddi.github.io/learn_bevy_ecs_by_ripping_o...>
 
reply



  
 
kriro 5 hours ago  
             | root | parent | next [–] 

Oh cool I wasn't aware of these. Thanks for sharing, both look very interesting. I like the idea of reusing projects to teach different tech :)
 
reply



  
 
ggrothendieck 16 minutes ago  
             | prev | next [–] 

Per Brinch Handsen, Programming a Personal Computer. Book is online. Interpreter, editor and OS in about 10 KLOC. http://pascal.hansotten.com/per-brinch-hansen/ Also https://news.ycombinator.com/item?id=24913959
 
reply



  
 
modernerd 6 hours ago  
             | prev | next [–] 

For Swift:https://www.apple.com/swift/playgrounds/ (I've had more success introducing programming with this than with any of the links below; it's a very compelling intro for those who already own an iPad/Mac, and the core concepts are generalisable to other languages/environments even if it's specific to Apple's APIs and hardware.)~~~For Python:https://automatetheboringstuff.comhttps://nostarch.com/big-book-small-python-projects~~~For JS:https://eloquentjavascript.net (project-based and for beginners)https://javascript30.com (not for total beginners or self-study, would need a friend/tutor)
 
reply



  
 
bpesquet 4 hours ago  
             | parent | next [–] 

I beg to differ: for all its qualities, Eloquent JavaScript is not a very beginner-friendly book. It goes quite deep into many JS intricaties and contains a lot of challenging tasks for people just discovering programming.More beginner-focused alternatives are https://www.freecodecamp.org and https://thejsway.net.Disclaimer: I've been teaching programming professionally for 15+ years and wrote the latter.
 
reply



  
 
Sn0wCoder 3 hours ago  
             | root | parent | next [–] 

I have recommended to beginners before and most of them thanked me.  I skimmed the page for your book but did not see the link to the free version.  I am on mobile so maybe just missed it.  Says read free here, but here is not a link.  Clicking the book takes you to Amazon.
 
reply



  
 
Sn0wCoder 3 hours ago  
             | root | parent | next [–] 

Never-mind the website is the book lol
 
reply



  
 
Sn0wCoder 5 hours ago  
             | parent | prev | next [–] 

I second eloquent JavaScript.  Been doing this book since v1 and recommend to just about anyone that asks for recommendations.  I use typescript daily and have been hoping that this book gets updated for TS although it would be hard since it uses lots of JS tricks that may or may not be best practices in TS.  Technically just renaming the files from .js to .TS would work the linter would go crazy
 
reply



  
 
samvher 4 hours ago  
             | prev | next [–] 

Software Foundations is a series of interactive books where you work through proofs in Coq, gradually increasing in complexity: https://softwarefoundations.cis.upenn.edu/This is a bit on the edge of the domain that you're asking about (not really for beginners, and proofs are perhaps a somewhat niche type of programming) but I learned a lot from this and many people don't seem to know it, so I think it belongs in the list.
 
reply



  
 
jtolds 39 minutes ago  
             | prev | next [–] 

Python Programming for the Absolute Beginner (https://smile.amazon.com/Python-Programming-Absolute-Beginne...) has you make a game at the end of every chapter with the concepts you've learned so far. I've bought this book for a number of people, though perhaps Automate the Boring Stuff with Python: Practical Programming for Total Beginners (https://smile.amazon.com/gp/product/1593275994/) is better if you don't want to make games.
 
reply



  
 
rwieruch 1 hour ago  
             | prev | next [–] 

Shameless plug if allowed: “The Road to React” goes through one large project to teach React.js from beginner to intermediate. I learned that going through one project it allows the teacher to go through a series of foundational aspects of the topic before implementing more  advanced domain specific features which make use of the earlier learned building blocks for the real world project. Does it make sense?https://www.amazon.com/Road-learn-React-pragmatic-React-js/d...
 
reply



  
 
CraigJPerry 5 hours ago  
             | prev | next [–] 

HTDPv2 https://htdp.org/ - it's about learning to program rather than teaching a specific language.I thought the little projects you build along the way struck a good balance of interest, e.g. you're building a snake game by section 2 (but crucially you've already been exposed to how to think about data so you're not just dropped into the deep end with some boilerplate to fiddle with).Anyway, i could write tons of praise for this book but it's convinced me to ditch python for teaching newbies and i LOVED python for getting newbies started real quick with projects for years.
 
reply



  
 
dns_snek 4 hours ago  
             | parent | next [–] 

That sound like an interesting approach and I'd love to hear more about it!One striking difference compared to most ""beginner languages"" is the Lisp-like syntax. I think focusing on problem solving rather than learning syntax is a great goal, but does this particular choice of a language make it harder for your ""students"" to continue learning in another language?I'm assuming they would switch to a more popular language at some point (with syntax that resembles C), so I'm curious what that transition is like for beginners.
 
reply



  
 
agumonkey 3 hours ago  
             | parent | prev | next [–] 

Interesting, I rarely see it mentioned. How do people react to the mindset it teaches ? I really like that it's near language-less programming book, really more about and/or :)
 
reply



  
 
newsoul 5 hours ago  
             | parent | prev | next [–] 

I wouldn't mind if you write some more about the book.
 
reply



  
 
rahuldan 7 hours ago  
             | prev | next [–] 

There is this excellent Github repo for this: https://github.com/codecrafters-io/build-your-own-xIt has a collection of blogs for building various small projects to learn different languages.
 
reply



  
 
asicsp 7 hours ago  
             | parent | next [–] 

Similar resources:* Projectbook https://projectbook.code.brettchalupa.com/* Project Based Learning https://github.com/practical-tutorials/project-based-learnin...
 
reply



  
 
jcon321 5 hours ago  
             | prev | next [–] 

No joke, ""C for Dummies"" - it's more teaching you CS101 concepts, less about ""C"". Tons of examples to go through and the attitude of the author is very helpful compared to a lot of other styles out there.I was failing my computer science classes first semester. Towards the end of the semester I sat down in the library for a few days and just went page by page typing every example and compiling it. Aced every IT class I had the next few years. I attribute that book to making everything ""click"" for me.Once you understand the beginner concepts that book shows, then you as an individual would know where to look to gain improvement.
 
reply



  
 
ojl 48 minutes ago  
             | prev | next [–] 

I mentioned The Ray Tracer Challenge in a comment below, and it seems quite popular so I’ll submit it here as well.http://raytracerchallenge.com/It’s not really for beginners though, but maybe possible to start it together with a beginners book in a language of choice. It starts with points, vectors and matrixes, but gets more advanced later on. It’s around 250 pages and there’s basically no code provided, just test scenarios and a bit of pseudo code.
 
reply



  
 
tsm 7 hours ago  
             | prev | next [–] 

Software Design by Example (https://www.routledge.com/Software-Design-by-Example-A-Tool-...)Available free online at https://third-bit.com/sdxjs/
 
reply



  
 
ajoseps 6 hours ago  
             | prev | next [–] 

This book is still in progress but I've gone through some of the chapters and have enjoyed it. Rust from the Ground Up: https://leanpub.com/rust-from-the-ground-upI was looking for a book that had offline projects I can work on while on flights, and this book focuses on rebuilding linux utilities using rust. The other nice part is that you get a better understanding of linux internals.I believe the author is also responsive on the rust subreddit.
 
reply



  
 
mprovost 4 hours ago  
             | parent | next [–] 

Thanks for the kind words! I'm desperately polishing the next chapter for publication...
 
reply



  
 
ajoseps 4 hours ago  
             | root | parent | next [–] 

great to see you on HN as well! I haven't revisited the book in a while but I really liked the chapters I did work on. I have another flight coming up soon so maybe I'll try another chapter :)
 
reply



  
 
patchorang 5 hours ago  
             | prev | next [–] 

Not a book and webdev focused, but I've been doing https://fullstackopen.com/en/ and it's been great.The chapters are structured so you are building 2 projects at once. One where you follow along with the chapter material with example code and explanations. And a second in the exercises.I really like it because it introduces the concepts in the context of a project, but you can't get stuck because the code is there too. Then you need to really apply it in the exercises on your own project, without the example code.
 
reply



  
 
weird_user 4 hours ago  
             | prev | next [–] 

For beginners, Daniel Holden's Build Your Own Lisp[1] is excellent material for learning C. It's a very succinct book.For intermediate programmers, Build Your Own Redis[2] is a WIP book I am currently writing.[1] https://buildyourownlisp.com/[2] https://build-your-own.org/
 
reply



  
 
friend_and_foe 1 hour ago  
             | prev | next [–] 

Write yourself a scheme in 48 hours by Johnothan Tang is a fantastic book, there's also the structure and interpretation of computer programs.https://upload.wikimedia.org/wikipedia/commons/a/aa/Write_Yo...https://web.mit.edu/6.001/6.037/sicp.pdf
 
reply



  
 
tjpnz 6 hours ago  
             | prev | next [–] 

MUD Game Programming. Not sure how easy obtaining a copy would be in 2023 but it has two projects which I recall being a lot of fun. I never did build a successful MUD but did learn a fair amount about network programming.https://www.goodreads.com/en/book/show/927128.Mud_Game_Progr...
 
reply



  
 
dartharva 6 hours ago  
             | prev | next [–] 

I don't know why jtimiclat's comment recommending Al Sweigart's books got downvoted and faded out, they are indeed quite good for beginners wanting to get their hands dirty with programming instead of cramming it in an academic way.Not to mention they're all freely available and give excellent value for both time and money: https://inventwithpython.com/
 
reply



  
 
eatonphil 5 hours ago  
             | prev | next [–] 

Sort of like that, I've been slowly working on a rosetta code for implementations of small problems. Rosettacode.org allows you to use standard library functions so many (but not all) of their examples are useless when you actually want to see how to code something from hand in the language.So my rosetta code only allows you to implement the main part of the thing yourself. But it's not appropriate for beginners. It's more appropriate for teaching a new language to someone who is already an intermediate programmer.https://tinyprograms.org/
 
reply



  
 
hiAndrewQuinn 5 hours ago  
             | parent | next [–] 

God, I hope you just found Rosetta.code off limits. That's a fantastic name
 
reply



  
 
nemoniac 7 hours ago  
             | prev | next [–] 

Nand2tetris https://www.nand2tetris.org/
 
reply



  
 
WoodenChair 3 hours ago  
             | prev | next [–] 

For intermediate programmer, that's what we do in the Classic Computer Science Problems series (https://classicproblems.com). We combine learning programming with learning computer science problem solving techniques.
 
reply



  
 
larve 6 hours ago  
             | prev | next [–] 

Peter norvig’s paradigms of artificial intelligence programming, despite its age, is a delight.https://norvig.github.io/paip-lisp/#/
 
reply



  
 
newsoul 6 hours ago  
             | parent | next [–] 

But is that suitable for a beginner?
 
reply



  
 
larve 5 hours ago  
             | root | parent | next [–] 

It depends on what you mean by beginner, but I would say for someone who has been futzing with a couple of js boot camps or building some php websites, it’s what takes you from hacking to software development. Completely new to programming, maybe, if someone has a background in maths or physics and has a sense of “rigor” in thinking.But, of all the books I worked through, this is the one I remember just having tons of fun because the examples are so cool and the code so… clear. I think norvigs programming course in python (which I only skimmed) could be a great modern equivalent.
 
reply



  
 
nesarkvechnep 6 hours ago  
             | root | parent | prev | next [–] 

Yes, it is. It’s not about machine learning, neural networks but the more approachable AI of the 80s.
 
reply



  
 
globalise83 6 hours ago  
             | root | parent | next [–] 

Haven't tried his, but can GPT3 write code that results in 1980s style AI?
 
reply



  
 
forrestbrazeal 6 hours ago  
             | prev | next [–] 

My Cloud Resume Challenge project [0] and book [1] uses a set of small, stackable mini-projects to introduce beginners to many of the pragmatic skills used in cloud software engineering.[0] https://cloudresumechallenge.dev/docs/the-challenge/
[1] https://cloudresumechallenge.dev/book/
 
reply



  
 
phowat 6 hours ago  
             | prev | next [–] 

http://landoflisp.com/Was a really fun read.
 
reply



  
 
rg111 6 hours ago  
             | parent | next [–] 

A similar great book is Realm of Racket.
 
reply



  
 
ljf 1 hour ago  
             | prev | next [–] 

The 'Arduino Projects Book' does a brilliant job if taking someone who knowing nothing about coding or electronics and setting out lessons and projects that take you from zero to somewhere fun.I still can't really code, but I can have a lot of fun tinkering because of that book.
 
reply



  
 
jonjacky 5 hours ago  
             | prev | next [–] 

The famous Structure and Interpretation of Computer Programs (SICP) does this, in the Scheme language.
The whole book is online here, with the table of contents right at the front:https://web.mit.edu/6.001/6.037/sicp.pdf
 
reply



  
 
bosch_mind 5 hours ago  
             | parent | next [–] 

There’s a JS edition too
 
reply



  
 
ilyt 2 hours ago  
             | prev | next [–] 

It's common approach because building something that you want is good motivation.I don't think that works as well when it is some random project from the book. Like, sure, better than dry code, but I'm far more motivated to learn when I also make something for me.
 
reply



  
 
Icathian 2 hours ago  
             | parent | next [–] 

The challenge I, and I think many others, run into with this is that there's almost never anything I ""want"".  I spend so much time racking my brain for what to build I never start.Having a nice curated problem put in front of me sets me off and it's easy from there.
 
reply



  
 
dgb23 1 hour ago  
             | root | parent | next [–] 

I agree in regards to complete beginners.But I think for more intermediate or advanced programmers it is very beneficial to go about a project in a more self-driven, free form way. Software design, exploration, research, reading specs and so on are half of the work (or more) in programming.What I would have loved to realize when I was younger is that there's nothing you can't write. It might be a drastically simplified version compared of the real thing, but you can absolutely do it.An operating system, a browser, a web server (""from scratch"") a programming language, a video game (with a custom engine)... It's all possible with enough effort. And even if you don't want to do the up-front work yourself, it is easier to find cool, guided projects out there if you already know what you want.
 
reply



  
 
tmtvl 6 hours ago  
             | prev | next [–] 

Common Lisp: A Gentle Introduction to Symbolic Computation(https://www.cs.cmu.edu/~dst/LispBook/) has you build a number of small applications, like a substitution decipher application, a plotting function, and more, which it calls ""keyboard exercises"".
 
reply



  
 
theshrike79 7 hours ago  
             | prev | next [–] 

If we're talking a complete beginner with maybe only ""hello world"" level skills I recommend the Lego BOOST set: https://www.lego.com/en-fi/product/boost-creative-toolbox-17...It teaches you step by step the basics of programming (loops, function calls etc) using a scratch-like programming language.
 
reply



  
 
oidar 3 hours ago  
             | parent | next [–] 

Any alternative to this? I think lego stopped making it.
 
reply



  
 
theshrike79 24 minutes ago  
             | root | parent | next [–] 

the Mindstorms Robot Inventor is a bit more advanced (Scratch-like vs Scratch Jr on the Boost) and twice as expensive: https://www.lego.com/en-us/product/robot-inventor-51515
 
reply



  
 
rcardo11 3 hours ago  
             | prev | next [–] 

This is the best programming book I have ever picked https://www.amazon.com/-/es/Noam-Nisan/dp/0262539802/ref=sr_...
 
reply



  
 
dan-g 4 hours ago  
             | prev | next [–] 

https://beautifulracket.com/ by Matthew Butterick - “An Introduction to Language-Oriented Programming Using Racket”
 
reply



  
 
saboot 1 hour ago  
             | prev | next [–] 

This was just released, I haven't picked it up yet but it's definitely on my wish listhttps://www.manning.com/books/tiny-c-projects
 
reply



  
 
RomanPushkin 4 hours ago  
             | prev | next [–] 

Shameless plug:FREE https://leanpub.com/rubyisforfun - Ruby Is For Fun, I made the book so you do a lot of exercise. There is around 80 exercises in total.Rewritten as a course:https://www.educative.io/courses/handbook-ruby-developers
 
reply



  
 
jtmiclat 7 hours ago  
             | prev | next [–] 

I found Al Sweigart's books nice for beginners.Highlights 
https://automatetheboringstuff.com
https://inventwithpython.com/bigbookpython
 
reply



  
 
throwawaybnb123 7 hours ago  
             | prev | next [–] 

Codecademy has projects page: https://www.codecademy.com/projects
 
reply



  
 
albertzeyer 1 hour ago  
             | prev | next [–] 

Same question, but not for beginners but for experienced programmers:Which book would you recommend to improve architectural design decisions, developing complex systems, etc? But on actual examples, e.g. by building a series of projects, or parts of it?
 
reply



  
 
elias94 6 hours ago  
             | prev | next [–] 

I would suggest Practical Common Lisphttps://gigamonkeys.com/book/You will create a MP3 database, a web server, a spam filter, a HTML generator. Really practical!
 
reply



  
 
fedeb95 5 hours ago  
             | prev | next [–] 

Parenthesis: keep in mind that while learning by doing might be a good way to begin, even if I wouldn't be so sure as wether one ""should"" or ""could"" start along such a path, at some point you'll be stuck without studying the ""boring"" stuff you've delayed. True programming is realising the boring stuff weren't so boring in the first place
 
reply



  
 
gen_greyface 7 hours ago  
             | prev | next [–] 

check thishttps://news.ycombinator.com/item?id=22299180
Ask HN: What are some books where the reader learns by building projects?
 
reply



  
 
MarkusWandel 3 hours ago  
             | prev | next [–] 

""Software Tools"" by Kernighan & Plauger, perhaps.The original was in Ratfor, which is a Fortran dialect.  That being similar syntax but more limited than C is not a bad intro language.
 
reply



  
 
abecedarius 2 hours ago  
             | parent | next [–] 

There's a later edition Software Tools in Pascal which had some improvements. (According to the preface iirc. I didn't read that one since I found the earlier book first.)
 
reply



  
 
fellowniusmonk 5 hours ago  
             | prev | next [–] 

It's not quite the same as what you are looking for, but I've known multiple people who were stuck in their attemp to learn to code before using railstutorial, I view it and the book ""sql: visual quickstart guide"" as two very well designed intros that manage to avoid having sections with random jumps in difficulty and terminology usage that can leave true beginners behind.
 
reply



  
 
hlship 4 hours ago  
             | prev | next [–] 

Not quite a beginner book, but you should check out https://www.amazon.com/Mazes-Programmers-Twisty-Little-Passa....
 
reply



  
 
tasuki 5 hours ago  
             | prev | next [–] 

I'm very much a fan of a project-based approach. However, I wouldn't recommend any book for that. I'd ask the apprentice what they'd like to achieve, and try to reduce scope as much as possible.There's a world of difference in motivation when building something you want to exist, versus building a project because a book told you so.
 
reply



  
 
azhenley 5 hours ago  
             | prev | next [–] 

Check out my blog series, ""Challenging projects every programmer should try"". Who knows, maybe I will turn it into a book that shows how to implement each project :)https://austinhenley.com/blog/challengingprojects.html
 
reply



  
 
cinntaile 4 hours ago  
             | parent | next [–] 

I like your list. Maybe you could break the projects down into smaller subproblems (you sort of do that already) but I personally don't like seeing the implementation. It gives you a false sense of understanding imo.
 
reply



  
 
olkyts 6 hours ago  
             | prev | next [–] 

Learning Rust, I use these 2:
1. Command-Line Rust: A Project-Based Primer for Writing Rust CLIs
2. Zero To Production In Rust (it's actually one project)
 
reply



  
 
eu 5 hours ago  
             | prev | next [–] 

http://tinypythonprojects.com/
 
reply



  
 
pixelmonkey 7 hours ago  
             | prev | next [–] 

Not a book, but CS50x is a freely available course with high quality videos and lecture notes. The lecture exercises are programming projects of sorts. It teaches programming from scratch with C and then Python. Plus, since a lot of students take it, there are lots of online resources with tips, like the CS50 subreddit.https://cs50.harvard.edu/x/2023/
 
reply



  
 
cehrlich 6 hours ago  
             | parent | next [–] 

CS50x and CS50web are fantastic. web is a bit outdated but it doesn't matter because the projects are so good.
 
reply



  
 
velmu 5 hours ago  
             | prev | next [–] 

The Symfony Fast Track book takes this approach: https://symfony.com/doc/6.2/the-fast-track/en/index.html
 
reply



  
 
LastTrain 6 hours ago  
             | prev | next [–] 

When I was young, I went about it the opposite way. I thought of some small project and researched how to implement it, then moved on to something bigger. Everybody is different of course, but I felt like I learned more having to figure out how to go about implementing each myself.
 
reply



  
 
maCDzP 2 hours ago  
             | prev | next [–] 

I like “The Elements of Computing Systems: Building a Modern Computer from First Principles”.You build a whole computer from scratch through small projects.Sure, you want learn a specific language, but you’ll grock programming in general.
 
reply



  
 
unrequited 5 hours ago  
             | prev | next [–] 

https://shop.jcoglan.com/building-git/This is an excellent resource.
 
reply



  
 
Taylor_OD 3 hours ago  
             | prev | next [–] 

Most udemy/MOOC courses are structured this way. I tend to just look for the highest rated ones on whatever language I'm interested in and make sure it says build projects or something similar.
 
reply



  
 
Plasmoid 2 hours ago  
             | prev | next [–] 

Actually, Advent of Code is pretty good.  The problems are challenging but you have a defined end condition and lots of help out on the internet.
 
reply



  
 
skizm 6 hours ago  
             | prev | next [–] 

While on the topic, how about any books or courses that teach building a 3D game engine from scratch? I'm language agnostic, so any language is fine.
 
reply



  
 
AlexeyBrin 2 hours ago  
             | parent | next [–] 

I think pikuma.com has a course about implementing a 3D rasterizer from scratch in C.
 
reply



  
 
atan2 3 minutes ago  
             | root | parent | next [–] 

The best!!!
 
reply



  
 
evandale 5 hours ago  
             | prev | next [–] 

How to Design Programs: https://htdp.org/2022-8-7/Book/index.htmlIt's lots of small problems that seem like projects and I've found it fun to expand on the examples on your own beyond what the book asks.
 
reply



  
 
boredemployee 6 hours ago  
             | prev | next [–] 

>> one should start with small projects to build something real rather than learning rules and syntax of the language only.Yes, but don't overlook the great learning that is gained, at the beginning of any learning, by studying from many different sources (aka good books). It seems that something magical happens when you do this: the confrontation of ideas from different sources can make you better absorb ideas and resolve any doubts that you didn't understand before.
 
reply



  
 
saperyton 7 hours ago  
             | prev | next [–] 

The Advanced Beginner blog post series by Robert Heaton is great.
 
reply



  
 
asicsp 7 hours ago  
             | parent | next [–] 

Link: https://robertheaton.com/2018/12/08/programming-projects-for...
 
reply



  
 
werber 7 hours ago  
             | prev | next [–] 

https://eloquentjavascript.net/ is nice and project based, it builds up from 0
 
reply



  
 
allie1 4 hours ago  
             | prev | next [–] 

Manning.com also has LiveProjects which are good for Machine Learning projects
 
reply



  
 
kdmitry 5 hours ago  
             | prev | next [–] 

I have done courses on udemy like this before for learning new languages and tech stacks. There are plenty of courses on there that you can usually get for $20 that will teach you by building a bunch of small apps or services that build up in complexity.
 
reply



  
 
numbsafari 2 hours ago  
             | prev | next [–] 

“Programming BASIC Adventure Games for the TRS-80”“Assembly Language, Step by Step”Check those out.
 
reply



  
 
autodev1 3 hours ago  
             | prev | next [–] 

- Pick a language- Find a recent book (< 2 yrs old) on LibGen.is- OR, a Udemy.com video series (note: never pay more than $12 or so-- the promo rate.  If you see a higher price, just create a new email & user account (or reset cookies?) and you'll see the promo price again)- Visit: Roadmap.sh to get a sense of a learning roadmap what knowledge to build.- Use a search engine to answer questions, such as ""free learning resource for learning <XYZ thing>""
 
reply



  
 
devd00d 6 hours ago  
             | prev | next [–] 

Not quite programming but certainly covers the ""small projects"" part. This is what taught me the basics of Unreal Engine: https://www.youtube.com/watch?v=k-zMkzmduqI
 
reply



  
 
anurags 5 hours ago  
             | prev | next [–] 

https://github.com/practical-tutorials/project-based-learnin...
 
reply



  
 
basicallydan 7 hours ago  
             | prev | next [–] 

If they don't exist, this is a good idea for anybody looking for a project to write a programming book
 
reply



  
 
cameron_b 5 hours ago  
             | parent | next [–] 

There are many, but the field is still wide open. Anyone with a particular domain experience should find a market.There are lots of starting from scratch guides, and few focused guides.for subject in [""Finance"", ""Contact Management"", ""Service Desk Automation"", ""Web Apps from scratch"", ""Web Apps on someone else's API"", ""Baseball Metrics""]:
   for language in [""R"", ""Go"", ""Python3"", ""Rust"", ""Lisp""]:
      print(f""Examples for {subject} in {language} would make a good book."")edit: no idea how to smash indent-formatting into MD-for-HN
 
reply



  
 
Nzen 3 hours ago  
             | root | parent | next [–] 

Prefix each line with two spaces, per the formatdoc page [0][0] https://news.ycombinator.com/formatdoc  for subject in [
   ""Finance"",
   ""Contact Management"",
   ""Service Desk Automation"",
   ""Web Apps from scratch"",
   ""Web Apps on someone else's API"",
   ""Baseball Metrics""
  ] : for language in [
   ""R"",
   ""Go"",
   ""Python3"",
   ""Rust"",
   ""Lisp""
  ] : print(f""Examples for {subject} in {language} would make a good book."")
 
reply



  
 
cameron_b 2 hours ago  
             | root | parent | next [–] 

Thanks!
 
reply



  
 
icco 4 hours ago  
             | prev | next [–] 

Pretty much all of the books at newline do this: https://www.newline.co/
 
reply



  
 
bartvk 6 hours ago  
             | prev | next [–] 

Learn Swift in 100 days. It start with the basics using Swift Playgrounds. But as soon as possible, it starts with building apps.https://www.hackingwithswift.com/100
 
reply



  
 
_jcrossley 2 hours ago  
             | parent | next [–] 

Came to say the same thing - plus, there's a great 100 Days of SwiftUI as well, which gets nontrivial apps running pretty quickly.
 
reply



  
 
robaye 5 hours ago  
             | prev | next [–] 

C Programming: A Modern Approach by K. N. King. I’ve never seen another book that contains as many exercises and projects as Kings book does.
 
reply



  
 
HellDunkel 5 hours ago  
             | prev | next [–] 

The Rust Programming Language by Steve Klabnik and Carol Nichols. I don’t think it’s a language for beginners though.
 
reply



  
 
feliixh 5 hours ago  
             | prev | next [–] 

Automate the boring stuff with Python
 
reply



  
 
poszlem 7 hours ago  
             | prev | next [–] 

I can recommend ""Programming: Principles and Practice Using C++"", although the fact that it uses C++ as a first language can be a downside to some.
 
reply



  
 
mesozoic 2 hours ago  
             | prev | next [–] 

I'd like the equivalent for electronics if anyone has any suggestions.
 
reply



  
 
kgwxd 6 hours ago  
             | prev | next [–] 

Programming Games for Atari 2600 https://forums.atariage.com/topic/339819-upcoming-book-on-at...
 
reply



  
 
rg111 6 hours ago  
             | prev | next [–] 

I recently finished the book that teaches programming by developing games using DragonRubyGameToolkit. Really loved this book._____Python Crash Course by Eric Matthes has a section dedicated to projects.I really liked this book.This is what taught me Python.I knew C before.
 
reply



  
 
cratermoon 3 hours ago  
             | prev | next [–] 

Practical Common Lisp by Peter Seibel
 
reply



  
 
volodarik_lemon 5 hours ago  
             | prev | next [–] 

All Swift by Sundell series are pretty good.
 
reply



  
 
keepquestioning 2 hours ago  
             | prev | next [–] 

raytracing weekend
 
reply



  
 
ThomPete 4 hours ago  
             | prev | next [–] 

ChatGPT is probably much better at doing that for you. You can start with anything you want and explore as deep as you want.
 
reply



  
 
hgsgm 6 hours ago  
             | prev [–] 

checkio.com for increasing challenges in Python or JS.But also pretty much every book does that.
 
reply







Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
Search:  


"
https://news.ycombinator.com/rss,Laser desorption mass spectrometry – Alien detector that fits in your hand,https://www.nature.com/articles/s41550-022-01866-x,Comments,"




Article
Published: 16 January 2023

Laser desorption mass spectrometry with an Orbitrap analyser for in situ astrobiology
Ricardo Arevalo Jr 
            ORCID: orcid.org/0000-0002-0558-50901, Lori Willhite1, Anais Bardyn1, Ziqin Ni1, Soumya Ray 
            ORCID: orcid.org/0000-0002-9591-93801, Adrian Southard2, Ryan Danell 
            ORCID: orcid.org/0000-0003-4863-19983, Andrej Grubisic4, Cynthia Gundersen5, Niko Minasola5, Anthony Yu4, Molly Fahey 
            ORCID: orcid.org/0000-0001-8551-96344, Emanuel Hernandez4, Christelle Briois 
            ORCID: orcid.org/0000-0002-5616-01806, Laurent Thirkell6, Fabrice Colin6 & …Alexander Makarov7 Show authors

Nature Astronomy

                         (2023)Cite this article




496 Altmetric


Metrics details





Subjects

Analytical chemistryPlanetary science




AbstractLaser desorption mass spectrometry (LDMS) enables in situ characterization of the organic content and chemical composition of planetary materials without requiring extensive sample processing. Coupled with an Orbitrap analyser capable of ultrahigh mass-resolving powers and accuracies, LDMS techniques facilitate the orthogonal detection of a wide range of biomarkers and classification of host mineralogy. Here an Orbitrap LDMS instrument that has been miniaturized for planetary exploration is shown to meet the performance standards of commercial systems and exceed key figures of merit of heritage spaceflight technologies, including those baselined for near-term mission opportunities. Biogenic compounds at area densities relevant to prospective missions to ocean worlds are identified unambiguously by redundant measurements of molecular ions (with and without salt adducts) and diagnostic fragments. The derivation of collision cross-sections serves to corroborate assignments and inform on molecular structure. Access to trace elements down to parts per million by weight levels provide insights into sample mineralogy and provenance. These analytical capabilities position the miniaturized LDMS described here for a wide range of high-priority mission concepts, such as those focused on life detection objectives (for example, Enceladus Orbilander) and progressive exploration of the lunar surface (for example, via the NASA Artemis Program).





Access through your institution




Buy or subscribe







This is a preview of subscription content, access via your institution


Access options





Access through your institution









Access through your institution




Change institution




Buy or subscribe



Subscribe to Nature+Get immediate online access to Nature and 55 other Nature journal$29.99monthlySubscribeSubscribe to JournalGet full journal access for 1 year$119.00only $9.92 per issueSubscribeAll prices are NET prices. VAT will be added later in the checkout.Tax calculation will be finalised during checkout.Buy articleGet time limited or full article access on ReadCube.$32.00BuyAll prices are NET prices.

Additional access options:


Log in


Learn about institutional subscriptions




Fig. 1: The highly miniaturized LDMS instrument described here leverages an Orbitrap mass analyser to achieve ultrahigh mass resolution and accuracy.Fig. 2: In both negative and positive mode, the miniaturized Orbitrap LDMS instrument achieves mass-resolving powers (m/Δm > 105, FWHM at m/z 100) comparable to commercial standards.Fig. 3: A single mass spectrum of an ocean world analogue sample illustrates the capability to detect and identify organic and inorganic components of planetary materials.Fig. 4: After successful injection into the Orbitrap analyser, the axial motions of the analyte ions are detected via image current in the time domain transient.Fig. 5: The Orbitrap LDMS instrument can detect trace elements down to ppmw concentrations, as illustrated by the measurement of REEs in NIST SRM610.


Data availability
Source data are provided with this paper. All other data presented in this study are available in the Supplementary Information.
ReferencesJohnson, S. S., Anslyn, E. V., Graham, H. V., Mahaffy, P. R. & Ellington, A. D. Fingerprinting non-terran biosignatures. Astrobiology 18, 915–922 (2018).Article 
    ADS 
    
                    Google Scholar 
                Marshall, S. M., Murray, A. R. G. & Cronin, L. A probabilistic framework for identifying biosignatures using Pathway Complexity. Philos. Trans. R. Soc. Lond. A 375, 20160342 (2017).ADS 
    
                    Google Scholar 
                Chan, M. A. et al. Deciphering biosignatures in planetary contexts. Astrobiology 19, 1075–1102 (2019).Article 
    ADS 
    
                    Google Scholar 
                Neveu, M., Hays, L. E., Voytek, M. A., New, M. H. & Schulte, M. D. The ladder of life detection. Astrobiology 18, 1375–1402 (2018).Article 
    ADS 
    
                    Google Scholar 
                Lukmanov, R. A. et al. On topological analysis of fs-LIMS data. Implications for in situ planetary mass spectrometry. Front. Artif. Intell. https://doi.org/10.3389/frai.2021.668163 (2021).Johnston, S., Gehrels, G., Valencia, V. & Ruiz, J. Small-volume U–Pb zircon geochronology by laser ablation-multicollector-ICP-MS. Chem. Geol. 259, 218–229 (2009).Article 
    ADS 
    
                    Google Scholar 
                Sagdeev, R. Z. & Zakharov, A. V. Brief history of the Phobos mission. Nature 341, 581–585 (1989).Article 
    ADS 
    
                    Google Scholar 
                Managadze, G. G. et al. Study of the main geochemical characteristics of Phobos’ regolith using laser time-of-flight mass spectrometry. Sol. Syst. Res. 44, 376–384 (2010).Article 
    ADS 
    
                    Google Scholar 
                Goesmann, F. et al. The Mars Organic Molecule Analyzer (MOMA) instrument: characterization of organic material in Martian sediments. Astrobiology 17, 655–685 (2017).Article 
    ADS 
    
                    Google Scholar 
                Grubisic, A. et al. Laser desorption mass spectrometry at Saturn’s moon Titan. Int. J. Mass Spectrom. 470, 116707 (2021).Article 
    
                    Google Scholar 
                Chumikov, A. E., Cheptsov, V. S., Managadze, N. G. & Managadze, G. G. LASMA-LR laser-ionization mass spectrometer onboard Luna-25 and Luna-27 missions. Sol. Syst. Res. 55, 550–561 (2021).Article 
    ADS 
    
                    Google Scholar 
                Briois, C. et al. Orbitrap mass analyser for in situ characterisation of planetary environments: performance evaluation of a laboratory prototype. Planet. Space Sci. 131, 33–45 (2016).Article 
    ADS 
    
                    Google Scholar 
                Willhite, L. et al. CORALS: a laser desorption/ablation Orbitrap mass spectrometer for in situ exploration of Europa. In 2021 IEEE Aerospace Conference 50100, 1–13 (2021).Makarov, A. A. Mass spectrometer US patent 5,886,346 (1999).Arevalo, R. Jr, Ni, Z. & Danell, R. M. Mass spectrometry and planetary exploration: a brief review and future projection. J. Mass Spectrom. 55, e4454 (2020).Article 
    ADS 
    
                    Google Scholar 
                Makarov, A. Electrostatic axially harmonic orbital trapping: a high-performance technique of mass analysis. Anal. Chem. 72, 1156–1162 (2000).Article 
    
                    Google Scholar 
                Arevalo, R. Jr et al. An Orbitrap-based laser desorption/ablation mass spectrometer designed for spaceflight. Rapid Commun. Mass Spectrom. https://doi.org/10.1002/rcm.8244 (2018).Article 
    
                    Google Scholar 
                Yu, A. W. et al. The Lunar Orbiter Laser Altimeter (LOLA) laser transmitter. In 2011 IEEE International Geoscience and Remote Sensing Symposium 3378–3379 (2011).Malloci, G., Mulas, G. & Joblin, C. Electronic absorption spectra of PAHs up to vacuum UV. Astron. Astrophys. 426, 105–117 (2004).Article 
    ADS 
    
                    Google Scholar 
                Cloutis, E. A. et al. Ultraviolet spectral reflectance properties of common planetary minerals. Icarus 197, 321–347 (2008).Article 
    ADS 
    
                    Google Scholar 
                Fahey, M. et al. Ultraviolet laser development for planetary lander missions. In 2020 IEEE Aerospace Conference 1–11 (2020).Büttner, A. et al. Optical design and characterization of the MOMA laser head flight model for the ExoMars 2020 mission. In Proc. SPIE 11180, International Conference on Space Optics—ICSO 2018, 111805H (12 July 2019); https://doi.org/10.1117/12.2536116Jenner, F. E. & O’Neill, H. S. C. Major and trace analysis of basaltic glasses by laser-ablation ICP-MS. Geochem. Geophys. Geosyst. https://doi.org/10.1029/2011GC003890 (2012).Humayun, M., Davis, F. A. & Hirschmann, M. M. Major element analysis of natural silicates by laser ablation ICP-MS. J. Anal. Spectrom. 25, 998–1005 (2010).Article 
    
                    Google Scholar 
                Longerich, H. P., Günther, D. & Jackson, S. E. Elemental fractionation in laser ablation inductively coupled plasma mass spectrometry. Fresenius J. Anal. Chem. 355, 538–542 (1996).Article 
    
                    Google Scholar 
                Alterman, M. A., Gogichayeva, N. V. & Kornilayev, B. A. Matrix-assisted laser desorption/ionization time-of-flight mass spectrometry-based amino acid analysis. Anal. Biochem. 335, 184–191 (2004).Article 
    
                    Google Scholar 
                Sarracino, D. & Richert, C. Quantitative MALDI-TOF MS of oligonucleotides and a nuclease assay. Bioorg. Med. Chem. Lett. 6, 2543–2548 (1996).Article 
    
                    Google Scholar 
                Chumbley, C. W. et al. Absolute quantitative MALDI imaging mass spectrometry: a case of rifampicin in liver tissues. Anal. Chem. 88, 2392–2398 (2016).Article 
    
                    Google Scholar 
                Zubarev, R. A. & Makarov, A. Orbitrap mass spectrometry. Anal. Chem. 85, 5288–5296 (2013).Article 
    
                    Google Scholar 
                Makarov, A., Denisov, E., Lange, O. & Horning, S. Dynamic range of mass accuracy in LTQ Orbitrap hybrid mass spectrometer. J. Am. Soc. Mass Spectrom. 17, 977–982 (2006).Hoegg, E. D. et al. Isotope ratio characteristics and sensitivity for uranium determinations using a liquid sampling–atmospheric pressure glow discharge ion source coupled to an Orbitrap mass analyzer. J. Anal. Spectrom. 31, 2355–2362 (2016).Article 
    
                    Google Scholar 
                Hofmann, A. E. et al. Using Orbitrap mass spectrometry to assess the isotopic compositions of individual compounds in mixtures. Int. J. Mass Spectrom. 457, 116410 (2020).Article 
    
                    Google Scholar 
                Hardouin, J. Protein sequence information by matrix-assisted laser desorption/ionization in-source decay mass spectrometry. Mass Spectrom. Rev. 26, 672–682 (2007).Article 
    ADS 
    
                    Google Scholar 
                Franchi, M., Ferris, J. P. & Gallori, E. Cations as mediators of the adsorption of nucleic acids on clay surfaces in prebiotic environments. Orig. Life Evol. Biosph. 33, 1–16 (2003); https://doi.org/10.1023/A:1023982008714Trumbo, S. K., Brown, M. E. & Hand, K. P. Sodium chloride on the surface of Europa. Sci. Adv. 5, eaaw7123 (2019).Article 
    ADS 
    
                    Google Scholar 
                Postberg, F., Schmidt, J., Hillier, J. et al. A salt-water reservoir as the source of a compositionally stratified plume on Enceladus. Nature 474, 620–622 (2011).De Sanctis, M. C. et al. Fresh emplacement of hydrated sodium chloride on Ceres from ascending salty fluids. Nat. Astron. 4, 786–793 (2020).Article 
    ADS 
    
                    Google Scholar 
                Hand, K. P. et al. Report of the Europa Lander Science Definition Team (NASA, 2017).Hendrix, A. R. et al. The NASA Roadmap to Ocean Worlds. Astrobiology 19, 1–27 (2018); https://doi.org/10.1089/ast.2018.1955MacKenzie, S. M. et al. The Enceladus Orbilander mission concept: balancing return and resources in the search for life. Planet. Sci. J. 2, 77 (2021).Article 
    
                    Google Scholar 
                Waite, J. H. Jr et al. Liquid water on Enceladus from observations of ammonia and 40Ar in the plume. Nature 460, 487–490 (2009).Article 
    ADS 
    
                    Google Scholar 
                Altwegg, K., Balsiger, H. & Fuselier, S. A. Cometary chemistry and the origin of icy solar system bodies: the view after Rosetta. Annu. Rev. Astron. Astrophys. 57, 113–155 (2019).Article 
    ADS 
    
                    Google Scholar 
                Guzman, M. et al. Collecting amino acids in the Enceladus plume. Int. J. Astrobiol. 18, 47–59 (2018).Article 
    ADS 
    
                    Google Scholar 
                Takayama, M. In-source decay characteristics of peptides in matrix-assisted laser desorption/ionization time-of-flight mass spectrometry. J. Am. Soc. Mass Spectrom. 12, 420–427 (2001).Article 
    ADS 
    
                    Google Scholar 
                Katta, V., Chow, D. T. & Rohde, M. F. Applications of in-source fragmentation of protein ions for direct sequence analysis by delayed extraction MALDI-TOF mass spectrometry. Anal. Chem. 70, 4410–4416 (1998).Article 
    
                    Google Scholar 
                Sanders, J. D. et al. Determination of collision cross-sections of protein ions in an Orbitrap mass analyzer. Anal. Chem. 90, 5896–5902 (2018).Article 
    
                    Google Scholar 
                Makarov, A. & Denisov, E. Dynamics of ions of intact proteins in the Orbitrap mass analyzer. J. Am. Soc. Mass Spectrom. 20, 1486–1495 (2009).Article 
    
                    Google Scholar 
                Anupriya, Jones, C. A. & Dearden, D. V. Collision cross sections for 20 protonated amino acids: Fourier transform ion cyclotron resonance and ion mobility results. J. Am. Soc. Mass Spectrom. 27, 1366–1375 (2016).Article 
    ADS 
    
                    Google Scholar 
                Chyba, C. & Sagan, C. Endogenous production, exogenous delivery and impact-shock synthesis of organic molecules: an inventory for the origins of life. Nature 355, 125–132 (1992).Article 
    ADS 
    
                    Google Scholar 
                Poppe, A. R. An improved model for interplanetary dust fluxes in the outer Solar System. Icarus 264, 369–386 (2016).Article 
    ADS 
    
                    Google Scholar 
                Taylor, S. R. & McLennan, S. M. in Handbook on the Physics and Chemistry of Rare Earths Vol. 11, 485–578 (eds Gschneidner, K. A. J. & Eyring, l.) (Elsevier, 1988).Jawin, E. R. et al. Lunar science for landed missions workshop findings report. Earth Space Sci. 6, 2–40 (2019).Article 
    ADS 
    
                    Google Scholar 
                National Academies of Sciences, Engineering, and Medicine. Origins, Worlds, and Life: A Decadal Strategy for Planetary Science and Astrobiology 2023–2032 (National Academies Press, 2022).Artemis III Science Definition Team Report (NASA, 2020).Steinbrügge, G. et al. Brine migration and impact-induced cryovolcanism on Europa. Geophys. Res. Lett. 47, e2020GL090797 (2020).Article 
    ADS 
    
                    Google Scholar 
                Danell, R. et al. A full featured, flexible, and inexpensive 2D and 3D ion trap control architecture and software package. In Proc. 58th ASMS Conference on Mass Spectrometry and Allied Topics 283889 (2010).Download referencesAcknowledgementsThis study was supported by the University of Maryland Faculty Incentive Program (PI: R.A. Jr), NASA Goddard Space Flight Center Internal Research and Development Program (PIs: A.G. and A.Y.), NASA ROSES ICEE 2 Grant 80NSSC19K0610 (PI: R.A. Jr), ROSES DALI Grant 80NSSC19K0768 (PI: R.A. Jr) and CRESST II Award Number 80GSFC21M0002 (PI: A.S.).Author informationAuthors and AffiliationsUniversity of Maryland, College Park, MD, USARicardo Arevalo Jr, Lori Willhite, Anais Bardyn, Ziqin Ni & Soumya RayCRESST II, College Park, MD, USAAdrian SouthardDanell Consulting, Winterville, NC, USARyan DanellNASA Goddard Space Flight Center, Greenbelt, MD, USAAndrej Grubisic, Anthony Yu, Molly Fahey & Emanuel HernandezAMU Engineering, Miami, FL, USACynthia Gundersen & Niko MinasolaLaboratoire de Physique et Chimie de l’Environnement et de l’Espace, Orléans, FranceChristelle Briois, Laurent Thirkell & Fabrice ColinThermo Fisher Scientific, Bremen, GermanyAlexander MakarovAuthorsRicardo Arevalo JrView author publicationsYou can also search for this author in
                        PubMed Google ScholarLori WillhiteView author publicationsYou can also search for this author in
                        PubMed Google ScholarAnais BardynView author publicationsYou can also search for this author in
                        PubMed Google ScholarZiqin NiView author publicationsYou can also search for this author in
                        PubMed Google ScholarSoumya RayView author publicationsYou can also search for this author in
                        PubMed Google ScholarAdrian SouthardView author publicationsYou can also search for this author in
                        PubMed Google ScholarRyan DanellView author publicationsYou can also search for this author in
                        PubMed Google ScholarAndrej GrubisicView author publicationsYou can also search for this author in
                        PubMed Google ScholarCynthia GundersenView author publicationsYou can also search for this author in
                        PubMed Google ScholarNiko MinasolaView author publicationsYou can also search for this author in
                        PubMed Google ScholarAnthony YuView author publicationsYou can also search for this author in
                        PubMed Google ScholarMolly FaheyView author publicationsYou can also search for this author in
                        PubMed Google ScholarEmanuel HernandezView author publicationsYou can also search for this author in
                        PubMed Google ScholarChristelle BrioisView author publicationsYou can also search for this author in
                        PubMed Google ScholarLaurent ThirkellView author publicationsYou can also search for this author in
                        PubMed Google ScholarFabrice ColinView author publicationsYou can also search for this author in
                        PubMed Google ScholarAlexander MakarovView author publicationsYou can also search for this author in
                        PubMed Google ScholarContributionsThe dataset presented in this study was collected and analysed by R.A. Jr, L.W., A.B., Z.N. and S.R. The system-level architecture of the miniaturized instrument and the operational sequence of the experiments conducted were defined by R.A. Jr, A.S., R.D., A.G., C.B., L.T., F.C. and A.M. Requirements for the ion optics and SIMION models of ion transmission were provided by A.S. The mechanical design of the mass analyser assembly and custom series of ion optics were led by C.G. and N.M. The design and build of the prototype UV laser system was led by A.Y. and M.F. All authors contributed to the interpretation of the results and editing of the manuscript.Corresponding authorCorrespondence to
                Ricardo Arevalo Jr.Ethics declarations
Competing interests
A.M. is an employee of Thermo Fisher Scientific, the manufacturer of the Orbitrap device leveraged in the miniaturized instrument described here.
Peer review
Peer review information
Nature Astronomy thanks Marek Tulej and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.
Additional informationPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Supplementary informationSupplementary InformationSupplementary discussion, Figs. 1–8 and Table 1.Source dataSource Data Fig. 2Raw time domain transients for Fig. 2.Source Data Fig. 3Raw time domain transient for Fig. 3.Source Data Fig. 5Raw time domain transient for Fig. 5.Rights and permissionsSpringer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.Reprints and PermissionsAbout this articleCite this articleArevalo, R., Willhite, L., Bardyn, A. et al. Laser desorption mass spectrometry with an Orbitrap analyser for in situ astrobiology.
                    Nat Astron  (2023). https://doi.org/10.1038/s41550-022-01866-xDownload citationReceived: 03 January 2022Accepted: 16 November 2022Published: 16 January 2023DOI: https://doi.org/10.1038/s41550-022-01866-xShare this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        




"
https://news.ycombinator.com/rss,The Amagasaki Derailment [video],https://www.youtube.com/watch?v=vfWUgkWh784&list=UULFFXad0mx4WxY1fXdbvtg0CQ,Comments,The Amagasaki Derailment | A Short Documentary | Fascinating Horror - YouTubeAboutPressCopyrightContact usCreatorsAdvertiseDevelopersTermsPrivacyPolicy & SafetyHow YouTube worksTest new features© 2023 Google LLC
https://news.ycombinator.com/rss,ChrysaLisp,https://github.com/vygr/ChrysaLisp,Comments,"








vygr

/

ChrysaLisp

Public




 

Notifications



 

Fork
    85




 


          Star
 1.3k
  









        Parallel OS, with GUI, Terminal, OO Assembler, Class libraries, C-Script compiler, Lisp interpreter and more...
      
License





     GPL-2.0 license
    






1.3k
          stars
 



85
          forks
 



 


          Star

  





 

Notifications












Code







Issues
29






Pull requests
1






Actions







Projects
4






Wiki







Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Wiki
 


                  Security
 


                  Insights
 







vygr/ChrysaLisp









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











master





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








3
branches





45
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




vygr

Update VP_ASSIGNMENT.md




        …
      




        61cf775
      

Jan 12, 2023





Update VP_ASSIGNMENT.md


61cf775



Git stats







6,690

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








apps



Update install.lisp



Dec 12, 2022









class



remove guard page option



Dec 14, 2022









cmd



change t -> :t and nil to :nil



Oct 1, 2022









docs



Update VP_ASSIGNMENT.md



Jan 12, 2023









fonts



smaller CTF files



Jan 20, 2020









gui



Rework (env-resize num [env[)



Dec 12, 2022









lib



be explicit in what is returned.



Jan 12, 2023









src



switch to snprintf



Jan 5, 2023









sys



move *debug_emit* to the VP code emit stage.



Jan 11, 2023









.gitignore



Removal of unsupported source



Sep 30, 2022









LICENSE



Basic cut/copy/paste functionality and clipboard service (#201)



Nov 17, 2020









Lisp.tmLanguage



a few doc corrections



Dec 23, 2022









Makefile



no need now for empty folders



Dec 21, 2022









Makefile_Homebrew



no need now for empty folders



Dec 21, 2022









README.md



remove guard page option



Dec 14, 2022









STATUS.md



remove guard page option



Dec 14, 2022









funcs.ps1



new GUI Terminal 2.0 !



Jun 12, 2021









funcs.sh



new GUI Terminal 2.0 !



Jun 12, 2021









install.bat



new GUI Terminal 2.0 !



Jun 12, 2021









run.bat



launch scripts for TUI only



Dec 24, 2020









run.ps1



latest Windows launch scripts from Mr Blyss



Apr 4, 2021









run.sh



better option handling



Mar 4, 2021









run_cube.ps1



latest Windows launch scripts from Mr Blyss



Apr 4, 2021









run_cube.sh



better option handling



Mar 4, 2021









run_mesh.bat



launch scripts for TUI only



Dec 24, 2020









run_mesh.ps1



latest Windows launch scripts from Mr Blyss



Apr 4, 2021









run_mesh.sh



better option handling



Mar 4, 2021









run_ring.ps1



latest Windows launch scripts from Mr Blyss



Apr 4, 2021









run_ring.sh



better option handling



Mar 4, 2021









run_star.ps1



latest Windows launch scripts from Mr Blyss



Apr 4, 2021









run_star.sh



better option handling



Mar 4, 2021









run_tree.ps1



latest Windows launch scripts from Mr Blyss



Apr 4, 2021









run_tree.sh



better option handling



Mar 4, 2021









run_tui.bat



new GUI Terminal 2.0 !



Jun 12, 2021









run_tui.ps1



Update run_tui.ps1



Apr 8, 2021









run_tui.sh



better option handling



Mar 4, 2021









screen_shot_1.png



refresh screen shots



Nov 3, 2020









screen_shot_2.png



refresh screen shots



Nov 3, 2020









screen_shot_3.png



refresh screen shots



Nov 3, 2020









screen_shot_4.png



refresh screen shots



Nov 3, 2020









screen_shot_5.png



refresh screen shots



Nov 3, 2020









screen_shot_6.png



refresh screen shots



Nov 3, 2020









screen_shot_7.png



current GUI screenshot



Nov 1, 2020









screen_shot_8.png



add screen_shot_8



Aug 14, 2021









snapshot.zip



Update snapshot.zip



Dec 12, 2022









stop.bat



split make in tui and gui options



Dec 24, 2020









stop.ps1



latest Windows launch scripts from Mr Blyss



Apr 4, 2021









stop.sh



correct TEMP location for Macs !



Dec 21, 2022




    View code
 


















ChrysaLisp
Getting Started
Make/Run/Stop
Installing
Make
Run
Stop
Snapshot
Clean





README.md




ChrysaLisp
Assembler/C-Script/Lisp 64 bit, MIMD, multi CPU, multi threaded, multi core,
multi user Parallel OS. With GUI, Terminal, OO Assembler, Class libraries,
C-Script compiler, Lisp interpreter, Debugger, Profiler, Vector Font engine and
more...








It runs on MacOS, Windows or Linux for x64, MacOS or Linux for Aarch64. Will
move to bare metal eventually but it's useful for now to run hosted while
experimenting. When time allows I will be doing a VM boot image for UniKernel
type appliances and a WebAssembly target to play around within the browser.
You can model various network topologies with point to point links. Each CPU in
the network is modelled as a separate host process, point to point links use
shared memory to simulate CPU to CPU, point to point, bi directional
connections. There is no global bus based networking on purpose.
The usb-links branch has the ability to use a usb3/usb2 Prolific chip 'copy'
cable to join heterogeneous host networks ! This demonstrates that the
simulated peer to peer network on a single machine does actually translate to
real world parallel hardware ! Plus it's rather cool to string your MacBook and
Windows laptops together with your PI4's to create your own dev network.
There is a virtual CPU instruction set to avoid use of x64/ARM/VP64 native
instructions. Currently it compiles directly to native code but there is no
reason it can't also go via a byte code form and runtime translation.
Register juggling for parameter passing is eliminated by having all functions
define their register interface and parameter source and destinations are
mapped automatically using a topological sort. Non DAG mappings are detected so
the user can break them with a temporary if required. Operators are provided to
simplify binding of parameters to dynamic bound functions, relative addresses,
auto defined string pools, references and local stack frame values. Unused
output parameters can be ignored with an _.
There is a powerful object and class system, not just for an assembler, but
quite as capable as a high level language. Static classes or virtual classes
with inline, virtual, final, static and override methods can be defined. The
GUI and Lisp are constructed using this class system.
It has function level dynamic binding and loading. Individual functions are
loaded and bound on demand as tasks are created and distributed. Currently
functions are loaded from the CPU file system on which the task finds itself,
but these will eventually come from the server object that the task was created
with and functions will be transported across the network as required.
Functions are shared between all tasks that share the same server object, so
only a single copy of a function is loaded regardless of how many tasks use
that function.
The interface to the system functions is provided as a set of static classes,
easing use and removing the need to remember static function locations, plus
decoupling the source from changes at the system level. Look in the
sys/xxx.inc files to see the interface definitions.
A command terminal with a familiar interface for pipe style command line
applications is provided with args vector, stdin, stdout, stderr etc. Classes
for easy construction of pipe masters and slaves, with arbitrary nesting of
command line pipes. While this isn't the best way to create parallel
applications it is very useful for the composition of tools and hides all the
message passing behind a familiar streams based API.
A Common Lisp like interpreter is provided. This is available from the command
line, via the command lisp. To build the entire system type (make),
calculates minimum compile workload, or (make-all) to do everything
regardless, at the Lisp command prompt. This Lisp has a C-Script 'snippets'
capability to allow mixing of C-Script compiled expressions within assignment
and function calling code. An elementary optimise pass exists for these
expressions. Both the virtual assembler and C-Script compiler are written in
Lisp, look in the sys/code.inc, lib/asm/xxx.inc, sys/func.inc,
lib/trans/x64.inc, lib/trans/arm64.inc and lib/asm/vp.inc for how this is
done. Some of the Lisp primitives are constructed via a boot script that each
instance of a Lisp class runs on construction, see class/lisp/boot.inc for
details. The compilation and make environment, along with all the compile and
make commands are created via the Lisp command line tool in lib/asm/asm.inc,
again this auto runs for each instance of the lisp command run from the
terminal. You can extend this with any number of additional files, just place
them after the lisp command and they will execute after the lib/asm/asm.inc
file and before processing of stdin.
Don't get the idea that due to being coded in interpreted Lisp the assembler
and compiler will be slow. A fully cleaned system build from source, including
creation of a full recursive pre-bound boot image file, takes on the order of 2
seconds on a 2014 MacBook Pro ! Dev cycle (make) and (remake) under 0.5
seconds. It ain't slow !
Network link routing tables are created on booting a link, and the process is
distributed in nature, each link starts a flood fill that eventually reaches
all the CPU's and along the way has marked all the routes from one CPU to
another. All shortest routes are found, messages going off CPU are assigned to
a link as the link becomes free and multiple links can and do route messages
over parallel routes simultaneously. Large messages are broken into smaller
fragments on sending and reconstructed at the destination to maximize use of
available routes.
The -run command line option launches tasks on booting that CPU, such as the
experimental GUI (a work in progress, -run gui/gui/gui.lisp). You can change
the network launch script to run more than one GUI session if you want, try
launching the GUI on more than CPU 0, look in funcs.sh at the boot_cpu_gui
function ! :)
The -l command line option creates a link, currently up to 1000 CPU's are
allowed but that's easy to adjust. The shared memory link files are created in
the tmp folder /tmp, so for example /tmp/000-001 would be the link file for
the link between CPU 000 and 001.
An example network viewed with ps looks like this for a 4x4 mesh network:
./main_gui -l 011-015 -l 003-015 -l 014-015 -l 012-015
./main_gui -l 010-014 -l 002-014 -l 013-014 -l 014-015
./main_gui -l 009-013 -l 001-013 -l 012-013 -l 013-014
./main_gui -l 008-012 -l 000-012 -l 012-015 -l 012-013
./main_gui -l 007-011 -l 011-015 -l 010-011 -l 008-011
./main_gui -l 006-010 -l 010-014 -l 009-010 -l 010-011
./main_gui -l 005-009 -l 009-013 -l 008-009 -l 009-010
./main_gui -l 004-008 -l 008-012 -l 008-011 -l 008-009
./main_gui -l 003-007 -l 007-011 -l 006-007 -l 004-007
./main_gui -l 002-006 -l 006-010 -l 005-006 -l 006-007
./main_gui -l 001-005 -l 005-009 -l 004-005 -l 005-006
./main_gui -l 000-004 -l 004-008 -l 004-007 -l 004-005
./main_gui -l 003-015 -l 003-007 -l 002-003 -l 000-003
./main_gui -l 002-014 -l 002-006 -l 001-002 -l 002-003
./main_gui -l 001-013 -l 001-005 -l 000-001 -l 001-002
./main_gui -l 000-012 -l 000-004 -l 000-003 -l 000-001 -run gui/gui

Getting Started
Take a look at the docs/INTRO.md for instructions to get started on all the
supported platforms.
The experimental GUI requires the SDL2 library to be installed.
Download these from the SDL web site.

SDL

Or get them via your package manager.
sudo apt-get install libsdl2-dev

Make/Run/Stop
Take a look at the docs/INTRO.md for platform specific instructions. The
following is for OSX and Linux systems. Windows has a pre-built main.exe
provided, or you can configure Visual Studio to compile things yourself if you
wish.
Installing
The first time you download ChrysaLisp you will only have the vp64 emulator
boot image. You must create the native boot images the first time round. This
is a little slower than subsequent boots and system compiles but allows us to
keep the snapshot.zip file as small as possible.
make install

Or on Windows
install.bat

Make
make

Run
./run_tui.sh [-n num_cpus] [-e] [-b base_cpu]

Text user interface based fully connected network. Each CPU has links to every
other CPU. Careful with this as you can end up with a very large number of link
files and shared memory regions. CPU 0 launches a terminal to the host system.
./run.sh [-n num_cpus] [-e] [-b base_cpu]

Fully connected network. Each CPU has links to every other CPU. Careful with
this as you can end up with a very large number of link files and shared memory
regions. CPU 0 launches a GUI.
./run_star.sh [-n num_cpus] [-e] [-b base_cpu]

Star connected network. Each CPU has a link to the first CPU. CPU 0 launches a
GUI.
./run_ring.sh [-n num_cpus] [-e] [-b base_cpu]

Ring connected network. Each CPU has links to the next and previous CPU's. CPU
0 launches a GUI.
./run_tree.sh [-n num_cpus] [-e] [-b base_cpu]

Tree connected network. Each CPU has links to its parent CPU and up to two
child CPU's. CPU 0 launches a GUI.
./run_mesh.sh [-n num_cpus on a side] [-e] [-b base_cpu]

Mesh connected network. Each CPU has links to 4 adjacent CPU's. This is similar
to Transputer meshes. CPU 0 launches a GUI.
./run_cube.sh [-n num_cpus on a side] [-e] [-b base_cpu]

Cube connected network. Each CPU has links to 6 adjacent CPU's. This is similar
to TMS320C40 meshes. CPU 0 launches a GUI.
Stop
Stop with:
./stop.sh

Snapshot
Snapshot with:
make snapshot

This will create a snapshot.zip file of the obj/ directory containing only
the host directory structures, the pre-compiled Windows main_gui.exe and
main_tui.exe plus the VP64 boot_image files !
Used to create the more compact snapshot.zip that goes up on Github. This
must come after creation of (make-all-platforms) boot_image set !
obj/x86_64/AMD64/Darwin/
obj/x86_64/AMD64/Linux/
obj/aarch64/ARM64/Linux/
obj/aarch64/ARM64/Darwin/
obj/vp64/VP64/sys/boot_image
obj/x86_64/WIN64/Windows/main_gui.exe
obj/x86_64/WIN64/Windows/main_tui.exe

Clean
Clean with:
make clean










About

      Parallel OS, with GUI, Terminal, OO Assembler, Class libraries, C-Script compiler, Lisp interpreter and more...
    
Topics



  linux


  gui


  vm


  lisp


  osx


  assembly


  x86-64


  os


  aarch64


  raspberry-pi-3



Resources





      Readme
 
License





     GPL-2.0 license
    



Stars





1.3k
    stars

Watchers





65
    watching

Forks





85
    forks







    Releases
      43







New Year Cheer

          Latest
 
Jan 12, 2023

 

        + 42 releases







    Packages 0


        No packages published 







    Contributors 9




















































Languages
















C++
48.1%







Common Lisp
23.9%







SourcePawn
6.7%







Assembly
6.5%







HTML
3.7%







NASL
3.6%







Other
7.5%











"
https://news.ycombinator.com/rss,UT-Austin blocks access to TikTok on campus Wi-Fi networks,https://www.texastribune.org/2023/01/17/ut-austin-tiktok-ban/,Comments,"






UT-Austin blocks access to TikTok on campus Wi-Fi networks



Students and faculty will not be able to access the popular social media app while connected to university internet servers. It’s the latest step to limit access to the service after Gov. Greg Abbott directed state agencies to ban the app on government-issued devices citing cybersecurity risks.



by Kate McGee
Jan. 17, 2023
    
      Updated: 4 hours ago
    
  















Copy link



Republish

















          The University of Texas at Austin campus. UT-Austin officials announced Tuesday the school is banning the use of TikTok on campus Wi-Fi and wired networks in response to Gov. Greg Abbott’s directive to ban the app on government devices amid growing cybersecurity concerns.
        

Credit:
 Tamir Kalifa for The Texas Tribune
        






Sign up for The Brief, The Texas Tribune’s daily newsletter that keeps readers up to speed on the most essential Texas news.
The University of Texas at Austin has blocked access to the video-sharing app TikTok on its Wi-Fi and wired networks in response to Gov. Greg Abbott’s recent directive requiring all state agencies to remove the app from government-issued devices, according to an email sent to students Tuesday.
“The university is taking these important steps to eliminate risks to information contained in the university’s network and to our critical infrastructure,” UT-Austin technology adviser Jeff Neyland wrote in the email. “As outlined in the governor’s directive, TikTok harvests vast amounts of data from its users’ devices — including when, where and how they conduct internet activity — and offers this trove of potentially sensitive information to the Chinese government.”
TikTok is owned by the Chinese company ByteDance Ltd. Last month, FBI Director Chris Wray raised national security concerns about the Chinese government’s ability to potentially collect data on users and use the app’s algorithms to “manipulate content” and “use it for influence operations.”











Abbott’s Dec. 7 directive stated that all state agencies must ban employees from downloading or using the app on government-issued devices, including cellphones, laptops and desktops, with exceptions for law enforcement agencies. He also directed the Texas Department of Public Safety and the Texas Department of Information Resources to create a plan to guide state agencies on how to handle the use of TikTok on personal devices, including those that have access to a state employee’s email account or connect to a state agency network. That plan was to be distributed to state agencies by Jan. 15.
Each state agency is expected to create its own policy regarding the use of TikTok on personal devices by Feb. 15.
More than half of states in the U.S. have banned the use of the social media app on government devices in some capacity in recent months, according to a CNN analysis. Across the country, a growing number of universities have banned the app on devices connected to campus networks, including Auburn University in Alabama, the University of Oklahoma and the schools within the University System of Georgia.
The ban could have broad impacts particularly at universities serving college-age students, a key demographic that uses the app. University admissions departments have used it to connect with prospective students, and many athletics departments have used TikTok to promote sporting events and teams. It’s also unclear how the ban will impact faculty who research the app or professors who teach in areas such as communications or public relations, in which TikTok is a heavily used medium.







In a statement, a representative from TikTok said they are disappointed by the news.
""We're disappointed that so many states are jumping on the political bandwagon to enact policies that will do nothing to advance cybersecurity in their states and are based on unfounded falsehoods about TikTok,"" spokesperson Jamal Brown wrote. ""We're especially sorry to see the unintended consequences of these rushed policies beginning to impact universities' ability to share information, recruit students, and build communities around athletic teams, student groups, campus publications, and more.""
Representatives for other large public universities in the state — including Texas A&M University, Texas Tech University and the University of Houston — did not immediately respond to questions about whether university leaders plan to take similar steps at their campuses.
Disclosure: Texas A&M University, Texas Tech University, University of Texas at Austin and University of Houston have been financial supporters of The Texas Tribune, a nonprofit, nonpartisan news organization that is funded in part by donations from members, foundations and corporate sponsors. Financial supporters play no role in the Tribune’s journalism. Find a complete list of them here.



































Your New Year’s resolution list isn’t complete without …





… supporting the Tribune. This new year, resolve to do your part to sustain trusted journalism in Texas. Join thousands of readers who power The Texas Tribune’s nonprofit newsroom.



Donate now





"
https://news.ycombinator.com/rss,Ask HN: Has anyone worked at the US National Labs before?,https://news.ycombinator.com/item?id=34414527,Comments,"

Ask HN: Has anyone worked at the US National Labs before? | Hacker News

Hacker News
new | past | comments | ask | show | jobs | submit 
login




 Ask HN: Has anyone worked at the US National Labs before?
117 points by science4sail 4 hours ago  | hide | past | favorite | 105 comments 

I have spent the last 10 years working for FAANG companies, but nowadays I find their performance-review and promotion obsessed cultures to be really draining. Worse, those negative feelings seem to be leaking into my personal life and slowly alienating friends and family.Therefore, I've been pondering a change of pace. The classic HN answer is of course ""create/join a startup"", but I've also been looking at areas more adjacent to scientific research.One option that has come up is the US Department of Energy's national laboratory network[0]. From what I understand, the pay is 33-50% of FAANG, but they do seem to have interesting projects (e.g. the nuclear fusion facility that was recently in the news).Has anyone here worked at one of them before? What is/was the day-to-day like?[0] https://www.energy.gov/national-laboratories 
 
  
 
floren 4 hours ago  
             | next [–] 

I worked for Sandia. Pay is pretty good by almost any standards except FAANG. The glory days where every staff member got a real office with a real door are over (shared offices are the norm) but it's still a pretty decent work environment.Things don't move fast, as another commenter said. In my area of work, projects tended to last 1-3 years and you'd be on several projects at any given time. In general, it is ICs rather than managers who run the projects. Your manager might say ""Bob over in 9876 has a neat project that could use somebody like you, send him an email if you're interested"".You have to acknowledge that the core mission of the DOE National Labs is nuclear weapons. You might not ever come in contact with the mission, but it is there. They have strong HPC programs--because HPC as we know it is basically driven by the need to simulate nuclear weapons. Some people have moral objections to this, and that's fine!I thought it was a good place to work, all in all.Edit: I'd like to stress that probably the biggest advantage of the labs is the opportunity for self-directed work. If you can convince somebody (external sponsors, internal R&D funding committees) to give you money, you can work on just about anything. If you can't get funding of your own, you are still more or less able to choose what you work on.Your work environment will depend highly on which group you're in. Some groups look like a university department without the students: you work in the unclassified area, you publish papers, you can even open-source software (with some effort). Other people spend their whole day in a windowless SCIF working on very sensitive stuff which they can never, ever discuss outside of a SCIF -- but while their public visibility is nil, their impact is arguably greater.
 
reply



  
 
kbarros 3 hours ago  
             | parent | next [–] 

I'm a computational physicist at Los Alamos and would echo these sentiments.Note that there are two main types of DOE labs: NNSA (Sandia, Los Alamos, Livermore) and Office of Science (Brookhaven, Berkeley, Oak Ridge, Argonne, ...). Although the former is more focused on ""nuclear weapons stockpile stewardship"", there is still much basic science at all DOE labs, especially where computer science meets physics and other domain sciences.Perhaps relevant to HN, I would mention the Applied Computer Science group at Los Alamos, which is in hiring mode (https://www.lanl.gov/org/ddste/aldsc/computer-computational-...). Besides supporting computational physicists in code development efforts, this group does a variety of researchy things like designing programming model, doing compiler development, building ML models, especially with an eye towards large scale scientific computing. The pay at a DOE lab is less than FAANG (PhD student interns might be around $80k/yr and starting staff scientists maybe $130k/yr), but the tradeoff for some people would be the research-flavor of the work, and the flexibility. Many of the LANL codes being developed are open source, for example. Other DOE labs have similar computer science divisions. For example, Oak Ridge, Argonne, and Berkeley all have ""leadership computing"" facilities.
 
reply



  
 
screwturner68 3 hours ago  
             | parent | prev | next [–] 

I've worked at a few DOE sites albeit as a consultant not an FTE, Fermi, Los Alamos and a couple others.  You are correct that the work is interesting and a like academic atmosphere, it was the academic atmosphere part that I found off putting.  Where I worked there was very much a hierarchy and if you didn't have a PHD your opinion didn't matter much, you just did what you were told.  Having a couple of decades of experience and being brought in to spin them up on their system and being talked down to on a daily basis like I was a Sophomore in college was really annoying -that said I'm a consultant so I get paid to be annoyed by the people who hire me.  Aside from that I liked working there, the tech was cool, as far as the moral issues I don't ask nor do I want to know about what I'm working on -I don't have a need to know.
 
reply



  
 
randcraw 2 hours ago  
             | root | parent | next [–] 

Having worked in R&D at a big pharma for 17 years and in university HPC groups for 10, I echo these comments.  I've found that corporate R&D is essentially investigative (rather than production driven), and as an IC computational scientist (or data scientist) you will move from project to project over the years, usually working solo 95% of the time.  You will have some opportunity to propose projects, especially in partnership with scientists (in pharma, those are chemists or biologists or work to improve manufacturing).  But without a PhD, advancement along the technical track will be limited.If you're not embedded in a science R&D group, you will be lumped in with general IT staff where tech support, database management, or software development of products are prized by management, but tackling individual R&D questions is not (though it is tolerated by IT brass since they know investigation is critical to finding & improving drugs).I found the same limitations when I worked in R&D-based military contracting (or for US gov't FFRDCs for various agencies).  There it's more important to develop a strong relationship with the gov't client, irrespective of the academic degree you have.
 
reply



  
 
neilv 2 hours ago  
             | root | parent | prev | next [–] 

Sounds similar to what I mentioned in comments a couple months ago, about science organizations: https://news.ycombinator.com/item?id=33673259(I'm not familiar with DoE sites.  And I actually had a good experience (other than the pay), as a high-end federal consultant doing challenging technical work, reporting to operations research PhDs at the Director level, who respected what I could do.  Where I've seen and heard of problems is other places.)
 
reply



  
 
scheme271 1 hour ago  
             | parent | prev | next [–] 

Not all DOE national labs do weapons stuff.  For example, Fermilab (FNAL) is pretty much all open science research.  FNAL doesn't even have fences and the guard booth just checks to make sure you have a drivers license if you're driving.  Argonne and Brookhaven are a bit more on the defensey side but not as much as say oak ridge or sandia.
 
reply



  
 
MadVikingGod 3 hours ago  
             | parent | prev | next [–] 

I worked, as a civilian, for part of the R&D arm of the Navy, but it has a very similar feel to what you have posted.  I would 100% agree with everything you have said, except the pay.One thing I will add is getting tools and resources to do the work you want to do can be a challenge.  There can be times when getting through the process to buy a $75 multimeter can be more difficult then the $16,000 signal generator.
 
reply



  
 
sthu11182 2 hours ago  
             | root | parent | next [–] 

I also worked as a civilian for the Navy as an EE (b.s.).  I think my pay was ~50k (in the mid 2000s).  I don't know about the national labs, but my experience working for the navy was that funding was a fight, especially everything being siphoned off for the Afghanistan war.  The equipment was pretty up to date (the computers were not the latest, but decent), the furniture was a mismatch collection of liquated stuff, lab reports were published to a confidential library, and we were on flex time, which made the hours great.
 
reply



  
 
rcpt 3 hours ago  
             | parent | prev | next [–] 

My friends who worked there say that the job was great but living in a remote science outpost made dating unbelievably difficult
 
reply



  
 
closeparen 1 hour ago  
             | root | parent | next [–] 

You’d think there would be plenty of pretty young Russian and Chinese women looking to meet handsome American nuclear scientists.
 
reply



  
 
kbarros 3 hours ago  
             | root | parent | prev | next [–] 

The town of Los Alamos is beautiful, but it's small. It can be a great place if you already a have family that enjoys outdoors activities. Many people prefer to commute from Santa Fe (45 min drive with mountain views, negligible traffic). Certain groups allow a flexible hybrid home/office working mode.https://www.usnews.com/news/healthiest-communities/articles/...
 
reply



  
 
neltnerb 3 hours ago  
             | root | parent | next [–] 

I was only a guest scientist at LBL, but LBL is practically in Berkeley.But yeah, generally I assume this is true, though you may just find that you need a car and that if you have one it's not so bad. For instance, NREL (where I was also briefly a guest scientist) is in an incredibly gorgeous area near Golden which isn't super small but everything is still very spread out and it'd take a while to walk anywhere for sure.I guess it also depends on your definition of ""middle of nowhere"" I suppose. Golden is hardly Oakland, but I am pretty sure I could find people to date as long as I included Denver... and if you have a car in Colorado you will find that Denver is considered close to a lot of things you might not at first consider it close to. It's only an hour drive from Boulder (where I lived and drove to NREL as needed).
 
reply



  
 
screwturner68 3 hours ago  
             | root | parent | prev | next [–] 

If you are talking about Los Alamos you can commute in from Santa Fe, it wasn't bad, about 45 minuets each way.  In my opinion it's kind of expensive for being in the middle of nowhere, you don't get much bang for your buck house-wise.  Social wise I couldn't imagine living in LA for an extended period of time if I were single.
 
reply



  
 
gautamcgoel 50 minutes ago  
             | root | parent | next [–] 

Took me a moment to realize that you meant LA = Los Alamos, not LA = Los Angeles :)
 
reply



  
 
dqpb 3 hours ago  
             | parent | prev | next [–] 

Is there a lot of work left in nuclear weapons?
 
reply



  
 
exmadscientist 2 hours ago  
             | root | parent | next [–] 

Plenty! Among other things, they literally decay over time, and that's fundamental to and completely inseparable from the way they work. Plus for some reason people always want them to be smaller and cheaper and more reliable.
 
reply



  
 
floren 2 hours ago  
             | root | parent | next [–] 

Plus, we're not allowed to actually blow them up to test any more, so they do all that in simulation on the world's biggest supercomputers. This leads to work in HPC system software, kernels, compilers, programming languages, HPC libraries, and so on.
 
reply



  
 
dlivingston 4 hours ago  
             | prev | next [–] 

I worked at LANL [0] for five years and enjoyed the hell out of it. I was a Research Technologist, which is basically an R&D Software Engineer.You will find a research group within a division to work for. For example, mine was the Computational Earth Science group (since renamed) within the Earth & Environmental Sciences division.You will be working with a handful (3+) of research scientists as their supporting engineer. On some projects, you may be doing machine learning work in Julia. On others, you may be coding a fluid dynamics simulation in FORTRAN or C++. On others, you 
might be doing data analytics in Python. It's highly, highly variable, depending strongly on the PIs you're working with, and can change as frequently or infrequently as you wish (within reason).Ultimately, I did the reverse: went from a DOE lab into a FAANG company. My reasons are particular to me, but if you're at all interested in a slower paced, more varied and collaborative environment, you can't do much better than working for the labs.For context, at LANL, I was making ~$100,000 / yr with 3 YOE (circa 2019). This is in northern New Mexico, with such a low cost of living that this amount of money goes about as far as $150k+ up in the Boston area (where I am now).[0]: https://www.lanl.gov/
 
reply



  
 
floren 4 hours ago  
             | parent | next [–] 

LANL is also in an absurdly beautiful place, and Los Alamos is a very pleasant little town.
 
reply



  
 
flatline 13 minutes ago  
             | root | parent | next [–] 

New Mexico can be a challenging place to live, Los Alamos even moreso. Like someone else mentioned: married with kids? Great place, but even then Los Alamos is isolated, it’s a 45 minute drive to Santa Fe which itself is not huge. For the national labs you will also need to maintain a clearance, and the DOE is somewhat regressive in its policies around past drug use and mental health treatment.There is very little in the way of night life. There are activities but this is a place where people live their whole lives and you often have to know someone to get involved. There is a paucity of health services. The airport is small and has had its routes reduced dramatically over the last 15 years. Los Alamos real estate is dated and expensive.On the other hand the state is beautiful and there are endless outdoor activities.
 
reply



  
 
subsubzero 32 minutes ago  
             | root | parent | prev | next [–] 

It is, I was just there this past weekend. The town itself is very small, and like some have said its getting more pricey. I can talk about this area for hours as I had a job offer from a startup spun out of LANL a few years back, I ended up not taking the job and sometimes regret it.But for Los Alamos itself I would consider it on an ""island"" so to speak as the surrounding communities(except santa fe) are not great, see espanola. There are reservations around the area and they have their own issues(see drugs alcohol etc - the drive from the valley to Los Alamos requires you to turn on daytime running lights due to many DUI's) but the town is extremely friendly and safe just not alot to do if its not outdoor related.Like some have said some LANL employees commute in from Santa Fe which is a nice town, it skews very old and rich(on the north and east sides) and if you have a family is not ideal(almost all the families live on the south side of town). Overall the area has excellent food(northern new mexican food is incredible!) and for outdoor enthusiasts it really can't be beat. Home prices in Santa Fe have risen alot in the past few years like most nice outdoor areas. But I think you can't go wrong with the area if you don't mind the few downsides.
 
reply



  
 
jessriedel 1 hour ago  
             | root | parent | prev | next [–] 

Strong emphasis on ""little"".  If you're married with kids, it can be great.  Much harder for single young adults, especially men since the gender ratio at the lab is unbalanced.  Most people in town are employed by the lab or contractors for the lab.
 
reply



  
 
dlivingston 2 hours ago  
             | root | parent | prev | next [–] 

Definitely. The town always gave me  'Stars Hollow' vibes, if you've ever seen Gilmore Girl.
 
reply



  
 
ianai 3 hours ago  
             | parent | prev | next [–] 

It’s much less affordable than it used to be. Lots of transplants seem to be driving prices up.
 
reply



  
 
dlivingston 2 hours ago  
             | root | parent | next [–] 

If you're talking about housing and property prices, that's a long-standing supply issue related to building a town on a mesa...
 
reply



  
 
ianai 1 hour ago  
             | root | parent | next [–] 

That just makes it a factor which long term diminishes the labs abilities and people. Consequences priced-in as it were.But the grocery stores and surrounding supportive industries like medical/dental are also going to reflect the towns population.I do love Los Alamos but knowing what a large city is capable of helps, too.
 
reply



  
 
ouid 4 hours ago  
             | parent | prev | next [–] 

>low cost of livingExcept for the nightly drive to Santa Fe.
 
reply



  
 
dlivingston 2 hours ago  
             | root | parent | next [–] 

If you live there, sure. There is also Los Alamos, White Rock, Jemez, Española, etc.
 
reply



  
 
femto113 1 hour ago  
             | root | parent | next [–] 

It gets only a sliver of attention compared to Los Alamos and Santa Fe, but Española is actually effectively another LANL company town--the lab is the town's biggest employer and a large portion of the lab's workforce lives there, so e.g. sharing the commute up the hill would be easy.  And compared to either of the others (or any FAANG city) real estate is cheap (like 3+BR 2000+sqft on half an acre for < $500K).
 
reply



  
 
mattpallissard 8 minutes ago  
             | prev | next [–] 

I worked at ANL, which didn't do any weapons research, and loved it.  Definitely had a lot of bureaucracy so if you haven't worked enterprise be prepared.  Things can move slow.  I knew some people at several other national labs and a few in DOD.  I got the sense that it was the same over there as well.That said, it was hands down the most interesting place I've ever worked.  I don't think it would have been nearly as interesting if I were a remote worker though.  Half the fun was taking to the ""lifers"" there.  Learning the lore of old experiments and asking them to show you around on lunch breaks.
 
reply



  
 
chemeril 3 hours ago  
             | prev | next [–] 

Did some time at LANL as an R&D engineer in the non-global security skunky areas, though wound up leaving for reasons not pertaining to the work. Participated in several projects involving Sandia and LLNL.Pros:
 - Pay was excellent, especially for the area
 - Incredibly beautiful country
 - Very interesting work
 - Infinite well of taxpayer dollars for equipment and materials
 - The best job security one can find
 - Crippling bureaucracy enforced a remarkably safe work environmentCons:
 - Crippling bureaucracy made it difficult to move quickly and hit tight deadlines
 - Internal politics (intra-lab and inter-lab) often adversely affected decision making and program success
 - Living in a company town
 - An inability to remove demonstrably problematic employees
 - A Q clearance limits certain extracurricular activitiesPersonal experiences with LANL were all over the place and highly, highly dependent on which group one works with. I was very lucky to get in with a group of wonderful people and immediate management that firewalled most adverse developments from higher up the food chain. This is not a common experience but organizational mobility is relatively free, so you can move to work and groups that are attractive.Worth noting for those coming from private industry: the national labs are institutions first and foremost, not businesses. Organizationally and operationally they exist in a very different mindset and within very different value systems than FAANG-like orgs. The adjustment can be a bit jarring.My work at LANL will likely be the most interesting and most fulfilling work I'll have done: every day was an adventure into the unknown. The work/life balance was also excellent. If you're a naturally curious person and have an inclination for basic science I'd recommend taking a look at the labs. If you have specific questions feel free to drop them here!
 
reply



  
 
qooiii2 2 hours ago  
             | prev | next [–] 

I interned at Sandia Livermore and Los Alamos in college, then worked at Sandia's main site for a few years before moving out to the Bay Area to work in the more dynamic world of consumer electronics.The labs are not for everyone, but it's the perfect job for some. If you want to work with fantastically smart people and don't mind following a lot of arbitrary rules, it can be a lot of fun. Most of my coworkers intended to spend their entire careers there.Just like anywhere else, a lot of the day-to-day experience depends on the group you work with. In general, it's somewhere between a university campus and a defense contractor, and the mix is different for each project. The good part is that once you get a security clearance and make some friends in other groups, you can move around.There might be some culture shock. Most employees have to be US citizens, so the labs are probably less diverse places than you might be used to. And you will really be hitting the brakes while you wait for a security clearance.I'd say look at the job postings and give it a try! It didn't end up being for me, I don't regret the time I spent at the labs. And it's tough to beat the work-life balance. You can't take a lot of the work home, and most people take every other Friday off (9/80 schedule).But do consider the location carefully. For example, Sandia and Los Alamos are both huge and have a huge variety of projects, but you're stuck in Albuquerque or Los Alamos which can be limiting unless you really enjoy hiking.
 
reply



  
 
q845712 3 hours ago  
             | prev | next [–] 

I interned at Sandia around '04-'05. That was a little bit before the current ""FAANG"" thing we have now (IIRC amazon was turning itself from a bookstore into an e-commerce startup, and google was still not being evil) but certainly it was already less glamorous than getting a Microsoft or even Apple internship.My recollection is that the whole experience depended on which group you were in, and mine was fortunately very chill. Smart, friendly people who arrived and left more or less at the same time every day. Lots of matrixing and loaning of people from different orgs -- I had the feeling that if I were making a career there I would wind up slowly drifting around between projects.The biggest surprise to naive me-in-my-early-20s was that ""Department of Energy"" is a euphemism for ""Department of Nukes."" Nuclear stockpile stewardship was a large portion of the activity there, and so a lot of your colleagues will be people who are at least vaguely comfortable with that.There was a ton of ""basic research"" too -- some high-energy group had a daily experiment that would deliver a ""whomp"" of a shockwave around 3:15pm most afternoons, there was a room temp. fusion group, lots of interest in assisted driving cars and unmanned aerial vehicles...  you just had to appreciate that all the first applications of all this tech was going to be military.Also the security clearances.... the joke was that the ""L"" clearance stood for ""Lavatory pass"" because in our building until you got one, you needed a line-of-sight escort at all times, even in the bathroom. Even for the ""L"" the process was quite onerous, and I understood that the 'Q' clearance held by nearly all full-time staff was even more burdensome. I heard stories of people waiting for their clearance getting stuck in rooms with nothing to work on. One person in my group basically got sent offsite to some ""think tank"" or something for several months while he waited for his clearance - I only met him once the whole summer, at a conference.
 
reply



  
 
madcaptenor 3 hours ago  
             | parent | next [–] 

The biggest surprise to naive me-in-my-early-20s was that ""Department of Energy"" is a euphemism for ""Department of Nukes.""It also surprised Rick Perry when he got to be Secretary of Energy (he thought it had to do with oil)
 
reply



  
 
aplsoftwaredev 1 hour ago  
             | prev | next [–] 

I work as a software engineer at the JHU applied physics lab and absolutely love my job.* Pay is very comfortable to live on in the area* The large majority of my teammates are self motivated and driven which keeps me motivated and on my toes* I get to constantly experiment with new tech, work on prototypes, and pursue work I'm interested in.* Almost all of my work is software development but it's rarely pure software work. I'm almost always working with other SMEs and helping them develop their ideas into codeIf you're a curious, hard working person it (and I imagine other UARCs) are great places to be
 
reply



  
 
UniverseHacker 18 minutes ago  
             | prev | next [–] 

I highly recommend it- the culture is much better than academia or industry in my opinion, having experienced all three. You get to work on big projects that directly target big problems in the world, with the best equipment and facilities in the world. There is less politics vs academia, and less careerism- people mostly love what they are doing, and are excited about it. Generally great benefits, pay, and work-life balance.Also, the national labs model gives you a chance to work on big teams with people from diverse backgrounds, which is a lot of fun. For example, a software engineer may find themselves working day to day with physicists, biologists, etc. and learning enough about these fields over time to make novel contributions to them.
 
reply



  
 
tsbischof 4 hours ago  
             | prev | next [–] 

I worked at LBNL, at the Molecular Foundry. The day to day was a mix of typical nanoscience work (chemical synthesis, electron microscopy, etc) and work in support of the user facility. In my case that involved consulting on projects involving our users (design of high-throughput screens, teaching spectroscopy, etc), setting up and maintaining instrumentation, and developing workflows for our chemical synthesis robots.I liked the work and really enjoyed getting to be a consultant on many projects. Turnover is massive among the researchers because there are few permanent positions, and most groups are heavy on postdocs since graduate students tended to be primarily on campus (UC Berkeley).If pay is a concern, look closely for the open databases of salaries. At LBNL there is the ""book of tears"" at the library under the cafeteria, listing every employee and their salary. The exact amount you get varies wildly with the department: prior to unionization in 2016, the range was from 20k to 125k annual salary for postdocs. I hear they raised the floor to NIH levels at least, but I assume they did not make NERSC take a paycut.
 
reply



  
 
uberman 4 hours ago  
             | prev | next [–] 

Just a heads up that most of the interesting work will require a ""current DOE security clearance"".  Many positions at places like LLL or any DOE lab really are going to require the more intense Q clearance.Sometimes prior clearance can be negotiated and a well qualified candidate who is likely to clear might be accepted and placed in a holding pen until they clear but I'm not sure what the backlog is now or even if they do that anymore. At the very least you will almost certainly need to be a US citizen proper.
 
reply



  
 
floren 4 hours ago  
             | parent | next [–] 

When I worked at Sandia, most of us were hired without any clearance but with the expectation that we would get one soon.It is not difficult to get a Q clearance, just annoying. You have to fill out a massive document listing everywhere you've lived in the last 10 years (a real hassle for a recent grad) and give all sorts of info about people you know. They will drive out and interview people.There was plenty of work that did not require a clearance, but so much of the sites are cleared-only that it just makes your life easier to have it.edit: oh yeah good point made in the dead comment below, if you've smoked weed in the last 7 (? something like that) years you're gonna have to tell them. Even if it was legal in the state where you did it. I've heard it's not a deal-killer these days, but they want to know and you will have to stop using it. There are not a ton of ways to lose your job at a National Lab but failing a drug test is one. Do not toy around with it.
 
reply



  
 
oppanoppen 3 hours ago  
             | root | parent | next [–] 

> It is not difficult to get a Q clearance, just annoying.Will be if: you do drugs (including pot), have debt problems, have dual citizenship esp. if you have made use of it in some way.> You have to fill out a massive documentSF-86. https://www.opm.gov/forms/pdf_fill/sf86.pdf
 
reply



  
 
SoftTalker 2 hours ago  
             | root | parent | next [–] 

These are the forms that the OPM leaked some years ago, BTW. So there's that risk also.
 
reply



  
 
justinzollars 3 hours ago  
             | parent | prev | next [–] 

Is this something you can apply for prior to getting a job?
 
reply



  
 
actinium226 1 hour ago  
             | root | parent | next [–] 

Certainly not. This is a government security clearance, not an industry certification. Zero need to distribute these to anyone who doesn't specifically require it.That said, it stays with you afterwards, so you can take another job that requires clearance at a different organization/company (provided you have the right level of clearance for the job). I think it expires after some time.
 
reply



  
 
dguest 2 hours ago  
             | root | parent | prev | next [–] 

Not that I've heard: it's not unusual to have the start of the job delayed by a year or so while someone checks in with everyone you've lived with / worked for for the last 10 years.
 
reply



  
 
hakkoru 3 hours ago  
             | root | parent | prev | next [–] 

No, a company or organization must sponsor your clearance. You cannot start the process on your own.
 
reply



  
 
fryz 3 hours ago  
             | prev | next [–] 

Haven't seen anyone mention a non-DOE lab, so figured I'd weigh in.I interned twice with MIT Lincoln Labs, which among other things, helped build and deploy Radar for WWII which turned into building/managing the technology for Air-Traffic Control, and then turned towards space.They are primarily a DOD-associated research lab (even located on an US Air Force Base), and so most of the projects have some military-oriented mission. Their mission is entrepreneurial-minded (which I found cool), in that they do the ""basic research"" and prototyping to prove viability and then the DOD turns over the project to a contractor to make feasible.While I was there I worked in their GeoIntelligence and Natural Language groups, doing research which I'd ultimately come to understand as being relevant for Project Maven (year 1) and PRISM (year 2). While I'm sure as an intern my contributions weren't directly related to or otherwise leveraged for these programs, in hindsight it was clear that this was the bigger picture that the work was contributing to. Take from this what you will.Most of the anecdotes that I've read through in the comments mirrors my experience. However, one thing I see missing was how opportunity was ""metered"" out. Each group I was in was organized like a research lab and the level of your academic progression limited (or opened) your ability to get access to specific projects/work. Their pay scale was also dictated based on this as well. So if you have a BS, your ability to ""move up"", doesn't exist, but it does if you have PhD.Ultimately, I was given an offer to work there, but ended up taking a SWE position in the Bay Area because I wasn't interested in continuing my education and felt like my ability to have a career progression at MITLL would have necessitated that.
 
reply



  
 
neilv 1 hour ago  
             | parent | next [–] 

The glass ceiling for non-PhDs sounds like a caste system, not the meritocracy that the broader MIT-ish community sometimes professes.
 
reply



  
 
indigochill 3 hours ago  
             | prev | next [–] 

I haven't, but outside the DoE (but still in national lab land) my brother worked at APL and I interviewed there (after hearing all the praise he had for it). I loved interviewing with them. Everyone, even when they severely outclassed my own education level (I have a bachelor's in journalism and am a self-taught software engineer, and I was talking to a couple of PhDs with many years of experience), were super personable, humble, and passionate about their area of expertise. They made me feel like I was already part of their team even when I was just interviewing (they even went out of their way to make international interviewing work since I live abroad). If they'd made me an offer, I almost certainly would have taken it. Great people and by far the best interviewing experience I've ever had (though in retrospect, interviewing for a position requiring clearance while living abroad was probably an uphill battle).As others have said, the work was highly self-directed. As for the need for software engineers, it was definitely there according to my brother. The scientists he worked with were capable in their field, but they needed someone capable of translating their models into something that would execute on a computer. I don't know what exactly he worked on, of course, but he's an ML specialist and was pretty interested in CUDA programming around that time, so maybe that's a clue what kind of skills he was applying.Anyway, maybe something to check out similar to the DoE network.
 
reply



  
 
mdmglr 3 hours ago  
             | prev | next [–] 

Speaking from my experience. All comments here are my own.- Exceptional work/life balance that you will not find anywhere else.- I started at 100k fresh out of masters program, at 5 YOE I was 150k. Goes up steadily YoY.- The labs operate like a variety of small businesses. This is because there are many projects and funding sources from a variety of customers.- From what I hear some labs are super relaxed. Like Los Alamos. The working environment is unlike any other.-  It is typical for teams to be in the same building but have no idea about each other. Overlap and rework is common, but is improving.- Performance reviews depends on the lab and whatever review process du jour HR wants. Where I am you are in competition with your peers. Limited bonus money. So you’ll need to go above and beyond your peers to get it.- day to day is: you work on one or more projects that last anywhere from weeks to years and report to that projects principle investigator. The PI will interface with the customer and get funding.- for software development we need to go through strict security processes that dictate what libraries and dev tools we can use. We use self hosted versions of popular tools like Mattermost and Gitlab. No cloud. GovCloud is typically too expensive for most customers unless your working on very well funded projects.- Managers are hands off and mostly there to ensure corporate compliance activities get done. E.g training, timesheet, perf reviews,etc.- High level of autonomy. So your expected to be knowledgeable in your area, able to learn quickly and able to work with and network with others to deliver results quickly to customers. For example, you might be tasked with implementing an algo a staff scientist came up with in C++ for an ARM board. It doesn’t matter if you haven’t done that before. Your expected to learn C++, get a demo out, and maybe you can pull in some colleagues you previously worked with who are experts to help.- There is a political structure in place and reputation is important. While it may not be as intense as FAANG, and underperforming is likely not to get you fired, if you consistently underperform folks will remember and your reputation will be permanently ruined. Which will result in not being picked for more desirable projects. And likely shunned. I’ve seen it happen a few times.The lab was going to be a brief stint on my way to FAANG but will likely turn into my career.
 
reply



  
 
Huntsecker 5 minutes ago  
             | prev | next [–] 

tinfoil hat, but couldnt this be an easy way for foreign agencies to easily find whos working in a sensitive industry
 
reply



  
 
eslaught 29 minutes ago  
             | prev | next [–] 

Experiences can vary a lot. For example, we're bootstrapping a CS research group at SLAC; our goal is to do fundamental CS research. Depending on where you land, that may or may not be typical of your experience. A lot of labs are science-focused (which they should be, but sometimes it comes at the cost of awareness of the CS side).(We're hiring by the way; a bit stale but [1] is still relevant.)[1]: https://news.ycombinator.com/item?id=33424570
 
reply



  
 
wpasc 4 hours ago  
             | prev | next [–] 

I'm not sure my response qualifies, but I see no other responses yet so:I interned in high school at Brookhaven National Lab working on a team that analyzed STAR (Solenoidal Tracker at RHIC) data from RHIC (Relativistic Heavy Ion Collider). I didn't contribute all that much as a high school intern but the program director said at the end that he liked the high school program because he wanted to help funnel and bring people back to help build up the labs.My experience was that everyone there was extremely smart, but all post-doc and top scientists in their field (the team I worked on was looking for Anti-Alpha particles from gold-gold particle collisions that also helped create Quark Gluon Plasma). So I'm not sure their relative need for regular software engineers.In terms of bureaucracy, you're still working for the government. The scientists all complained about the layers of government bureaucracy but were mostly okay with it. High-tier science moves at a pretty slow pace; coming from a tech background you might not be used to the slow pace around the actual physical construction of some of these devices, let alone the fund-seeking, approvals, testing, runs, and data collection. and 33-50% is a hopeful estimate. Let's say one is a 500k a year senior/staff SWE at FAANG. at a similar level of experience, one's pay would be very lucky to break 150k.So fascinating science, layers of bureaucracy, slow moving stuff, PhD's in their fields, and reduced pay. Again I was only a high school intern, but I spoke with the scientists about their experiences so take my recollection with massive salt. I walked away from the summer fascinated by the work and I had a love of physics at the time; but I also left (this was 2010 IIRC?) watching the world of tech explode at a massive pace and thought that I didn't like physics enough ( I had spent my junior/senior year of high school doing a capstone project on theoretical physics and having taken a lot of physics classes). When I went to college the next year, I tried a few engineering courses, and switched to CS. I'm glad I made the switch.
 
reply



  
 
AlotOfReading 4 hours ago  
             | parent | next [–] 

There's a pretty decent need for software engineers at national labs and they occasionally do some cool stuff. If you've used ZFS on Linux for example, you've used something produced by LLNL. They do some pretty massive software projects and have a huge number of software testers. The ones I've interacted with were pretty exceptional.
 
reply



  
 
dekhn 4 hours ago  
             | prev | next [–] 

I worked at LBNL (in Berkeley) and it was great.  It's like academia but with no teaching responsibilities.  Yes, the pay was lower and the expectations were roughly the same as MAAA (Microsoft-Amazon-Apple-Alphabet) but you didn't get fired if you didn't meet expectations.
 
reply



  
 
aksjdglkjlk 4 hours ago  
             | parent | next [–] 

LBL is a special place. It's still managed by UC without the involvement of any other orgs (aside from the DoE). It's basically just another UC campus, but research oriented and federally funded. The other labs can be quite different, managed by LLCs that involve a few universities and some private companies.Things might have improved now that Bechtel is out of the picture, but for many years LBL was hands down the best lab to work for, purely because of the management situation.
 
reply



  
 
dekhn 3 hours ago  
             | root | parent | next [–] 

I wonder if the same thing can be said about Argonne (run by U of Chicago)?My manager at LBNL worked there because he was kicked out of one of the other labs (run by Batelle or Bechtel or one of those) for refusing to take a drug test.  They said he'd fit in well at LBL- and he did.
 
reply



  
 
jvanderbot 3 hours ago  
             | prev | next [–] 

I spent 7 years at JPL, which is run mostly like a national lab, based on this comment [https://news.ycombinator.com/item?id=34414812]Is it a change of pace? Yes, but not in workload. You can easily work yourself to death if you let yourself. And politics happens everywhere -- usually in the form of missing the good projects. But the promotion frenzy is minimized, there's a sense of greater purpose in all the projects that is impossible to replicate anywhere else, and the technologists run everything. Managers help connect, but they don't typically determine your day to day priorities. In that sense, you can continuously shop around for good projects and teams without any formal change of position.
 
reply



  
 
xvedejas 4 hours ago  
             | prev | next [–] 

I've known several people who worked at LANL, and they reported you might sit around for six months or more waiting on bureaucratic approval for your project. They all agreed: very slow moving, lots of red tape, otherwise fine.
 
reply



  
 
mrpf1ster 4 hours ago  
             | prev | next [–] 

I worked at Argonne National Lab for two years as a web developer for a few projects (https://afleet.es.anl.gov/home/, https://energyjustice.egs.anl.gov/).The people I worked with were super smart in their fields, but were pretty bad at writing code / handling data outside of Excel so they usually hired interns to help with code-related stuff. Some of the divisions had full-time software developer teams, but I was the only software developer in mine.The pace was extremely relaxed, deadlines were not tight at all.I worked remotely, but came in to the office a few times a month. The campus is beautiful, as it is right inside a nature preserve. Everyone there is doing scientific work, so it feels like a real scientific think tank atmosphere and I loved it.
 
reply



  
 
tcpekin 57 minutes ago  
             | prev | next [–] 

I worked as a grad student in LBL/UC Berkeley for 5 years. At the time I didn't want to make a career of it, but if I had to go back to a national lab, that might be the one. The culture was department dependent, but at the Molecular Foundry was pretty good. The campus is gorgeous, it's not isolated like other national labs at all, pay is low compared to tech for the Bay Area, but not pennies, and the conversations and people you can meet are fantastic. Always lots of new faces with students and visitors, but the core group that I interacted with were all very kind, helpful, hardworking, and just fun to talk to about science! I can definitely recommend it.
 
reply



  
 
actinium226 2 hours ago  
             | prev | next [–] 

Just to add a couple places to your list that are similar to the national labs:Simons Foundation (offices in NYC and Berkeley)
Allens Institute (Seattle)OpenAI would be more research focused as wellI have no affiliation with any of these nor have I worked at any of them, but I'm also looking towards a career in scientific research.
 
reply



  
 
actinium226 1 hour ago  
             | parent | next [–] 

This thread is totally government/national lab heavy so I made a separate one about private research institutes: https://news.ycombinator.com/item?id=34417257
 
reply



  
 
jlturner 4 hours ago  
             | prev | next [–] 

My dad works in the Molecular Foundry division at LBNL / Lawrence Berkeley Labs (Dept of Energy and UC Berkeley), and loves it. He started there 40 years ago working in electron microscopy and oversaw the transition to digital imaging (you’d be surprised how much code they write). Good work life balance (he comes home for lunch everyday), a pension (rare these days!). His favorite part of the job is the revolving door of very smart people using/visiting the lab and getting to interact with so many ambitious (and not yet jaded) younger grad students from UC Berkeley.
 
reply



  
 
tcpekin 1 hour ago  
             | parent | next [–] 

I am 99% sure I know him - shares your same first initial? Tell him hello from me (TC Pekin), I always liked talking to him during some down time, and the feeling was mutual. As a grad student I always liked talking to him and hearing about all the history and his time at NCEM!
 
reply



  
 
seanlane 3 hours ago  
             | prev | next [–] 

It's been a minute since I was an intern there, but haven't seen this one mentioned yet. I spent a summer at Pacific Northwest National Laboratory (PNNL, https://pnnl.gov) and really enjoyed my time. It seemed like compensation was pretty good, especially considering the cost of living in the area. There were a number of times when we needed some help from a subject matter expert, and we could go down the hall or to another building and speak with someone who recently published on the topic. There was a lot of interesting work going on, from national security issues to storing nuclear waste, etc.The campus there was also different than many other national lab campuses in that it's an open campus and doesn't have the military entrances that others have. It felt like the culture was much more laid back than the FAANG and other corporate cultures that OP mentioned, but perhaps more bureaucratic as well. Again, I was an intern, so didn't have much visibility into that aspect. Overall, definitely a positive experience and I could see myself there if things lined up right.
 
reply



  
 
hprotagonist 4 hours ago  
             | prev | next [–] 

Your experience will vary strongly depending on which PI you work for.Expect a very different work culture.  If academia is cozy to you, you'll fit in fine.
 
reply



  
 
evanb 3 hours ago  
             | prev | next [–] 

I was a postdoc at Lawrence Livermore National Lab (LLNL) from 2013-2016, working on computational particle physics (lattice QCD).  Pay was extremely high (for a postdoc) but not high by comparison to other bay-area employers.  What you get instead is reliable job security, and some sense of civil service (as most of the programs are federally funded).LQCD is kind of funny because it doesn't YET have anything practical to say about nuclear physics, which is what the lab cares about, but it will someday.  So I was pretty insulated from all the weapons+complex integration stuff; my work was 'pure research', which is not that common (though it is more common at the postdoc level, which the lab views as a way of recruiting talent).  But unless you can find your own funding (usually from a DOE grant), you're working on something that advances the lab's programs.  I can't bring myself to work on nuclear weapons, which is why I didn't stay [there's a lot more to LLNL than that, of course, but it's what my field funnels into, broadly speaking].The computational expertise for HPC is really unparalleled, especially at Livermore (and Oak Ridge, which I've only visited).  They're consistently pushing the envelope in terms of high-performance machines which can address scientific questions that require extremely tight coupling between computing resources, rather than a cluster, and they have a lot of experimental architectures and things like that.  LLNL publishes a lot of open-source software; if you've used a cluster in a scientific setting you might be most familiar with SLURM or spack.The day-to-day can be a bit surreal.  At the defense labs people with enormous machine guns thoroughly check your badge on the way in.  On your walk to the cafeteria you might pass a beach volleyball court that's inside the superblock [an extra-high-security area where they've got plutonium etc.], next to a machine gun turret.  Very few employers have teams that regularly win SWAT competitions.The food was fine.  No luxuries like free snacks or anything else I'd seen my tech-company friends enjoy.  No dogs allowed.  LLNL has a lot of employee organizations for sports, charities, exercise, etc.  Transportation around the LLNL site is via sporadic shuttles but more practically there's a bike share, which is just a bunch of bikes you can leave anywhere (on the sidewalk / by a building).
 
reply



  
 
altintx 4 hours ago  
             | prev | next [–] 

I've personally done both Sandia (2004-05) and LANL (2006-10). I'd do LANL again, but Sandia was very political with dull work. Interesting ideas, but dull work. So much bikeshedding. So slow moving.I did workflow management systems, environmental controls in labs, and lightning prediction software.
 
reply



  
 
2OEH8eoCRo0 4 hours ago  
             | prev | next [–] 

FYI- there is also the US Digital Service. My interviewer was a former SWE manager @ Microsoft who wanted to help people in a ""civic duty"" type of manner.https://usds.gov/
 
reply



  
 
jhart99 3 hours ago  
             | parent | next [–] 

In addition to USDS(which has issues being under the White House), there is 18F and PIF at GSA which can also be good choices.
 
reply



  
 
nukenuke 3 hours ago  
             | prev | next [–] 

I worked at LBNL as a research scientist and spun a startup out of there. It was a great place to work in some respects, lots of really smart people, awesome brainstorming, seeing Nobel prize winners around. But if you like getting things done quickly it’s quite a challenge due the the bureaucracy (ex ordering simple things could take an extra couple weeks going through lab purchasing). I once described it to a friend who worked at a FAANG company and they said “Oh so it’s like working at a big company but without the advantages of a big company”.Working at our startup almost feels like working at the lab (ie we have scientists and are doing hard tech), but we can also move fast and don’t have the bureaucracy. So maybe consider working at a hard tech startup with a heavy science base!
 
reply



  
 
dopeboy 4 hours ago  
             | prev | next [–] 

I have a friend who works at Sandia. I remember him saying the pay is lower and there is way more work life balance. There is less of a sink or swim attitude around perf there. I got the sense you can come in, do your thing, and be out by 4 or 5 - everyday.
 
reply



  
 
dehrmann 3 hours ago  
             | parent | next [–] 

My dad works at a national lab. The other side of this is that underperformers stick around longer, and working with them can be frustrating.
 
reply



  
 
chinchilla2020 1 hour ago  
             | prev | next [–] 

I'm not sure how you would end up on the fusion program as a FAANG engineer.Software engineering is vastly different than the physical sciences (I've worked in both).I worked a bit with the Sandia and some folks from national labs on the corporate side in the energy industry. The only software engineers I was aware of were ones that did IT work (Integrated workday, salesforce, etc). The scientists and mech/chem/EE engineers I met were the ones doing all the physical sciences work.
 
reply



  
 
throwawy_gfdh 3 hours ago  
             | prev | next [–] 

Throwaway since I'm going to say some negative things.Overall I agree with almost all the positive things people say in other comments - the national labs have a lot of very smart and kind people, there are interesting things (or at least interesting ideas), and if you're at home at a university, you'll find a lot of kindred spirits.But I made the opposite move you're considering and quit my national lab job and moved to FAANG. Why? Because I wasn't a scientific superstar with a clear vision, and IMO my field (applied math, computational science, etc.) seemed to asking (by which I mean funding) people to do software engineering without many engineers and at the same time still be academics: scientists and mathematicians who spend a lot of time writing grants and trying to publish papers, etc. This made me feel like a liar, writing the grants, and a hack, writing the code. Not to mention that like all academic-type and ""interesting"" jobs, you are supposed to be happy with the idea that you're going to expend a lot of your free time and energy, and perhaps not be paid quite as much as you would if you weren't pursuing your (supposed) passion.Industry is for sure a whole other pile of bullshit, but don't assume that the performance review and promotion stuff is any more draining than the things you will have to do to get funding (you'll likely either be spending a lot of time writing proposals, or you'll be funded by weapons money). Don't make trying to get away from money and politics your reason for moving, though there are plenty of good reasons. Good luck!
 
reply



  
 
abadger9 2 hours ago  
             | prev | next [–] 

I'm surprised more people haven't brought up NASA? I was offered 180k/yr salary in 2020 doing fullstack development. While the offer was brought through a recruiting agency, I believe it was a full time position as a NASA Employee - I was surprised by the competitive salary, maybe worth looking into for others interested in the space.
 
reply



  
 
compsciphd 2 hours ago  
             | parent | next [–] 

doubtful it was as a NASA employee (i.e. US Govt).  It was probably as a contractor.  I believe (someone can correct me if I'm wrong) that NASA employees are on the GS schedule, as GS-15 step 10 in the DC area, only gets ~176K (after cost of area increase).  i.e. a very senior (but not executive) manager with many years at that level.The value of working at a national lab (vs say a military one) as an employee is that when one works at a military lab as an employee, one is a govt employee and on the GS schedule (which makes it hard for them to compete).  When one works for a ""national lab"", one is an employee of the lab which is under contract with the US govt, but they are not paid according to the GS schedule.
 
reply



  
 
ArtWomb 3 hours ago  
             | prev | next [–] 

Fascinating thread. Love reading the oral histories. I heard some tales of covered up nuclear accidents happening perilously close to Manhattan at Brookhaven National Lab back in the day, and its clear from the fusion results, government science is still the largest research outlay. If you are comfortable with payscale, lack of advance, essentially academic role in natsec context, then its a dream job! I feel like the only people I know anymore who have multi-decade careers for the same entity (DoE/PPPL) are gov scientists ;)
 
reply



  
 
rqtwteye 3 hours ago  
             | prev | next [–] 

I live in NM and meet quite a few people who work at Sandia Labs. I am sure it's not perfect but they seem pretty content. It's definitely not a high stress place. If anything, it's too slow and relaxed for some people.
 
reply



  
 
schoen 3 hours ago  
             | prev | next [–] 

I was a summer intern at LBNL as a student. Beautiful views and very smart people (and very big computers).The cafeteria is way less fancy than tech company cafeterias. :-)As a few posts have pointed out, there are national labs that do only unclassified work (LBNL is one). So you don't have to get a clearance or be prohibited from accessing lots of places or conversations, and you don't have to work on weapons. You do still have to sign a loyalty oath as a state government employee (the lab being managed under contract by the University of California), something that became highly objectionable to me in retrospect.
 
reply



  
 
floren 3 hours ago  
             | parent | next [–] 

> The cafeteria is way less fancy than tech company cafeterias. :-)The cafeteria at Sandia in Albuquerque always had extremely good posole, though.
 
reply



  
 
rheone 2 hours ago  
             | root | parent | next [–] 

The posole is a thing of the past. The latest food contract switch over didn't come with the recipe as I understand it.
 
reply



  
 
gautamcgoel 24 minutes ago  
             | parent | prev | next [–] 

I thought that loyalty oaths were not allowed in the UC system? I recall there was a brouhaha many years ago where they were requiring employees to disavow communism; several people refused and were fired. As a result, the UC system adopted a rule that political litmus tests would not be allowed when making hiring/firing decisions.
 
reply



  
 
whalesalad 4 hours ago  
             | prev | next [–] 

Reminds me that I started re-reading The Cuckoo's Egg over the holiday break and need to keep up the habit.
 
reply



  
 
hypeatei 1 hour ago  
             | prev | next [–] 

I know someone who works at LLNL and it sounds pretty typical but he has to go on trips every so often that can span anywhere from a week to a month for testing their designs and such.
 
reply



  
 
JackFr 3 hours ago  
             | prev | next [–] 

I worked at National Center for Biotechnology Information at the National Library of Medicine at the National Institutes of Health.  It was a nice entry level job, pay was OK, environment was fantastic (low stress, extremely collegial), but without an MD or a Phd (many had both) my path there was relatively limited.One other thing, I was absolutely blown away by the number of outrageously brilliant people I got to work with.  Really some next level people.
 
reply



  
 
boringg 3 hours ago  
             | prev | next [–] 

Always had a grass is greener mentality of the Labs but i suspect there are certainly their own set of challenges to overcome.  Also i imagine if you are trying to level up competition is subtle but fierce at the higher levels given that its zero sum for the positions.  I imagine multi year to decade long political campaigns for top positions in the orgs.Staff levels probably pretty good if your motivation isn't to move upwards and focus on whatever your interests are.These are my speculations.
 
reply



  
 
bargle0 1 hour ago  
             | prev | next [–] 

There are also FFRDCs and other similar organizations outside of the DOE. For example, JPL is a NASA FFRDC.
 
reply



  
 
scdlbx 4 hours ago  
             | prev | next [–] 

The projects are more interesting and feel like they are actually benefiting humanity, rather than making money for some random company. Though there can be a lot more bureaucracy and friction. Pay is certainly less, though sometimes the benefits can be better.There's also annoyances coming from political things, such as the budget not being done on time so no one gets paid or there are furlough days.
 
reply



  
 
secabeen 4 hours ago  
             | prev | next [–] 

The national labs are pretty good, as is Research Facilitation at most of the R1 research universities. You might find good leads through the Campus Research Computing Consortium.  We're still in the early days of developing these positions, but it is starting to snowgball.https://carcc.org/
 
reply



  
 
robg 3 hours ago  
             | prev | next [–] 

There’s a cool accelerator that is partnering with Oakridge and out of TN that might be a good middle ground:https://www.techstars.com/accelerators/industries-of-the-fut...
 
reply



  
 
elil17 3 hours ago  
             | prev | next [–] 

I have heard universally positive things from my friends working at NREL, Oak Ridge, and Argonne.You might also want to check out the US Digital Service, which might be more aligned with traditional SWE skills.
 
reply



  
 
metalforever 3 hours ago  
             | prev | next [–] 

I had a bad experience. I have top tech companies on my resume and they made the role look cool and I was trying to slow down a little . They instead put me into support . The codebase wasn’t in source control ; it was just a few scripts in any case . They didn’t have a sane deploy. The people that they had me working under had been there for decades but were not good . They talked down to me and did not give me tasks that aligned to my experience. I thought I would ruin my career if I stayed so I left. Much happier with a principal role where I’m at now .
 
reply



  
 
myself248 3 hours ago  
             | prev | next [–] 

Side question: How are these positions affected by things like government shutdowns and other Washington bullshit?
 
reply



  
 
Gh0stRAT 5 minutes ago  
             | parent | next [–] 

These days it seems like management tries to keep a buffer so they can operate for a little while during a government shutdown. That being said, there are a variety of funding sources and it wouldn't be fair for some teams to be allowed to keep working while others could not, so they'll typically shut basically the whole lab down once they run out of extraordinary budget measures.Also, if there is an actual shutdown Congress usually retroactively pays the federal employees who were furloughed but it doesn't typically apply to contractors. (DOE labs are staffed by contractors rather than government employees so that they can pay more than the government pay scales)
 
reply



  
 
ssully 2 hours ago  
             | parent | prev | next [–] 

It depends on how your projects/team is funded. I’ve been on teams where a shutdown had a pretty immediate impact (people had to be shuffled around to other projects in the short term). I’ve also been on teams that get all their funding at the start of fiscal, so a shutdown doesn’t impact them until they need to be funded again.
 
reply



  
 
tinglymintyfrsh 2 hours ago  
             | prev | next [–] 

Former neighbors had good things to say about Sandia. Albuquerque isn't so great though.
 
reply



  
 
kxyvr 2 hours ago  
             | prev | next [–] 

I spent about a decade working for one of the NNSA labs.  I think they're a good place to work, but it's a very different work environment than startup culture.  Most of what I'm seeing in the discussion is pretty accurate.  I'll add a few things in case they're deal breakers.It's likely that you'll either be required to or pressured into obtaining a security clearance.  Have a look at Standard Form 86 (SF86) and see if you're fine with these kinds of questions.  Depending on how comfortable you are with this line of questioning, do note there will be more digging especially if you've lived abroad, had prior drug use, had prior legal issues, or a variety of other items that pique their interest.  Failure to obtain a clearance is grounds for termination in most circumstances.A clearance comes with certain responsibilities.  You will have to report certain kinds of travel even if on vacation.  You will have to report certain kinds of contacts even if not work related.  You may not be allowed to freely publish even non-technical documents or books without prior approval depending on the level of clearance.  There is a way to accommodate the lab in a way where you can mostly live your life freely.  Most of this is just paperwork.  However, you do give up some of your autonomy.There is a proprietary innovation form they will push you to sign that will assign to them all innovations even on your personal time without company resources.  Likely, this is not valid in California.  Not sure if this is currently negotiable.  They were pretty insistent in the past.Pay for a starting Ph.D. was a little over $100k about 10 years ago.  It capped out for most people at around the $130-140k region after 10 years of experience.  Likely a bit different now, but not tremendously so.While it depends on the group, internal politics heavily favors pedigree and degree level.  Meaning, you are treated better if you have a Ph.D. even if it's not really necessary for your position.  I'm not sure I would consider a job there with less than a masters.  Practically, this means you're more likely to be the PI with a higher degree.  More likely to be selected for promotion.  More likely to be able to move to management if that's of interest.  More likely to obtain internal research funding.  To be sure, I don't agree with this, but it's a reality of the culture.Things like time card fraud are taken very seriously.  Meaning, if you work extra hours, you will be compensated for it either in terms of flex time or pay with overtime.  Management doesn't like to pay overtime, so this means that you'll likely be out on time most of the time.It's a professional workplace where people come to work and then leave.  As such, no alcohol on the facility, no gaming tables, or sleep cubicles or cutesy architecture.  It's not that people are completely serious the entire time, but they are professionals and there is a pretty strict separation between what's considered business activity and personal activity.The research projects are great.  There's easier access to grant money than in academics and it provides the opportunity to work in areas that are not necessarily commercially viable, but have broader impact.  Getting access to the money either means being good at proposal writing, and having the right pedigree, or making friends with someone who is and likes to farm out work to you.  When I say money here, you're not going to make anything more on the grant.  It means being able to buy out your time to work on this particular project.That's already probably too much, so I'll stop here.  In short, I think they're a great place to work if you can fit into the culture with them.  They tend to play by the book professionally and tend to hold by their agreements with you.  However, there is a lot of paperwork and you will necessarily give up some of your personal autonomy.
 
reply



  
 
throwaway84592 3 hours ago  
             | prev | next [–] 

I've worked at Sandia Labs as a software developer my entire career, so I can't compare to FAANG or SV in general. Obviously I like it, or I wouldn't have been here for 20 years.Their job classification system is such that you will want a Masters degree. I joined with a BS in CS, and until I got my MS, I was categorized as a technician and was paid the same as say someone who soldered and assembled electronics - just over half of what someone with masters in CS would get with simular rankings. I've heard it has improved since then, but there is still stong bias towards those with a masters. For many engineering jobs this makes sense, but it is out-of-touch for computer programmers and security researchers.Apart from that while the pay is less than SV, all the labs except LLNL are in parts of the country with much lower cost of living as well, so the pay is pretty darn good for the area IMO. Benifits are good, but not exceptional. Work-life balance is great. I've hand a handfull of month-or-two long crunches in a 20 year career where I had to work 60 hours a week. The rest of the time I work my normal 40 hour schedule and go home. They have standard, 9-80, and 4-10 schedules as options (which nearly all managers will approve). After years of having every other Friday off it would be hard for me to go back to a normal schedule.The actual work varies wildly with the project you are on. Nuclear Weapons work is extremely slow and process heavy as you might imagine, others are more nimble. Projects I've been on have varied from solo development writing software for the engineering next office over, to small agile teams on quarterly releases, to 5-year waterfall development cycles which a huge team. I've done everything from microcontroller software for sensor systems, realtime streaming data processing, desktop data analysis software, web tools for managing data stores, to pure algorithm research. And that is just a small sample of projects going not even touching supercomputer simulation or security work, that I have no experience with. I feel like I have had a great balance of interesting and stimulating technical work and necessary grunt work. Needless to say it is hard give a single ""this is what working at the labs is like"", and individual experiences will vary.There are some differences from industry that are independent of the project, particularly around security. It is not uncommon for software development to be done at the unclassified Official Use Only level, but (production and test) data to be at the classified level, which is done on separate networks (or stand-alone computers). Moving between the two environments can be a time sink. Getting approval to use third-party libraries on classified systems can be a very slow process (weeks at a minimum) depending on the network. If the generic security plans won't work for your project developing a custom one can take the better part of a year. There are many security processes for which I completely respect the purpose, but am flabbergasted at the inefficiency of the execution. Contributing patches back to open-source projects is painful enough that it is rarely done. There is some third-party software that is prohibited (like JetBrains due to connections with Russia), and cloud based tools (on the public internet) are obviously not allowed.  You need to be constantly mindful of what you type/say to maintain OPSEC and avoid leaking classified onto OUO systems, or leaking OUO to  friends and family.They are allowing WFH now, but most managers for most jobs will want you start on-site to help acclimate to the security culture, and to live in town to be able to come on-site to work in the classified environments when needed. You will need to apply for a security clearance once hired, and some projects are better than others at finding meaningful work for you to do while waiting for the security clearance to be granted.As far as ethics go, on one hand you won't be asked to write dark-pattern advertising-driven manipulative spyware. On the other, most work will be related to defense applications directly or indirectly. There are some projects related strictly to energy generation and power-grid security and the like, but they are the exception. The best way to advance your career in the Labs is to move around between departments every several years, so you will be limited in your options to do that if you have reservations about defense work.
 
reply



  
 
devoutsalsa 3 hours ago  
             | prev | next [–] 

What are the qualifications to get a software job at a national lab?
 
reply



  
 
wedn3sday 3 minutes ago  
             | parent | next [–] 

From what I remember from some old all-hands slides, in the R&D groups (so not facilities, not administration etc) there was a pretty even breakdown of 1/3 PhD, 1/3 MSc, 1/3 BS, and like 2% with no degree.
 
reply



  
 
shagie 3 hours ago  
             | parent | prev | next [–] 

Likely depends on the lab and the project.For example, here's a software developer at Idaho National Lab - https://inl.taleo.net/careersection/inl_external/jobdetail.f... which is bachelors + 5 years professional.Same lab, Cybersecurity Researcher  https://inl.taleo.net/careersection/inl_external/jobdetail.f... which has bachelors + 0-2 years.Over at LLNL Embedded Software Developer  https://www.llnl.gov/join-our-team/careers/find-your-job/all... is ""just need a bachelors""while Software Developer https://www.llnl.gov/join-our-team/careers/find-your-job/all... asks for a masters.
 
reply



  
 
kincl 3 hours ago  
             | prev | next [–] 

Great thread! I noticed a bunch of the comments are from Sandia/LLNL/LANL all of which are mostly focused on the National Nuclear Security Administration side of the Department of Energy which is focused on the various aspects of maintaining the nuclear stockpile of the US.I worked at Oak Ridge National Laboratory in High Performance Computing and did not work with anything directly nuclear at all. The HPC efforts of the DOE are under the Office of Science (separate and at the same level as the NNSA) which is focused on more wider scientific impact and application than just nuclear. The Office of Science has a number of program offices that focus on all different kinds of science from basic energy sciences/physics to biological/environmental and scientific computing (where HPC is funded in DOE).I agree that the work/life balance is great and it is definitely a slower pace than what you would find in industry. The lab system is huge and there are plenty of opportunities but on the Office of Science side I like to break it down between what I think of as a research group and a user facility.Working in a research group is much like academia, they mostly require a PhD and from what I could tell performance is judged on publication output. These folks also write grant proposals that come from DOE program offices for funding their own research. In some cases I have seen these groups employ non-PhDs to be computational scientists and write code.The user facilities are long-running projects funded by the DOE at the labs to provide specific capabilities to researchers, sometimes just for DOE scientists but a number of them are open to scientific researchers all over the world. This is where I have the most experience where I worked at ORNL's National Center for Computational Sciences on the Oak Ridge Leadership Computing Facility (OLCF). These projects are generally well funded and have all kinds of interesting challenges to solve.  For example, the OLCF has consistently deployed the number one supercomputers on the Top500 list and it offers those computational resources to anyone through their allocation program INCITE which supports many different computational modeling and simulation experiments. Other examples of user facilities at ORNL are the Spallation Neutron Source and the High Flux Isotope Reactor.One thing I have noticed since moving from ORNL to industry is that the sense of shared purpose does not extend as far in the lab system as it does in company. What I mean is that with the small research group and with a user facility like the OLCF there is shared purpose with the people in those groups but it does not go much beyond that. A lab is generally made up of lots of different research groups and a few facilities but beyond the drive for ""Science!"" there is not a lot of shared purpose or collaboration at a macro level. The analogy I use is that a lab is a bunch of small dinghy boats that are all generally moving in a similar direction but a company is a single ship with a specific purpose driving it forward.Overall I loved my experience at ORNL, I learned so much working with so many smart people and made friends that I will have for life.
 
reply



  
 
oppanoppen 3 hours ago  
             | prev | next [–] 

Have worked at Sandia for 12 years. Will probably retire in another 20.Pay is good. I make $145,000. That is low for Sandians with 12 years experience I think, but have had some atypically low points in my Sandia career. You can make more in 'private industry' (other defense contractors) even in Albuquerque but you will lose work-life-balanceBenefits are very good. 3 weeks paid vacation. 2 weeks unpaid. Flexible work schedule: normal hours, or 9/80, or 4/10. Generous 401k match, plan supports roth mega backdoor, HDHP+HSA available. Good WFH was slow to arrive, but corona fixed that. Nobody has ever disturbed me on vacation or implied I should not take one.Location is ok. Abq is high crime and NM is a poor state with poor outcomes but it is very rugged beautiful. Don't knock LCOL, it provides wonderful peace of mind, and the bad parts can be easily avoided, but maybe you won't like it. Relative to NM, Los Alamos is outlier with very good outcomes (crime nil, public schools among best in nation), because it is a place that only exists due to LANL. Sandia CA is option but while Sandia Abq pays well for Abq, Sandia CA pays very bad for CA. Californians often poached by FAANG.Clearance means govt will look up your ass, and often. You must report all 'meaningful' foreign interactions, including friends/family. Investigations occur every five years. Random drug tests: get a phone call that tells you to go to the medical facility and piss today, or you're fired. 'Forgetting' to pick up your phone only holds them off for so long. Some (rare) clearances have worse requirements: must report all dual citizens not just foreigners; random polygraphs; must ask permission to leave the country; but again, these are rare. Easy to opt out of such a clearance and it will hardly limit your opportunities at all.Project work varies. Nuclear is the mission but Sandia has expanded to wider govt contracts. Nuclear weapons, non-nuclear weapons/military, CIA/FBI/NSA partnerships, all are possible. If this is against your morals, there are other options, but probably better to work elsewhere. Would be like working for Google while hating ads. Myself, I am okay with thise things. I have not encountered govt abuses, nor any projects I consider inherently immoral, but perhaps I am naive.I have had good projects and bad projects. Sandia is broadly 10+ years behind the curve at software engineering. There are pockets that are better, but eg version control is spotty in some places, lots of crufty old codebases. I think a symptom of being primarily an elec/mech/chem eng shop since the 50s. But there are 12000 people and if you find the right department, combined with the job benefits it is heaven. I have, and will stay till I retire. It's the perfect job for me.
 
reply



  
 
wedn3sday 3 hours ago  
             | prev [–] 

I worked at LLNL for a little over 6 years, and recently took a job at a ""real"" software company. Other peoples comments here are completely valid so Im not going to rehash them all, but they all seem to be focusing on the positive so here's a few take aways from my time behind the curtain (taken with a grain of salt, there was a reason I left):1. Extremely high bureaucracy, very poor facilities management. Expect a terrible shared office in a temporary building with asbestos. Dont expect to be reimbursed for your travel expenses for like 3 months, and you're very likely going to have to travel for conferences (i.e. fly to DC to make your sponsors happy).2. The old joke around LLNL was, ""hey, you know how many people work here? About half."" Half the people you work with will be the smartest most productive (and friendly!) people you've ever met, the other half are also smart but have realized that their productivity has no bearing on their advancement and so have decided to not give a fuck. Since hiring is such a nightmare, its very hard to get fired. From a program managers perspective, they'd rather keep people around who massively underperform then fire them since getting a replacement could take years. The upside is that if you want to get paid to do nothing, this is an amazing place to work!3. You dont need a clearance to get hired, although it helps, you get to do the incredibly invasive FBI-agents-knocking-on-your-moms-door clearance process after starting, and then again every 5 years for the rest of your time there. The upside is that you get to make really fun Qanon jokes with all your coworkers. Be ready for random drug tests as this is a Federal facility and Cali's pot laws have no affect. Eat a gummy at a party? kiss your career goodbye.4. If you're cool working on nuclear weapons, you're set for life. If you dont want to operate the gas chamber at Auschwitz (how I see nuke people), then your funding will perpetually be in danger, and you will likely spend more time chasing grants than writing code. (Nearly) Everyone you work with will be totally OK working on a tool with the express purpose of killing 100MM people. I had friends/co-workers who couldnt talk to their spouces about what they did during the day. The nuke people are also very enthusiastically pro-america in a creepy way that always set my teeth on edge. All diversity/inclusivity programs here forcibly killed by the GOP, which leads me to:5. Politics affects everything. Government disfunction is annoying enough, but if there's ever a fight over the federal budget or a government lockdown expect it shutdown your work as well. If you work on something politically sensitive (hello climate program!) expect your funding to be on the chopping block. Real shit, when Trump came into office my entire program changed its name to exchange ""climate"" with ""earth system science"" to try to run under the radar.6. (LLNL Specific) Livermore is very expensive (about the same as the rest of the Bay, but still pretty expensive by anyones measure) with none of the things that make the Bay nice. No BART stop and massive traffic means more than 2 hour round trip to Berkeley and back. Your Bay friends are not going to come out to visit, and in terms of travel time Sacramento is closer than SF. Unlike the rest of the Bay, its temperature is not regulated by the ocean so it regularly hits 115 degrees for weeks on end during the summer.All that said, I still (mostly) enjoyed working there. The reason I left is because during covid I moved to SoCal and after the climate program got its funding reduced the HPC/Nuke people who wanted to hire me onto their team wanted me to come back into the office (there are certain terminals for accessing classified material that are fixed in place and cant move).
 
reply







Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
Search:  


"
https://news.ycombinator.com/rss,My grandfather was almost shot down at the White House (2018),https://nones-leonard.medium.com/flying-high-8536fa403324,Comments,"Leonard and Sondra NonesFollowJan 12, 2018·4 min readFlying HighWhen I owned Piper N47943, a four seat single engine airplane, I thought it prudent not to fly myself to assignments. I did not want the stress of being the pilot and the photographer. But, when I got an assignment to go to Washington DC to simply take straight on photographs of several monuments, I thought no stress why not fly, take the pictures and fly home. It was a perfect summer day. CAVU all the way. (Cieling and Visability Unlimited). I filed a visual flight plan and we were on our way. I was very naive in my selection of airports in the DC area. My choise was Washington National, right in the middle of town, and a very busy place.The trip down was beautiful and I made a smooth landing on the 6000 foot runway. As I was taxing to my parking place, I received a radio call from ground control. The caller advised me to make a reservation for my departure. He gave me a phone number and instructed me to call one hour before we would be ready to take off. The art director and I rented a car and we drove around Washington. I took the photographs and we went to lunch.When we were ready to head back to Teterboro Airport, I got to a phone, [cell phones had not not yet been invented], and called Departure. A very busy air traffic controller spit out departure instructions. I read them back and we were on our way to the airport.As instructed, I contacted departure control when we were ready to taxi. When finally I was able to break into the frantic jumble of pilots also trying to reach departure, a hectic voice said “we have an amendment to your departure are you ready to copy”. While still in my parking space I read back the departure instructions.I was told to start my engine. The jumble of voices on the radio of pilots waiting to taxi was without interruption. Then ground control was on the radio saying “N47943 taxi to the active runway, we have an amendment to your departure, are you ready to copy”. I acknowledged my latest departure instructions and was ready to roll.Seat belts fastened, I rolled forward made a right turn and taxied to runway 18 and took my place behind a 727, a large commercial airliner. When I looked back I saw another 727 roll onto the taxiway behind me and then as I slowly rolled forward there was another one behind that one. I was about twelth for departure. The radio traffic was constant. Like Jeopardy contestents the one that was quickest on the button got heard and those guys in the big planes were really fast. Then I heard “N47943 we have an amendment to your departure”. I wrote it down on my knee pad, read it back and squeezed along between the big airliners.Finally, I was number one for departure. Just as I was making my turn onto runway 18 through all the radio chatter I heard “N47493 we have an amendment to your departure“. Without pause, the controller announced “maintain runway heading, climb to 1500 feet and then follow the river”. When I reached 1500 feet I looked down. I was crossing the river. It went right and left. I tried to break into the radio clatter, but was unsuccessful. I had to make a decision with 50–50 chance of making the right decision. I turned left.After about 45 seconds an agitated controller’s voice was on the radio. “November47943 execute immediate right turn, immediate right turn”. I made the turn and then was cleared to climb to 3000 feet, was handed off to air traffic control and headed home to sleepy Teterboro.Some two weeks later I received a phone call from the FAA in Oklahoma City. What did I think I was doing in one of the busiest airports in the country, was his first question. Then how much flying time did I have? Did I consider myself a weekend pilot? Did I think I could handle the stress at an airport like National? Then he told me the left turn was going to take me into restricted zone-357, which was directly over the White House. I listened as a large lump formed in my throat and I was having difficulty swollowing. Then came the worst of it. Jets had been scrambled and ground to air missels were aimed at ME. He said “when I made the right turn the alert was cancelled”. After berating me for some time he finally let up and said “stay away from big commercial airports” I learned that day that there is more to flying than smooth take offs and landings and straight and level flight. Airline pilots are the best at what they do and have spent years honing their craft. My hat is off to them.Aviation----More from Leonard and Sondra NonesFollowRecommended from MediumLisa TisdaleEscape to Italy: Finding a Way to HealJosé Olascoaga OrtegaAs I wander: The traveler artist in emotional troubleLes MillerThere are things that go ‘bump’ in the night.D.O.T. MarceoFeeling of The AngelsMeenakshi BharathanWater Everywhere- A Story of LossChris Lowry5 Free Things to do in White CountyToiletopsToilet Luxury, Hygiene, & Usability - A Toiletops InfographicCourtney BurryinWorld Traveler’s BlogThe Lowdown on Three Very Different Sides to JapanAboutHelpTermsPrivacyGet the Medium app"
https://news.ycombinator.com/rss,Diskless infrastructure in beta (System Transparency: stboot) (2022),https://mullvad.net/en/blog/2022/1/12/diskless-infrastructure-beta-system-transparency-stboot/,Comments,"




















Diskless infrastructure in beta (System Transparency: stboot) - Blog | Mullvad VPN



















































About
Policies
Blog
Pricing
Servers
Apps
Help


Account

Get started









Diskless infrastructure in beta (System Transparency: stboot)
12 January 2022 
        
            SYSTEM TRANSPARENCY


Diskless infrastructure using stboot (in beta) is now available on a pair of WireGuard servers in Sweden.
Today we are introducing our first VPN servers booted with our new bootloader - stboot. This marks the start of our long-running public-facing journey to make our VPN infrastructure transparent and user-auditable.
Diskless infrastructure for VPN servers
Today we announce an early beta release of a part of our System Transparency technology running on one VPN server in Gothenburg and one in Stockholm, Sweden. Both of these servers are listed in a “System Transparency [BETA]” city in our server list, viewable within our app as well as on our website.
You can find these servers by selecting: Switch Location → Sweden → System Transparency [BETA]
Make sure you are using the WireGuard protocol (applies to desktop app only).
This means that we now have two servers running entirely on RAM, without any disks in use.
What does “without any disks in use” mean?

If the computer is powered off, moved or confiscated, there is no data to retrieve.
We get the operational benefits of having fewer breakable parts. Disks are among the components that break often. Therefore, switching away from them makes our infrastructure more reliable.
The operational tasks of setting up and upgrading package versions on servers become faster and easier.
Running the system in RAM does not prevent the possibility of logging. It does however minimise the risk of accidentally storing something that can later be retrieved.

Where do you pull data from if you have no disks to store it on?
For these servers we make use of provisioning servers in order to download an “OS Package”. These provisioning servers have disks but they contain only the signed images and some base configuration data that our System Transparency (or stbooted) servers will use.
Our VPN servers launch the System Transparency bootloader (stboot) which downloads the OS package from a provisioning server and verifies that it originates from relevant Mullvad VPN staff by checking its signatures. If the OS package is valid, the OS is booted. The server then waits for an authorised member of staff to provision and deploy it for customer use.
By and large, these servers will be configured in a similar manner to our other WireGuard servers, except we use no disks, and RAM is the only location where data is kept.

Debug output stboot starting up

Debug output OS package signatures verified
What happens when the server is restarted?
At this point, the server would boot up, unaware about its past history due to using no disk. The process would be the same as in the previous step (download, verify, wait for authorisation).
In other words, we have amnesia for servers.
If this is the first of many steps, what happens next?
We get your feedback, if any, on how well it works!
We will continue to develop our provisioning and deployment process of stbooted VPN servers, starting with the ones providing WireGuard tunnels. We will start adding more servers in different locations as we get more comfortable and the projects' moving parts become more mature.
End goal: Trustworthiness through transparency
We are continuously striving to strengthen the trustworthiness of all aspects of our service. This is why our VPN apps have been open source since we started over 12 years ago. Achieving transparency on the server side is a very different challenge, as merely open sourcing our server software is not enough. We want our users to be able to verify and audit what is currently running on the VPN server they are connected to. This is our end goal with System Transparency.
Note
During this beta, WireGuard keys will be wiped on each server restart. If you are using configuration files to connect to the servers you will need to download new ones each time this happens. This does not affect the Mullvad App.
Read more

System Transparency is the future
Open Source Firmware is the future
System Transparency home page









Mullvad

About
Help
Servers
Pricing
Blog
What is privacy?
Why Mullvad VPN?
What is a VPN?
Download
Press


Jobs






Policies

Open source
Privacy policy
Cookies
Terms of service
Partnerships and resellers
Reviews, ads and affiliates
Reporting a bug or vulnerability



Address

Mullvad VPN AB
Box 53049
400 14 Gothenburg
Sweden





support@mullvad.net





GPG key








Onion service






Follow us




@mullvadnet





@mullvadnet





MullvadNet





Mullvad VPN





mullvad




Language





English








العربيّة
Dansk
Deutsch
English
Español
فارسی
Suomi
Français
Italiano
日本語
한국어
Nederlands
Norsk
Polski
Português
Русский
Svenska
ภาษาไทย
Türkçe
繁體中文
简体中文










"
https://news.ycombinator.com/rss,The Underground Cooks of Singapore’s Prisons,https://www.atlasobscura.com/articles/prison-cooking-singapore,Comments,"






















































The Underground Cooks of Singapore's Prisons - Gastro Obscura








































Want to see fewer ads? Become a Member.





















 






 


























Trips
 






Upcoming Trips


All Trips
Trips Blog








Get the Atlas Obscura book
Shop Now »







Upcoming Trips


View All Trips »







Iceland in Winter: Natural Wonders & Ancient Legends

 




Vietnam: A Culinary Adventure from Hanoi to Saigon

 




Oaxaca: Tastes of Past & Present

 




Delhi and Rajasthan: Colors of India

 








Experiences
 




Upcoming Experiences


View All Experiences »







Members only



Antiques and Their Afterlives: Osteological Specimens

 



Members only



Accidental Discoveries: A Celebration of Historical Mistakes

 



New York

Inside the Holographer's Laboratory

 



Members only



Monster of the Month w/ Colin Dickey: St. Foy

 



Members only



Accidental Discoveries: A Celebration of Historical Mistakes

 






Courses
 





Upcoming Courses


View All Courses »








Mindful Mixology: Making Non-Alcoholic Cocktails During Dry January With Derek Brown

 




Tree Communication With Annie Novak

 




 Designing Immersive Gatherings With Zach Morris of Third Rail Projects

 




Raising the Bar: Chocolate's History, Art, and Taste With Sophia Contreras Rea

 




Once Upon a Time: Fairy Tale Writing With Anca Szilágyi

 







Places
 







Top Destinations




Latest Places




Most Popular Places




Random Place




Lists




Itineraries






Add a Place









Download the App








Latest Places


View All Places »





Glasgow, Scotland
The Govan Cat

55.8633, -4.3133

 

Falmouth, Massachusetts
Rachel Carson Memorial

41.5250, -70.6770

 

Elmira, New York
Eldridge Park

42.1145, -76.8197

 

Pune, India
Baneshwar Cave Temple

18.5588, 73.7871

 




Top Destinations


View All Destinations »






Countries



Australia


Canada


China


France


Germany


India


Italy


Japan





Cities



Amsterdam


Barcelona


Beijing


Berlin


Boston


Budapest


Chicago


London


Los Angeles


Mexico City


Montreal


Moscow


New Orleans


New York City


Paris


Philadelphia


Rome


San Francisco


Seattle


Stockholm


Tokyo


Toronto


Vienna


Washington, D.C.












Foods
 





Latest Places to Eat & Drink


View All Places to Eat »






Tiny Bread Box
 


Mission Garden
 


The Stand
 


Books for Cooks
 


Herta Heuwer Currywurst Memorial
 







Stories
 






Recent Stories


All Stories


Puzzles


Video


Podcast






Most Recent Stories


View All Stories »





How a Groundbreaking Pastry Chef Bakes Outside the Lines

 

Can an Old Coin Solve the Mystery of a Lost Roman Emperor? 

 

Podcast: Songs of Ice

 

The Scientist Bringing Fresh Fish Back to Philadelphia's Underserved Neighborhoods 

 








Newsletters







Sign In


Join













Explore Newsletters











Sign In

Join









 






 



















What's near me?
 

Random Place Icon





Random Place
 




































The Underground Cooks of Singapore’s Prisons

In the 1970s and 80s, when the guards stepped away, the pots and flames came out.



by Sam Lin-Sommer
January 12, 2023




























The Underground Cooks of Singapore's Prisons




































In the 1970s and 80s, illicit cooking sessions known as ""masak"" were widespread in Singaporean prisons and rehabilitation centers. EyeEm/Alamy






In This Story












 Destination Guide

Singapore


14 Articles


57 Places









Shrimp sambal, reconstituted milk, and fried noodles bubble away in a pot, filling the air with the aroma of laksa, Singapore’s beloved noodle soup. For the cooks working in careful silence, the smell is a reminder of life outside of the prison they are stuck in, and it is hard-won: To make the dish, one man lit a flame on a candle made of his T-shirt and a melted-down food tray; another purchased the can of sambal from the commissary before scraping it open against the concrete wall; while yet another sacrificed the noodles of his paltry prison lunch.
In 1970s and 1980s Singapore, scenes like this abounded in men’s prisons and Drug Rehabilitation Centers (DRCs). Behind guards’ backs, many men smuggled ingredients to secretly cook in their barracks on makeshift stoves during kitchen jam sessions that were so common they had their own nickname: masak, which means “to cook” in Malay.
Cooks “baked” birthday cakes from melted chocolate, margarine, and soda biscuits in a mold made of magazines. Courtesy of Sheere Ng. Photos by Don Wong.
“Masak provided a space for autonomy, despite the circumstances,” explains Singaporean food writer Sheere Ng in When Cooking Was a Crime: Masak in the Singapore Prisons, 1970s-80s. In institutions that could tell a person how to dress, how to behave, and what to eat, cooking became a means of self-expression.
Ng began researching masak after hearing about the practice from a formerly incarcerated Singaporean restaurateur. She tracked down eight men who were incarcerated during the period and each of them revealed during his interview that he had participated in masak. Her reporting led her to believe that masak was widespread.
Denied access to utensils, fuel, and most ingredients, incarcerated cooks—most of whom had little prior experience—devised ingenious cooking methods. They would make bubbling stews from ingredients such as commissary anchovies, luncheon meat reserved from prison lunch, and clean toilet bowl water, heating them over makeshift fuel such as plastic bags and cut-outs of blankets that they rolled together and set aflame.
In DRCs, cooks would light cotton balls with razor blades struck against flint. Networks of bribed guards and other incarcerated men smuggled ingredients and cooking supplies out of clinics, workshops, and kitchens, often along gang networks.
To open a can purchased from the commissary, masak cooks would shave the rim off against the concrete floor before slamming it down to remove the lid. Courtesy of Sheere Ng. Photos by Don Wong.
For the freshest ingredients, men hunted and foraged on prison and rehabilitation center grounds. Many of the latter were located in jungles. Several of Ng’s interviewees described a roadside mango tree that hung over the yard of a DRC. They picked the mangoes and pickled them in salt, sugar, and water from a cleaned toilet bowl. Men in DRCs would also hunt rabbits that had wandered into the yard, and catch pigeons by scattering breadcrumbs in patches of grass laced with needles.
These exploits succeeded because prisoners were less surveilled than their modern-day counterparts. The prisons and DRCs of the 70s and 80s were scattered across the island in the former barracks of the British Army. “These facilities weren’t built for surveillance,” Ng says. Singapore, a nexus in Southeast Asia’s “Golden Triangle” of drug trafficking, began arresting huge numbers of people for drug- and gang-related charges starting in the 70s, and couldn’t hire enough prison staff to keep up. Many guards were not comfortable writing in English, the country’s official language, so many avoided checking on the captees for fear of having to write a report.
To make a sweet condiment, cooks would rinse preserved Sichuan vegetable of salt, then re-pickle it in a solution of sugar and clean water from a clean toilet bowl. Courtesy of Sheere Ng. Photos by Don Wong.
Still, as Ng’s book title attests, cooking was not allowed within prison walls. Masak cooks risked additional charges such as theft, possession of forbidden property, and destruction of prison property if they were caught. Such infractions could mean solitary confinement, and an extension of their prison sentence. To avoid being detected, cooks waited until they were left alone for the night, and made sure to only cook when lenient guards were on patrol.
For their bravery, incarcerated cooks were rewarded with a momentary escape. They found nostalgia in hot, boiled dishes that, in addition to being easy to make, reminded them of their lives outside of prison. “After that you lie down on the floor, smoke a cigarette, you [feel like] are not in prison, you know!” Ng recounts one interviewee telling her in When Cooking Was a Crime.
In addition to escape, masak offered camaraderie to men torn from the social fabric of home. Cooks brought food to newcomers who had trouble adjusting, and baked cakes made of melted chocolate, margarine, and soda biscuits to those celebrating their birthdays. Men with more money would shoulder the cost of commissary items, and those who already had served close to the maximum prison sentence of 36 months would take the blame for those with more to lose. As one former masak cook told Ng, “We won’t ge gau (“to be miserly” in Hokkien) because we are all under one banner.”
Men would lay orange peels in the sun to dry after marinating them in soy sauce. Courtesy of Sheere Ng. Photos by Don Wong.
As they dug into homemade food, men would have “heart-to-heart-talk about criminal techniques and girlfriend woes,” Ng writes. One interviewee compared the gangs that permeated the prison to impersonal country clubs, while “your masak buddies are like your golf buddies.”
Masak began to fade in the 1990s. Wardens started beating those caught cooking and barring them from working. At the same time, the government began installing closed-circuit television cameras in correctional facilities, making it difficult to cook undetected.
Today, most of the people incarcerated in Singapore are sequestered in the Changi Prison Complex, which the nonprofit Prison Insider describes as “a high-rise, high-tech and claustrophobic facility, where prisoners rarely see the sunlight.” Today, Singaporeans behind bars can no longer get away with cooking over heat, but they continue to craft dishes that do not require fire, such as pickled vegetables and cakes.
Changi Prison Complex does offer an opportunity for 30 people per year to hone their job skills in their Tearoom, a training kitchen where participants make dishes like chicken roulade and florentine blueberry vanilla cheesecake. The technique behind these dishes may be industry-ready, but it’s a far cry from the resourcefulness, communality, and daring that went into a pot of laksa cooked over a flaming cotton ball during the age of masak.
Gastro Obscura covers the world’s most wondrous food and drink.

Sign up for our email, delivered twice a week.






Read next

Capturing the Mysterious Antarctic Whale Gathering on Film


No one was sure about the size of these almost-mythical wildlife events—until we got it all on camera.








cookingprisonsfoodhistory








Want to see fewer ads?
Become a Member.










Want to see fewer ads?
Become a Member.




























Using an ad blocker?
We depend on ad revenue to craft and curate stories about the world’s hidden wonders. Consider supporting our work by becoming a member for as little as $5 a month.

Continue Using Ad-Block
Support Us 


















Keep Exploring









food

The Unsung Women of the Betty Crocker Test Kitchens


For many Crockettes, the job was glamorous, fulfilling, and ""almost subversive.""



Anne Ewbank

March 21, 2022













food

The Curious Case of Colonial India's Breakfast Curries


Even as the British Empire appropriated curry, authentic versions conquered local officials' tables.



Lily Kelting

February 16, 2022













military history

So What Is a British Biscuit Really? 


And why does it need to ""snap""?



Dan Nosowitz

March 19, 2021













libraries

Now Online: A Free Library Devoted to West Africa's Food Heritage


A new internet archive is making the region's culinary history more accessible than ever.



Vonnie Williams

February 22, 2021














Video • Gastro Obscura

The Secret to China's Bounciest Meatballs






3:47











Video • Wonder From Home

Show & Tell: Inside a House of Hot Sauce With Vic Clinco






13:16











Video • Gastro Obscura

The Beautiful World of Microscopic Food






3:04











Video • Gastro Obscura

Eating Lunch 14,000 Feet Below Sea Level






3:07











Video • AO Docs

Hawaiʻi’s Native-Language Newspaper Archive






3:35

Sponsored by Olukai
















Video • AO Docs

'Discovering' Mexico's Monarch Butterfly Migration






6:46











Video • Object of Intrigue

The Real Story Behind George Washington's Dentures






3:30











Video • Object of Intrigue

Behold, the Vampire-Killing Kit






4:19











Video

See the Mysterious Horned Helmet of Henry VIII






4:00















More Food & Drink
















Singapore

Hawker Chan Soya Sauce Chicken Rice & Noodle


This Singaporean food-court stall is home to the world's cheapest Michelin-starred meal.


















Singapore

World's First Salmon ATM


Frozen in Norway, vended in Singapore.


















Singapore

Krishna's Kitchen


Little India's hidden gem of kindness and good food.


















Vernon, Vermont

Tiny Bread Box


Every Saturday morning, this microbakery fills up with freshly baked pastries and sourdough breads.


















Tucson, Arizona

Mission Garden


A living museum of curated gardens honors 4,000 years of agriculture in the Santa Cruz River basin.


Sponsored by Visit Arizona


















Want to see fewer ads? Become a Member.


From Around the Web










THE GASTRO OBSCURA BOOK


Taste the World!
An eye-opening journey through the history, culture, and places of the culinary world.
Order Now





















































See Fewer Ads
Become an Atlas Obscura member and experience far fewer ads and no pop-ups.

Learn More









Get Our Email Newsletter











Thanks for subscribing!
View all newsletters »




 





Follow Us












Get the app









Download the App








Places 



All Places
Latest Places
Most Popular
Places to Eat
Random
Nearby
Add a Place





Editorial 



Stories
Food & Drink
Itineraries
Lists
Puzzles
Video
Podcast
Newsletters





Trips 



All Trips
Trips Blog
Art & Culture Trips
Food Trips
Hidden City Trips
History Trips
Photography Trips
Wildlife & Nature Trips
FAQ





Experiences



Experiences
Online Courses
Online Experience FAQ
Online Course FAQ





Community 



Membership
Feedback & Ideas
Community Guidelines
Product Blog
Unique Gifts
Work With Us





Company 



About
Contact Us
FAQ
Advertise With Us
Advertising Guidelines
Press
Privacy Policy
Cookie Policy
Terms of Use















© 2023 Atlas Obscura. All rights reserved.







Questions or Feedback? Contact Us





































Thanks for sharing!
Follow us on Twitter to get the latest on the world's hidden wonders.
Like us on Facebook to get the latest on the world's hidden wonders.
Follow us on Twitter
Like us on Facebook





















Want a Free Book?

Sign up for our newsletter and enter to win the second edition of our book, Atlas Obscura: An Explorer’s Guide to the World’s Hidden Wonders.




Subscribe


No Thanks
Visit AtlasObscura.com

 











Stay in Touch!
Follow us on social media to add even more wonder to your day.
Follow us on Twitter
Like us on Facebook
Follow Us on Instagram
No Thanks
Visit AtlasObscura.com





No purchase necessary. Winner will be selected at random on 02/01/2023. Offer available only in the U.S. (including Puerto Rico). Offer subject to change without notice. See contest rules for full details.

















Add Some Wonder to Your Inbox
Every weekday we compile our most wondrous stories and deliver them straight to you.






Subscribe
 


No Thanks

 













We'd Like You to Like Us
Like Atlas Obscura and get our latest and greatest stories in your Facebook feed.




No Thanks




































aoc-full-screen



aoc-heart-solid



aoc-compass



aoc-flipboard



aoc-globe



aoc-pocket



aoc-share



aoc-cancel



aoc-video



aoc-building



aoc-clock



aoc-clipboard



aoc-help



aoc-arrow-right



aoc-arrow-left



aoc-ticket



aoc-place-entry



aoc-facebook



aoc-instagram



aoc-reddit



aoc-rss



aoc-twitter



aoc-accommodation



aoc-activity-level



aoc-add-a-photo



aoc-add-box



aoc-add-shape



aoc-arrow-forward



aoc-been-here



aoc-chat-bubbles



aoc-close



aoc-expand-more



aoc-expand-less



aoc-forum-flag



aoc-group-size



aoc-heart-outline



aoc-heart-solid



aoc-home



aoc-important



aoc-knife-fork



aoc-library-books



aoc-link



aoc-list-circle-bullets



aoc-list



aoc-location-add



aoc-location



aoc-mail



aoc-map



aoc-menu



aoc-more-horizontal



aoc-my-location



aoc-near-me



aoc-notifications-alert



aoc-notifications-mentions



aoc-notifications-muted



aoc-notifications-tracking



aoc-open-in-new



aoc-pencil



aoc-person



aoc-pinned



aoc-plane-takeoff



aoc-plane



passport-plane



aoc-print



aoc-reply



aoc-search



aoc-shuffle



aoc-star



aoc-subject



aoc-trip-style



aoc-unpinned



aoc-send




aoc-phone




aoc-apps




aoc-lock




aoc-verified











"
https://news.ycombinator.com/rss,The FBI Identified a Tor User,https://www.schneier.com/blog/archives/2023/01/the-fbi-identified-a-tor-user.html,Comments,"





The FBI Identified a Tor User - Schneier on Security






































							Schneier on Security						






						Menu						

Blog
Newsletter
Books
Essays
News
Talks
Academic
About Me
 



Search

Powered by DuckDuckGo





Blog

Essays

Whole site

Subscribe



 



HomeBlog 


The FBI Identified a Tor User
No details, though:
According to the complaint against him, Al-Azhari allegedly visited a dark web site that hosts “unofficial propaganda and photographs related to ISIS” multiple times on May 14, 2019. In virtue of being a dark web site—­that is, one hosted on the Tor anonymity network—­it should have been difficult for the site owner’s or a third party to determine the real IP address of any of the site’s visitors.
Yet, that’s exactly what the FBI did. It found Al-Azhari allegedly visited the site from an IP address associated with Al-Azhari’s grandmother’s house in Riverside, California. The FBI also found what specific pages Al-Azhari visited, including a section on donating Bitcoin; another focused on military operations conducted by ISIS fighters in Iraq, Syria, and Nigeria; and another page that provided links to material from ISIS’s media arm. Without the FBI deploying some form of surveillance technique, or Al-Azhari using another method to visit the site which exposed their IP address, this should not have been possible.
There are lots of ways to de-anonymize Tor users. Someone at the NSA gave a presentation on this ten years ago. (I wrote about it for the Guardian in 2013, an essay that reads so dated in light of what we’ve learned since then.) It’s unlikely that the FBI uses the same sorts of broad surveillance techniques that the NSA does, but it’s certainly possible that the NSA did the surveillance and passed the information to the FBI.

Tags: dark web, de-anonymization, FBI, hacking, NSA, privacy, surveillance, Tor 

Posted on January 17, 2023 at 7:02 AM			•
			12 Comments 



Comments



thorvold •

					
						January 17, 2023 8:27 AM					

The filing mentions that it is referencing a purported Top Secret document “Exhibit 2” from the timeframe of 2013.  Based on that info, I am assuming this is a document purportedly from the Edward Snowden leak.  The current policy of the government is that a classified document that is leaked is still classified until officially de-classified at a later date. Public access != Unclassified.  The government is not going to acknowledge that the document is indeed classified in an open context because that would then confirm that the information contained in the document is likely true.  Potentially the “fact of” information that the lawyer obtained in that document and then references in his motion may also be classified.
This would make the motion a derivatively classified document based on the inclusion of classified information in it.  If the government managed to convince the judge that the information was still classified, then that would show the need required to seal the motion, without actually stating in open writing that the document was indeed true.






Will •

					
						January 17, 2023 9:29 AM					

The gist of the article is that the US government could have compromised the website, or the website may have been a honeypot, or they may have ways of unmasking TOR traffic generally.
But isn’t it more likely that they compromised the machine he used to access the dark web instead?






Winter •

					
						January 17, 2023 9:44 AM					

A known way to re-identify an IP address over Tor is when the user enables javascript support. If you do so, it is advised to use the browser in a VM with the IP address shielded. This is especially true when the user does not use the Tor browser, but accesses Tor using SOCKS5 on a regular browser.
If the FBI already had access to the dark web site, it could install Javascript code to get at the IP address.
Another, fairly unlikely way, is to look for searches on public fora in the open for certain websites just before the access.
A real killer would be asking for translating the offending page in open Google Translate just after you accessed it via Tor. Google can be fickle when used over Tor, it generally blocks access from Tor.






Clive Robinson •

					
						January 17, 2023 10:46 AM					

@ Bruce, ALL,
Re : The cost of catching a tiddler.
“There are lots of ways to de-anonymize Tor users.”
Yes there are[1] but non of them are “resource inexpensive”, thus as with angling,
“You only throw the bait where you’ve a good idea there is a fish that will bite.”
Thus I’d be more interested in what the suspect allegedly did to first attract attention to themselves.
After that we know it’s more likely to be “Methods” rather than “sources”.
It’s knowing if the first flagging up event was just technical or involved human agency. If the latter wether it was an error by the suspect or somebody else “provided confidential information”.
1, If technical, We all have a problem.
2, If Suspect error, Some others have a problem.
3, If Confidential source, Similar others have a problem.
The last does not overly concern me on the “If you can’t do the time…” principle I avoid doing that sort of “crime” thing.
The second I suspect is actuall quite probable on the “Johnny can’t encrypt” principle. This unfortunately is a major failing of most encryption systems going all the way to BC times.
If it’s the first then I’m very concerned because that has the implication that there is a fault in the standards, protocols or algorithms, which is likely to effect a great number of other systems not just Tor.
Hopefully we get to find out, and find out fairly soon.
[1] When you think about it at a fundemental level there are two issues,
1.1, All traffic is point to point.
1.2, All traffic is bidirectional.
Together these guarenty that all such connections are tracable, with enough resources to gather the needed information.
Due to Tors fixation with “low latency” this makes “tracking in the time domain” relatively painless and no amount of encryption no matter how clever can hide the time domain information. Nor can encryption hide the data flow domain information, all it can hide and often not well at all is the “traffic content”.






TRX •

					
						January 17, 2023 11:22 AM					

This all assumes it wasn’t something so simple as a piece of malware running that passed all his keystrokes on to an FBI host.






Winter •

					
						January 17, 2023 11:27 AM					

Another discussion of the OP at Hacker NEws:
‘https://news.ycombinator.com/item?id=34412080
Scroll down:

  I wouldn’t get so excited about this. There have been tons of javascript exploits to leak IP addresses in the past, it’s more likely that than the FBI running thousands of servers.







Winter •

					
						January 17, 2023 11:42 AM					

To counter my own arguments, it seems it is not that easy anymore to leak your IP address over Javascript.
See this discussion on Reddit:
‘https://www.reddit.com/r/TOR/comments/om5aiv/what_exactly_is_the_risk_of_running_javascript/
In the grey old days, long ago, it used to be parallel calls of web addresses in plain TCP/IP traffic inside complicated pages. But the Tor browser seems to block all know ways to do that.
Best guess to me, beside that they did not break Tor, but only use that as a cover for what they really did, is that the FBI used a zero-day in the browser of the accused. Or that he logged into a honeypot site and his credentials gave him away.






JonKnowsNothing •

					
						January 17, 2023 12:19 PM					

@All
re: It’s unlikely that the FBI uses the same sorts of broad surveillance techniques that the NSA does, but it’s certainly possible that the NSA did the surveillance and passed the information to the FBI. 
The high probability is that the FISA Courts (1) granted warrants for the overseas surveillance, which is the province of the NSA.
All such photos and documents as contained on that site are watermarked by USA LEAs (and maybe other LEAs).  This is similar to the watermarks on other “illegal materials”.  MITM insertion and interception, allows the LEAs to collect the images and replace any intercepted requests for download or browser pre-fetch (2) with a watermarked version (aka Honey Pot).  There are teams of LEAs that are dedicated to finding and tagging all such illicit images, as in moderation decisions, there are always more to discover, watermark and/or block.
Since the end user is inside the USA, that requires a special FISC Warrant, which is a boomerang warrant.  Normally the NSA stops at USA borders, but with a boomerang warrant they can follow the pipe directly inside the USA.  In theory the NSA doesn’t do this often but in practice it is done a lot. The NSA Collect It All strategy is that the collection is not “viewed by humans” and is sorted by computer, so it remains within their legal boundaries to collect.  If they are going to use any of this already collected material and if the target is inside the USA, they need a boomerang warrant to access and analyze the USA end of the pipe.  They do not need any authorization to analyze the other end of the pipe outside of the USA.
After 9/11, some aspects of this revealed the scope of investigations inside the USA and in foreign countries. The FBI, Dept of Defense (Dod), the CIA and NSA all have presence in the field.  It was a bit of a surprise when it was disclosed that the DoD was doing interrogations of renditioned/kidnapped US Citizens in foreign countries.
afaik All the agencies continue to work outside of the USA.
===
1) Foreign Intelligence Surveillance Act (FISA)
The Act created the Foreign Intelligence Surveillance Court (FISC) to oversee requests for surveillance warrants by federal law enforcement and intelligence agencies.
2) Browsers do a pre-fetech for images. All you need to do is access a page and the browser presumes you are going to look at all the images.  They pre-fetch an image to reduce the display render time as you look at the page.  The browser pings the source image host for their “counters”.  This process is done as if you visited and viewed the item even if you closed the window without ever seeing the displayed image.






AlexT •

					
						January 17, 2023 12:50 PM					

I don’t know much about the specifics of this case but is he “only” charged for visiting this site or was the de-anonymization the basis for probable cause for a search warrant that tuned up further evidence ?
If the former I guess they will have to somehow come clean about how they did it (even in a semi restricted setting): they  can’t just say “we know he did it because we tell you so”.
If the latter parallel construction comes to mind.
Certainly a story to follow.






SpaceLifeForm •

					
						January 17, 2023 1:03 PM					

Re: NSA passed info to FBI
Yes, that would be a top choice given EO 12333.
Related to this I believe:
‘https://slate.com/news-and-politics/2023/01/biden-cybersecurity-inglis-neuberger.html
A new policy will empower U.S. agencies to hack into the networks of criminals and foreign governments, among other changes.






Sam •

					
						January 17, 2023 1:45 PM					

I wonder if this is connected to KAX17.
The fact that a well-resourced and persistent group has been running hundreds of entry and middle Tor nodes since at least 2017 has been stuck in my head since the group was first noticed by Nusenu in Dec 2021.
KAX17’s significant resources, use of Azure hosting, and seemingly non-monetary motive for intercepting Tor traffic has always made me wonder if KAX17 is a three-letter agency of the US government.
https://www.schneier.com/blog/archives/2021/12/someone-is-running-lots-of-tor-relays.html






Clive Robinson •

					
						January 17, 2023 3:57 PM					

@ Bruce, ALL,
A quick lookup via search engines gives, the alledged person via an other article[1] as,
“Muhammed Momtaz Al-Azhari, of Tampa, Florida”
Searching on that pulls up a whole bunch of interesting information.
Judging from which it looks like he, his father, and sister[2] were very probably on various watch lists. He having returned to the US having spent three years in a Saudi Jail for trying to join ISIS.
Apparently he was actualy arrested in a faux weapons buy sting operation where he had been given money and talked into purchasing a hand gun and silencer.
There is a whole lot more about him looking up other information etc.
So there is apparently quite a bit more to the story.
Any way it’s 9PM and my vegi-pasta sauce is cooked, so time to go make the pasta and “slurp on the shirt” 😉
[1] From https://gizmodo.com/fbi-tor-ip-address-muhammed-momtaz-al-azhari-isis-1849975153
[2] Apparently the sister was shot dead by the action of a couple of police officers less than a couple weeks after her brothers arrest. There is apparently “overhead” video –which I’ve not seen– showing a police officer approaching her saying something and then she pulls a knife from her bag, a chase follows and she was fatally shot by another police officer.






			Subscribe to comments on this entry		


Leave a comment Cancel replyLoginName 
Email 
URL: 
 Remember personal info?


		Fill in the blank: the name of this blog is Schneier on ___________ (required):	



Comments:





Allowed HTML
	<a href=""URL""> • <em> <cite> <i> • <strong> <b> • <sub> <sup> • <ul> <ol> <li> • <blockquote> <pre>
	Markdown Extra syntax via https://michelf.ca/projects/php-markdown/extra/




 

Δ 

← Hacked Cellebrite and MSAB Software Released 
Sidebar photo of Bruce Schneier by Joe MacInnis.



About Bruce SchneierI am a public-interest technologist, working at the intersection of security, technology, and people. I've been writing about security issues on my blog since 2004, and in my monthly newsletter since 1998. I'm a fellow and lecturer at Harvard's Kennedy School, a board member of EFF, and the Chief of Security Architecture at Inrupt, Inc. This personal website expresses the opinions of none of those organizations.
Related Entries

ChatGPT-Written MalwareIdentifying People Using Cell Phone Location DataUkraine Intercepting Russian Soldiers' Cell Phone CallsTrojaned Windows Installer Targets UkraineLeaked Signing Keys Are Being Used to Sign MalwareCryWiper Data Wiper Targeting Russian Sites

Featured Essays

The Value of EncryptionData Is a Toxic Asset, So Why Not Throw It Out?How the NSA Threatens National SecurityTerrorists May Use Google Earth, But Fear Is No Reason to Ban ItIn Praise of Security TheaterRefuse to be TerrorizedThe Eternal Value of PrivacyTerrorists Don't Do Movie Plots 
More EssaysBlog Archives

Archive by Month100 Latest Comments
Blog Tags3d printers9/11A Hacker's MindAaron Swartzacademicacademic papersaccountabilityACLUactivismAdobeadvanced persistent threatsadwareAESAfghanistanair marshalsair travelairgapsal QaedaalarmsalgorithmsalibisAmazonAndroidanonymityAnonymousantivirusApacheAppleApplied Cryptographyartificial intelligenceMore TagsLatest BookMore Books




 




Blog
Newsletter
Books
Essays
News
Talks
Academic
About Me
 

















"
