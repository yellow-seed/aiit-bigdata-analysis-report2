url,title,link,summary,content
https://news.ycombinator.com/rss,I'm Shadow Banned by DuckDuckGo (and Bing),https://daverupert.com/2023/01/shadow-banned-by-duckduckgo-and-bing/,Comments,"




I'm Shadow Banned by DuckDuckGo (and Bing)


January 14, 2023




It came to my attention that my site does not appear on DuckDuckGo search results. Even when searching for “daverupert.com” directly. After some digging, DuckDuckGo used to get their site index from Yandex, but now gets their site index from Bing and sure enough… I didn’t appear on Bing either.
First of all… rude. I’m the one person I know who actually uses Bing and I started using DuckDuckGo on my Mac… and they have the audacity —nay, the cowardice!— to shadow ban me and my contributions to the Web!? I —a southern gentleman— take the highest offense at this slighting and misconstruing of my character. I do declare.
SEO isn’t one of my top objectives with this site, so initially I dismissed it. But that nerdsnipe shot a signal flare up in my brain that spun into mystery I needed to solve. I mean… there can be money from blogging. Surely I’ve built some clout for my blog over the years… right?

I have been blogging, almost weekly, for over a decade…
I co-host a somewhat successful web development podcast…
I’ve been back linked from popular blogs like CSS-Tricks…
I’ve been on hacker news a handful of times…
And probably most important… I show up on Google!

Why on earth would Bing not index my site at all? To solve this, I took the first step and signed up for Bing Webmaster Tools to try to know what Bing knows about my site and sure enough: zero clicks, zero impressions, and zero indexed pages for my site. Awful.

The one clue I have to go off are some “Errors” according to Bing’s Crawler. 100% of those errors are “missing meta description”. That doesn’t seem like an SEO dealbreaker to me (I get a 91 on Lighthouse SEO), but does Bing super care about meta descriptions? Doesn’t seem like I should have 0 out of 418 pages in my sitemap.xml though.
One “out there” reason I can think is that I use Amazon Affiliate links on my Bookshelf and my /Uses page and that triggers a shadow ban? I could see how that appears spammy and I question the ethics of Amazon links sometimes myself, but what would I do without those $ones of dollars that I make each year!? I keep it around as a money carrot incentive to motivate me to update the bookshelf, but perhaps it’s time I retire that monetization avenue.
Anyways, a mystery is afoot… let the investigation begin! I will post a follow up if I ever solve this.



"
https://news.ycombinator.com/rss,SLT – A Common Lisp Language Plugin for Jetbrains IDE Lineup,https://github.com/Enerccio/SLT,Comments,"








Enerccio

/

SLT

Public




 

Notifications



 

Fork
    0




 


          Star
 26
  









        SLT is an IDE Plugin for Itellij/Jetbrains IDE lineup implementing support for Common Lisp via SBCL and Slime/Swank 
      
License





     Apache-2.0 license
    






26
          stars
 



0
          forks
 



 


          Star

  





 

Notifications












Code







Issues
4






Pull requests
0






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







Enerccio/SLT









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











master





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








3
branches





1
tag







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit









Peter Vanusanik

fixed wrong import on StringUtils, tested building distributions




        …
      




        4681185
      

Jan 15, 2023





fixed wrong import on StringUtils, tested building distributions


4681185



Git stats







19

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.run



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









gradle/wrapper



init



Jan 6, 2023









src



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









.gitignore



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









CodeOfConduct.md



rename file



Jan 14, 2023









LICENSE.txt



readme and such



Jan 14, 2023









README.md



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









build-distributions.sh



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









build.gradle.kts



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









gradle.properties



fixed wrong import on StringUtils, tested building distributions



Jan 14, 2023









gradlew



init



Jan 6, 2023









gradlew.bat



init



Jan 6, 2023









settings.gradle.kts



init



Jan 6, 2023




    View code
 















SLT - A Common Lisp Language Plugin for Jetbrains IDE lineup
Requirements
Getting started
Compiling source
Planned features / goals
License





README.md




SLT - A Common Lisp Language Plugin for Jetbrains IDE lineup



THIS PLUGIN IS EXPERIMENTAL and can crash at any time! Please report all bugs!
This plugin is providing support for Common Lisp for JetBrains IDEs.
Using modified SLIME/Swank protocol to commmunicate with SBCL providing
IDE capabilities for Common Lisp.

Requirements

Intellij based IDE - tested on Intellij Idea Community/Ultimate but should workd on all major IDEs
Steel Bank Common Lisp installed
Quicklisp

Getting started
Download plugin for your IDE from releases and install it via file.
To find out which release applies to you check this table:



Jetbrains IDE Variant
Plugin name pattern




CLion
slt-version-signed-CL.zip


GoLand
slt-version-signed-GO.zip


Intellij Community
slt-version-signed-IC.zip


Intellij Ultimate
slt-version-signed-IU.zip


PyCharm
slt-version-signed-PY.zip


PyCharm Community
slt-version-signed-PC.zip


Rider
slt-version-signed-RD.zip



PhpStorm is coming when I read how to build it correctly since just swapping
the type does not work.
Compiling source
Clone the repository and change gradle.properties for your IDE.
Then use gradle to build the plugin.
You can also open this as a project in Intellij Idea.
Planned features / goals

 Upload to marketplace when it has enough features
 REPL
 Interactive debugging
 Walkable debugger without actions
 Breakpoints
 Documentation
 Macro expand in documentation
 Find function by symbol name
 Search for symbols
 Back references
 Refactoring
 List of quicklisp installed packages / ASDF packages
 List of modified top level forms that are yet to be evaluated

License
This project is licensed under Apache License v2.









About

      SLT is an IDE Plugin for Itellij/Jetbrains IDE lineup implementing support for Common Lisp via SBCL and Slime/Swank 
    
Topics



  lisp


  integrated-development-environment


  jetbrains


  common-lisp


  sbcl


  intellij-plugin


  jetbrains-plugin



Resources





      Readme
 
License





     Apache-2.0 license
    



Stars





26
    stars

Watchers





3
    watching

Forks





0
    forks







    Releases





1
tags







    Packages 0


        No packages published 











Languages













Java
91.4%







Common Lisp
4.4%







Lex
3.3%







Other
0.9%











"
https://news.ycombinator.com/rss,Ask HN: How do you trust that your personal machine is not compromised?,https://news.ycombinator.com/item?id=34388866,Comments,"

Ask HN: How do you trust that your personal machine is not compromised? | Hacker News

Hacker News
new | past | comments | ask | show | jobs | submit 
login




 Ask HN: How do you trust that your personal machine is not compromised?
44 points by coderatlarge 1 hour ago  | hide | past | favorite | 29 comments 

""Compromised"" meaning that malware hasn't been installed or that it's not being accessed by malicious third parties.  This could be at the BIOS, firmware, OS, app or any other other level. 
 
  
 
gnfargbl 31 minutes ago  
             | next [–] 

Here's a short, fairly practical guide that you might find helpful: https://www.ncsc.gov.uk/files/Cyber-Essentials-Requirements-.... It is aimed mostly at small businesses, but I find a lot of the guidance to be pretty relevant to my personal IT.My even shorter (and incomplete) summary of the document would be: configure your router and firewall; remove default passwords and crapware from your devices; use a lock screen; don't run as root; use a password manager and decent passwords; enable 2FA everywhere you can; enable anti-malware if your OS has it built it; don't run software from untrusted sources; patch regularly.There are also other controls that you can choose to impose on yourself. For example, I require full-disk encryption, and I will only use mobile devices which get regular updates. Would be interested in hearing other things that HN'ers do to limit risk.
 
reply



  
 
amelius 16 minutes ago  
             | parent | next [–] 

Do you lock your computer every time you leave your desk?And do you always check for keylogger thumbdrives and such?
 
reply



  
 
Semaphor 5 minutes ago  
             | root | parent | next [–] 

For me: No, and no. But as that would require someone breaking into my apartment, I don’t worry too much.
 
reply



  
 
k3liutZu 5 minutes ago  
             | root | parent | prev | next [–] 

Yes (I don't bother when I'm working from home)I haven't used a use usb stick in +10 years.
 
reply



  
 
greggyb 14 minutes ago  
             | root | parent | prev | next [–] 

Yes. Why wouldn't you?
 
reply



  
 
NikolaNovak 22 minutes ago  
             | prev | next [–] 

Great question. I don't anymore. Decades ago when I had a 286 and knew what each file did and what all the software was, and threats were limited and crude, I had good confidence of controlling my machine. Today, when my laptop has millions of files and each website - even hacker news - could inject something malicious and my surface is so broad (browsers applications extensions libraries everything) and virtually anything I do involves network connections... I just don't have the confidence.FWIW, I try to segregate my machines for different categories of behaviour - this laptop is for work, this one is for photos and personal documents, this one is for porn, this one is if I want to try something. But even still my trust in e. G. software vlan on my router and access controls on my NAS etc are limited in this day and age.I feel today it's not about striving for zero risk (for 99.99 of people) , but picking the ratio of overhead and risk you're ok with. And backups. (bonus question - how to make backups safe in age of encrypting ransom ware).
 
reply



  
 
jl6 9 minutes ago  
             | parent | next [–] 

On the backup question, this is one reason why I have a set of backups that are physically disconnected and not automated.
 
reply



  
 
h2odragon 42 minutes ago  
             | prev | next [–] 

You really can't, anymore. You can watch traffic and hope that anything nasty isn't communicating with the outside world, but then there's all sorts of side channels that you may not know to watch.At some point you just have to admit there's limits to privacy and work with them. You paper journal could be stolen and read / rewritten too, yaknow? It's not a new problem, its just in a new context.
 
reply



  
 
ramraj07 3 minutes ago  
             | prev | next [–] 

There are two levels here: compromised by some national agency vs. compromised by anyone else.For the former, I don’t assume anything especially since I’m not an American citizen. I still believe with some certainty that my iPhone is safe from the government but not 100%
 
reply



  
 
adriancr 25 minutes ago  
             | prev | next [–] 

Just some generic things that should help avoid or clean up after a compromise.- clean reinstall every month, just pick a new flavor of Linux to try out. (also helps ensure I have proper backups and scripts for setting up environment)- Dev work I usually do in docker containers, easy to set up/nuke environments.- Open source router with open source bios (apu2), firewall on it, usually reinstall once in a while.- Spin up VMs via scripts for anything else. (games - windows VM with passthrough GPU for example)- automatic updates everywhere.
 
reply



  
 
867-5309 22 minutes ago  
             | parent | next [–] 

>Spin up VMs via scripts for .. gamesthis is not sustainable. you do this once and then pray nothing breaks!
 
reply



  
 
adriancr 20 minutes ago  
             | root | parent | next [–] 

I just have a clone of a clean windows VM.If something breaks or I get bored, nuke the active one and start clone, update it and make another backup, then reinstall games again.On the other hand, gpu pass-through breaks once in a while and is annoying to fix.
 
reply



  
 
albntomat0 6 minutes ago  
             | prev | next [–] 

The biggest thing is being deliberate about your threat model.  Who would want to get onto your systems, and how much do they care about you in particular?From there, take appropriate actions.  For the vast, vast majority of us, that means using good passwords, updating software, and not running weird things from the internet.If you’re worried about 0 click RCE in Chrome/Windows/iOS, you either should be getting better advice from folks outside of HN, or are being unrealistic about who is coming after you.
 
reply



  
 
lifthrasiir 36 minutes ago  
             | prev | next [–] 

I'm reasonably sure that my personal machine is less compromised than the average, but I can't and will never be able to ensure that it is not compromised because I have no way to know everything the machine trying to do. This remains true even when you have an entirely free and directly inspectable hardware; you simply have no knowledge and time to verify everything. Just keep a reasonable amount of precaution and skepticism.
 
reply



  
 
jl6 30 minutes ago  
             | prev | next [–] 

I don’t have ultimate trust in any software or hardware, but I get to “good enough” by deciding which providers I trust:* Software: Canonical, Google, Microsoft, Valve, Oracle, Dropbox. I install software from their official repos. Anything 3rd-party/unofficial/experimental/GitHub goes in a VM.* Hardware: I built my main PC from mainstream commodity components. I have no way of knowing if there are secret backdoors but I consider it unlikely.I’m also privileged enough to not be a “person of interest” so don’t feel the need to take any extraordinary precautions.Yes, I’m aware of VM escapes. Yes, I’ve read Reflections on Trusting Trust. I choose to trust regardless because life’s too short for paranoia. As Frank Drebin said:“You take a chance getting up in the morning, crossing the street, or sticking your face in a fan.”
 
reply



  
 
rhn_mk1 17 minutes ago  
             | parent | next [–] 

What about publicly known backdoors in your hardware?https://www.techrepublic.com/article/is-the-intel-management...There is hardware that doesn't contain those at least, but it doesn't break power records.
 
reply



  
 
codetrotter 38 minutes ago  
             | prev | next [–] 

Noone has drained my crypto from my wallets yet.So either my personal machine is not compromised, or they think the amount of crypto in the wallets is too low.Jokes on them though, cause I am moving my crypto to a hardware wallet eventually
 
reply



  
 
progval 14 minutes ago  
             | parent | next [–] 

Joke's on you, you just told them they should hurry up before you do ;)
 
reply



  
 
tluyben2 31 minutes ago  
             | parent | prev | next [–] 

Quite an interesting honeypot really.
 
reply



  
 
mimimi31 23 minutes ago  
             | root | parent | next [–] 

More like a canary I think.
 
reply



  
 
adg001 25 minutes ago  
             | prev | next [–] 

The reality is that you cannot trust that your machines are not compromised.The only option we are left with is to operate under the assumption that, indeed, our machines are permanently compromised.
 
reply



  
 
rekrsiv 4 minutes ago  
             | prev | next [–] 

You don't. Treat your personal machine(s) as compromised by default and take it from there.
 
reply



  
 
PaulHoule 35 minutes ago  
             | prev | next [–] 

Reminds me of the time I was watching a creepypasta horror movie about some guy who gets strange phone calls and my phone rang.I think this guy had gotten my phone number from my HN profile and he thought I might be able to help him.  He thought his android phone was infected by malware and he knew who did it.  I told him the people who repair cell phones at the mall could do a system reset on his phone…. Unless he was dealing with state-level actors in which case it might be an advanced persistent threat and it might be permanent.
 
reply



  
 
treebeard901 43 minutes ago  
             | prev | next [–] 

You should assume all devices are compromised
 
reply



  
 
wadayano 34 minutes ago  
             | parent | next [–] 

*compromisable
 
reply



  
 
baobabKoodaa 40 minutes ago  
             | parent | prev | next [–] 

Not helpful
 
reply



  
 
twaw 14 minutes ago  
             | root | parent | next [–] 

Why not? It still possible to communicate securely using compromised devices and networks.
 
reply



  
 
cube2222 11 minutes ago  
             | prev | next [–] 

I try to follow what others already mentioned, but still, for any personal high-security stuff I use a device whose OS  puts strong limits on apps, like an iPad.
 
reply



  
 
crims0n 22 minutes ago  
             | prev [–] 

Keep it air gapped, only way to be sure!Only half kidding, unfortunately.
 
reply







Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
Search:  


"
https://news.ycombinator.com/rss,Ubuntu 22.04 LTS servers and phased apt updates,https://utcc.utoronto.ca/~cks/space/blog/linux/Ubuntu2204ServerPhasedUpdates,Comments,"
 
 Chris's Wiki :: blog/linux/Ubuntu2204ServerPhasedUpdates 






Chris Siebenmann ::
CSpace »
       blog »
       linux »
       Ubuntu2204ServerPhasedUpdates
Welcome, guest.




Ubuntu 22.04 LTS servers and phased apt updates
January 13, 2023

I was working on getting one of our 22.04 LTS servers up to date,
even for packages we normally hold, when I hit a mystery and
posted about it on the Fediverse:
Why does apt on this 22.04 Ubuntu machine want to hold back a bunch of
package updates even with '--with-new-pkgs --ignore-hold'? Who knows,
it won't tell me why it doesn't like any or all of:
open-vm-tools openssh-client openssh-server openssh-sftp-server
osinfo-db python3-software-properties software-properties-common
(Apt is not my favorite package manager for many reasons, this among
them.)

Steve suggested that it was Ubuntu's ""Phased Update"" system, which is what it turned
out to be. This set me off to do some investigations, and it turns
out that phased (apt) updates explain some other anomalies we've
seen with package updates on our Ubuntu 22.04 machines.
The basic idea of phased updates is explained in the ""Phasing""
section of Ubuntu's page on Stable Release Updates (SRUs); it's a
progressive rollout of the package to more and more of the system
base. Ubuntu introduced phased updates in 2013 (cf) but initially they weren't
directly supported by apt, only by the desktop upgrade programs.
Ubuntu 21.04 added apt support for phased updates and
Ubuntu 22.04 LTS is thus the first LTS version to subject servers
to phased updates. More explanations of phased updates are in this
askubuntu answer, which includes
one way to work around them.
(Note that as far as I know and have seen, security updates are not
released as phased updates; if it's a security update, everyone
gets it right away. Phased updates are only used for regular,
non-security updates.)
Unfortunately apt (or apt-get) won't tell you if an update is being
held back because of phasing. This user-hostile apt issue is tracked
in Ubuntu bug #1988819 and
you should add yourself as someone it affects if this is relevant
to you. Ubuntu has a web page on what updates are currently in
phased release,
although packages are removed from this page once they reach 100%.
Having reached 100%, such a package is no longer a phased update,
which will become relevant soon. If you can't see a reason for a
package to be held back, it's probably a phased update but you can
check the page
to be sure.
(As covered in the ""Phasing"" section, packages
normally move forward through the phased rollout every six hours,
so you can have a package held back on some server in the morning
and then be not-held in the afternoon. This is great fun for
troubleshooting why a given server didn't get a particular update.)
Your place in a phased update is randomized across both different
servers and different packages. If you have a fleet of servers,
they will get each phased update at different times, and the order
won't be consistent from package to package. This explains an anomaly
we've been seeing in our package updates for some time, where
different 22.04 servers would get updates at different times without
any consistent pattern.
The phased update related apt settings available and some of the
technical details are mostly explained in this askubuntu answer. If you want to opt out of phased
updates entirely, you have two options; you can have your servers
install all phased updates right away (basically putting you at the
0% start line), or you can skip all phased updates and only install
such packages when they reach 100% and stop being considered phased
updates at all. Unfortunately, as of 22.04 there's no explicit
option to set your servers to have a particular order within all
updates (so that you can have, for example, a 'canary' server that
always installs updates at 0% or 10%, ahead of the rest of the
fleet).
For any given package update, machines are randomized based on the
contents of /etc/machine-id, which
can be overridden for apt by setting APT::Machine-ID to a 32 hex
digit value of your choice (the current version of apt appears to
only use the machine ID for phased updates).  If you set this to
the same value across your fleet, your fleet will update in sync
(although not at a predictable point in the phase process); you can
also set subsets of your fleet to different shared values so that
the groups will update at different times.  The assignment of a
particular machine to a point in the phased rollout is done through
a relatively straightforward approach; the package name, version,
and machine ID are all combined into a seed for a random number
generator, and then the random number generator is used to produce
a 0 to 100 value, which is your position in the phased rollout. The
inclusion of the package name and version means that a given machine
ID will be at different positions in the phased update for different
packages. All of this turns out to be officially documented in the
""Phased Updates"" section of apt_preferences(5),
although not in much detail.
(There is a somewhat different mechanism for desktop updates, covered
in the previously mentioned askubuntu answer.)
As far as I can see from looking at the current apt source code, apt doesn't log anything
at any verbosity if it holds a package back because the package is
a phased update and your machine doesn't qualify for it yet. The
fact that a package was a phased update the last time apt looked
may possibly be recorded in /var/log/apt/eipp.log.xz, but documentation
on this file is sparse.
Now that I've looked at all of this and read about APT::Machine-ID,
we'll probably set it to a single value across all of our fleet
because we find different machines getting updates at different
times to be confusing and annoying (and it potentially complicates
troubleshooting problems that are reported to us, since we normally
assume that all 22.04 machines have the same version of things like
OpenSSH). If we could directly control the position within a phased
rollout we'd probably set up some canary machines, but since we
can't I don't think there's a strong reason to have more than one
machine-id group of machines.
(We could set some very important machines to only get updates when
packages reach 100% and stop being phased updates, but Ubuntu has
a good record of not blowing things up with eg OpenSSH updates.)

(4 comments.)
Written on 13 January 2023. 

     «   A browser tweak for system administrators doing (web) network debugging    
    Your server BMCs can need to be rebooted every so often   »     



 These are my WanderingThoughts 
(About the blog)
Full index of entries 
Recent comments
This is part of CSpace, and is written by ChrisSiebenmann. 
Mastodon: @cks 
Twitter: @thatcks
* * *
Categories: links, linux, programming, python, snark, solaris, spam, sysadmin, tech, unix, web 
Also: (Sub)topics
This is a DWiki. 
GettingAround 
(Help)
 
 Search:  



 Page tools: View Source, Add Comment. 

Search: 

Login: 
Password: 


 

Atom Syndication: Recent Comments.
 Last modified: Fri Jan 13 22:56:18 2023 
This dinky wiki is brought to you by the Insane Hackers
Guild, Python sub-branch.


"
https://news.ycombinator.com/rss,Single-file scripts that download their dependencies,https://dbohdan.com/scripts-with-dependencies,Comments,"



Single-file scripts that download their dependencies · DBohdan.com











Toggle navigation




dbohdan



Home





 




Homescripts-with-dependencies





Single-file scripts that download their dependencies
An ideal distributable script is fully contained in a single file. It runs on any compatible operating system with an appropriate language runtime. It is plain text, and you can copy and paste it. It does not require mucking about with a package manager, or several, to run. It does not conflict with other scripts’ packages or require managing a project environment to avoid such conflicts.
The classic way to get around all of these issues with scripts is to limit yourself to using the scripting language’s standard library. However, programmers writing scripts don’t want to; they want to use libraries that do not come with the language by default. Some scripting languages, runtimes, and environments resolve this conflict by offering a means to download and cache a script’s dependencies with just declarations in the script itself. This page lists such languages, runtimes, and environments. If you know more, drop me a line.
Contents



Anything with a Nix package


D


Groovy


JavaScript (Deno)


Kotlin (kscript)


Racket (Scripty)


Scala (Ammonite)



Anything with a Nix package
The Nix package manager can act as a #! interpreter and start another program with a list of dependencies available to it.
#! /usr/bin/env nix-shell
#! nix-shell -i python3 -p python3
print(""Hello, world!"".rjust(20, ""-""))
D
D’s official package manager DUB supports single-file packages.
#! /usr/bin/env dub
/+ dub.sdl:
name ""foo""
+/
import std.range : padLeft;
import std.stdio : writeln;
void main() {
    writeln(padLeft(""Hello, world!"", '-', 20));
}
Groovy
Groovy comes with an embedded JAR dependency manager.
#! /usr/bin/env groovy
@Grab(group='org.apache.commons', module='commons-lang3', version='3.12.0')
import org.apache.commons.lang3.StringUtils
println StringUtils.leftPad('Hello, world!', 20, '-')
JavaScript (Deno)
Deno downloads dependencies like a browser. Deno 1.28 and later can also import from NPM packages. Current versions of Deno require you to pass a run argument to deno. One way to accomplish this from a script is with a form of “exec magic”. Here the magic is modified from a comment by Rafał Pocztarski.
#! /bin/sh
"":"" //#; exec /usr/bin/env deno run ""$0"" ""$@""
import leftPad from ""npm:left-pad"";
console.log(leftPad(""Hello, world!"", 20, ""-""));
On Linux systems with recent GNU env(1) and on FreeBSD you can replace the magic with env -S.
#! /usr/bin/env -S deno run
import leftPad from ""npm:left-pad"";
console.log(leftPad(""Hello, world!"", 20, ""-""));
Kotlin (kscript)
kscript is an unofficial scripting tool for Kotlin that understands several comment-based directives, including one for dependencies.
#! /usr/bin/env kscript
//DEPS org.apache.commons:commons-lang3:3.12.0
import org.apache.commons.lang3.StringUtils
println(StringUtils.leftPad(""Hello, world!"", 20, ""-""))
Racket (Scripty)
Scripty interactively prompts you to install the missing dependencies for a script in any Racket language.
#! /usr/bin/env racket
#lang scripty
#:dependencies '(""base"" ""typed-racket-lib"" ""left-pad"")
------------------------------------------
#lang typed/racket/base
(require left-pad/typed)
(displayln (left-pad ""Hello, world!"" 20 ""-""))
Scala (Ammonite)
The scripting environment in Ammonite lets you import Ivy dependencies.
#! /usr/bin/env amm
import $ivy.`org.apache.commons:commons-lang3:3.12.0`,
  org.apache.commons.lang3.StringUtils
println(StringUtils.leftPad(""Hello, world!"", 20, ""-""))

Tags: list, programming.
 
 
 
 


Copyright 2013–2022 D. Bohdan.




 


"
https://news.ycombinator.com/rss,"The Fourier Transform, explained in one sentence",https://blog.revolutionanalytics.com/2014/01/the-fourier-transform-explained-in-one-sentence.html,Comments,"


































The Fourier Transform, explained in one sentence (Revolutions)













Revolutions

			Milestones in AI, Machine Learning, Data Science, and visualization with R and Python since 2008
		









« Forecasting By Combining Expert Opinion |
	Main
	| Predictive Models in R Clustered By Tag Similarity »



January 03, 2014


The Fourier Transform, explained in one sentence


If, like me, you struggled to understand the Fourier Transformation when you first learned about it, this succinct one-sentence colour-coded explanation from Stuart Riffle probably comes several years too late:

Stuart provides a more detailed explanation here. This is the formula for the Discrete Fourier Transform, which converts sampled signals (like a digital sound recording) into the frequency domain (what tones are represented in the sound, and at what energies?). It's the mathematical engine behind a lot of the technology you use today, including mp3 files, file compression, and even how your old AM radio stays in tune.
The daunting formula involves imaginary numbers and complex summations, but Stuart's idea is simple. Imagine an enormous speaker, mounted on a pole, playing a repeating sound. The speaker is so large, you can see the cone move back and forth with the sound. Mark a point on the cone, and now rotate the pole. Trace the point from an above-ground view, if the resulting squiggly curve is off-center, then there is frequency corresponding the pole's rotational frequency is represented in the sound. This animated illustration (click to see it in action) illustrates the process:

The upper signal is make up of three frequencies (""notes""), but only the bottom-right squiggle is generated by a rotational frequency matching one of the component frequencies of the signal.
By the way, no-one uses that formula to actually calculate the Discrete Fourier Transform — use the Fast Fourier Transform instead, as implemented by the fft function in R. As the name suggests, it's much faster.
AltDevBlog: Understanding the Fourier Transform (note: updated link 20 Oct 2015 with active mirror)





Posted by David Smith at 13:30 in R, random  | Permalink











Comments

 You can follow this conversation by subscribing to the comment feed for this post.





Very interesting article, thank you. Please take a moment to rephrase the following key statement, if you would: ""...then there is frequency corresponding the pole's rotational frequency is represented in the sound.""


		Posted by:
		C. Griffith |
		January 04, 2014 at 10:09




polar form e^iθ is equal to the rectangular form cosθ+isinθ and corresponds to the coordinates (cosθ,sinθ) such that 
e^i0    =  1 = (1,0)
e^iτ/4  =  i = (0,1)
e^iτ/2  = -1 = (-1,0)
e^iτ3/4 = -i = (0,-1)
e^iτ    =  1 = (1,0)


		Posted by:
		MasterG |
		January 04, 2014 at 16:33




May I suggest a  minor exception to your claim about FFT: most modern languages, R included, use some variation of the ""pure"" 2^N Cooley-Tukey FFT algorithm as appropriate to support factors of 3, 5, etc. in the length of the dataset, and even default to the ""raw"" DFT for other data lengths (unless specifically suppressed by the user).   
And, of course, the FFT is in fact that equation, just with gobs of like terms grouped together. :-)


		Posted by:
		Carl Witthoft |
		January 06, 2014 at 08:40











	The comments to this entry are closed.







Information


About this blog
Comments Policy
About Categories
About the Authors
Local R User Group Directory
Tips on Starting an R User Group





Search Revolutions Blog






















Got comments or suggestions for the blog editor? 
Email David Smith.
    




 Follow David on Twitter: @revodavid





Get this blog via email with 




Categories


academia (41)
advanced tips (218)
AI (62)
airoundups (20)
announcements (201)
applications (288)
beginner tips (106)
big data (272)
courses (60)
current events (126)
data science (227)
developer tips (90)
events (280)
finance (126)
government (25)
graphics (378)
high-performance computing (115)
life sciences (35)
Microsoft (314)
mlops (4)
open source (78)
other industry (58)
packages (388)
popularity (54)
predictive analytics (163)
profiles (15)
python (69)
R (2442)
R is Hot (8)
random (464)
reviews (22)
Revolution (422)
Rmedia (136)
roundups (121)
sports (55)
statistics (297)
user groups (127)


See More




R links


R on AzureDeveloper's guide and documentation
Find R packagesCRAN package directory at MRAN
Download Microsoft R OpenFree, high-performance R
R Project siteInformation about the R project






Recommended Sites


@RLangTipDaily tips on using R
FlowingDataModern data visualization
Probability and statistics blogMonte Carlo simulations in R
R BloggersDaily news and tutorials about R, contributed by R bloggers worldwide.
R Project group on analyticbridge.comCommunity and discussion forum
Statistical Modeling, Causal Inference, and Social ScienceAndrew Gelman's statistics blog






Archives


January 2023
August 2022
September 2021
July 2021
June 2021
April 2021
March 2021
February 2021
January 2021
December 2020





 Subscribe to this blog's feed




​
    













 








"
https://news.ycombinator.com/rss,Go FOSS: Information is power,https://gofoss.net/,Comments,"          gofoss.net      Home      Get started      Get started     Protect your digital freedom         Browse privately      Browse privately     Free your browser     Firefox     Tor Browser     VPN         Speak freely      Speak freely     Keep conversations private     Encrypted messages     Encrypted emails         Store safely      Store safely     Secure your data     Safe passwords     Backups     Encrypted files         Stay mobile & free      Stay mobile & free     Free your phone     FOSS apps     CalyxOS     LineageOS for microG         Unlock your computer      Unlock your computer     Free your computer     Ubuntu     Ubuntu apps         Own your cloud      Own your cloud     Free your cloud     Fediverse     Alternative cloud providers     Server hosting     Basic server security     Advanced server security     Secure access     Cloud storage     Photo gallery     Contacts, calendars & tasks     Media streaming     Server backups         About      About     Big Tech threatens privacy     The project     The team     Thanks     Contributing     Roadmap     Disclaimer         Origins      Origins     Hackers, 1984     The GNU Manifesto, 1985     The Techno-Revolution, 1986     The Conscience of a Hacker, 1986     The Crypto Anarchist Manifesto, 1988     A Cypherpunk's Manifesto, 1993     A Declaration of the Independence of Cyberspace, 1996     A Cyberpunk Manifesto, 1997     The Cathedral & The Bazaar, 1999     The dotCommunist Manifesto, 2003     A Hacker Manifesto, 2004     The Maker's Bill of Rights, 2006     Guerilla Open Access Manifesto, 2008     The Apache Way, 2009     Repair Manifesto, 2009     The Cult of Done Manifesto, 2009     Self-Repair Manifesto, 2010     The Hardware Hacker Manifesto, 2010     An Anonymous Manifesto, 2011     The Declaration of the Independence of the people of the Internet, 2012                          Back to top  "
https://news.ycombinator.com/rss,Constrain – Interactive figures using declarative constraint solving,https://github.com/andrewcmyers/constrain,Comments,"








andrewcmyers

/

constrain

Public




 

Notifications



 

Fork
    2




 


          Star
 50
  









        Responsive, animated figures in JavaScript/HTML canvases
      





andrewcmyers.github.io/constrain







50
          stars
 



2
          forks
 



 


          Star

  





 

Notifications












Code







Issues
18






Pull requests
0






Discussions







Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Discussions
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







andrewcmyers/constrain









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











master





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








7
branches





6
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




andrewcmyers

Fix backprop for Smooth/Linear.




        …
      




        130cac8
      

Jan 15, 2023





Fix backprop for Smooth/Linear.

Remove unnecessary tree constraints.
Add necessary constraints to example.

130cac8



Git stats







516

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








doc



doc improvement



Jan 13, 2023









examples



Fix backprop for Smooth/Linear.



Jan 15, 2023









images



Update image



Jun 27, 2022









reveal.js @ 9430a98



latest



Jul 25, 2019









tests



un/refix test



Jan 15, 2023









.gitignore



Reorg/clean up repo



Dec 25, 2020









.gitmodules



change to my version of Reveal



Jul 18, 2019









README.md



tree example



Jan 13, 2023









constrain-graph.js



argument checking



May 11, 2022









constrain-mathjax.js



formatting



Jul 27, 2022









constrain-pdf.js



Monkey-patch broken jsPDF calls



Apr 9, 2022









constrain-ps.js



Implement missing methods for PS rendering



Apr 9, 2022









constrain-reveal.js



Clean up comment



Apr 19, 2021









constrain-slide.css



More refactoring of Reveal mods



Aug 21, 2019









constrain-trees.js



Fix backprop for Smooth/Linear.



Jan 15, 2023









constrain.js



Fix backprop for Smooth/Linear.



Jan 15, 2023









numeric-1.2.6.js



execute bit should not be on



Jul 22, 2019




    View code
 















Constrain - a JS (ES6) library for animated, interactive figures, based on declarative constraint solving
Demos
Requirements





README.md




Constrain - a JS (ES6) library for animated, interactive figures, based on declarative constraint solving


Responsive, animated figures embedded in web pages
Figures implemented declaratively with time-dependent constraints on graphical objects
Integrates with Reveal.js presentations
GitHub repository
Reference manual
A short talk about Constrain

Demos
Using constraints to compute the Golden Ratio (Drag the diamond!)
A short talk about Constrain, using Reveal
Cornell University course notes using Constrain for embedded figures: CS 2112,
CS 4120/lexer generation,
CS 4120/bottom-up parsing
Interactive Pythagorean Theorem
Interactively computing centers of a triangle
TeX-style text formatting
Simple template page for using Constrain
Animated trees
Requirements

ES6-capable web browser

Tested on Chrome, Opera, Brave, Firefox, Safari (runs best on the first three)
Does not work on Internet Explorer or Opera Mini


Numeric.js version 1.2.6 (included)










About

      Responsive, animated figures in JavaScript/HTML canvases
    





andrewcmyers.github.io/constrain


Topics



  constraints


  web-pages


  animated-figures


  embedded-figures



Resources





      Readme
 


Stars





50
    stars

Watchers





4
    watching

Forks





2
    forks







    Releases
      5







Release 0.3.0

          Latest
 
Apr 21, 2021

 

        + 4 releases







    Packages 0


        No packages published 





Languages












JavaScript
95.1%







HTML
4.8%







CSS
0.1%











"
https://news.ycombinator.com/rss,Flightcontrol (YC W22) is hiring Developer Advocate (remote/fulltime),https://jobs.flightcontrol.dev/developer-advocate,Comments,"🛫Developer Advocate at a Calm, Ambitious DevTools Startup https://www.flightcontrol.dev Flightcontrol is a 4 person devtool startup cofounded by the creator of Blitz.js. We’re not the macho, overworked team that “startup” might bring to mind. We’re intentionally building the most life-giving and fulfilling company possible, and we want you to join us! :) Photos from our last retreat in Italy — contrary to appearances, we also did real work :)Flightcontrol is transforming the app deployment landscapeFlightcontrol provides the deployment experience of a Platform-as-a-Service but without the limitations.Traditionally, users had to choose between a PaaS like Heroku, which is easy to use but is a closed-box with limitations and restrictions, or AWS which gives you full power and control but is a royal pain to set up. But with Flightcontrol they get the best of both worlds: great developer experience and full control and scalabilityOur product is so compelling that most of our users are migrating existing applications from Heroku, Render, Railway, Vercel and from custom AWS setups.Before you cringe at our current design, know that we are currently working with Overnice to completely overhaul our brand design, marketing site, and UI/UX. So rest assured, you’ll soon have world-class brand materials to work with :)Since launching in January, over 350 users have deployed 30,000+ timesUsers of all sizes love the product, from solo indie hackers to enterprisesWe went through Y Combinator in W22 and have raised $3.5M.We’re default alive — on track to become profitable this yearIntrigued? Read more about our company here.Deployments Per Month Meet Our Team of 4Brandon Bayer, Cofounder and CEO. Dayton, Ohio. You might know me as the creator of Blitz.js. Although highly technical, my strengths are product design and marketing. My superpower is simplicity. My top values that define everything I do are excellence, equality, inclusion, and freedom. Outside of work I love traveling, flying airplanes and helicopters, and rock climbing. My intention is to build the best company to work for in the world. I’m here to support you and help make your dreams come true.Mina Abadir, Cofounder and CTO. Toronto, Canada. Mina is the technical genius that brings our core product features to life. He’s deeply authentic and caring, loves to laugh, and greatly enjoys a good video game. His superpower is empathy.Blake Bayer, Junior Software Engineer. Dayton, Ohio. Formerly a nurse, Blake joined at the end of April. This is his first job in tech, and he continually impresses everyone on the team with his ability to learn and implement complex things quickly. He loves rock climbing and learning new things.Camila Rondinini, Senior Backend Software Engineer. Spain. Since joining this past June, she is already an extremely foundational part of our team, having designed and shipped some of our most important features. She’s an incredible engineer and has made a massive impact on our engineering culture.You? 😉 We need you to help us grow through awareness & educationAs our first devrel hire, this is your chance to really shine and help propel Flightcontrol into one of the most loved developer companies of this decade. This is a critical role in shaping FlightcontrolYour high level goal is to grow “top of funnel” traffic through awareness and education, with written content as the foundation.We are building a product-led growth flywheel. Your primary role is feeding the flywheel via new top of funnel users. Your secondary role is improving the flywheel through education and documentation.So far Brandon has been doing all the devrel related tasks. You’ll work closely with him to figure out where and how to invest your efforts. We don’t yet have the perfect content strategy. So we’ll be experimenting to see what works and what doesn’t.In time, we want to be known as the cloud education company. The place where all devs turn to become maestros of the cloud. We believe that most engineers would love working with native cloud providers like AWS if only it was more approachable and more easily understood. Our vision is to empower an entirely new generation of engineers through our product and through our educational content.Essential Responsibilities Deeply understand our product, its strengths and weaknessesCreate and distribute compelling technical content, including documentation, documentation, guides, demos, and videosWe care about quality over quantityIdentify and work on collaborations and integrations with other companies and projectsExample: how to use Flightcontrol preview environments to run isolated Cypress e2e testsSecondary ResponsibilitiesHelp with customer success. As a small team, we all share this responsibility. Helping with this is one of the best ways to understand nitty-gritty details of our product, product improvement ideas, and documentation ideas.A customer recently said that we “have amazing support and developer success” and that it provides a tremendous amount of value to themPossibly, but not required: represent Flightcontrol at eventsIdentifying relevant events for Flightcontrol and organizing our participation (meetups, conferences, hackathons, workshops, etc.),Ideally participating in 4-8 of these events per year, as a speaker or sponsorRequirements1 year full-time devrel experience2 years full-time software engineering experience and are comfortable with fullstackSome working knowledge of some basic AWS services like EC2, S3, RDS, LambdaTeacher — can explain complex things in as simple a manner as possibleWriter — great at writing technical content, ideally for 1+ yearsGrit — can ship content consistently over time through thick and thinEmpathy — can learn and understand what’s important to developers and engineering organizationsCredible — produces content that acknowledges the trade-offs and complexities of the real worldCan overlap with 10a-noon US Eastern time (EST) . You can work from anywhere in the world, but we have our company wide meetings in the 10a-12p EST time range.Nice to haveExperience in the AWS or cloud spaceGreat at creating videosConference speaking experienceGreat knowledge of the application hosting/deployment ecosystemAble to work with basic demos in several programming languagesExperience with our stack: Typescript, React, Next.js You Are Someone WhoIs Kind. We are a team that seeks to work really well together by building deep relationships. We have each other’s backs. We care about and check in on each other, and we enjoy being together. We have company retreats 2-3 times per year for a week at a time.Is Collaborative. We all work closely together to design and develop the best product possible. We want someone who is humble but will bring your own ideas on how to be more excellent.Takes Ownership. We offer significant equity because we want you to think at a higher level than just your daily tasks. We want you to help us shape the business. We need someone who loves to dig in and do what it takes to figure things out. And we want someone who is good at turning vague ideas into magnificence. Has a Growth Mindset. It matters more where you are going than where you are today. We’re looking for someone who loves to grow, improve, and learn new things. Your Typical Week at FlightcontrolOn Monday, depending on your timezone, you’ll start your morning or afternoon with a coffee chat where everyone is together for causal conversation. After that, you’ll join our Flightcontrol planning session with the entire Flightcontrol product team.Tuesday is usually meeting free, so you’ll be focused on your work.On Wednesday you’ll have your weekly 1 on 1 with Brandon, the CEO. This is your time to ask for what you want, bring up issues, ask hard questions, and give and receive feedback. Brandon takes feedback very seriously and is quick to make needed changes. Thursday and Friday are your time for deep work.Aside from being available 9a-12p EST, your work hours are flexible and up to you. Some of us work a standard 9-5 type of deal while others have varying schedules.You’ll collaborate with Brandon as much as is needed.Since we’re a startup, the journey from idea to building to shipping to growing is certainly a bit of a roller coaster. But we're all on the roller coaster together, learning and iterating as quickly as we can. As long as we stick to our values and show up for each other with curiosity, compassion, and collaboration, we can likely overcome just about anything together.Every month we have a tech-debt cleanup day. Every other month we have a company hackathon. You Can Grow With UsWe want you to grow with us as much as you desire. As we scale, you’ll be able to grow into almost any role you can imagine. Want to become a team lead? We’ll help train you. Want to become a manager? We’ll make it happen. Want to be an executive? Let’s figure that out. We want you to be with us as long as you are extremely happy. If we get to place were you aren’t happy, we’ll do everything we can to help you find a place where you are. Our Code of ExcellenceGo above and beyond. We’re not here to half-way do anything. If we’re going to do something, we’re going to do a stellar job.Tell the truth even when it hurts. We don’t tell white lies, and we don’t deceive. Even when it costs.Take care of you and yours first, work second. Nothing matters more than family and close relationships. We never sacrifice them for work.Treat people better than they deserve. Kindness and generosity guides how we treat everyone, including teammates and customers.Give and receive feedback. Feedback is essential for growth. We highly value giving and receiving informal, constructive feedback between all members of the team, and then taking prompt action on that feedback.Have a life outside work. It can be anything, hobbies, side projects, reading, etc. As long as you have something and work isn’t all you live for.Eradicate stress. Stress is a killer, and we work to eliminate it through any means, including systems, exercise, and meditation.Nothing is impossible. We believe we can create any future we imagine, and we lean into solving the things that seem impossible.Build a legacy. We are here to do our very best work. Work that will inspire generations for years to come. Salary & Benefits32 Hour Work Week - More and more companies are finding that people accomplish the same amount of work in 32 hours as in 40 hours.Salary: $110k — $145k USD (same as our engineering roles)0.75% — 1% Equity Stock Options. You’ll be a $20+ millionaire if our growth continues like it isMinimum 4 Weeks PTO - It's critical to have good work life balance, so you must take at least 4 weeks PTO each year.Fully RemoteHealth Insurance Fully Paid For401k - We’re still working out the details on this, but will get it nailed down asap if it’s important to youMenstrual Leave - There's no use trying to be productive when you are suffering. Take the day(s) off as PTO, no explanation needed.Unlimited Sick Leave - If you are feeling crappy, you aren't going to be doing your best work. So rest, get better, then come back energized.2+ In-Person Company Retreats Per YearOpen Source - We are passionate about open-source and encourage you to contribute on company time to anything that will benefit the company.Equipment - We'll make sure you have all the equipment you need to have an ergonomic, productive environment, including a standing desk and external monitors.Conferences - We're a big fan of in-person conference experiences, and encourage you to speak at and attend them. We'll fully pay for you to attend 2 conferences per year.Education - Budget for books or courses that are at least tangentially related to your work.  🔥Please apply here 👉 https://airtable.com/shrPet5euUinQ0uP4 👈
Our process:You submit the application45 minute zoom with Brandon, CEO45 minute technical interview with Mina, CTONo LeetCode garbage — we’ll offer you a range of options so you can choose a style that you’ll do best at45 minute technical interview with Camila1 hour zoom with Brandon, CEO — a deep dive on your experience, devrel strategy, tactics, and information architectureAnother short call with Brandon for both of us to ask and answer questions in preparation for making an offer 📣If you’d like to hear about future job openings, sign up here. "
https://news.ycombinator.com/rss,"Porth, It's Like Forth but in Python",https://gitlab.com/tsoding/porth,Comments,"






P



porth






Project ID: 30419193








Star
250






1,189 Commits

1 Branch

0 Tags

16.1 MB Project Storage








Concatenative Programming Language for Computers


Read more
























Find file




Select Archive Format




Download source code


zip
tar.gz
tar.bz2
tar









Clone






Clone with SSH










Clone with HTTPS











Open in your IDE



Visual Studio Code (SSH)




Visual Studio Code (HTTPS)




IntelliJ IDEA (SSH)




IntelliJ IDEA (HTTPS)







Copy HTTPS clone URL





Copy SSH clone URLgit@gitlab.com:tsoding/porth.git


Copy HTTPS clone URLhttps://gitlab.com/tsoding/porth.git








README

MIT License

CONTRIBUTING





"
https://news.ycombinator.com/rss,I analyzed shuffling in a million games of MtG Arena (2020),https://old.reddit.com/r/MagicArena/comments/b21u3n/i_analyzed_shuffling_in_a_million_games/,Comments,"



Too Many Requests



whoa there, pardner!
we're sorry, but you appear to be a bot and we've seen too many requests
from you lately. we enforce a hard speed limit on requests that appear to come
from bots to prevent abuse.
if you are not a bot but are spoofing one via your browser's user agent
string: please change your user agent string to avoid seeing this message
again.
please wait 1 second(s) and try again.
as a reminder to developers, we recommend that clients make no
    more than one
    request every two seconds to avoid seeing this message.


"
https://news.ycombinator.com/rss,Faster than the filesystem (2021),https://www.sqlite.org/fasterthanfs.html,Comments,"




35% Faster Than The Filesystem









Small. Fast. Reliable.Choose any three.



Home
Menu
About
Documentation
Download
License
Support
Purchase

Search




About
Documentation
Download
Support
Purchase





Search Documentation
Search Changelog










35% Faster Than The Filesystem



►
Table Of Contents

1. Summary
1.1. Caveats
1.2. Related Studies
2. How These Measurements Are Made
2.1. Read Performance Measurements
2.2. Write Performance Measurements
2.3. Variations
3. General Findings
4. Additional Notes
4.1. Compiling And Testing on Android




1. Summary
SQLite reads and writes small blobs (for example, thumbnail images)
35% faster¹ than the same blobs
can be read from or written to individual files on disk using
fread() or fwrite().

Furthermore, a single SQLite database holding
10-kilobyte blobs uses about 20% less disk space than
storing the blobs in individual files.

The performance difference arises (we believe) because when
working from an SQLite database, the open() and close() system calls
are invoked only once, whereas
open() and close() are invoked once for each blob
when using blobs stored in individual files.  It appears that the
overhead of calling open() and close() is greater than the overhead
of using the database.  The size reduction arises from the fact that
individual files are padded out to the next multiple of the filesystem
block size, whereas the blobs are packed more tightly into an SQLite
database.


The measurements in this article were made during the week of 2017-06-05
using a version of SQLite in between 3.19.2 and 3.20.0.  You may expect
future versions of SQLite to perform even better.

1.1. Caveats


¹The 35% figure above is approximate.  Actual timings vary
depending on hardware, operating system, and the
details of the experiment, and due to random performance fluctuations
on real-world hardware.  See the text below for more detail.
Try the experiments yourself.  Report significant deviations on
the SQLite forum.


The 35% figure is based on running tests on every machine
that the author has easily at hand.
Some reviewers of this article report that SQLite has higher 
latency than direct I/O on their systems.  We do not yet understand
the difference.  We also see indications that SQLite does not
perform as well as direct I/O when experiments are run using
a cold filesystem cache.


So let your take-away be this: read/write latency for
SQLite is competitive with read/write latency of individual files on
disk.  Often SQLite is faster.  Sometimes SQLite is almost
as fast.  Either way, this article disproves the common
assumption that a relational database must be slower than direct
filesystem I/O.

1.2. Related Studies

Jim Gray
and others studied the read performance of BLOBs
versus file I/O for Microsoft SQL Server and found that reading BLOBs 
out of the 
database was faster for BLOB sizes less than between 250KiB and 1MiB.
(Paper).
In that study, the database still stores the filename of the content even
if the content is held in a separate file.  So the database is consulted
for every BLOB, even if it is only to extract the filename.  In this
article, the key for the BLOB is the filename, so no preliminary database
access is required.  Because the database is never used at all when
reading content from individual files in this article, the threshold
at which direct file I/O becomes faster is smaller than it is in Gray's
paper.


The Internal Versus External BLOBs article on this website is an
earlier investigation (circa 2011) that uses the same approach as the
Jim Gray paper — storing the blob filenames as entries in the
database — but for SQLite instead of SQL Server.



2. How These Measurements Are Made
I/O performance is measured using the
kvtest.c program
from the SQLite source tree.
To compile this test program, first gather the kvtest.c source file
into a directory with the SQLite amalgamation source
files ""sqlite3.c"" and ""sqlite3.h"".  Then on unix, run a command like
the following:

gcc -Os -I. -DSQLITE_DIRECT_OVERFLOW_READ \
  kvtest.c sqlite3.c -o kvtest -ldl -lpthread

Or on Windows with MSVC:

cl -I. -DSQLITE_DIRECT_OVERFLOW_READ kvtest.c sqlite3.c

Instructions for compiling for Android
are shown below.


Use the resulting ""kvtest"" program to
generate a test database with 100,000 random uncompressible
blobs, each with a random
size between 8,000 and 12,000 bytes
using a command like this:

./kvtest init test1.db --count 100k --size 10k --variance 2k


If desired, you can verify the new database by running this command:

./kvtest stat test1.db


Next, make copies of all the blobs into individual files in a directory
using a command like this:

./kvtest export test1.db test1.dir


At this point, you can measure the amount of disk space used by
the test1.db database and the space used by the test1.dir directory
and all of its content.  On a standard Ubuntu Linux desktop, the
database file will be 1,024,512,000 bytes in size and the test1.dir
directory will use 1,228,800,000 bytes of space (according to ""du -k""),
about 20% more than the database.


The ""test1.dir"" directory created above puts all the blobs into a single
folder.  It was conjectured that some operating systems would perform 
poorly when a single directory contains 100,000 objects.  To test this,
the kvtest program can also store the blobs in a hierarchy of folders with no
more than 100 files and/or subdirectories per folder.  The alternative
on-disk representation of the blobs can be created using the --tree
command-line option to the ""export"" command, like this:

./kvtest export test1.db test1.tree --tree


The test1.dir directory will contain 100,000 files
with names like ""000000"", ""000001"", ""000002"" and so forth but the
test1.tree directory will contain the same files in subdirectories like
""00/00/00"", ""00/00/01"", and so on.  The test1.dir and test1.test
directories take up approximately the same amount of space, though
test1.test is very slightly larger due to the extra directory entries.


All of the experiments that follow operate the same with either 
""test1.dir"" or ""test1.tree"".  Very little performance difference is
measured in either case, regardless of operating system.


Measure the performance for reading blobs from the database and from
individual files using these commands:

./kvtest run test1.db --count 100k --blob-api
./kvtest run test1.dir --count 100k --blob-api
./kvtest run test1.tree --count 100k --blob-api


Depending on your hardware and operating system, you should see that reads 
from the test1.db database file are about 35% faster than reads from 
individual files in the test1.dir or test1.tree folders.  Results can vary
significantly from one run to the next due to caching, so it is advisable
to run tests multiple times and take an average or a worst case or a best
case, depending on your requirements.

The --blob-api option on the database read test causes kvtest to use
the sqlite3_blob_read() feature of SQLite to load the content of the
blobs, rather than running pure SQL statements.  This helps SQLite to run
a little faster on read tests.  You can omit that option to compare the
performance of SQLite running SQL statements.
In that case, the SQLite still out-performs direct reads, though
by not as much as when using sqlite3_blob_read().
The --blob-api option is ignored for tests that read from individual disk
files.


Measure write performance by adding the --update option.  This causes
the blobs are overwritten in place with another random blob of
exactly the same size.

./kvtest run test1.db --count 100k --update
./kvtest run test1.dir --count 100k --update
./kvtest run test1.tree --count 100k --update


The writing test above is not completely fair, since SQLite is doing
power-safe transactions whereas the direct-to-disk writing is not.
To put the tests on a more equal footing, add either the --nosync
option to the SQLite writes to disable calling fsync() or
FlushFileBuffers() to force content to disk, or using the --fsync option
for the direct-to-disk tests to force them to invoke fsync() or
FlushFileBuffers() when updating disk files.


By default, kvtest runs the database I/O measurements all within
a single transaction.  Use the --multitrans option to run each blob
read or write in a separate transaction.  The --multitrans option makes
SQLite much slower, and uncompetitive with direct disk I/O.  This
option proves, yet again, that to get the most performance out of
SQLite, you should group as much database interaction as possible within
a single transaction.


There are many other testing options, which can be seen by running
the command:

./kvtest help

2.1. Read Performance Measurements
The chart below shows data collected using 
kvtest.c on five different
systems:


Win7: A circa-2009 Dell Inspiron laptop, Pentium dual-core
    at 2.30GHz, 4GiB RAM, Windows7.
Win10: A 2016 Lenovo YOGA 910, Intel i7-7500 at 2.70GHz,
    16GiB RAM, Windows10.
Mac: A 2015 MacBook Pro, 3.1GHz intel Core i7, 16GiB RAM,
    MacOS 10.12.5
Ubuntu: Desktop built from Intel i7-4770K at 3.50GHz, 32GiB RAM,
    Ubuntu 16.04.2 LTS
Android: Galaxy S3, ARMv7, 2GiB RAM

All machines use SSD except Win7 which has a
hard-drive. The test database is 100K blobs with sizes uniformly
distributed between 8K and 12K, for a total of about 1 gigabyte
of content.  The database page size
is 4KiB.  The -DSQLITE_DIRECT_OVERFLOW_READ compile-time option was
used for all of these tests.
Tests were run multiple times.
The first run was used to warm up the cache and its timings were discarded.


The chart below shows average time to read a blob directly from the
filesystem versus the time needed to read the same blob from the SQLite 
database.
The actual timings vary considerably from one system to another 
(the Ubuntu desktop is much
faster than the Galaxy S3 phone, for example).  
This chart shows the ratio of the
times needed to read blobs from a file divided by the time needed to
from the database.  The left-most column in the chart is the normalized
time to read from the database, for reference.


In this chart, an SQL statement (""SELECT v FROM kv WHERE k=?1"") 
is prepared once.  Then for each blob, the blob key value is bound 
to the ?1 parameter and the statement is evaluated to extract the
blob content.


The chart shows that on Windows10, content can be read from the SQLite
database about 5 times faster than it can be read directly from disk.
On Android, SQLite is only about 35% faster than reading from disk.






Chart 1:  SQLite read latency relative to direct filesystem reads.
100K blobs, avg 10KB each, random order using SQL


The performance can be improved slightly by bypassing the SQL layer
and reading the blob content directly using the
sqlite3_blob_read() interface, as shown in the next chart:






Chart 2:  SQLite read latency relative to direct filesystem reads.
100K blobs, avg size 10KB, random order
using sqlite3_blob_read().


Further performance improves can be made by using the
memory-mapped I/O feature of SQLite.  In the next chart, the
entire 1GB database file is memory mapped and blobs are read
(in random order) using the sqlite3_blob_read() interface.
With these optimizations, SQLite is twice as fast as Android
or MacOS-X and over 10 times faster than Windows.






Chart 3:  SQLite read latency relative to direct filesystem reads.
100K blobs, avg size 10KB, random order
using sqlite3_blob_read() from a memory-mapped database.


The third chart shows that reading blob content out of SQLite can be
twice as fast as reading from individual files on disk for Mac and
Android, and an amazing ten times faster for Windows.

2.2. Write Performance Measurements

Writes are slower.
On all systems, using both direct I/O and SQLite, write performance is
between 5 and 15 times slower than reads.


Write performance measurements were made by replacing (overwriting)
an entire blob with a different blob.  All of the blobs in these
experiment are random and incompressible.  Because writes are so much
slower than reads, only 10,000 of the 100,000 blobs in the database
are replaced.  The blobs to be replaced are selected at random and
are in no particular order.


The direct-to-disk writes are accomplished using fopen()/fwrite()/fclose().
By default, and in all the results shown below, the OS filesystem buffers are
never flushed to persistent storage using fsync() or
FlushFileBuffers().  In other words, there is no attempt to make the
direct-to-disk writes transactional or power-safe.
We found that invoking fsync() or FlushFileBuffers() on each file
written causes direct-to-disk storage
to be about 10 times or more slower than writes to SQLite.


The next chart compares SQLite database updates in WAL mode
against raw direct-to-disk overwrites of separate files on disk.
The PRAGMA synchronous setting is NORMAL.
All database writes are in a single transaction.
The timer for the database writes is stopped after the transaction
commits, but before a checkpoint is run.
Note that the SQLite writes, unlike the direct-to-disk writes,
are transactional and power-safe, though because the synchronous
setting is NORMAL instead of FULL, the transactions are not durable.






Chart 4:  SQLite write latency relative to direct filesystem writes.
10K blobs, avg size 10KB, random order,
WAL mode with synchronous NORMAL,
exclusive of checkpoint time


The android performance numbers for the write experiments are omitted
because the performance tests on the Galaxy S3 are so random.  Two
consecutive runs of the exact same experiment would give wildly different
times.  And, to be fair, the performance of SQLite on android is slightly
slower than writing directly to disk.


The next chart shows the performance of SQLite versus direct-to-disk
when transactions are disabled (PRAGMA journal_mode=OFF)
and PRAGMA synchronous is set to OFF.  These settings put SQLite on an
equal footing with direct-to-disk writes, which is to say they make the
data prone to corruption due to system crashes and power failures.






Chart 5:  SQLite write latency relative to direct filesystem writes.
10K blobs, avg size 10KB, random order,
journaling disabled, synchronous OFF.


In all of the write tests, it is important to disable anti-virus software
prior to running the direct-to-disk performance tests.  We found that
anti-virus software slows down direct-to-disk by an order of magnitude
whereas it impacts SQLite writes very little.  This is probably due to the
fact that direct-to-disk changes thousands of separate files which all need
to be checked by anti-virus, whereas SQLite writes only changes the single
database file.

2.3. Variations
The -DSQLITE_DIRECT_OVERFLOW_READ compile-time option causes SQLite
to bypass its page cache when reading content from overflow pages.  This
helps database reads of 10K blobs run a little faster, but not all that much
faster.  SQLite still holds a speed advantage over direct filesystem reads
without the SQLITE_DIRECT_OVERFLOW_READ compile-time option.

Other compile-time options such as using -O3 instead of -Os or
using -DSQLITE_THREADSAFE=0 and/or some of the other
recommended compile-time options might help SQLite to run even faster
relative to direct filesystem reads.

The size of the blobs in the test data affects performance.
The filesystem will generally be faster for larger blobs, since
the overhead of open() and close() is amortized over more bytes of I/O,
whereas the database will be more efficient in both speed and space
as the average blob size decreases.


3. General Findings


SQLite is competitive with, and usually faster than, blobs stored in
separate files on disk, for both reading and writing.


SQLite is much faster than direct writes to disk on Windows
when anti-virus protection is turned on.  Since anti-virus software
is and should be on by default in Windows, that means that SQLite
is generally much faster than direct disk writes on Windows.


Reading is about an order of magnitude faster than writing, for all
systems and for both SQLite and direct-to-disk I/O.


I/O performance varies widely depending on operating system and hardware.
Make your own measurements before drawing conclusions.


Some other SQL database engines advise developers to store blobs in separate
files and then store the filename in the database.  In that case, where
the database must first be consulted to find the filename before opening
and reading the file, simply storing the entire blob in the database
gives much faster read and write performance with SQLite.
See the Internal Versus External BLOBs article for more information.

4. Additional Notes

4.1. Compiling And Testing on Android

The kvtest program is compiled and run on Android as follows.
First install the Android SDK and NDK.  Then prepare a script
named ""android-gcc"" that looks approximately like this:

#!/bin/sh
#
NDK=/home/drh/Android/Sdk/ndk-bundle
SYSROOT=$NDK/platforms/android-16/arch-arm
ABIN=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin
GCC=$ABIN/arm-linux-androideabi-gcc
$GCC --sysroot=$SYSROOT -fPIC -pie $*

Make that script executable and put it on your $PATH.  Then
compile the kvtest program as follows:

android-gcc -Os -I. kvtest.c sqlite3.c -o kvtest-android

Next, move the resulting kvtest-android executable to the Android
device:

adb push kvtest-android /data/local/tmp

Finally use ""adb shell"" to get a shell prompt on the Android device,
cd into the /data/local/tmp directory, and begin running the tests
as with any other unix host.
This page last modified on  2021-03-01 12:55:48 UTC 
"
https://news.ycombinator.com/rss,The Inner Beauty of Basic Electronics,https://spectrum.ieee.org/open-circuits,Comments,"The Inner Beauty of Basic Electronics - IEEE SpectrumIEEE.orgIEEE Xplore Digital LibraryIEEE StandardsMore SitesSign InJoin IEEEThe Inner Beauty of Basic ElectronicsShareFOR THE TECHNOLOGY INSIDERSearch: Explore by topicAerospaceArtificial IntelligenceBiomedicalComputingConsumer ElectronicsEnergyHistory of TechnologyRoboticsSemiconductorsSensorsTelecommunicationsTransportationIEEE SpectrumFOR THE TECHNOLOGY INSIDERTopicsAerospaceArtificial IntelligenceBiomedicalComputingConsumer ElectronicsEnergyHistory of TechnologyRoboticsSemiconductorsSensorsTelecommunicationsTransportationSectionsFeaturesNewsOpinionCareersDIYThe Big PictureEngineering ResourcesMoreSpecial ReportsCollectionsExplainersPodcastsVideosNewslettersTop Programming LanguagesRobots GuideFor IEEE MembersCurrent IssueMagazine ArchiveThe InstituteTI ArchiveFor IEEE MembersCurrent IssueMagazine ArchiveThe InstituteTI ArchiveIEEE SpectrumAbout UsContact UsReprints & PermissionsAdvertisingFollow IEEE SpectrumSupport IEEE SpectrumIEEE Spectrum is the flagship publication of the IEEE — the world’s largest professional organization devoted to engineering and applied sciences. Our articles, podcasts, and infographics inform our readers about developments in technology, engineering, and science.Join IEEESubscribeAbout IEEEContact & SupportAccessibilityNondiscrimination PolicyTermsIEEE Privacy Policy© Copyright 2023 IEEE — All rights reserved. A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
                        view privacy policy
                    
                    accept & close
                Enjoy more free content and benefits by creating an accountSaving articles to read later requires an IEEE Spectrum accountThe Institute content is only available for membersDownloading full PDF issues is exclusive for IEEE MembersAccess to Spectrum's Digital Edition is exclusive for IEEE MembersFollowing topics is a feature exclusive for IEEE MembersAdding your response to an article requires an IEEE Spectrum accountCreate an account to access more content and features on IEEE Spectrum, including the ability to save articles to read later, download Spectrum Collections, and participate in conversations with readers and editors. For more exclusive content and features, consider Joining IEEE.Join the world’s largest professional organization devoted to engineering and applied sciences and get access to all of Spectrum’s articles, archives, PDF downloads, and other benefits. Learn more →CREATE AN ACCOUNTSIGN INJOIN IEEESIGN INCloseAccess Thousands of Articles — Completely FreeCreate an account and get exclusive content and features: Save articles, download collections, and talk to tech insiders — all free! For full access and benefits, join IEEE as a paying member.CREATE AN ACCOUNTSIGN INConsumer ElectronicsTopicMagazineTypeFeature
        The Inner Beauty of Basic Electronics
    Open Circuits showcases the surprising complexity of passive componentsEric SchlaepferWindell H. Oskay20h5 min readBlueEric Schlaepfer was trying to fix a broken piece of test equipment when he came across the cause of the problem—a troubled tantalum capacitor. The component had somehow shorted out, and he wanted to know why. So he polished it down for a look inside. He never found the source of the short, but he and his collaborator, Windell H. Oskay, discovered something even better: a breathtaking hidden world inside electronics. What followed were hours and hours of polishing, cleaning, and photography that resulted in Open Circuits: The Inner Beauty of Electronic Components (No Starch Press, 2022), an excerpt of which follows. As the authors write, everything about these components is deliberately designed to meet specific technical needs, but that design leads to “accidental beauty: the emergent aesthetics of things you were never expected to see.”

	From a book that spans the wide world of electronics, what we at 
	IEEE Spectrum found surprisingly compelling were the insides of things we don’t spend much time thinking about, passive components. Transistors, LEDs, and other semiconductors may be where the action is, but the simple physics of resistors, capacitors, and inductors have their own sort of splendor. 
            
                High-Stability Film Resistor
            
            
        All photos by Eric Schlaepfer & Windell H. OskayThis high-stability film resistor, about 4 millimeters in diameter, is made in much the same way as its inexpensive carbon-film cousin, but with exacting precision. A ceramic rod is coated with a fine layer of resistive film (thin metal, metal oxide, or carbon) and then a perfectly uniform helical groove is machined into the film.Instead of coating the resistor with an epoxy, it’s hermetically sealed in a lustrous little glass envelope. This makes the resistor more robust, ideal for specialized cases such as precision reference instrumentation, where long-term stability of the resistor is critical. The glass envelope provides better isolation against moisture and other environmental changes than standard coatings like epoxy.
            
                15-Turn Trimmer Potentiometer
            
            
        It takes 15 rotations of an adjustment screw to move a 15-turn trimmer potentiometer from one end of its resistive range to the other. Circuits that need to be adjusted with fine resolution control use this type of trimmer pot instead of the single-turn variety.The resistive element in this trimmer is a strip of cermet—a composite of ceramic and metal—silk-screened on a white ceramic substrate. Screen-printed metal links each end of the strip to the connecting wires. It’s a flattened, linear version of the horseshoe-shaped resistive element in single-turn trimmers.Turning the adjustment screw moves a plastic slider along a track. The wiper is a spring finger, a spring-loaded metal contact, attached to the slider. It makes contact between a metal strip and the selected point on the strip of resistive film.
            
                Ceramic Disc Capacitor
            
            
        Capacitors are fundamental electronic components that store energy in the form of static electricity. They’re used in countless ways, including for bulk energy storage, to smooth out electronic signals, and as computer memory cells. The simplest capacitor consists of two parallel metal plates with a gap between them, but capacitors can take many forms so long as there are two conductive surfaces, called electrodes, separated by an insulator.A ceramic disc capacitor is a low-cost capacitor that is frequently found in appliances and toys. Its insulator is a ceramic disc, and its two parallel plates are extremely thin metal coatings that are evaporated or sputtered onto the disc’s outer surfaces. Connecting wires are attached using solder, and the whole assembly is dipped into a porous coating material that dries hard and protects the capacitor from damage.
            
                Film Capacitor
            
            
        Film capacitors are frequently found in high-quality audio equipment, such as headphone amplifiers, record players, graphic equalizers, and radio tuners. Their key feature is that the dielectric material is a plastic film, such as polyester or polypropylene.The metal electrodes of this film capacitor are vacuum-deposited on the surfaces of long strips of plastic film. After the leads are attached, the films are rolled up and dipped into an epoxy that binds the assembly together. Then the completed assembly is dipped in a tough outer coating and marked with its value.Other types of film capacitors are made by stacking flat layers of metallized plastic film, rather than rolling up layers of film.
            
                Dipped Tantalum Capacitor
            
            
        At the core of this capacitor is a porous pellet of tantalum metal. The pellet is made from tantalum powder and sintered, or compressed at a high temperature, into a dense, spongelike solid.Just like a kitchen sponge, the resulting pellet has a high surface area per unit volume. The pellet is then anodized, creating an insulating oxide layer with an equally high surface area. This process packs a lot of capacitance into a compact device, using spongelike geometry rather than the stacked or rolled layers that most other capacitors use.The device’s positive terminal, or anode, is connected directly to the tantalum metal. The negative terminal, or cathode, is formed by a thin layer of conductive manganese dioxide coating the pellet.
            
                Axial Inductor
            
            
        Inductors are fundamental electronic components that store energy in the form of a magnetic field. They’re used, for example, in some types of power supplies to convert between voltages by alternately storing and releasing energy. This energy-efficient design helps maximize the battery life of cellphones and other portable electronics.Inductors typically consist of a coil of insulated wire wrapped around a core of magnetic material like iron or ferrite, a ceramic filled with iron oxide. Current flowing around the core produces a magnetic field that acts as a sort of flywheel for current, smoothing out changes in the current as it flows through the inductor.This axial inductor has a number of turns of varnished copper wire wrapped around a ferrite form and soldered to copper leads on its two ends. It has several layers of protection: a clear varnish over the windings, a light-green coating around the solder joints, and a striking green outer coating to protect the whole component and provide a surface for the colorful stripes that indicate its inductance value.
            
                Power Supply Transformer
            
            
        This transformer has multiple sets of windings and is used in a power supply to create multiple output AC voltages from a single AC input such as a wall outlet.The small wires nearer the center are “high impedance” turns of magnet wire. These windings carry a higher voltage but a lower current. They’re protected by several layers of tape, a copper-foil electrostatic shield, and more tape.The outer “low impedance” windings are made with thicker insulated wire and fewer turns. They handle a lower voltage but a higher current.All of the windings are wrapped around a black plastic bobbin. Two pieces of ferrite ceramic are bonded together to form the magnetic core at the heart of the transformer.From Your Site ArticlesDell Tried to Hide Bad Capacitors Problem 2003-2005 ›Hands On - IEEE Spectrum ›Watch: Laser Origami Makes Inductors ›Related Articles Around the WebOpen Circuits: The Inner Beauty of Electronic Components: Oskay ... ›Open Circuits | No Starch Press ›Open Circuits ›passive componentsArt of Electronicsresistorscapacitorsinductorsbooks{""imageShortcodeIds"":[]}Eric SchlaepferEric Schlaepfer runs the popular engineering Twitter account @TubeTimeUS, where he posts cross-section photos, shares his retrocomputing and reverse engineering projects, investigates engineering accidents, and even features the occasional vacuum tube or two. He is coauthor of Open Circuits: The Inner Beauty of Electronic Components (No Starch Press, 2022).Windell H. OskayWindell H. Oskay is the cofounder of Evil Mad Scientist Laboratories, where he designs robots for a living. He is coauthor of Open Circuits: The Inner Beauty of Electronic Components (No Starch Press, 2022).The Conversation (0)
        Video Friday: Robots at Night
    13 Jan 20233 min readAerospaceTopicTypeRoboticsNews
        Relativity Space Aims for Orbit
    13 Jan 20234 min readConsumer ElectronicsTopicTypeNews
        Paper Batteries, Blue Quantum Dots, and Other Enabling Technologies from CES 2023
    12 Jan 20233 min readThe InstituteTopicArticleTypeHistory of Technology
        How This Record Company Engineer Invented the CT Scanner
    The machine, made to image the human brain, won him a Nobel PrizeJoanna GoodrichJoanna Goodrich is the associate editor of The Institute, covering the work and accomplishments of IEEE members and IEEE and technology-related events. She has a master's degree in health communications from Rutgers University, in New Brunswick, N.J.12 Jan 20234 min readResearch engineer Godfrey Hounsfield invented the CT scanner to create three-dimensional brain images.
        PA Images/Getty Images
    ieee historyieee tech historyhistory of technologyct scannermedical devicesieee milestonetype:tiThe inspiration for computed tomography (CT) came from a chance conversation that research engineer Godfrey Hounsfield had with a doctor while on vacation in the 1960s. The physician complained that X-ray images of the brain were too grainy and only two-dimensional.Hounsfield worked at Electrical and Musical Industry in Hayes, England. Best known for producing and selling Beatles records, EMI also developed electronic equipment. Keep Reading ↓Show lessConsumer ElectronicsTopicTypeComputingSponsored Article
        Building the Future of Smart Home Security
    Engineers must invent new technology to enhance security products’ abilitiesNate WilfertNate Wilfert is Vice President of Software Engineering at SimpliSafe.22 Mar 20224 min readIn this article, SimpliSafe’s VP of Software Engineering discusses his team’s focus on creating a safer future through enhanced technology.
        SimpliSafe
    smart homeiotconnected homesecuritysimplisafeThis is a sponsored article brought to you by SimpliSafe.It’s nearly impossible to find a household today that doesn’t have at least one connected smart home device installed. From video doorbells to robot vacuums, automated lighting, and voice assistants, smart home technology has invaded consumers’ homes and shows no sign of disappearing anytime soon. Indeed, according to a study conducted by consulting firm Parks Associates, smart home device adoption has increased by more than 64 percent in the past two years, with 23 percent of households owning three or more smart home devices. This is particularly true for devices that provide security with 38 percent of Americans owning a home security product. This percentage is likely to increase as 7 in 10 homebuyers claimed that safety and security was the primary reason, after convenience, that they would be seeking out smart homes, according to a report published by Security.org last year.As the demand for smart home security grows, it’s pertinent that the engineers who build the products and services that keep millions of customers safe continue to experiment with new technologies that could enhance overall security and accessibility. At SimpliSafe, an award-winning home security company based in Boston, Mass., it is the pursuit of industry-leading protection that drives the entire organization to continue innovating.In this article, Nate Wilfert, VP of Software Engineering at SimpliSafe, discusses the complex puzzles his team is solving on a daily basis—such as applying artificial intelligence (AI) technology into cameras and building load-balancing solutions to handle server traffic—to push forward the company’s mission to make every home secure and advance the home security industry as a whole.Keep Reading ↓Show less
        Trending Stories
    The most-read stories on IEEE Spectrum right nowThe InstituteTopicArticleTypeHistory of Technology
        How This Record Company Engineer Invented the CT Scanner
    12 Jan 20234 min readAerospaceTopicTypeRoboticsNews
        Relativity Space Aims for Orbit
    13 Jan 20234 min readConsumer ElectronicsTopicTypeNews
        Paper Batteries, Blue Quantum Dots, and Other Enabling Technologies from CES 2023
    12 Jan 20233 min readConsumer ElectronicsTopicTypeNews
        CES 2023’s Four Wildest—and Catchiest—Gadgets
    11 Jan 20233 min readThe InstituteTopicTypeOpinionTelecommunications
        Examining the Impact of 6G Telecommunications on Society
    10 Jan 20233 min readSensorsTopicArtificial IntelligenceTypeNews
        Spray-on Smart Skin Reads Typing and Hand Gestures
    11 Jan 20233 min readTelecommunicationsTopicMagazineTypeFeature
        How Police Exploited the Capitol Riot’s Digital Records
    06 Jan 202311 min readConsumer ElectronicsTopicTypeNewsTransportation
        The Best Tech of CES 2023
    09 Jan 20236 min read"
https://news.ycombinator.com/rss,Running KDE Plasma on RISC-V VisionFive-2,https://cordlandwehr.wordpress.com/2023/01/14/running-plasma-on-visionfive-2/,Comments,"


Running Plasma on VisionFive-2 

New year, new RISC-V Yocto blog post \o/ When I wrote my last post, I did really not expect my brand new VisionFive-2 board to find its way to me so soon… But well, a week ago it was suddenly there. While unpacking I shortly pondered over my made plans to prepare a Plasma Bigscreen RaspberryPi 4 demo board for this year’s FOSDEM.
Obvious conclusion: “Screw it! Let’s do the demo on the VisionFive-2!” — And there we are:
After some initial bumpy steps to boot up a first self-compiled U-boot and Kernel (If you unbox a new board, you need to do a bootloader and firmware update first! Otherwise it will not boot the latest VisionFive Kernel) it was surprisingly easy to prepare Yocto to build a core-image-minimal that really boots the whole way up.
Unfortunately after these first happy hours, the last week was full of handling the horrors of closed-source binary drivers for the GPU. Even though Imagination promised to provide an open source driver at some time, right now there is only the solution to use the closed source PVR driver. After quite a lot of trying, guessing and and comparing the boot and init sequences of the reference image to the dark screen in front of me, I came up with:

a new visionfive2-graphics Yocto package for the closed source driver blobs
a fork of Mesa that uses a very heavy patch set for the PVR driver adaptions; all patches are taken from the VisionFive 2 buildroot configurations
and a couple of configs for making the system start with doing an initial modeset

The result right now:

VisionFive-2 device with Plasma-Bigscreen (KWin running via Wayland), SD card image built via Yocto, KDE software via KDE’s Yocto layers, Kernel and U-Boot being the latest fork versions from StarFive
Actually, the full UI even feels much smoother than on my RPi4, which is quite cool. I am not sure where I will end in about 3 weeks with some more debugging and patching. But I am very confident that you can see a working RISC-V board with onboard GPU and running Plasma Shell, when you visit the KDE stall at FOSDEM in February 😉
For people who are interested in Yocto, here is the WIP patch set: https://github.com/riscv/meta-riscv/pull/382
Share this:TwitterFacebookLike this:Like Loading...

Related
 

Posted on January 14, 2023January 14, 2023Author cordlandwehrCategories KDE, YoctoTags KDE 



Leave a Reply Cancel reply


Enter your comment here...




Fill in your details below or click an icon to log in:







 



 



 






 
 


Email (required) (Address never made public)



Name (required)



Website
















			You are commenting using your WordPress.com account.			
				( Log Out / 
				Change )
			
















			You are commenting using your Twitter account.			
				( Log Out / 
				Change )
			
















			You are commenting using your Facebook account.			
				( Log Out / 
				Change )
			






Cancel
Connecting to %s




 Notify me of new comments via email. Notify me of new posts via email.
 



Δ 



Post navigation
Previous Previous post: Getting a First Picture on my Nezha RISC-V Board

"
https://news.ycombinator.com/rss,The Bibites: Artificial Life Simulation,https://leocaussan.itch.io/the-bibites,Comments,"The Bibites by The BibitesFollow The BibitesFollowFollowing The BibitesFollowingAdd To CollectionCollectionCommentsDevlogRelated gamesRelatedThe BibitesA downloadable project for Windows, macOS, and LinuxDownload NowName your own priceWelcome everyone! 
This is The Bibites  
A simulation where you are able to watch evolution happen before your very eyes! 
Each bibite (the small critters you see on the screen) starts off with an empty brain (they do nothing) and pretty basic genes (they all look alike). 
Through random mutations, one can be spawned with a brain connection that will link two neurons and might trigger a behavior, like going forward, which will allow them to eat food, and then reproduce with the energy gained.
You have reproduction, mutations, and natural selection, which leads to ... 

With time, this develops into complex behaviors, like following pheromone trails to hunt other bibites, or stockpiling food in a specific area of the map. 

Present Features
VisionProcedural Sprites (generating a custom sprite for each bibite from their genes)
Self-awareness (state, health, energy, etc.)Pheromones (producing and sensing)Grabbing and Throwing stuff (pellets and other bibites)Materials and Digestion SimulationRealistic Energy System
The simulation is also interactive, allowing you to YEET bibites and pellets around. You can selectively kill bibites, feed them, force the laying of eggs, and so much more.
It's also highly customizable, allowing you to test a nearly infinite number of scenarios. How will they evolve if there is no drag (no friction)? What about if moving is extremely energy-costly? It's your job to test it all, I sure can't do it by myself.
I'LL STATE CLEARLY THAT THIS IS THE REGULAR VERSION. I TRIED TO DISABLE ""name your own price"" AND SET IT TO 0.00$ BUT IT DON'T SEEM TO WORK...I ENCOURAGE YOU TO DOWNLOAD THIS FOR FREE, ONLY PAY SOMETHING IF YOU WANT TO THROW MONEY AT ME FOR NO OTHER REASON THAN TO SUPPORT THIS PROJECT. The best way to do so is to subscribe to my Patreon to provide me with reliable support and have access to the alpha updates as I develop them: 
Become a Patron to get alpha updates! 

Follow the development and see additional content on Youtube

Follow me on Twitter to see... whatever I do there

Join the subreddit community

Upcoming features 
Module-based systems for unbounded evolution and incredible performancesBiomes (environmental simulation)Evolving ecosystems (the plants/food evolves too)Rocks (Movable objects)And much more!

After trying it out, please give me some feedback

Or report bugs
More informationUpdated 28 days agoStatusIn developmentPlatformsWindows, macOS, LinuxRatingRated 4.6 out of 5 stars(69 total ratings)AuthorThe BibitesGenreSimulationMade withUnityTags2D, artificial-intelligence, evolution, interactive, Life Simulation, Pixel Art, Procedural Generation, Sandbox, UnityAverage sessionA few hoursLanguagesEnglishInputsKeyboard, MouseMultiplayerLocal multiplayerPlayer countSingleplayerLinksYouTube, Patreon, Twitter, CommunityDownloadDownload NowName your own priceClick download now to get access to the following files:The Bibites 0.4.2 - Windows 64x.zip 30 MB  The Bibites 0.4.2 - Linux.zip 41 MB  The Bibites 0.4.2 - Mac Universal.zip 36 MB  The Bibites 0.4.2 - Windows 32x.zip 27 MB  The Bibites 0.5.0 - Linux.zip 84 MB  The Bibites 0.5.0 - Windows 32x.zip 70 MB  The Bibites 0.5.0 - Mac Universal.zip 98 MB  The Bibites 0.5.0 - Windows 64x.zip 73 MB  Development logThe Bibites 0.5.0: Modernity and Progress 28 days agoThe Bibites  v0.4.2: Balance and stability Jun 19, 2022The Bibites  v0.4.1 Mar 28, 2022The Bibites 0.3.0 : Artificial Life With Herding and Viruses Jun 25, 2021It's official, this is launch 🚀🚀🚀! Full-time on The Bibites May 20, 2021Roadmap for the future of the project Ep.3 Procedural Sprites! Jan 24, 2021Roadmap for the future of the project Ep.2 Modules! Jan 11, 2021Roadmap of the future of the Project Ep.1 Dec 28, 2020View all postsCommentsLog in with itch.io to leave a comment.Viewing most recent comments 1 to 40 of 112 · Next page · Last page Waterloo057 hours agoso dowload the game... from where do i enter to it?Reply Davket00520 hours ago!!!Reply Davket00520 hours agohow the hell do I download itReply Victoria_the_cool4 days agoi cant figure out how to save my progressReply Victoria_the_cool4 days agoyou should probably implement in-game save filesReply Riptides_storm2 days agohit settings top right, save gameleads to a menu where u can name the save file for 0.5 btwReply JoeKing295 days agoWill you add android version? (if it is possible)Reply TheSmartBanana5 days agoIs there an opotion that allows you to paint or upload your own bit parts like texture packs in minecraft?Reply Filipcucumer18 days agoIf i have a problem how do i report it?Reply R0fael25 days ago(+1)It's the best simulator of lifeReply The Bibites25 days ago(+1)Thanks!Reply R0fael23 days ago (1 edit) Can you fix 0 fps when you have more than 100 bibitesReply johnnysmith10 days agoprobably a hardware issue (bad computer)Reply R0fael7 days agono, it can run windows 11Reply Riptides_storm2 days agothat dosn't really determine how good you computer is.Reply coryedora28 days ago(+1)(-3)how I download the game?Reply Crknite!26 days ago(+10)By completing elementary school.Reply Nikki_Devil31 days ago(+2)I wanted to know, will the Linux version also be compiled to Arm64 processors ? I'd really like to use this on my server but as of now I can't and am stuck with my 15yo 32bit pc :,)Reply EKKN38 days ago(+5)Great game, recommended.Even though there are bugs and uncompleted features, it is a decent and very interesting game.Good luck on developing the game!Reply tosety56 days ago(+5)(-1)running on Ubuntu 20 I get a grey screen and cursor, but nothing elseReply Methisa53 days ago (1 edit) (+4)(-1)same for me on steamdeck running steam os. Happened on v 3.0 and 4.2Reply geomagas27 days ago(+1)Same here!Reply Friday_13th56 days ago(+1)I think we rly need some multi core optimizations. Program struggles a lot when there is a high birth rate bibite developedReply baulerbonduc63 days ago(+2)(-1)hey this don't work on linux.please fix if you canReply raktul89 days ago(+1)I like the o & g binding to find oldest and highest generation respectfully, but would love to add more search features/ toggles between multiple bit bits of the same generation. Not sure how to support the development(as in offering my own time/skills to learn and implement)Reply Garyizcool103 days ago(+2)I got to play it once but now its not letting me go on. not sure if its my computer or some sort of glitch but I'm getting a new computer today so we will see if it works then :). if this has happened to anyone else and they know how to solve it could you please help?Reply ThemonstousBibiteengineer111 days ago(-2)how download simReply Skyper111 days ago(+1)Is there a way to save the simulation?Reply Victoria_the_cool2 days agoi hope he adds in-game save filesReply Magnet Boi117 days ago(+5)When will neural netork editor be available?Reply bloodytomb122 days ago(-1)my own thing is how do i use neural network editorReply Phoenix_185128 days ago(+2)How do you download this on linux/chromebook? It will be very helpful if someone can tell me.Reply sssemil130 days ago(+4)To fix the blue screen on Linux, run with the following parameter: -force-vulkan. Cheers.Reply HotNoob130 days ago(+3)
./'The Bibites.x86_64' -force-vulkanWorks! thx.Reply jinnturtle130 days ago (1 edit) (+5)Stuck on a dark blue screen immediately after running the executable, nothing seems to change even if I let it sit there for a while.OS: distro is ArchLinux running on kernell v5.18.15GPU: Nvidia GeForce 1060s ; driver version 515.57CPU: Intel i5-9400FGame version in question: The Bibites 0.4.2The game/sim looks quite interesting from what I've seen and read of it, well done!Reply juega331131 days ago(+1)Is this going to be on Android at some point?Reply Fiddeou131 days ago(+4)I'm on mac Mojave. I open the game, and when loading it just stops at 50%Reply damiantyler8a58 days ago(+2)same, why does this happen?Reply ViyWolf56 days ago(+3)You need to put the file into the applications folder.Reply IIDisruptII133 days ago (1 edit) (+4)(-1)Linux version is broken.
After the unity flash screen goes away it get's stuck on a dark blue screen.
Nothing at all, gotta alt f4 or tab out to close.I'm on Ubuntu 22.04.1 LTS x86_64, I hope this get's fixed the game looks super dope.Reply TeDe3152 days ago (1 edit) (+2)Add bodyplans and abylity to change them!!!!!!!! PLSReply Morado161 days ago(+3)I'm having difficulties on macOS, the game hangs on 50%; any suggestion?Reply sunusl157 days ago(+1)I am having the same issueReply RottenLynx165 days ago(+3)The game does not work on linux. There's just a dark blue screen after the unity splash screen.Reply someguyplaysitchgames132 days ago(+1)yeah same hereReply R333999174 days ago(+2)make an android version pleaseReply lilyhavok179 days ago(+1)As a fan of Framsticks, Artificial LIfe ENvironment, and AL:RE this is definitely on my watchlist. I love everything so far, and it runs quite well. Do you plan to allow creating Bibites through genetic programming?Reply String Studios184 days agoWhere source code?Reply helsy185 days ago(+1)wont load :( stuck on 50% permanentlyReply neccarus180 days ago(+1)Had this happen. Moved it to another folder location and it worked. Seems like it was a permissions errorReply helsy180 days ago(+1)i'll try it out! :D thanks for the replyReply Daevan189 days ago(+1)I tried to download it, sadly my Mac says it can't search for malware and the software needs to be updated to do that. Reply Robotex4193 days ago (2 edits) (+4)some of my bibites evolved to ""herd"" with individual prey, chasing it to eat its meat once it dies. I kind of think that the herding node is too advanced and powerful (it seems to completely overpower things like ""pellet concentration angle""), it could have an internal neural network which can also evolve and change, or a better option would be to make it an input node like pellet concentration angle which would simply cause the bibite to accelerate towards its herd (but only when moving slower) if connected to the accelerate node, or turn towards its herd, if connected to the rotate node, this option could also come with making all input/output nodes also being able to be modifier nodes, allowing for strands like """"pellet concentration angle""-""herding""-""rotate"""" which would cause the bibite to turn towards pellets by a value altered by how close/far said pellet is from the herdReply Robotex4193 days ago(+3)Here's my idea to make bibites able to be just a bit better: memory, for example,  if they see a pellet somewhere but pass by it (and no longer see it). They could, with this adaptation, remember where it is anyway, acting as though they can see it even if it's out of view. A memorized thing would no longer need to be sensed to trigger something like ""pellet concentration angle"". This would all work through a ""commit to memory"" node, which would save the bibites location, direction, and everything it senses to a single memory slot, a ""bite"" if you will, but it would not need any extra nodes to call this info, instead, it would be called by the normal sensing nodes if nothing else is found. there would be a ""memory"" stat, which by default would be 0, and would control the number of ""bites"" a bibite can remember, and once a bibite fills its memory older memories are deleted. There could also be a node that would update/replace a memory slot, and another that would delete a memory slot. This memory system would save many splendid bibites from a lonely fate in the void.Reply Jognh199 days ago(+1)I have a glitch with the latest windows version where no matter how much energy my bibites are getting they have a minimum energy loss. If they end up deep in the negative of energy consumption they still lose energy faster, but there seems to be a point (around 0.1 e/s) where it just doesnt lose less or gain any and all my bibites just dieReply EnchantedAxolotl204 days ago (1 edit) (+1)my strong bibites keep throwing themselves into the void :( they are kinda dumb, but i love this game!Reply Kammcorder205 days ago(+3)i am having the hardest time modding the game, can you please make the guide more easy to understand?Reply Lrapava205 days ago(+2)Fix Linux version plzReplyViewing most recent comments 1 to 40 of 112 · Next page · Last pageitch.io·View all by The Bibites·Report·Embed·Updated  28 days agoGames › Simulation › Free"
https://news.ycombinator.com/rss,VToonify: Controllable high-resolution portrait video style transfer,https://github.com/williamyang1991/VToonify,Comments,"








williamyang1991

/

VToonify

Public




 

Notifications



 

Fork
    238




 


          Star
 2.2k
  









        [SIGGRAPH Asia 2022] VToonify: Controllable High-Resolution Portrait Video Style Transfer
      
License





     View license
    






2.2k
          stars
 



238
          forks
 



 


          Star

  





 

Notifications












Code







Issues
7






Pull requests
1






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







williamyang1991/VToonify









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





0
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




williamyang1991

Update train_vtoonify_d.py




        …
      




        cf993aa
      

Nov 15, 2022





Update train_vtoonify_d.py


cf993aa



Git stats







166

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








checkpoint



Update README.md



Sep 12, 2022









data



Add files via upload



Oct 3, 2022









environment



Add files via upload



Sep 14, 2022









model



Update align_all_parallel.py



Oct 2, 2022









notebooks



使用 Colaboratory 创建



Oct 7, 2022









output



Update readme.md



Sep 12, 2022









LICENSE.md



Update LICENSE.md



Sep 14, 2022









README.md



Update README.md



Oct 13, 2022









smooth_parsing_map.py



Add files via upload



Sep 12, 2022









style_transfer.py



Add files via upload



Sep 12, 2022









train_vtoonify_d.py



Update train_vtoonify_d.py



Nov 15, 2022









train_vtoonify_t.py



Update train_vtoonify_t.py



Sep 16, 2022









util.py



Update util.py



Oct 1, 2022









vtoonify_model.py



Update vtoonify_model.py



Oct 4, 2022




    View code
 


















VToonify - Official PyTorch Implementation
Updates
Web Demo
Installation
(1) Inference for Image/Video Toonification
Inference Notebook
Pre-trained Models
Style Transfer with VToonify-D
Style Transfer with VToonify-T
(2) Training VToonify
Train VToonify-D
Train VToonify-T
(3) Results
Citation
Acknowledgments





README.md




VToonify - Official PyTorch Implementation





overview.mp4





This repository provides the official PyTorch implementation for the following paper:
VToonify: Controllable High-Resolution Portrait Video Style Transfer
Shuai Yang, Liming Jiang, Ziwei Liu and Chen Change Loy
In ACM TOG (Proceedings of SIGGRAPH Asia), 2022.
Project Page | Paper | Supplementary Video | Input Data and Video Results 




Abstract: Generating high-quality artistic portrait videos is an important and desirable task in computer graphics and vision.
Although a series of successful portrait image toonification models built upon the powerful StyleGAN have been proposed,
these image-oriented methods have obvious limitations when applied to videos, such as the fixed frame size, the requirement of face alignment, missing non-facial details and temporal inconsistency.
In this work, we investigate the challenging controllable high-resolution portrait video style transfer by introducing a novel VToonify framework.
Specifically, VToonify leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details. The resulting fully convolutional architecture accepts non-aligned faces in videos of variable size as input, contributing to complete face regions with natural motions in the output.
Our framework is compatible with existing StyleGAN-based image toonification models to extend them to video toonification, and inherits appealing features of these models for flexible style control on color and intensity.
This work presents two instantiations of VToonify built upon Toonify and DualStyleGAN for collection-based and exemplar-based portrait video style transfer, respectively.
Extensive experimental results demonstrate the effectiveness of our proposed VToonify framework over existing methods in generating high-quality and temporally-coherent artistic portrait videos with flexible style controls.

Features:
High-Resolution Video (>1024, support unaligned faces) | Data-Friendly (no real training data) | Style Control

Updates

[10/2022] Integrate Gradio interface into Colab notebook. Enjoy the web demo!
[10/2022] Integrated to 🤗 Hugging Face. Enjoy the web demo!
[09/2022] Input videos and video results are released.
[09/2022] Paper is released.
[09/2022] Code is released.
[09/2022] This website is created.

Web Demo
Integrated into Huggingface Spaces 🤗 using Gradio. Try out the Web Demo 
Installation
Clone this repo:
git clone https://github.com/williamyang1991/VToonify.git
cd VToonify
Dependencies:
We have tested on:

CUDA 10.1
PyTorch 1.7.0
Pillow 8.3.1; Matplotlib 3.3.4; opencv-python 4.5.3; Faiss 1.7.1; tqdm 4.61.2; Ninja 1.10.2

All dependencies for defining the environment are provided in environment/vtoonify_env.yaml.
We recommend running this repository using Anaconda (you may need to modify vtoonify_env.yaml to install PyTorch that matches your own CUDA version following https://pytorch.org/):
conda env create -f ./environment/vtoonify_env.yaml
If you have a problem regarding the cpp extention (fused and upfirdn2d), or no GPU is available, you may refer to CPU compatible version.

(1) Inference for Image/Video Toonification
Inference Notebook

To help users get started, we provide a Jupyter notebook found in ./notebooks/inference_playground.ipynb that allows one to visualize the performance of VToonify.
The notebook will download the necessary pretrained models and run inference on the images found in ./data/.
Pre-trained Models
Pre-trained models can be downloaded from Google Drive, Baidu Cloud (access code: sigg) or Hugging Face:


BackboneModelDescription


DualStyleGANcartoonpre-trained VToonify-D models and 317 cartoon style codes


caricaturepre-trained VToonify-D models and 199 caricature style codes


arcanepre-trained VToonify-D models and 100 arcane style codes


comicpre-trained VToonify-D models and 101 comic style codes


pixarpre-trained VToonify-D models and 122 pixar style codes


illustrationpre-trained VToonify-D models and 156 illustration style codes


Toonifycartoonpre-trained VToonify-T model


caricaturepre-trained VToonify-T model


arcanepre-trained VToonify-T model


comicpre-trained VToonify-T model


pixarpre-trained VToonify-T model


Supporting model 


encoder.ptPixel2style2pixel encoder to map real faces into Z+ space of StyleGAN


faceparsing.pthBiSeNet for face parsing from face-parsing.PyTorch


The downloaded models are suggested to be arranged in this folder structure.
The VToonify-D models are named with suffixes to indicate the settings, where

_sXXX: supports only one fixed style with XXX the index of this style.

_s without XXX means the model supports examplar-based style transfer


_dXXX: supports only a fixed style degree of XXX.

_d without XXX means the model supports style degrees ranging from 0 to 1


_c: supports color transfer.

Style Transfer with VToonify-D
✔ A quick start HERE
Transfer a default cartoon style onto a default face image ./data/077436.jpg:
python style_transfer.py --scale_image
The results are saved in the folder ./output/, where 077436_input.jpg is the rescaled input image to fit VToonify (this image can serve as the input without --scale_image) and 077436_vtoonify_d.jpg is the result.

Specify the content image and the model, control the style with the following options:

--content: path to the target face image or video
--style_id: the index of the style image (find the mapping between index and the style image here).
--style_degree (default: 0.5): adjust the degree of style.
--color_transfer(default: False): perform color transfer if loading a VToonify-Dsdc model.
--ckpt: path of the VToonify-D model. By default, a VToonify-Dsd trained on cartoon style is loaded.
--exstyle_path: path of the extrinsic style code. By default, codes in the same directory as --ckpt are loaded.
--scale_image: rescale the input image/video to fit VToonify (highly recommend).
--padding (default: 200, 200, 200, 200): left, right, top, bottom paddings to the eye center.

Here is an example of arcane style transfer:
python style_transfer.py --content ./data/038648.jpg \
       --scale_image --style_id 77 --style_degree 0.5 \
       --ckpt ./checkpoint/vtoonify_d_arcane/vtoonify_s_d.pt \
       --padding 600 600 600 600     # use large padding to avoid cropping the image

Specify --video to perform video toonification:
python style_transfer.py --scale_image --content ./data/YOUR_VIDEO.mp4 --video
The above style control options (--style_id, --style_degree, --color_transfer) also work for videos.
Style Transfer with VToonify-T
Specify --backbone as ''toonify'' to load and use a VToonify-T model.
python style_transfer.py --content ./data/038648.jpg \
       --scale_image --backbone toonify \
       --ckpt ./checkpoint/vtoonify_t_arcane/vtoonify.pt \
       --padding 600 600 600 600     # use large padding to avoid cropping the image

In VToonify-T, --style_id, --style_degree, --color_transfer, --exstyle_path are not used.
As with VToonify-D, specify --video to perform video toonification.

(2) Training VToonify
Download the supporting models to the ./checkpoint/ folder and arrange them in this folder structure:



Model
Description




stylegan2-ffhq-config-f.pt
StyleGAN model trained on FFHQ taken from rosinality


encoder.pt
Pixel2style2pixel encoder that embeds FFHQ images into StyleGAN2 Z+ latent code


faceparsing.pth
BiSeNet for face parsing from face-parsing.PyTorch


directions.npy
Editing vectors taken from LowRankGAN for editing face attributes


Toonify | DualStyleGAN
pre-trained stylegan-based toonification models



To customize your own style, you may need to train a new Toonify/DualStyleGAN model following here.
Train VToonify-D
Given the supporting models arranged in the default folder structure, we can simply pre-train the encoder and train the whole VToonify-D by running
# for pre-training the encoder
python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_d.py \
       --iter ITERATIONS --stylegan_path DUALSTYLEGAN_PATH --exstyle_path EXSTYLE_CODE_PATH \
       --batch BATCH_SIZE --name SAVE_NAME --pretrain
# for training VToonify-D given the pre-trained encoder
python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_d.py \
       --iter ITERATIONS --stylegan_path DUALSTYLEGAN_PATH --exstyle_path EXSTYLE_CODE_PATH \
       --batch BATCH_SIZE --name SAVE_NAME                  # + ADDITIONAL STYLE CONTROL OPTIONS
The models and the intermediate results are saved in ./checkpoint/SAVE_NAME/ and ./log/SAVE_NAME/, respectively.
VToonify-D provides the following STYLE CONTROL OPTIONS:

--fix_degree: if specified, model is trained with a fixed style degree (no degree adjustment)
--fix_style: if specified, model is trained with a fixed style image (no examplar-based style transfer)
--fix_color: if specified, model is trained with color preservation (no color transfer)
--style_id: the index of the style image (find the mapping between index and the style image here).
--style_degree (default: 0.5): the degree of style.

Here is an example to reproduce the VToonify-Dsd on Cartoon style and the VToonify-D specialized for a mild toonification on the 26th cartoon style:
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_d.py \
       --iter 30000 --stylegan_path ./checkpoint/cartoon/generator.pt --exstyle_path ./checkpoint/cartoon/refined_exstyle_code.npy \
       --batch 1 --name vtoonify_d_cartoon --pretrain      
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_d.py \
       --iter 2000 --stylegan_path ./checkpoint/cartoon/generator.pt --exstyle_path ./checkpoint/cartoon/refined_exstyle_code.npy \
       --batch 4 --name vtoonify_d_cartoon --fix_color 
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_d.py \
       --iter 2000 --stylegan_path ./checkpoint/cartoon/generator.pt --exstyle_path ./checkpoint/cartoon/refined_exstyle_code.npy \
       --batch 4 --name vtoonify_d_cartoon --fix_color --fix_degree --style_degree 0.5 --fix_style --style_id 26
Note that the pre-trained encoder is shared by different STYLE CONTROL OPTIONS. VToonify-D only needs to pre-train the encoder once for each DualStyleGAN model.
Eight GPUs are not necessary, one can train the model with a single GPU with larger --iter.
Tips: [how to find an ideal model] we can first train a versatile model VToonify-Dsd,
and navigate around different styles and degrees. After finding the ideal setting, we can then train the model specialized in that setting for high-quality stylization.
Train VToonify-T
The training of VToonify-T is similar to VToonify-D,
# for pre-training the encoder
python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_t.py \
       --iter ITERATIONS --finetunegan_path FINETUNED_MODEL_PATH \
       --batch BATCH_SIZE --name SAVE_NAME --pretrain       # + ADDITIONAL STYLE CONTROL OPTION
# for training VToonify-T given the pre-trained encoder
python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_t.py \
       --iter ITERATIONS --finetunegan_path FINETUNED_MODEL_PATH \
       --batch BATCH_SIZE --name SAVE_NAME                  # + ADDITIONAL STYLE CONTROL OPTION
VToonify-T only has one STYLE CONTROL OPTION:

--weight (default: 1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0): 18 numbers indicate how the 18 layers of the ffhq stylegan model and the finetuned model are blended to obtain the final Toonify model. Here is the --weight we use in the paper for different styles. Please refer to toonify for the details.

Here is an example to reproduce the VToonify-T model on Arcane style:
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_t.py \
       --iter 30000 --finetunegan_path ./checkpoint/arcane/finetune-000600.pt \
       --batch 1 --name vtoonify_t_arcane --pretrain --weight 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1 1 1 1 1 1 1 1 1 1 1
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_t.py \
       --iter 2000 --finetunegan_path ./checkpoint/arcane/finetune-000600.pt \
       --batch 4 --name vtoonify_t_arcane --weight 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1 1 1 1 1 1 1 1 1 1 1

(3) Results
Our framework is compatible with existing StyleGAN-based image toonification models to extend them to video toonification, and inherits their appealing features for flexible style control. With DualStyleGAN as the backbone, our VToonify is able to transfer the style of various reference images and adjust the style degree in one model.





joint.style.and.degree.control.mp4





Here are the color interpolated results of VToonify-D and VToonify-Dc on Arcane, Pixar and Comic styles.





styles.mp4





Citation
If you find this work useful for your research, please consider citing our paper:
@article{yang2022Vtoonify,
  title={VToonify: Controllable High-Resolution Portrait Video Style Transfer},
  author={Yang, Shuai and Jiang, Liming and Liu, Ziwei and Loy, Chen Change},
  journal={ACM Transactions on Graphics (TOG)},
  volume={41},
  number={6},
  articleno={203},
  pages={1--15},
  year={2022},
  publisher={ACM New York, NY, USA},
  doi={10.1145/3550454.3555437},
}
Acknowledgments
The code is mainly developed based on stylegan2-pytorch, pixel2style2pixel and DualStyleGAN.









About

      [SIGGRAPH Asia 2022] VToonify: Controllable High-Resolution Portrait Video Style Transfer
    
Topics



  style-transfer


  face


  siggraph-asia


  stylegan2


  toonify


  video-style-transfer



Resources





      Readme
 
License





     View license
    



Stars





2.2k
    stars

Watchers





54
    watching

Forks





238
    forks







    Releases

No releases published






    Packages 0


        No packages published 







    Contributors 4





 



 



 



 







Languages












Jupyter Notebook
91.0%







Python
8.3%







Other
0.7%











"
https://news.ycombinator.com/rss,ZSWatch – Open-source Zephyr-based smartwatch,https://github.com/jakkra/ZSWatch,Comments,"








jakkra

/

ZSWatch

Public




 

Notifications



 

Fork
    5




 


          Star
 204
  









        ZSWatch - the Open Source Zephyr™ based Smartwatch, including both HW and FW.
      
License





     MIT license
    






204
          stars
 



5
          forks
 



 


          Star

  





 

Notifications












Code







Issues
1






Pull requests
1






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







jakkra/ZSWatch









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





0
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




jakkra

Merge remote-tracking branch 'origin/main' into main




        …
      




        b0bbf79
      

Jan 14, 2023





Merge remote-tracking branch 'origin/main' into main


b0bbf79



Git stats







123

                      commits
                    







Files

Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github


 


 









CAD


 


 









ZSWatch-kicad


 


 









app


 


 









schematic


 


 









.gitignore


 


 









LICENCE


 


 









README.md


 


 




    View code
 



















ZSWatch
Hardware Features in ZSWatch v1
Upcoming Hardware features in ZSWatch v2
Charger/Dock
Enclosure/Casing
Software Features
Larger not yet implemented SW Features and TODOs
Android phone communication
PCB
ZSWatch in action
Writing apps for the Application Manager
Dock





README.md





ZSWatch


  The ZSWatch v1



Smartwatch built from scratch, both hardware and software. Built on the Zephyr™ Project RTOS, hence the name ZSWatch - Zephyr Smartwatch.

Hardware Features in ZSWatch v1

nRF52833 BLE chip (u-blox ANNA-B402 module).
1.28"" 240x240 IPS TFT Circular Display with GC9A01 driver.
Accelerometer for step counting etc. (LIS2DS12TR).
Pulse oximetry and heartrate using (MAX30101EFD)).
Vibration motor with haptics driver to give better vibration control (DRV2603RUNT).
External 8MB flash (MX25R6435FZNIL0).
Battery charger and battery supervisor (MAX1811ESA+ datasheet, TLV840MAPL3).
3 buttons for navigation (prev/next/enter)
220 mAh Li-Po battery.
Sapphire Crystal Glass to protect the display.

Upcoming Hardware features in ZSWatch v2

nRF5340 BLE chip (u-blox NORA-B10 module)
Touch screen with same size and features as v1
8MB external flash will probably be removed due to larger size of u-blox NORA-B10 vs. ANNA-B402.
Find another way to dock the clock for charging and programming, maybe can find some connector similar to what smartwatches normally have.

Charger/Dock
Basic pogo-pin dock that connects the power and SWD pins to the bottom of the watch.
Enclosure/Casing
3D printed casing with 3D printed buttons. Does it's job, but for revision v2 of the watch I'll probably do something CNC'd for nicer looks.
Software Features

Bluetooth LE communications with GadgetBridge Android app.
Also support Bluetooth Direction Finding so the watch can act as a tag and is trackable using any u-blox AoA antenna board
Watchface that shows:

Standard stuff as time, date, battery
Weather
Step count
Number unread notifications
Heart rate (not implemented yet however)


Pop-up notifications
Setting menu system, with easy extendability
Application picker and app concept

Music control app
Settings app
etc.


Step counting

Larger not yet implemented SW Features and TODOs

Heart rate, right now only samples the raw data, but no heart rate is calculated from it.
Proper BLE pairing, currently removed due to flash constraints (fixed by nRF5340 upgrade).
Watchface should also be an application.
Refactoring of main.c, should have way less logic, utlize Zephyr architecture more.

Android phone communication
Fortunately there is a great Android app called GadgetBridge which handles everything needed on the phone side, such as notifications management, music control and so much more... The ZSWatch right now pretends to be one of the supported Smart Watches in Gadgetbridge, following the same API as it does. In future there may be a point adding native support, we'll see.
PCB
A 4 layer board which measures 36mm in diameter designed in KiCad.








ZSWatch in action



Music control
Accelerometer for step count and tap detection




 object-fit=""cover""



Notifications from phone (Gmail here)
Settings







Writing apps for the Application Manager
Check out the sample application for the general app design. The main idea is each app have an <app_name>_app.c file which registers the app, chooses icon and drives the logic for the app. Then there should be one or more files named for example <app_name>_ui.c containing pure LVGL code with no dependencies to Zephyr or the watch software. The idea is that this UI code should be runnable in a LVGL simulator to speed up development of UI, however right now that's not set up yet. The <app_name>_app.c will do all logic and call functions in <app_name>_ui.c to update the UI accordingly.
Each application needs to have a way to close itself, for example a button, and then through callback tell the application_manager.c to close the app:
When user clicks an app in the app picker:

application_manager.c deletes it's UI elements and calls the application_start_fn.
<app_name>_app.c will do necessary init and then call the <app_name>_ui.c to draw the app UI.
User can now navigate arund and the application and do whatever.

When user for example presses a close button in the application:

Typically a callback from the UI code in <app_name>_ui.c will call <app_name>_app.c to tell that user requested to close the app. <app_name>_app.c will notify application_manager.c that it want to close itself. application_manager.c will then call <app_name>_app.c application_stop_fn and <app_name>_app.c will tell UI to close then do necessary de-init and return.
application_manager.c will now draw the app picker again.

The application manager can also at any time close a running application by calling it's application_stop_fn.
Dock
Very basic, will be re-worked for next watch revision v2.












About

      ZSWatch - the Open Source Zephyr™ based Smartwatch, including both HW and FW.
    
Topics



  bluetooth


  ble


  smartwatch


  zephyr


  nrf52


  lvgl


  angle-of-arrival


  nrf-connect


  nordic-semiconductor


  nrf53


  nrf-connect-sdk


  zswatch



Resources





      Readme
 
License





     MIT license
    



Stars





204
    stars

Watchers





5
    watching

Forks





5
    forks







    Releases

No releases published




Languages











C
99.9%







Other
0.1%











"
https://news.ycombinator.com/rss,Four thousand weeks,https://leebyron.com/4000/,Comments,"



















Four Thousand Weeks










FourThousandWeeks


          A tribute to
          the book by
          Oliver Burkeman, an exploration of time management in the
          face of human finitude, and addressing the anxiety of “getting
          everything done.”
        

          To begin, enter when were you born
          This site uses no cookies nor saves your information




Scroll on...




We live our livesweek by week

















Yet a week feels frustratingly limited

        The pressure to be more productive and fit ever-increasing quantities of
        activity into a stubbornly non-increasing quantity of time leads to
        productivity anxiety, shriveled attention spans, and burn-out.
      


And there are alarmingly few of them

        You would feel less anxious about wasting an evening doom-scrolling if
        you had an infinite amount of them. Somehow either doing too much or too
        little can create the sense of wasting time.
      

        Despite all this activity we sense there are important and fulfilling
        ways we could be spending our time, even if we can’t say exactly what
        they are. Yet, we systematically spend our time doing other things to
        get by instead.
      


          I (like many others) felt a wrongness in the world. Life, I knew, was
          supposed to be more joyful than this, more real, more meaningful. We
          were not supposed to hate Mondays and live for the weekends and
          holidays. We were not supposed to have to raise our hands to be
          allowed to pee.
        
Charles Eisenstien




The average human life is only four thousand weeks


        Scientists estimate that life, in some form, will persist for another
        1.5 billion years or more, until the intensifying heat of the sun
        condemns the last organism to death.
      

        But you? Assuming you live to be eighty, you’ll have had about four
        thousand weeks. The rare few lucky enough to become a centenarian will
        see only five thousand.
      
That’s absurdly, terrifyingly, insultingly short.




          You have lived  of them so far
        


        You likely have many more weeks ahead of you. The psychologist
        Erik Erikson suggests that at this phase of life you focus
        on the virtues of competence and fidelity. Allow yourself failures in
        the spirit of discovering and developing your personal identity and
        priorities so that your future weeks can be lived well with intention
        and purpose.
      

        That’s a significant amount of the weeks you’ll see. The psychologist
        Erik Erikson suggests that at this phase of life you focus
        on the virtue of love. Share yourself more intimately with others and
        invest in happy relationships so that your future weeks can be lived
        well with companionship and purpose.
      

        That’s likely a majority of the weeks you’ll see. The psychologist
        Erik Erikson suggests that at this phase of life you focus
        on the virtue of care. Spend your weeks “making your mark” by
        intentionally nurturing things that will outlast you, raising children,
        mentoring others, becoming involved in your community and organizations,
        and creating positive change that benefits others.
      

        You’re likely well aware of your own finitude having lived the large
        majority of the weeks you’ll see. The psychologist
        Erik Erikson suggests that at this phase of life you focus
        on the virtue of wisdom. Accept and appreciate your accomplishments so
        far as a life well lived. Continue to nurture things that will outlast
        you and mentor others, spend your weeks intentionally on your true
        priorities, and appreciate novelty in the mundane.
      

        You’re no doubt well aware of your own finitude as one of the lucky ones
        to live well past four thousand weeks. The psychologist
        Erik Erikson suggests that at this phase of life you focus
        on the virtue of wisdom. Accept and appreciate your accomplishments so
        far as a life well lived. Spend every remaining week intentionally on
        your true priorities and appreciate novelty in the mundane.
      


Productivity is a trap

        There are numerous techniques, products, and services to squeeze the
        most productivity from your week. The problem isn’t that these don’t
        work, it’s that they do work. And yet paradoxically you only
        feel busier, more anxious, and somehow emptier as a result.
      

        The day will never arrive when you finally have everything under
        control—when the flood of emails has been contained; when your to-do
        lists have stopped getting longer; when you’re meeting all your
        obligations at work and in your home life; when nobody’s angry with you
        for missing a deadline or dropping the ball; and when the fully
        optimized person you’ve become can turn, at long last, to the things
        life is really supposed to be about.
      

        Let’s start by admitting defeat: none of this is ever going to happen.
      


          Time feels like an unstoppable conveyer belt, bringing us new tasks as
          fast as we can dispatch the old ones; and becoming “more productive”
          just seems to cause the belt to speed up.
        
Edward T. Hall



Adopt a limit-embracing attitude

        If you truly don’t have time for everything you want to do, or feel you
        ought to do, or that others are badgering you to do, then, well, you
        don’t have time—no matter how grave the consequences of failing to do it
        all might prove to be. So, technically, it’s irrational to feel troubled
        by an overwhelming to-do list. You’ll do what you can, you won’t do what
        you can’t, and the tyrannical inner voice insisting that you must do
        everything is simply mistaken.
      

        We rarely stop to consider things so rationally, though, because that
        would mean confronting the painful truth of our limitations.
      

        Surrender to the reality that things just take the time they take, and
        that you can’t quiet your anxieties by working faster, because it isn’t
        within your power to force reality’s pace as much as you feel you need
        to, and because the faster you go, the faster you’ll feel you need to
        go.
      


          Which of us truly lives on twenty-four hours a day? Which of us is not
          saying: “I shall alter that when I have a little more time?” We never
          shall have any more time. We have, and we have always had, all the
          time there is.
        
Arnold Bennett



How you spend your time is a choice

        We are forced to accept that there will always be too much to do; that
        you can’t make the world run at your preferred speed and so there are
        tough choices to be made: which balls to let drop, which people to
        disappoint, which cherished ambitions to abandon, which roles to fail
        at.
      

        Once you truly understand that you’re guaranteed to miss out on almost
        every experience the world has to offer, the fact that there are so many
        you still haven’t experienced stops feeling like a problem. Instead, you
        get to focus on fully enjoying the tiny slice of experiences you
        actually do have time for. Digging in to a challenging project that
        can’t be hurried becomes not a trigger for stressful emotions but a
        bracing act of choice.
      


The importance of rest

        A real risk of doing too much is finding your work time, in attempt to
        be productive, encroaching on an evening’s rest. Rest as it turns
        out—whether in the evening, over a weekend, or a long vacation—is
        critical for productive creative work. Its absence can lead to stress,
        burnout, and counterintuitively overall poor performance despite the
        extra hours worked.
      

        Though why should vacations or lazy mornings need defending in terms of
        improved work performance? Enjoying leisure for its own sake—which is
        the whole point of leisure—should not feel as though you’re failing at
        life. Leisure is not merely an opportunity for recovery and
        replenishment for the purposes of further work, but for its intrinsic
        satisfactions.
      


          I have to die. If it is now, well then I die now; if later, then now I
          will take my lunch, since the hour for lunch has arrived - and dying I
          will tend to later.
        
Epictetus



The loneliness of temporal sovereignty

        Other human beings are always impinging on your time in countless
        frustrating ways. In an ideal world the only person making decisions
        about your time is you. However this comes at a cost that’s not worth
        paying.
      

        It’s good to have plenty of time, but having all the time in the world
        isn’t much use if you’re forced to experience it all on your own. To do
        countless important things with time: to socialize, go on dates, raise
        children, launch businesses, start movements; it has to be synchronized
        with other people. In fact, having large amounts of time but no
        opportunity to use it collaboratively can be actively unpleasant.
      

        We treat our time as something to hoard, when it’s better approached as
        something to share. Even if that means surrendering some of your power
        to decide exactly what you do with it and when.
      


          However, the two things must be mingled and varied, solitude and
          joining a crowd: the one will make us long for people and the other
          for ourselves, and each will be a remedy for the other; solitude will
          cure our distaste for a crowd, and a crowd will cure our boredom with
          solitude.
        
Seneca




        Ten tools for embracing finitude
      


1.
Adopt a fixed volume approach to productivity

        Tough choices are inevitable; focus on making them consciously and well.
      

        Keep two to-do lists: an “open” one for everything on your plate,
        doubtlessly nightmarishly long, and “closed” with a fixed number of
        entries, only moving tasks onto it when previous ones have been
        completed.
      

        You’ll never get through all the tasks on the open list, but you were
        never going to in any case. The choice to leave them there is hard, but
        time spent on them is time not spent on the things you chose to focus
        on.
      

        Establish pre-determined time boundaries on your work, and make
        decisions in light of those limits. If your primary goal is to do what’s
        required to be finished by 5:30 you’ll be aware of the constraints on
        your time and motivated to use it wisely.
      


2.
Serialize,serialize,serialize

        Focus on one big project at a time, and see it to completion before
        moving onto the next.
      

        It’s alluring to try to alleviate the anxiety of having too many
        responsibilities or ambitions by getting started on them all at once,
        but you’ll make little progress that way. Instead, train yourself to get
        incrementally better at tolerating that anxiety by consciously
        postponing everything you possibly can except for one thing.
      

        Soon the satisfaction of completing important projects will make that
        anxiety feel worthwhile, and as you complete them you’ll have less to be
        anxious about anyway.
      


3.
Strategic underachievement

        Simply because your time is finite, you’ll inevitably underachieve at
        something. When you can’t do it all, you can feel ashamed and give up.
        When you decide in advance what to fail at, you remove the sting of
        shame.
      

        Nominate in advance whole areas of life in which you won’t expect
        excellence from yourself. Instead focus that time more effectively, and
        you won’t be surprised when you fail at what you planned to fail at all
        along.
      


4.
Celebrate wins

        The to-do list will never be finished. Inbox zero will inevitably
        refill. There’s an unhelpful assumption that you begin each morning with
        a productivity debt that you must pay off with hard work to achieve a
        zero-balance by evening.
      

        Keep a “done” list which starts empty and fills up over the day. You
        could have spent the day doing nothing remotely constructive, and look
        what you did instead! Lower the bar for what gets to count as an
        accomplishment; small wins accrue.
      


5.
Consolidate care

        The attention economy demands urgency, bringing a litany of demands for
        your care every day. Consciously choose your battles in industry,
        charity, activism, and politics.
      

        To make a real difference, you must focus your finite capacity for care.
      


6.
Embrace boring & single-purpose technology

        Modern digital devices offer distraction to a place where painful human
        limitations do not apply; you need never feel bored or constrained in
        your freedom of action—which isn’t the case when it comes to work that
        matters.
      

        Combat this by making your devices boring. Remove apps that distract
        (even consider Slack or Email). Switch your screen to grayscale. Use
        time-limiting reminders.
      

        Choose single-purpose devices like an e-reader where it’s tedious and
        awkward to do anything but read. If distracting apps are only a swipe
        away they’ll prove impossible to resist when the first twinge of boredom
        or difficulty of focus arises.
      


7.
Seek novelty in the mundane

        The fewer weeks we have left the faster we seem to lose them. The
        likeliest explanation for this phenomenon is that our brains encode the
        passing of time on the basis of how much information we process in any
        given interval.
      

        Cramming your life with novel experiences does work, but can also lead
        to existential overwhelm and is also impractical, especially if you have
        a job or children.
      

        Alternatively pay more attention to every moment no matter how mundane.
        Plunge into the life you already have with twice the intensity and your
        life will feel twice as full and will be remembered as lasting twice as
        long. Meditation, going on unplanned walks, photography, journaling,
        anything that draws your attention more fully to the present.
      


8.
Be a researcher in relationships

        When presented with a challenging or boring moment with another person,
        deliberately adopt an attitude of curiosity in which your goal isn’t to
        achieve any particular outcome or explain your position but to figure
        out who this human being is who we’re with.
      

        This curiosity is well suited to the unpredictability of life with
        others because it can be satisfied by their behaving in ways you like or
        dislike whereas the stance of demanding a certain result is frustrated
        each time things fail to go your way.
      


9.
Cultivate instantaneous generosity

        Whenever a generous impulse arises your mind: to give money, to check in
        on a friend, send an email praising someone’s work, act on that impulse
        right away. If you put it off for whatever reason, you’ll likely not get
        back to it. The only acts of generosity that count are the ones you’re
        actually making.
      

        People are social creatures, and generous action reliably makes us feel
        much happier.
      


10.
Practice doing nothing

        When it comes to the challenge of using your four thousand weeks well,
        the capacity to do nothing is indispensable. If you can’t bear the
        discomfort of not acting you’re far more likely to make poor choices
        with your time simply to feel as if you’re acting. Calm down, gain
        autonomy over your choices, and make better ones.
      
Do nothing meditation

Set a timer, even for only five minutes.
Sit in a chair and then stop trying to do anything.

          Every time you notice you’re doing something, including thinking or
          focusing on your breathing, stop doing it.
        

          If you notice you’re criticizing yourself inwardly for doing things
          well… that’s a thought too so stop doing that.
        
Keep on stopping until the timer goes off.





        Thanks for reading this tribute to
        Four Thousand Weeks, by Oliver Burkeman.
      

        This page is comprised of themes and excerpts from the book. If you’ve
        scrolled this far you should absolutely read it in its entirety.
      

        Set in
        Playfair 2.0
        by Claus Eggers


        Made and open-sourced by
        Lee Byron
        with ♥ in San Francisco.
      
✌︎



"
https://news.ycombinator.com/rss,Use.GPU Goes Trad,https://acko.net/blog/use-gpu-goes-trad/,Comments,"



Use.GPU Goes Trad — Acko.net































Hackery, Math & Design
Steven Wittens i













Home







Home






January 14, 2023
Use.GPU Goes Trad


Old is new again




I've released a new version of Use.GPU, my experimental reactive/declarative WebGPU framework, now at version 0.8.
My goal is to make GPU rendering easier and more sane. I do this by applying the lessons and patterns learned from the React world, and basically turning them all up to 11, sometimes 12. This is done via my own Live run-time, which is like a martian React on steroids.
The previous 0.7 release was themed around compute, where I applied my shader linker to a few challenging use cases. It hopefully made it clear that Use.GPU is very good at things that traditional engines are kinda bad at.
In comparison, 0.8 will seem banal, because the theme was to fill the gaps and bring some traditional conveniences, like:

Scenes and nodes with matrices
Meshes with instancing
Shadow maps for lighting
Visibility culling for geometry






These were absent mostly because I didn't really need them, and they didn't seem like they'd push the architecture in novel directions. That's changed however, because there's one major refactor underpinning it all: the previously standard forward renderer is now entirely swappable. There is a shiny deferred-style renderer to showcase this ability, where lights are rendered separately, using a g-buffer with stenciling.
This new rendering pipeline is entirely component-driven, and fully dogfooded. There is no core renderer per-se: the way draws are realized depends purely on the components being used. It effectively realizes that most elusive of graphics grails, which established engines have had difficulty delivering on: a data-driven, scriptable render pipeline, that mortals can hopefully use.





Root of the App



Deep inside the tree


I've spent countless words on Use.GPU's effect-based architecture in prior posts, which I won't recap. Rather, I'll just summarize the one big trick: it's structured entirely as if it needs to produce only 1 frame. Then in order to be interactive, and animate, it selectively rewinds parts of the program, and reactively re-runs them. If it sounds crazy, that's because it is. And yet it works.
So the key point isn't the feature list above, but rather, how it does so. It continues to prove that this way of coding can pay off big. It has all the benefits of immediate-mode UI, with none of the downsides, and tons of extensibility. And there are some surprises along the way.
Real Reactivity
You might think: isn't this a solved problem? There are plenty of JS 3D engines. Hasn't React-Three-Fiber (R3F) shown how to make that declarative? And aren't these just web versions of what native engines like Unreal and Unity already do well, and better?
My answer is no, but it might not be clear why. Let me give an example from my current job.







My client needs a specialized 3D editing tool. In gaming terms you might think of it as a level design tool, except the levels are real buildings. The details don't really matter, only that they need a custom 3D editing UI. I've been using Three.js and R3F for it, because that's what works today and what other people know.
Three.js might seem like a great choice for the job: it has a 3D scene, editing controls and so on. But, my scene is not the source of truth, it's the output of a process. The actual source of truth being live-edited is another tree that sits before it. So I need to solve a two-way synchronization problem between both. This requires careful reasoning about state changes.





Change handlers in Three.js and R3F


Sadly, the way Three.js responds to changes is ill-defined. As is common, its objects have ""dirty"" flags. They are resolved and cleared when the scene is re-rendered. But this is not an iron rule: many methods do trigger a local refresh on the spot. Worse, certain properties have an invisible setter, which immediately triggers a ""change"" event when you assign a new value to it. This also causes derived state to update and cascade, and will be broadcast to any code that might be listening.
The coding principle applied here is ""better safe than sorry"". Each of these triggers was only added to fix a particular stale data bug, so their effects are incomplete, creating two big problems. Problem 1 is a mix of old and new state... but problem 2 is you can only make it worse, by adding even more pre-emptive partial updates, sprinkled around everywhere.
These ""change"" events are oblivious to the reason for the change, and this is actually key: if a change was caused by a user interaction, the rest of the app needs to respond to it. But if the change was computed from something else, then you explicitly don't want anything earlier to respond to it, because it would just create an endless cycle, which you need to detect and halt.


R3F introduces a declarative model on top, but can't fundamentally fix this. In fact it adds a few new problems of it own in trying to bridge the two worlds. The details are boring and too specific to dig into, but let's just say it took me a while to realize why my objects were moving around whenever I did a hot-reload, because the second render is not at all the same as the first.
Yet this is exactly what one-way data flow in reactive frameworks is meant to address. It creates a fundamental distinction between the two directions: cascading down (derived state) vs cascading up (user interactions). Instead of routing both through the same mutable objects, it creates a one-way reverse-path too, triggered only in specific circumstances, so that cause and effect are always unambigious, and cycles are impossible.
Three.js is good for classic 3D. But if you're trying to build applications with R3F it feels fragile, like there's something fundamentally wrong with it, that they'll never be able to fix. The big lesson is this: for code to be truly declarative, changes must not be allowed to travel backwards. They must also be resolved consistently, in one big pass. Otherwise it leads to endless bug whack-a-mole.
What reactivity really does is take cache invalidation, said to be the hardest problem, and turn the problem itself into the solution. You never invalidate a cache without immediately refreshing it, and you make that the sole way to cause anything to happen at all. Crazy, and yet it works.
When I tell people this, they often say ""well, it might work well for your domain, but it couldn't possibly work for mine."" And then I show them how to do it.


Figuring out which way your cube map points:just gfx programmer things.

And... Scene
One of the cool consequences of this architecture is that even the most traditional of constructs can suddenly bring neat, Lispy surprises.
The new scene system is a great example. Contrary to most other engines, it's actually entirely optional. But that's not the surprising part.
Normally you just have a tree where nodes contain other nodes, which eventually contain meshes, like this:
<Scene>
  <Node matrix={...}>
    <Mesh>
    <Mesh>
  <Node matrix={...}>
    <Mesh>
    <Node matrix={...}>
      <Mesh>
      <Mesh>


It's a way to compose matrices: they cascade and combine from parent to child. The 3D engine is then built to efficiently traverse and render this structure.
But what it ultimately does is define a transform for every mesh: a function vec3 => vec3 that maps one vertex position to another. So if you squint, <Mesh> is really just a marker for a place where you stop composing matrices and pass a composed matrix transform to something else.
Hence Use.GPU's equivalent, <Primitive>, could actually be called <Unscene>. What it does is escape from the scene model, mirroring the Lisp pattern of quote-unquote. A chain of <Node> parents is just a domain-specific-language (DSL) to produce a TransformContext with a shader function, one that applies a single combined matrix transform.
In turn, <Mesh> just becomes a combination of <Primitive> and a <FaceLayer>, i.e. triangle geometry that uses the transform. It all composes cleanly.
So if you just put meshes inside the scene tree, it works exactly like a traditional 3D engine. But if you put, say, a polar coordinate plot in there from the plot package, which is not a matrix transform, inside a primitive, then it will still compose cleanly. It will combine the transforms into a new shader function, and apply it to whatever's inside. You can unscene and scene repeatedly, because it's just exiting and re-entering a DSL.
In 3D this is complicated by the fact that tangents and normals transform differently from vertices. But, this was already addressed in 0.7 by pairing each transform with a differential function, and using shader fu to compose it. So this all just keeps working.
Another neat thing is how this works with instancing. There is now an <Instances> component, which is exactly like <Mesh>, except that it gives you a dynamic <Instance> to copy/paste via a render prop:
<Instances
   mesh={mesh}
   render={(Instance) => (<>
     <Instance position={[1, 2, 3]} />
     <Instance position={[3, 4, 5]} />
   </>)
 />


As you might expect, it will gather the transforms of all instances, stuff all of them into a single buffer, and then render them all with a single draw call. The neat part is this: you can still wrap individual <Instance> components in as many <Node> levels as you like. Because all <Instance> does is pass its matrix transform back up the tree to the parent it belongs to.





This is done using Live captures, which are React context providers in reverse. It doesn't violate one-way data flow, because captures will only run after all the children have finished running. Captures already worked previously, the semantics were just extended and formalized in 0.8 to allow this to compose with other reduction mechanisms.


But there's more. Not only can you wrap <Instance> in <Node>, you can also wrap either of them in <Animate>, which is Use.GPU's keyframe animator, entirely unchanged since 0.7:









<Instances
  mesh={mesh}
  render={(Instance) => (

    <Animate
      prop=""rotation""
      keyframes={ROTATION_KEYFRAMES}
      loop
      ease=""cosine""
    >
      <Node>
        {seq(20).map(i => (
          <Animate
            prop=""position""
            keyframes={POSITION_KEYFRAMES}
            loop
            delay={-i * 2}
            ease=""linear""
          >
            <Instance
              rotation={[
                Math.random()*360,
                Math.random()*360,
                Math.random()*360,
              ]}
              scale={[0.2, 0.2, 0.2]}
            />
          </Animate>
        ))}
      </Node>
    </Animate>

  )}
/>


The scene DSL and the instancing DSL and the animation DSL all compose directly, with nothing up my sleeve. Each of these <Components> are still just ordinary functions. On the inside they look like constructors with all the other code missing. There is zero special casing going on here, and none of them are explicitly walking the tree to reach each other. The only one doing that is the reactive run-time... and all it does is enforce one-way data flow by calling functions, gathering results and busting caches in tree order. Because a capture is a long-distance yeet.
Personally I find this pretty magical. It's not as efficient as a hand-rolled scene graph with instancing and built-in animation, but in terms of coding lift it's literally O(0) instead of OO. I needed to add zero lines of code to any of the 3 sub-systems, in order to combine them into one spinning whole.
The entire scene + instancing package clocks in at about 300 lines and that's including empties and generous formatting. I don't need to architect the rest of the framework around a base Object3D class that everything has to inherit from either, which is a-ok in my book.
This architecture will never reach Unreal or Unity levels of hundreds of thousands of draw calls, but then, it's not meant to do that. It embraces the idea of a unique shader for every draw call, and then walks that back if and when it's useful. The prototype map package for example does this, and can draw a whole 3D vector globe in 2 draw calls: fill and stroke. Adding labels would make it 3. And it's not static: it's doing the usual quad-tree of LOD'd mercator map tiles.










Multi-Pass
Next up, the modular renderer passes. Architecturally and reactively-speaking, there isn't much here. This was mainly an exercise in slicing apart the existing glue.
The key thing to grok is that in Use.GPU, the <Pass> component does not correspond to a literal GPU render pass. Rather, it's a virtual, logical render pass. It represents all the work needed to draw some geometry to a screen or off-screen buffer, in its fully shaded form. This seems like a useful abstraction, because it cleanly separates the nitty gritty rendering from later compositing (e.g. overlays).
For the forward renderer, this means first rendering a few shadow maps, and possibly rendering a picking buffer for interaction. For the deferred renderer, this involves rendering the g-buffer, stencils, lights, and so on.
My goal was for the toggle between the two to be as simple as replacing a <ForwardRenderer> with a <DeferredRenderer>... but also to have both of those be flexible enough that you could potentially add on, say, SSAO, or bloom, or a Space Engine-style black hole, as an afterthought. And each <Pass> can have its own renderer, rather than shoehorning everything into one big engine.
Neatly, that's mostly what it is now. The basic principle rests on three pillars.



Deferred rendering


First, there are a few different rendering modes, by default solid vs shaded vs ui. These define what kind of information is needed at every pixel, i.e. the classic varying attributes. But they have no opinion on where the data comes from or what it's used for: that's defined by the geometry layer being rendered. It renders a <Virtual> draw call, which it gives e.g. a getVertex and getFragment shader function with a particular signature for that mode. These functions are not complete shaders, just the core functions, which are linked into a stub. There are a few standard 'tropes' used here, not just these two.
Second, there are a few different rendering buckets, like opaque, transparent, shadow, picking and debug. These are used to group draws into. Different GPU render passes then pick and choose from that. opaque and transparent are drawn to the screen, while shadow is drawn repeatedly into all the shadow maps. This includes sorting front-to-back and back-to-front, as well as culling.
Finally, there's the renderer itself (forward vs deferred), and its associated pass components (e.g. <ColorPass>, <ShadowPass>, <PickingPass>, and so on). The renderer decides how to translate a particular ""mode + bucket"" combination into a concrete draw call, by lowering it into render components (e.g. <ShadedRender>). The pass components decide which buffer to actually render stuff to, and how. So the renderer itself doesn't actually render, it merely spawns and delegates to other components that do.


The forward path works mostly the same as before, only the culling and shadow maps are new... but it's now split up into all its logical parts. And I verified this design by adding the deferred renderer, which is a lot more convoluted, but still needs to do some forward rendering.
It works like a treat, and they use all the same lighting shaders. You can extend any of the 3 pillars just by replacing or injecting a new component. And you don't need to fork either renderer to do so: you can just pick and choose à la carte by selectively overriding or extending its ""mode + bucket"" mapping table, or injecting a new actual render pass.










To really put a bow on top, I upgraded the Use.GPU inspector so that you can directly view any render target in a RenderDoc-like way. This will auto-apply useful colorization shaders, e.g. to visualize depth. This is itself implemented as a Use.GPU Live canvas, sitting inside the HTML-based inspector, sitting on top of Live, which makes this a Live-in-React-in-Live scenario.
For shits and giggles, you can also inspect the inspector's canvas, recursively, ad infinitum. Useful for debugging the debugger:







There are still of course some limitations. If, for example, you wanted to add a new light type, or add support for volumetric lights, you'd have to reach in more deeply to make that happen: the resulting code needs to be tightly optimized, because it runs per pixel and per light. But if you do, you're still going to be able to reuse 90% of the existing components as-is.
I do want a more comprehensive set of light types (e.g. line and area), I just didn't get around to it. Same goes for motion vectors and TXAA. However, with WebGPU finally nearing public release, maybe people will actually help out. Hint hint.







Port of a Reaction Diffusion system by Felix Woitzel.



A Clusterfuck of Textures
A final thing to talk about is 2D image effects and how they work. Or rather, the way they don't work. It seems simple, but in practice it's kind of ludicrous.
If you'd asked me a year ago, I'd have thought a very clean, composable post-effects pipeline was entirely within reach, with a unified API that mostly papered over the difference between compute and render. Given that I can link together all sorts of crazy shaders, this ought to be doable.
Well, I did upgrade the built-in fullscreen conveniences a bit, so that it's now easier to make e.g. a reaction diffusion sim like this (full code):



The devil here is in the details. If you want to process 2D images on a GPU, you basically have several choices:

Use a compute shader or render shader?
Which pixel format do you use?
Are you sampling one flat image or a MIP pyramid of pre-scaled copies?
Are you sampling color images, or depth/stencil images?
Use hardware filtering or emulate filtering in software?

The big problem is that there is no single approach that can handle all cases. Each has its own quirks. To give you a concrete example: if you wrote a float16 reaction-diffusion sim, and then decided you actually needed float32, you'd probably have to rewrite all your shaders, because float16 is always renderable and hardware filterable, but float32 is not.
Use.GPU has a pretty nice set of Compute/Stage/Kernel components, which are elegant on the outside; but they require you to write pretty gnarly shader code to actually use them. On the other side are the RenderToTexture/Pass/FullScreen components which conceptually do the same thing, and have much nicer shader code, but which don't work for a lot of scenarios. All of them can be broken by doing something seemingly obvious, that just isn't natively supported and difficult to check ahead of time.
Even just producing universal code to display any possible texture type on screen becomes a careful exercise in code-generation. If you're familiar with the history of these features, it's understandable how it got to this point, but nevertheless, the resulting API is abysmal to use, and is a never-ending show of surprise pitfalls.
Here's a non-exhaustive list of quirks:

Render shaders are the simplest, but can only be used to write those pixel formats that are ""renderable"".
Compute shaders must be dispatched in groups of N, even if the image size is not a multiple of N. You have to manually trim off the excess threads.
Hardware filtering only works on some formats, and some filtering functions only work in render shaders.
Hardware filtering (fast) uses [0..1] UV float coordinates, software emulation in a shader (slow) uses [0..N] XY uint coordinates.
Reading and writing from/to the same render texture is not allowed, you have to bounce between a read and write buffer.
Depth+stencil images have their own types and have an additional notion of ""aspect"" to select one or both.
Certain texture functions cannot be called conditionally, i.e. inside an if.
Copying from one texture to another doesn't work between certain formats and aspects.

My strategy so far has been to try and stick to native WGSL semantics as much as possible, meaning the shader code you do write gets inserted pretty much verbatim. But if you wanted to paper over all these differences, you'd have to invent a whole new shader dialect. This is a huge effort which I have not bothered with. As a result, compute vs render pretty much have to remain separate universes, even when they're doing 95% the same thing. There is also no easy way to explain to users which one they ought to use.
While it's unrealistic to expect GPU makers to support every possible format and feature on a fast path, there is little reason why they can't just pretend a little bit more. If a texture format isn't hardware filterable, somebody will have to emulate that in a shader, so it may as well be done once, properly, instead of in hundreds of other hand-rolled implementations.
If there is one overarching theme in this space, it's that limitations and quirks continue to be offloaded directly onto application developers, often with barely a shrug. To make matters worse, the ""next gen"" APIs like Metal and Vulkan, which WebGPU inherits from, do not improve this. They want you to become an expert at their own kind of busywork, instead of getting on with your own.
I can understand if the WebGPU designers have looked at the resulting venn-diagram of poorly supported features, and have had to pick their battles. But there's a few absurdities hidden in the API, and many non-obvious limitations, where the API spec suggests you can do a lot more than you actually can. It's a very mixed bag all things considered, and in certain parts, plain retarded. Ask me about minimum binding size. No wait, don't.
* * *
Most promising is that as Use.GPU grows to do more, I'm not touching extremely large parts of it. This to me is the sign of good architecture. I also continue to focus on specific use cases to validate it all, because that's the only way I know how to do it well.
There are some very interesting goodies lurking inside too. To give you an example... that R3F client app I mentioned at the start. It leverages Use.GPU's state package to implement a universal undo/redo system in 130 lines. A JS patcher is very handy to wrangle the WebGPU API's deep argument style, but it can do a lot more.
One more thing. As a side project to get away from the core architecting, I made a viewer for levels for Dark Engine games, i.e. Thief 1 (1998), System Shock 2 (1999) and Thief 2 (2000). I want to answer a question I've had for ages: how would those light-driven games have looked, if we'd had better lighting tech back then? So it actually relights the levels. It's still a work in progress, and so far I've only done slow-ass offline CPU bakes with it, using a BSP-tree based raytracer. But it works like a treat.









I basically don't have to do any heavy lifting if I want to draw something, be it normal geometry, in-place data/debug viz, or zoomable overlays. Integrating old-school lightmaps takes about 10 lines of shader code and 10 lines of JS, and the rest is off-the-shelf Use.GPU. I can spend my cycles working on the problem I actually want to be working on. That to me is the real value proposition here.
I've noticed that when you present people with refined code that is extremely simple, they often just do not believe you, or even themselves. They assume that the only way you're able to juggle many different concerns is through galaxy brain integration gymnastics. It's really quite funny. They go looking for the complexity, and they can't find it, so they assume they're missing something really vital. The realization that it's simply not there can take a very long time to sink in.
Visit usegpu.live for more and to view demos in a WebGPU capable browser.






Compute  Data Flow  GPU  Latest  Use.GPU

January 14, 2023











Home







Home









Subscribe










About
© 2003–2023







This article contains graphics made with WebGL, which your browser does not seem to support.
  Try Google Chrome or Mozilla Firefox.
  
  ×



"
https://news.ycombinator.com/rss,DragonFlyBSD's HAMMER2 File-System Being Ported to NetBSD,https://www.phoronix.com/news/NetBSD-HAMMER2-Port,Comments,"







DragonFlyBSD's HAMMER2 File-System Being Ported To NetBSD - Phoronix






































 













Articles & Reviews
News Archive
Forums
Premium  Categories
Computers Display Drivers Graphics Cards Linux Gaming Memory Motherboards Processors Software Storage Operating Systems Peripherals Close








Articles & Reviews


News Archive


Forums


Premium
 
Contact


 Categories


Computers Display Drivers Graphics Cards Linux Gaming Memory Motherboards Processors Software Storage Operating Systems Peripherals 






























Show Your Support:  This site is primarily supported by advertisements. Ads are what have allowed this site to be maintained on a daily basis for the past 18+ years. We do our best to ensure only clean, relevant ads are shown, when any nasty ads are detected, we work to remove them ASAP. If you would like to view the site without ads while still supporting our work, please consider our ad-free Phoronix Premium.
DragonFlyBSD's HAMMER2 File-System Being Ported To NetBSD
Written by Michael Larabel in BSD on 11 January 2023 at 06:26 AM EST. 21 Comments


NetBSD continues using the FFS file-system by default while it's offered ZFS support that has been slowly improving -- in NetBSD-CURRENT is the ability to use ZFS as the root file-system if first booting to FFS, for example.  There may be another modern file-system option soon with an effort underway to port DragonFlyBSD's HAMMER2 over to NetBSD. HAMMER2 has been built-up over the past decade by Matthew Dillon and other DragonFlyBSD developers. HAMMER2 has been the default and working rather well with recent releases of DragonFlyBSD while now there is a port underway to try to get the file-system working good for NetBSD.HAMMER2 on DragonFlyBSD. NetBSD developer  Tomohiro Kusumi has started working on a HAMMER2 port to NetBSD, who had also worked on porting HAMMER2 to FreeBSD as another exercise. This port is intended to be built against recent NetBSD code, initially is only read-only support but write support will be tackled once the read support has stabilized.  More details on this still very early stage port of HAMMER2 to NetBSD can be found via this GitHub repository. It will be interesting to see how the HAMMER2 port to NetBSD goes and if eventually could become a viable file-system option for NetBSD installations.









21 Comments 




Tweet







Related News
DragonFlyBSD 6.4 Released With Many FixesNetBSD 10 Beta Brings Much Improved Performance, Long Overdue Hardware SupportFreeBSD 12.4 Released With Various Fixes & ImprovementsTrying Out The BSDs On The Intel Core i9 13900K ""Raptor Lake""FreeBSD Re-Introduces WireGuard Support Into Its KernelFreeBSD 12.4-BETA1 Released, Q3-2022 Status Report Issued
 






About The Author

Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via Twitter, LinkedIn, or contacted via MichaelLarabel.com.


Popular News This Week
DragonFlyBSD's HAMMER2 File-System Being Ported To NetBSDOpenZFS Lands A Very Nice Performance OptimizationA Developer Hopes To Restore GCC's Java Front-EndOBS Studio 29 Released With AV1 Encode Additions, Upward Compression FilterUbuntu's Real-Time Kernel Approaching GA StatusLinux Preparing To Disable Drivers For Microsoft's RNDIS ProtocolLinux 4.9.337 Released To End Out The 2016 LTS SeriesWine 8.0-rc3 Released With 28 Known Bug Fixes
 













Latest Linux News
Removing Some Old Arm Drivers & Board/Machine Code To Lighten The Kernel By 154k Lines


Linux 6.3 To Support Making Use Of Intel's New LKGS Instruction (Part Of FRED)


Linux 6.3 Will Better Handle Missing AMD Radeon Firmware / Unsupported Hardware


Basic OpenGL ES Compute Shader Support Begins Working For The Apple GPU Linux Driver


GNU Binutils 2.40 Released With AMD Zen 4 & Upcoming Intel Instructions, Zstd Support


MSI PRO Z690-A WiFi DDR5 Support Upstreamed To Coreboot


Intel Posts Linux Patches For Linear Address Space Separation (LASS)


AMD ROCm 5.4.2 Released As Another Small Update To The Compute Stack


KDE This Week: ""Pretty Juicy In The Eye Candy Department""


Linux Developers Eye Orphaning The JFS File-System










Show Your Support, Go Premium
Phoronix Premium allows ad-free access to the site, multi-page articles on a single page, and other features while supporting this site's continued operations.


Latest Featured Articles
Setting Up Intel 4th Gen Xeon Scalable ""Sapphire Rapids"" For Accelerator Use


AMD Radeon vs. Intel Arc Graphics With Linux 6.2 + Mesa 23.0


Intel Xeon Platinum 8490H ""Sapphire Rapids"" Performance Benchmarks


Intel Launches 4th Gen Xeon Scalable ""Sapphire Rapids"", Xeon CPU Max Series


AMD Ryzen 5 7600 / Ryzen 7 7700 / Ryzen 9 7900 Linux Performance







 


Support Phoronix
The mission at Phoronix since 2004 has centered around enriching the Linux hardware experience. In addition to supporting our site through advertisements, you can help by subscribing to Phoronix Premium. You can also contribute to Phoronix through a PayPal tip or tip via Stripe.











Phoronix Media


Contact
Michael Larabel
OpenBenchmarking.org



Phoronix Premium


Support Phoronix
While Having Ad-Free Browsing,
Single-Page Article Viewing



Share


Facebook
Twitter







Legal Disclaimer, Privacy Policy, Cookies | Contact
Copyright © 2004 - 2023 by Phoronix Media.
All trademarks used are properties of their respective owners. All rights reserved.







"
https://news.ycombinator.com/rss,The joy of sets,https://www.prospectmagazine.co.uk/arts-and-books/the-joy-of-sets,Comments,"
403 Forbidden

403 Forbidden


"
https://news.ycombinator.com/rss,Server BMCs can need to be rebooted every so often,https://utcc.utoronto.ca/~cks/space/blog/sysadmin/BMCsCanNeedRebooting,Comments,"
 
 Chris's Wiki :: blog/sysadmin/BMCsCanNeedRebooting 






Chris Siebenmann ::
CSpace »
       blog »
       sysadmin »
       BMCsCanNeedRebooting
Welcome, guest.




Your server BMCs can need to be rebooted every so often
January 14, 2023

Over on the Fediverse I said:
A sysadmin tip: if your BMC/IPMI
is doing weird things, restart (reboot) it. Server BMCs are little
computers running ancient versions of Linux with software that's
probably terribly written and they stay running forever, which means
all sorts of opportunities for slow bugs. Reboot away!
This is brought to you by the BMC with a KVM-over-IP that wouldn't
accept '2' entered on the (virtual) keyboard in any way or form. Until
I rebooted the BMC. 
PS: Our IP addresses have 2s in them.

(This probably isn't the only weird BMC glitch we've experienced,
but it's the first one where I tried rebooting the BMC and that
fixed it.)
A number of people shared additional stories in the replies, and I
especially 'liked' @frederic@chaos.social's:
Same for IPMI hardware sensors: Thought the motherboard was damaged
because half the sensors were reported as ""n/a"".  Rebooting magically
fixed this. ðŸ™ˆ

This happens for more or less the reasons I mentioned above. BMCs
naturally accumulate very large uptimes because they don't normally
reboot when your server reboots; if you don't do anything special,
your BMC will normally stay up for as long as the server has power.
In many places this can amount to years of uptime, and it's a rare
set of software that can stand up to that even if you don't use
them much. Server vendors typically don't want you to think about
this, and I don't believe 'BMC uptime' is generally exposed anywhere.
(Routinely querying the BMC's sensor readings via IPMI may actually
make this worse, since then the BMC's software is active to answer
those queries. I should probably make our metrics system notice when a server decreases the
number of IPMI metrics it exposes without a reboot.)
Modern BMCs can generally reboot themselves without rebooting their
host (the actual server), although you may want to test this to be
sure since apparently some vendors can do that differently.
PS: How I encountered this is that I was reinstalling a server using
KVM-over-IP, and I hit the portion of the base Ubuntu 22.04 install
when I had to enter the subnet and various associated IP addresses.
Our network has a '2' in it, so all of that failed. Helpfully, the
KVM-over-IP software had a virtual keyboard so I could see it wasn't
just some browser weirdness intercepting a '2' from my real keyboard;
even the virtual keyboard's '2' key wouldn't get through to the
Ubuntu 22.04 installer running on the server being reinstalled. Since
rebooting the BMC didn't reboot the host, I could verify that rebooting
the BMC alone fixed the problem; when the BMC rebooted, my KVM-over-IP
session could now enter all digits.
(I'm glad that it occurred to me to reboot the BMC, instead of just
grumble and go down to the machine room to do the install with the
physical console.)

(One comment.)
Written on 14 January 2023. 

     «   Ubuntu 22.04 LTS servers and phased apt updates    
  



 These are my WanderingThoughts 
(About the blog)
Full index of entries 
Recent comments
This is part of CSpace, and is written by ChrisSiebenmann. 
Mastodon: @cks 
Twitter: @thatcks
* * *
Categories: links, linux, programming, python, snark, solaris, spam, sysadmin, tech, unix, web 
Also: (Sub)topics
This is a DWiki. 
GettingAround 
(Help)
 
 Search:  



 Page tools: View Source, Add Comment. 

Search: 

Login: 
Password: 


 

Atom Syndication: Recent Comments.
 Last modified: Sat Jan 14 22:02:49 2023 
This dinky wiki is brought to you by the Insane Hackers
Guild, Python sub-branch.


"
https://news.ycombinator.com/rss,Bayesian statistics and machine learning: How do they differ?,https://statmodeling.stat.columbia.edu/2023/01/14/bayesian-statistics-and-machine-learning-how-do-they-differ/,Comments,"
403 Forbidden

403 Forbidden
nginx


"
https://news.ycombinator.com/rss,Money creation in the modern economy (2014) [pdf],https://www.bankofengland.co.uk/-/media/boe/files/quarterly-bulletin/2014/money-creation-in-the-modern-economy.pdf?la=en&hash=9A8788FD44A62D8BB927123544205CE476E01654,Comments,"14  Quarterly Bulletin  2014 Q1  Money creation in the modern economy  By Michael McLeay, Amar Radia and Ryland Thomas of the Bank’s Monetary Analysis Directorate.(1)    This article explains how the majority of money in the modern economy is created by commercial  banks making loans.    Money creation in practice differs from some popular misconceptions — banks do not act simply as intermediaries, lending out deposits that savers place with them, and nor do they ‘multiply up’ central bank money to create new loans and deposits.    The amount of money created in the economy ultimately depends on the monetary policy of the central bank.  In normal times, this is carried out by setting interest rates.  The central bank can also affect the amount of money directly through purchasing assets or ‘quantitative easing’.  Overview  In the modern economy, most money takes the form of bank deposits.  But how those bank deposits are created is often misunderstood:  the principal way is through commercial banks making loans.  Whenever a bank makes a loan, it simultaneously creates a matching deposit in the borrower’s bank account, thereby creating new money.  The reality of how money is created today differs from the description found in some economics textbooks:    Rather than banks receiving deposits when households save and then lending them out, bank lending creates deposits.    In normal times, the central bank does not fix the amount  of money in circulation, nor is central bank money ‘multiplied up’ into more loans and deposits.  Although commercial banks create money through lending, they cannot do so freely without limit.  Banks are limited in how much they can lend if they are to remain profitable in a competitive banking system.  Prudential regulation also acts as a constraint on banks’ activities in order to maintain the resilience of the financial system.  And the households and companies who receive the money created by new lending may take actions that affect the stock of money — they could quickly ‘destroy’ money by using it to repay their existing debt, for instance.  Monetary policy acts as the ultimate limit on money creation.  The Bank of England aims to make sure the amount of money creation in the economy is consistent with  low and stable inflation.  In normal times, the Bank of England implements monetary policy by setting the interest rate on central bank reserves.  This then influences a range of interest rates in the economy, including those on bank loans.  In exceptional circumstances, when interest rates are at theireffective lower bound, money creation and spending in the economy may still be too low to be consistent with the central bank’s monetary policy objectives.  One possible response is to undertake a series of asset purchases, or‘quantitative easing’ (QE).  QE is intended to boost theamount of money in the economy directly by purchasingassets, mainly from non-bank financial companies. QE initially increases the amount of bank deposits those companies hold (in place of the assets they sell).  Those companies will then wish to rebalance their portfolios ofassets by buying higher-yielding assets, raising the price ofthose assets and stimulating spending in the economy. As a by-product of QE, new central bank reserves arecreated.  But these are not an important part of thetransmission mechanism.  This article explains how, just as innormal times, these reserves cannot be multiplied into moreloans and deposits and how these reserves do not represent‘free money’ for banks. Click here for a short video filmed in the Bank’s gold vaults that discusses some of the key topics from this article.  (1)  The authors would like to thank Lewis Kirkham for his help in producing this article.  Topical articles  Money creation in the modern economy  15  Introduction  ‘Money in the modern economy:  an introduction’, a companion piece to this article, provides an overview of what is meant by money and the different types of money that exist in a modern economy, briefly touching upon how each type of money is created.  This article explores money creation in the modern economy in more detail.  The article begins by outlining two common misconceptions about money creation, and explaining how, in the modern economy, money is largely created by commercial banks making loans.(1)  The article then discusses the limits to the banking system’s ability to create money and the important role for central bank policies in ensuring that credit and money growth are consistent with monetary and financial stability in the economy.  The final section discusses the role of money in the monetary transmission mechanism during periods of quantitative easing (QE), and dispels some myths surrounding money creation and QE.  A short video explains some of the key topics covered in this article.(2)  Two misconceptions about money creation  The vast majority of money held by the public takes the form of bank deposits.  But where the stock of bank deposits comes from is often misunderstood.  One common misconception is that banks act simply as intermediaries, lending out the deposits that savers place with them.  In this view deposits are typically ‘created’ by the saving decisions of households, and banks then ‘lend out’ those existing deposits to borrowers, for example to companies looking to finance investment or individuals wanting to purchase houses.  In fact, when households choose to save more money in bank accounts, those deposits come simply at the expense of deposits that would have otherwise gone to companies in payment for goods and services.  Saving does not by itself increase the deposits or ‘funds available’ for banks to lend. Indeed, viewing banks simply as intermediaries ignores the fact that, in reality in the modern economy, commercial banks are the creators of deposit money.  This article explains how, rather than banks lending out deposits that are placed with them, the act of lending creates deposits — the reverse of the sequence typically described in textbooks.(3)  Another common misconception is that the central bank determines the quantity of loans and deposits in the economy by controlling the quantity of central bank money — the so-called ‘money multiplier’ approach.  In that view, central banks implement monetary policy by choosing a quantity of reserves.  And, because there is assumed to be a constant ratio of broad money to base money, these reserves are then ‘multiplied up’ to a much greater change in bank  loans and deposits.  For the theory to hold, the amount of reserves must be a binding constraint on lending, and the central bank must directly determine the amount of reserves. While the money multiplier theory can be a useful way of introducing money and banking in economic textbooks, it is not an accurate description of how money is created in reality. Rather than controlling the quantity of reserves, central banks today typically implement monetary policy by setting the price of reserves — that is, interest rates.  In reality, neither are reserves a binding constraint on lending, nor does the central bank fix the amount of reserves that are available.  As with the relationship between deposits and loans, the relationship between reserves and loans typically operates in the reverse way to that described in some economics textbooks.  Banks first decide how much to lend depending on the profitable lending opportunities available to them — which will, crucially, depend on the interest rate set by the Bank of England.  It is these lending decisions that determine how many bank deposits are created by the banking system.  The amount of bank deposits in turn influences how much central bank money banks want to hold in reserve (to meet withdrawals by the public, make payments to other banks, or meet regulatory liquidity requirements), which is then, in normal times, supplied on demand by the Bank of England.  The rest of this article discusses these practices in more detail.  Money creation in reality  Lending creates deposits — broad money determination at the aggregate level As explained in ‘Money in the modern economy:  an introduction’, broad money is a measure of the total amount of money held by households and companies in the economy. Broad money is made up of bank deposits — which are essentially IOUs from commercial banks to households and companies — and currency — mostly IOUs from the central bank.(4)(5)  Of the two types of broad money, bank deposits make up the vast majority — 97% of the amount currently in circulation.(6)  And in the modern economy, those bank deposits are mostly created by commercial banks themselves.  (1)  Throughout this article, ‘banks’ and ‘commercial banks’ are used to refer to banks and  building societies together.  (2)  See www.youtube.com/watch?v=CvRAqR2pAgw. (3)  There is a long literature that does recognise the ‘endogenous’ nature of money  creation in practice.  See, for example, Moore (1988), Howells (1995) and Palley (1996).  (4)  The definition of broad money used by the Bank of England, M4ex, also includes a  wider range of bank liabilities than regular deposits;  see Burgess and Janssen (2007) for more details.  For simplicity, this article describes all of these liabilities as deposits. A box later in this article provides details about a range of popular monetary aggregates in the United Kingdom.  (5)  Around 6% of the currency in circulation is made up of coins, which are produced by The Royal Mint.  Of the banknotes that circulate in the UK economy, some are issued by some Scottish and Northern Irish commercial banks, although these are fully matched by Bank of England money held at the Bank.  (6)  As of December 2013.  16  Quarterly Bulletin  2014 Q1  Commercial banks create money, in the form of bank deposits, by making new loans.  When a bank makes a loan, for example to someone taking out a mortgage to buy a house, it does not typically do so by giving them thousands of pounds worth of banknotes.  Instead, it credits their bank account with a bank deposit of the size of the mortgage.  At that moment, new money is created.  For this reason, some economists have referred to bank deposits as ‘fountain pen money’, created at the stroke of bankers’ pens when they approve loans.(1)  This process is illustrated in Figure 1, which shows how new lending affects the balance sheets of different sectors of the economy (similar balance sheet diagrams are introduced in ‘Money in the modern economy:  an introduction’).  As shown in the third row of Figure 1, the new deposits increase the assets of the consumer (here taken to represent households and companies) — the extra red bars — and the new loan increases their liabilities — the extra white bars.  New broad money has been created.  Similarly, both sides of the commercial banking sector’s balance sheet increase as new money and loans are created.  It is important to note that although the simplified diagram of Figure 1 shows the amount of new money created as being identical to the amount of new lending, in practice there will be several factors that may subsequently cause the amount of deposits to be different from the amount of lending.  These are discussed in detail in the next section.  While new broad money has been created on the consumer’s balance sheet, the first row of Figure 1 shows that this is without — in the first instance, at least — any change in the amount of central bank money or ‘base money’.  As discussed earlier, the higher stock of deposits may mean that banks want, or are required, to hold more central bank money in order to meet withdrawals by the public or make payments to other banks.  And reserves are, in normal times, supplied ‘on demand’ by the Bank of England to commercial banks in exchange for other assets on their balance sheets.  In no way does the aggregate quantity of reserves directly constrain the amount of bank lending or deposit creation.  This description of money creation contrasts with the notion that banks can only lend out pre-existing money, outlined in the previous section.  Bank deposits are simply a record of how much the bank itself owes its customers.  So they are a liability of the bank, not an asset that could be lent out.  A related misconception is that banks can lend out their reserves. Reserves can only be lent between banks, since consumers do not have access to reserves accounts at the Bank of England.(2)  Other ways of creating and destroying deposits Just as taking out a new loan creates money, the repayment of bank loans destroys money.(3)  For example, suppose a consumer has spent money in the supermarket throughout the month by using a credit card.  Each purchase made using the  Figure 1  Money creation by the aggregate banking sector making additional loans(a)  Before loans are made  After loans are made  Assets  Liabilities  Assets  Liabilities  Central bank(b)  Non-money  Reserves  Currency  Base money  Non-money  Reserves  Currency  Base money  Commercial banks(c)  Assets  Liabilities  New loans  New deposits  Assets  Liabilities  Reserves  Deposits  Reserves  Deposits  Currency  Currency  Consumers(d)  Assets  Liabilities  New deposits  New loans  Assets  Liabilities  Broad money  Broad money  Deposits  Non-money  Deposits  Non-money  Currency  Currency  (a)  Balance sheets are highly stylised for ease of exposition:  the quantities of each type of  money shown do not correspond to the quantities actually held on each sector’s balance sheet.  (b)  Central bank balance sheet only shows base money liabilities and the corresponding assets. In practice the central bank holds other non-money liabilities.  Its non-monetary assets are mostly made up of government debt.  Although that government debt is actually held by the Bank of England Asset Purchase Facility, so does not appear directly on the balance sheet. (c)  Commercial banks’ balance sheets only show money assets and liabilities before any loans  are made.  (d)  Consumers represent the private sector of households and companies.  Balance sheet only shows broad money assets and corresponding liabilities — real assets such as the house being transacted are not shown.  Consumers’ non-money liabilities include existing secured and unsecured loans.  credit card will have increased the outstanding loans on the consumer’s balance sheet and the deposits on the supermarket’s balance sheet (in a similar way to that shown in Figure 1).  If the consumer were then to pay their credit card  (1)  Fountain pen money is discussed in Tobin (1963), who mentions it in the context of making an argument that banks cannot create unlimited amounts of money in practice.  (2)  Part of the confusion may stem from some economists’ use of the term ‘reserves’ when referring to ‘excess reserves’ — balances held above those required by regulatory reserve requirements.  In this context, ‘lending out reserves’ could be a shorthand way of describing the process of increasing lending and deposits until the bank reaches its maximum ratio.  As there are no reserve requirements in the United Kingdom the process is less relevant for UK banks.  (3)  The fall in bank lending in the United Kingdom since 2008 is an important reason why  the growth of money in the economy has been so much lower than in the years leading up to the crisis, as discussed in Bridges, Rossiter and Thomas (2011) and Butt et al (2012).  Topical articles  Money creation in the modern economy  17  bill in full at the end of the month, its bank would reduce the amount of deposits in the consumer’s account by the value of the credit card bill, thus destroying all of the newly created money.  Banks making loans and consumers repaying them are the most significant ways in which bank deposits are created and destroyed in the modern economy.  But they are far from the only ways.  Deposit creation or destruction will also occur any time the banking sector (including the central bank) buys or sells existing assets from or to consumers, or, more often, from companies or the government.  Banks buying and selling government bonds is one particularly important way in which the purchase or sale of existing assets by banks creates and destroys money.  Banks often buy and hold government bonds as part of their portfolio of liquid assets that can be sold on quickly for central bank money if, for example, depositors want to withdraw currency in large amounts.(1)  When banks purchase government bonds from the non-bank private sector they credit the sellers with bank deposits.(2)  And, as discussed later in this article, central bank asset purchases, known as quantitative easing (QE), have similar implications for money creation.  Money can also be destroyed through the issuance of long-term debt and equity instruments by banks.  In addition to deposits, banks hold other liabilities on their balance sheets. Banks manage their liabilities to ensure that they have at least some capital and longer-term debt liabilities to mitigate certain risks and meet regulatory requirements.  Because these ‘non-deposit’ liabilities represent longer-term investments in the banking system by households and companies, they cannot be exchanged for currency as easily as bank deposits, and therefore increase the resilience of the bank.  When banks issue these longer-term debt and equity instruments to non-bank financial companies, those companies pay for them with bank deposits.  That reduces the amount of deposit, or money, liabilities on the banking sector’s balance sheet and increases their non-deposit liabilities.(3)  Buying and selling of existing assets and issuing longer-term liabilities may lead to a gap between lending and deposits in a closed economy.  Additionally, in an open economy such as the United Kingdom, deposits can pass from domestic residents to overseas residents, or sterling deposits could be converted into foreign currency deposits.  These transactions do not destroy money per se, but overseas residents’ deposits and foreign currency deposits are not always counted as part of a country’s money supply.  Limits to broad money creation Although commercial banks create money through their lending behaviour, they cannot in practice do so without limit. In particular, the price of loans — that is, the interest rate (plus  any fees) charged by banks — determines the amount that households and companies will want to borrow.  A number of factors influence the price of new lending, not least the monetary policy of the Bank of England, which affects the level of various interest rates in the economy.  The limits to money creation by the banking system were discussed in a paper by Nobel Prize winning economist James Tobin and this topic has recently been the subject of debate among a number of economic commentators and bloggers.(4)  In the modern economy there are three main sets of constraints that restrict the amount of money that banks can create.  (i)  Banks themselves face limits on how much they can  lend.  In particular:    Market forces constrain lending because individual  banks have to be able to lend profitably in a competitive market.    Lending is also constrained because banks have to take  steps to mitigate the risks associated with making additional loans.    Regulatory policy acts as a constraint on banks’  activities in order to mitigate a build-up of risks that could pose a threat to the stability of the financial system.  (ii)  Money creation is also constrained by the behaviour of the money holders — households and businesses. Households and companies who receive the newly created money might respond by undertaking transactions that immediately destroy it, for example by repaying outstanding loans.  (iii)  The ultimate constraint on money creation is monetary policy.  By influencing the level of interest rates in the economy, the Bank of England’s monetary policy affects how much households and companies want to borrow. This occurs both directly, through influencing the loan rates charged by banks, but also indirectly through the overall effect of monetary policy on economic activity in  (1)  It is for this reason that holdings of some government bonds are counted towards meeting prudential liquidity requirements, as described in more detail by Farag, Harland and Nixon (2013).  (2)  In a balance sheet diagram such as Figure 1, a purchase of government bonds from consumers by banks would be represented by a change in the composition of consumers’ assets from government bonds to deposits and an increase in both deposits and government bonds on the commercial banks’ balance sheet.  (3)  Commercial banks’ purchases of government bonds and their issuance of long-term  debt and equity have both been important influences on broad money growth during the financial crisis as discussed in Bridges, Rossiter and Thomas (2011) and Butt et al (2012).  (4)  Tobin (1963) argued that banks do not possess a ‘widow’s cruse’, referring to a biblical story (earlier referenced in economics by John Maynard Keynes) in which a widow is able to miraculously refill a cruse (a pot or jar) of oil during a famine.  Tobin was arguing that there were limits to how many loans could be automatically matched by deposits.  18  Quarterly Bulletin  2014 Q1  the economy.  As a result, the Bank of England is able to ensure that money growth is consistent with its objective of low and stable inflation.  make many such loans every day.  So if a given bank financed all of its new loans in this way, it would soon run out of reserves.  The remainder of this section explains how each of these mechanisms work in practice.  (i) Limits on how much banks can lend Market forces facing individual banks Figure 1 showed how, for the aggregate banking sector, loans are initially created with matching deposits.  But that does not mean that any given individual bank can freely lend and create money without limit.  That is because banks have to be able to lend profitably in a competitive market, and ensure that they adequately manage the risks associated with making loans.  Banks receive interest payments on their assets, such as loans, but they also generally have to pay interest on their liabilities, such as savings accounts.  A bank’s business model relies on receiving a higher interest rate on the loans (or other assets) than the rate it pays out on its deposits (or other liabilities). Interest rates on both banks’ assets and liabilities depend on the policy rate set by the Bank of England, which acts as the ultimate constraint on money creation.  The commercial bank uses the difference, or spread, between the expected return on their assets and liabilities to cover its operating costs and to make profits.(1)  In order to make extra loans, an individual bank will typically have to lower its loan rates relative to its competitors to induce households and companies to borrow more.  And once it has made the loan it may well ‘lose’ the deposits it has created to those competing banks.  Both of these factors affect the profitability of making a loan for an individual bank and influence how much borrowing takes place.  For example, suppose an individual bank lowers the rate it charges on its loans, and that attracts a household to take out a mortgage to buy a house.  The moment the mortgage loan is made, the household’s account is credited with new deposits. And once they purchase the house, they pass their new deposits on to the house seller.  This situation is shown in the first row of Figure 2.  The buyer is left with a new asset in the form of a house and a new liability in the form of a new loan. The seller is left with money in the form of bank deposits instead of a house.  It is more likely than not that the seller’s account will be with a different bank to the buyer’s.  So when the transaction takes place, the new deposits will be transferred to the seller’s bank, as shown in the second row of Figure 2.  The buyer’s bank would then have fewer deposits than assets.  In the first instance, the buyer’s bank settles with the seller’s bank by transferring reserves.  But that would leave the buyer’s bank with fewer reserves and more loans relative to its deposits than before.  This is likely to be problematic for the bank since it would increase the risk that it would not be able to meet all of its likely outflows.  And, in practice, banks  Banks therefore try to attract or retain additional liabilities to accompany their new loans.  In practice other banks would also be making new loans and creating new deposits, so one way they can do this is to try and attract some of those newly created deposits.  In a competitive banking sector, that may involve increasing the rate they offer to households on their savings accounts.  By attracting new deposits, the bank can increase its lending without running down its reserves, as shown in the third row of Figure 2.  Alternatively, a bank can borrow from other banks or attract other forms of liabilities, at least temporarily.  But whether through deposits or other liabilities, the bank would need to make sure it was attracting and retaining some kind of funds in order to keep expanding lending.  And the cost of that needs to be measured against the interest the bank expects to earn on the loans it is making, which in turn depends on the level of Bank Rate set by the Bank of England.  For example, if a bank continued to attract new borrowers and increase lending by reducing mortgage rates, and sought to attract new deposits by increasing the rates it was paying on its customers’ deposits, it might soon find it unprofitable to keep expanding its lending.  Competition for loans and deposits, and the desire to make a profit, therefore limit money creation by banks.  Managing the risks associated with making loans Banks also need to manage the risks associated with making new loans.  One way in which they do this is by making sure that they attract relatively stable deposits to match their new loans, that is, deposits that are unlikely or unable to be withdrawn in large amounts.  This can act as an additional limit to how much banks can lend.  For example, if all of the deposits that a bank held were in the form of instant access accounts, such as current accounts, then the bank might run the risk of lots of these deposits being withdrawn in a short period of time.  Because banks tend to lend for periods of many months or years, the bank may not be able to repay all of those deposits — it would face a great deal of liquidity risk. In order to reduce liquidity risk, banks try to make sure that some of their deposits are fixed for a certain period of time, or term.(2)  Consumers are likely to require compensation for the inconvenience of holding longer-term deposits, however, so these are likely to be more costly for banks, limiting the amount of lending banks wish to do.  And as discussed earlier, if banks guard against liquidity risk by issuing long-term liabilities, this may destroy money directly when companies pay for them using deposits.  (1)  See Button, Pezzini and Rossiter (2010) for an explanation of how banks price new  loans.  (2)  Banks also guard against liquidity risk by holding liquid assets (including reserves and currency), which either can be used directly to cover outflows, or if not can quickly and cheaply be converted into assets that can.  Although if banks purchase liquid assets such as government bonds from non-banks, this could create further deposits.  Topical articles  Money creation in the modern economy  19  Figure 2  Money creation for an individual bank making an additional loan(a)  Changes to the balance sheets of the house buyer and seller  House buyer  House seller  House buyer  House seller  House buyer  House seller  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Non-money (house)  GovernmentDeposits debt Currency  Deposits  Currency  Non-money  New deposit  New loan  Non-money (house)  Non-money (house)  New loan  New deposit  Non-money  Non-money  Non-money  Deposits  Currency  Non-money  GovernmentDeposits debt Currency  Deposits  Currency  Non-money  GovernmentDeposits debt Currency  Balance sheets before the loan is made.  The house buyer takes out a mortgage…  …and uses its new deposits to pay the house seller.  Changes to the balance sheets of the house buyer and seller’s banks  Buyer’s bank  Seller’s bank  Buyer’s bank  Seller’s bank  Buyer’s bank  Seller’s bank  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  Assets  Liabilities  New loan  New deposit  Transferred reserves  New deposit  Reserves  Deposits  Reserves  Deposits  Reserves  Deposits  Reserves  Deposits  Currency  Currency  Currency  Currency  Balance sheets before the loan is made.  The mortgage lender creates new deposits…  New loan  Reserves  Currency  Deposits  Reserves  Currency  Deposits  …which are transferred to the seller’s bank, along with reserves, which the buyer’s bank uses to settle the transaction.  But settling all transactions in this way would be unsustainable: •  The buyer’s bank would have fewer reserves to meet its possible  outfows, for example from deposit withdrawals.  •  And if it made many new loans it would eventually run out  of reserves.  Buyer’s bank  Seller’s bank  Assets  Liabilities  Assets  Liabilities  New loan  New deposit  Reserves  Reserves  Currency  Deposits  Reserves  Currency  Deposits  So the buyer’s bank will in practice seek to attract or retain new deposits (and reserves) — in the example shown here, from the seller’s bank — to accompany their new loans.  (a)  Balance sheets are highly stylised for ease of exposition:  the quantities of each type of money shown do not correspond to the quantities actually held on each sector’s balance sheet.  Individual banks’ lending is also limited by considerations of credit risk.  This is the risk to the bank of lending to borrowers who turn out to be unable to repay their loans.  In part, banks can guard against credit risk by having sufficient capital to absorb any unexpected losses on their loans.  But since loans will always involve some risk to banks of incurring losses, the cost of these losses will be taken into account when pricing loans.  When a bank makes a loan, the interest rate it charges will typically include compensation for the average level of credit losses the bank expects to suffer.  The size of this component of the interest rate will be larger when banks estimate that they will suffer higher losses, for example when lending to mortgagors with a high loan to value ratio.  As banks expand lending, their average expected loss per loan is likely to increase, making those loans less profitable.  This further limits the amount of lending banks can profitably do, and the money they can therefore create.  Market forces do not always lead individual banks to sufficiently protect themselves against liquidity and credit risks.  Because of this, prudential regulation aims to ensure that banks do not take excessive risks when making new loans, including via requirements for banks’ capital and liquidity positions.  These requirements can therefore act as an additional brake on how much money commercial banks create by lending.  The prudential regulatory framework, along with more detail on capital and liquidity, is described in Farag, Harland and Nixon (2013).  So far this section has considered the case of an individual bank making additional loans by offering competitive interest rates — both on its loans and deposits.  But if all banks simultaneously decide to try to do more lending, money growth may not be limited in quite the same way.  Although an individual bank may lose deposits to other banks, it would itself be likely to gain some deposits as a result of the other banks making loans.     20  Quarterly Bulletin  2014 Q1  There are a number of reasons why many banks may choose to increase their lending markedly at the same time.  For example, the profitability of lending at given interest rates could increase because of a general improvement in economic conditions.  Alternatively, banks may decide to lend more if they perceive the risks associated with making loans to households and companies to have fallen.  This sort of development is sometimes argued to be one of the reasons why bank lending expanded so much in the lead up to the financial crisis.(1)  But if that perception of a less risky environment were unwarranted, the result could be a more fragile financial system.(2)  One of the responses to the crisis in the United Kingdom has been the creation of a macroprudential authority, the Financial Policy Committee, to identify, monitor and take action to reduce or remove risks which threaten the resilience of the financial system as a whole.(3)  (ii) Constraints arising from the response of households and companies In addition to the range of constraints facing banks that act to limit money creation, the behaviour of households and companies in response to money creation by the banking sector can also be important, as argued by Tobin.  The behaviour of the non-bank private sector influences the ultimate impact that credit creation by the banking sector has on the stock of money because more (or less) money may be created than they wish to hold relative to other assets (such as property or shares).  As the households and companies who take out loans do so because they want to spend more, they will quickly pass that money on to others as they do so.  How those households and companies then respond will determine the stock of money in the economy, and potentially have implications for spending and inflation.  There are two main possibilities for what could happen to newly created deposits.  First, as suggested by Tobin, the money may quickly be destroyed if the households or companies receiving the money after the loan is spent wish to use it to repay their own outstanding bank loans.  This is sometimes referred to as the ‘reflux theory’.(4) For example, a first-time house buyer may take out a mortgage to purchase a house from an elderly person who, in turn, repays their existing mortgage and moves in with their family.  As discussed earlier, repaying bank loans destroys money just as making loans creates it.  So, in this case, the balance sheet of consumers in the economy would be returned to the position it was in before the loan was made.  in the economy.(5)  Instead, the money may initially pass to households or companies with positive holdings of financial assets:  the elderly person may have already paid off their mortgage, or a company receiving money as a payment may already have sufficient liquid assets to cover possible outgoings.  They may then be left holding more money than they desire, and attempt to reduce their ‘excess’ money holdings by increasing their spending on goods and services. (In the case of a company it may instead buy other, higher-yielding, assets.)  These two scenarios for what happens to newly created money — being quickly destroyed or being passed on via spending — have very different implications for economic activity.  In the latter, the money may continue to be passed between different households and companies each of whom may, in turn, increase their spending.  This process — sometimes referred to as the ‘hot potato’ effect — can lead, other things equal, to increased inflationary pressure on the economy.(6)  In contrast, if the money is quickly destroyed as in the former scenario, there need be no further effects on the economy.  This section has so far discussed how the actions of banks, households and companies can affect the amount of money in the economy, and therefore inflationary pressure.  But the ultimate determinant of monetary conditions in the economy is the monetary policy of the central bank.  (iii) Monetary policy — the ultimate constraint on money creation One of the Bank of England’s primary objectives is to ensure monetary stability by keeping consumer price inflation on track to meet the 2% target set by the Government.  And, as discussed in the box on pages 22–23, over some periods of time, various measures of money have grown at a similar rate to nominal spending, which determines inflationary pressure in the economy in the medium term.  So setting monetary policy appropriately to meet the inflation target should ultimately ensure a stable rate of credit and money creation consistent with meeting that target.  This section explains the relationship between monetary policy and different types of money.  In normal times, the Monetary Policy Committee (MPC), like most of its equivalents in other countries, implements monetary policy by setting short-term interest rates, specifically by setting the interest rate paid on central bank reserves held by commercial banks.  It is able to do so because  The second possible outcome is that the extra money creation by banks can lead to more spending in the economy.  For newly created money to be destroyed, it needs to pass to households and companies with existing loans who want to repay them.  But this will not always be the case, since asset and debt holdings tend to vary considerably across individuals  (1)  See, for example, Haldane (2009). (2)  Tucker (2009) discusses the possibility of such ‘risk illusion’ in the financial system. (3)  Tucker, Hall and Pattani (2013) describe the new powers for macroprudential policymaking in the United Kingdom in the wake of the recent financial crisis.  (4)  See Kaldor and Trevithick (1981). (5)  See Kamath et al (2011). (6)  This mechanism is explained in more detail in papers including Laidler (1984),  Congdon (1992, 2005), Howells (1995), Laidler and Robson (1995), Bridges, Rossiter and Thomas (2011) and Bridges and Thomas (2012).  Topical articles  Money creation in the modern economy  21  of the Bank’s position as the monopoly provider of central bank money in the United Kingdom.  And it is because there is demand for central bank money — the ultimate means of settlement for banks, the creators of broad money — that the price of reserves has a meaningful impact on other interest rates in the economy.  The interest rate that commercial banks can obtain on money placed at the central bank influences the rate at which they are willing to lend on similar terms in sterling money markets — the markets in which the Bank and commercial banks lend to each other and other financial institutions.  The exact details of how the Bank uses its money market operations to implement monetary policy has varied over time, and central bank operating procedures today differ somewhat from country to country, as discussed in Clews, Salmon and Weeken (2010).(1)  Changes in interbank interest rates then feed through to a wider range of interest rates in different markets and at different maturities, including the interest rates that banks charge borrowers for loans and offer savers for deposits.(2)  By influencing the price of credit in this way, monetary policy affects the creation of broad money.  This description of the relationship between monetary policy and money differs from the description in many introductory textbooks, where central banks determine the quantity of broad money via a ‘money multiplier’ by actively varying the quantity of reserves.(3)  In that view, central banks implement monetary policy by choosing the quantity of reserves.  And, because there is assumed to be a stable ratio of broad money to base money, these reserves are then ‘multiplied up’ to a much greater change in bank deposits as banks increase lending and deposits.  Neither step in that story represents an accurate description of the relationship between money and monetary policy in the modern economy.  Central banks do not typically choose a quantity  of reserves to bring about the desired short-term interest rate.(4)  Rather, they focus on prices — setting interest rates.(5)  The Bank of England controls interest rates by supplying and remunerating reserves at its chosen policy rate.  The supply of both reserves and currency (which together make up base money) is determined by banks’ demand for reserves both for the settlement of payments and to meet demand for currency from their customers — demand that the central bank typically accommodates.  This demand for base money is therefore more likely to be a consequence rather than a cause of banks making loans and creating broad money.  This is because banks’ decisions to extend credit are based on the availability of profitable lending opportunities at any given point in time.  The profitability of making a loan will depend on a number of factors, as discussed earlier.  One of these is the cost of funds that banks face, which is closely related to the interest rate paid on reserves, the policy rate.  In contrast, the quantity of reserves already in the system does not constrain the creation of broad money through the act of lending.(6)  This leg of the money multiplier is sometimes motivated by appealing to central bank reserve requirements, whereby banks are obliged to hold a minimum amount of reserves equal to a fixed proportion of their holdings of deposits.  But reserve requirements are not an important aspect of monetary policy frameworks in most advanced economies today.(7)  A looser stance of monetary policy is likely to increase the stock of broad money by reducing loan rates and increasing the volume of loans.  And a larger stock of broad money, accompanied by an increased level of spending in the economy, may cause banks and customers to demand more reserves and currency.(8)  So, in reality, the theory of the money multiplier operates in the reverse way to that normally described.  QE — creating broad money directly with monetary policy  The previous section discussed how monetary policy can be seen as the ultimate limit to money creation by commercial banks.  But commercial banks could alternatively create too little money to be consistent with the economy meeting the inflation target.  In normal times, the MPC can respond by lowering the policy rate to encourage more lending and hence more money creation.  But, in response to the financial crisis, the MPC cut Bank Rate to 0.5% — the so-called effective lower bound.  Once short-term interest rates reach the effective lower bound, it is not possible for the central bank to provide further stimulus to the economy by lowering the rate at which reserves are remunerated.(9)  One possible way of providing further monetary stimulus to the economy is through a programme of asset purchases (QE).  Like reductions in Bank  (1)  The framework for the Bank’s operations in the sterling money markets is set out in  the Bank’s ‘Red Book’, available at www.bankofengland.co.uk/markets/Documents/money/publications/redbook.pdf. Recent developments in sterling money markets are discussed by Jackson and Sim (2013).  (2)  Bank of England (1999) discusses the transmission mechanism of monetary policy in  more detail.  (3)  Benes and Kumhof (2012) discuss the money multiplier myth in more detail. (4)  As discussed by Disyatat (2008). (5)  Bindseil (2004) provides a detailed account of how monetary policy implementation  works through short-term interest rates.  (6)  Carpenter and Demiralp (2012) show that changes in quantities of reserves are  unrelated to changes in quantities of loans in the United States.  (7)  The Bank of England currently has no formal reserve requirements, for example. (It does require banks to hold a proportion of non-interest bearing ‘cash ratio deposits’ with the Bank for a subset of their liabilities.  But the function of these cash ratio deposits is non-operational.  Their sole purpose is to provide income for the Bank.)  Bernanke (2007) discusses how reserve requirements now present less of a constraint than in the past in the United States.  (8)  Kydland and Prescott (1990) found that broad money aggregates led the cycle, while  base money aggregates tended to lag the cycle slightly.  (9)  If the central bank were to lower interest rates significantly below zero, banks could swap their bank reserves into currency, which would pay a higher interest rate (of zero, or slightly less after taking into account the costs of storing currency).  Or put another way, the demand for central bank reserves would disappear, so the central bank could no longer influence the economy by changing the price of those reserves.  22  Quarterly Bulletin  2014 Q1  The information content of different types of money and monetary aggregates  One of the Bank of England’s primary objectives is to ensure monetary stability by keeping inflation on track to meet the Government’s 2% target.  Milton Friedman (1963) famously argued that ‘inflation is always and everywhere a monetary phenomenon’.  So changes in the money supply may contain valuable information about spending and inflationary pressure in the economy.  Since money is essential for buying goods and services, it is likely to contain corroborative information about the current level of nominal spending in the economy.  It may also provide incremental information about future movements in nominal spending, and so can be a useful indicator of future inflationary pressure.  Finally, the behaviour of money may help to reveal the nature of the monetary transmission mechanism, especially when monetary policy is operated through ‘quantitative easing’ (QE).  In practice, a key difficulty is assessing which measures of money are the appropriate ones to look at for each of the different purposes.  The Bank currently constructs a number of monetary aggregates and publishes a range of data that allow to be created, summarised in Table 1.  Chart A shows some long-run historical time series of the growth of monetary aggregates compared with that of nominal spending in the economy.(1)  Given the various changes in the UK monetary regime over the past 150 years, it is unlikely that a single monetary indicator perfectly captures both the corroborative and incremental information in money.  The UK financial sector has also undergone various structural changes that need to be taken into account when considering the underlying link between money and spending.  For example, during periods when the financial sector has grown relative to the rest of the economy (such as in the early 1980s and the 2000s), broad money has tended to grow persistently faster than nominal spending.  Narrower measures of money, such as notes and coin and sight deposits (accounts that can be withdrawn immediately without penalty) are, in principle, better corroborative indicators of spending, as these are likely to be the types of money used to carry out the majority of transactions in goods and services in the economy.  The sum of notes and coin and sight deposits held by the non-bank private sector is sometimes known as zero maturity money or ‘MZM’.(2)  Broader measures of money might be more appropriate as incremental indicators of future spending and more revealing about the nature of the transmission mechanism.  M2, for example, additionally includes household time deposits such as savings accounts.(3)  And M4 is an even broader measure, including all sight and time deposits held by non-financial companies and non-bank financial companies.  The main article describes how QE works by first increasing the deposits of financial companies.  As these companies rebalance their  portfolios, asset prices are likely to increase and, with a lag, lead to an increase in households’ and companies’ spending.  So monitoring broad money has been an important part of assessing the effectiveness of QE.(4)  A number of econometric studies have suggested that sectoral movements in broad money may also provide valuable incremental information about spending in the economy.(5)  For example, non-financial companies’ deposits appear to be a leading indicator of business investment in the economy. One can also try and weight different types of narrow and broad money together using some metric of how much each type of money is used in transactions — known as a Divisia index.(6)  In practice, the interest paid on a given type of money is typically used as a weighting metric.  That is because individuals and companies are only likely to hold money which earns a low interest rate relative to other financial instruments if it compensates them by providing greater transactions services.  Identifying the appropriate measurement of money has been complicated by the continued development of the financial sector.  This has both expanded the range of instruments that might serve as money and the range of financial institutions that borrow from and deposit with the traditional banking system.  For example, sale and repurchase agreements (known as repos) — where a company agrees to buy a security from a bank with agreement to sell it back later — are currently included in M4 since the claim held on the bank can be thought of as a secured deposit.  In addition, some economists have argued that a range of instruments that provide collateral for various types of borrowing and lending could also be included in a broader measure of money.(7)  Moreover, many of the non-bank institutions that hold deposits mainly intermediate between banks themselves.  The deposits of these institutions, known as ‘intermediate other financial corporations’ (IOFCs), are likely to reflect activities within the banking system that are not directly related to spending in the economy.(8)  For this reason, the Bank’s headline measure of broad money is M4ex, which excludes IOFC deposits.  (1)  These series involve splicing together current Bank of England data with historic data  on monetary aggregates.  A spreadsheet of the data is available at www.bankofengland.co.uk/publications/Documents/quarterlybulletin/2014/ longrunmoneydata.xls.  (2)  A narrower measure known as non-interest bearing M1 can also be constructed.  This measure has become a less useful aggregate as most sight deposits now pay some form of interest.  For example, during the financial crisis when interest rates fell close to zero, the growth of non-interest bearing M1 picked up markedly as the relative cost of holding a non-interest bearing deposit fell sharply compared to an interest-bearing one.  Focusing on M1 would have given a misleading signal about the growth of nominal spending in the economy.  (3)  M2 contains the non-bank private sector’s holdings of notes and coin plus ‘retail’  deposits which are deposits that pay an advertised interest rate.  Those will largely be deposits held by households but will also apply to some corporate deposits.  (4)  See Bridges, Rossiter and Thomas (2011) and Butt et al (2012). (5)  See, for example, Astley and Haldane (1995), Thomas (1997a, b) and Brigden and  Mizen (2004).  (6)  See Hancock (2005), for example. (7)  See, for example, Singh (2013). (8)  See Burgess and Janssen (2007) and  www.bankofengland.co.uk/statistics/Pages/iadb/notesiadb/m4adjusted.aspx for more detail.  Topical articles  Money creation in the modern economy  23  Table 1  Popular monetary aggregates that can be constructed from available UK data(a)  Name  Definition  Description(b)  Availability  Notes and coin  M0  Notes and coin in circulation outside the Bank of England.  Notes and coin plus central bank reserves.  The narrowest measure of money and used as an indicator of cash-based transactions.  1870–present(c)  Historically the base measure of money used in money multiplier calculations.  Often used as an approximate measure of the size of the Bank of England’s balance sheet. No longer published by the Bank of England but can be reconstructed.(d)  1870–present(c)  Non-interest bearing M1  Notes and coin plus non-interest bearing sight deposits held by the non-bank private sector.  An indicator of transactions in goods and services in the economy, less useful now since most sight deposits pay some form of interest.  1921–present(c)  Not published by the Bank of England but can be constructed from published components.  MZM  Notes and coin plus all sight deposits held by the non-bank private sector.  M2 or retail M4  Notes and coin plus all retail deposits (including retail time deposits) held by the non-bank private sector.  Notes and coin plus all sight and time deposits held with banks (excluding building societies) by the non-bank private sector.  Notes and coin, deposits, certificates of deposit, repos and securities with a maturity of less than five years held by the non-bank private sector.  M3  M4  M4ex  Divisia  An indicator of transactions in goods and services in the economy.  1977–present  Not published by the Bank of England but can be constructed from published components. The Bank also produces a measure based on an ECB definition of M1.  A broader measure of money than MZM encompassing all retail deposits.  The key additions are household time deposits and some corporate retail time deposits.  1982–present  Published by the Bank of England.  The Bank also produces a measure based on an ECB definition of M2.  Up until 1987 the headline broad monetary aggregate constructed by the Bank of England.  1870–1990(c)  The Bank also produces a measure based on an ECB definition of M3.  Up until 2007 the headline broad monetary aggregate constructed by the Bank of England.  1963–present  M4 excluding the deposits of IOFCs.  Since 2007 the headline broad monetary aggregate constructed by the Bank of England.  1997–present  A weighted sum of different types of money.  Aims to weight the component assets of broad money according to the transactions services they provide.(e)  1977–present  (a)  All definitions refer to sterling instruments only.  Some of the definitions in this table were changed at various points in time.  For example the original M3 aggregate included public sector deposits and thesector’s holdings of deposits in foreign currency.  A more comprehensive history of the development of UK monetary aggregates can be found at www.bankofengland.co.uk/statistics/Documents/ms/articl  non-bank private es/art2jul03.pdf.  (b)  Published by the Bank of England unless otherwise stated. (c)  This series uses the data constructed by Capie and Webber (1985). (d)  Data on M0 were discontinued following reforms to the Bank of England’s money market operations in 2006.  See www.bankofengland.co.uk/statistics/Documents/ms/articles/artjun06.pdf for more detai(e)  The Divisia indices for other financial corporations and for the non-bank private sector were discontinued in 2013.  See www.bankofengland.co.uk/statistics/Documents/ms/articles/art1aug13.pdf for more  ls. details.  Chart A  Different monetary aggregates and nominal spending  Notes and coin(a) Non-interest bearing M1(b)  MZM(c) M2(d) Divisia M3/M4/M4ex(e)  Nominal GDP(f)  Percentage changes on a year earlier  1870  80  90  1900  10  20  30  40  50  60  70  80  90  2000 10  60  50  40  30  20  10 + 0 – 10  20  30  Sources:  Bank of England, Capie and Webber (1985), Mitchell (1988), ONS, Sefton and Weale (1995), Solomou and Weale (1991) and Bank calculations.  All series seasonally adjusted and break-adjusted where possible.  Historical data seasonally adjusted using X12.  (a)  1969 Q2 to 2013 Q4 — notes and coin in circulation.  1870 Q1 to 1969 Q2 — M0 from Capie and Webber (1985). (b)  1977 Q1 to 2013 Q4 — notes and coin held by the non-bank and building society private sector plus non-interest bearing deposits.  Prior to 2008 Q1, excludes deposits with  building societies.  1963 Q1 to 1977 Q1 — historical M1 data from Bank of England Quarterly Bulletins.  1921 Q4 to 1963 Q1 — Capie and Webber (1985). (c)  Notes and coin held by the non-bank and building society private sector plus total sight deposits.  Prior to 1998 Q4 excludes deposits with building societies. (d)  Notes and coin and retail deposits held by the non-bank and building society private sector. (e)  1997 Q4 to 2013 Q4 — M4 excluding intermediate OFCs.  1963 Q1 to 1997 Q4 — M4.  1870 Q2 to 1963 Q1 — M3 from Capie and Webber (1985). (f)  Composite estimate of nominal GDP at market prices.  See appendix of Hills, Thomas and Dimsdale (2010) for details.  24  Quarterly Bulletin  2014 Q1  Rate, asset purchases are a way in which the MPC can loosen the stance of monetary policy in order to stimulate economic activity and meet its inflation target.  But the role of money in the two policies is not the same.  QE involves a shift in the focus of monetary policy to the quantity of money:  the central bank purchases a quantity of assets, financed by the creation of broad money and a corresponding increase in the amount of central bank reserves. The sellers of the assets will be left holding the newly created deposits in place of government bonds.  They will be likely to be holding more money than they would like, relative to other assets that they wish to hold.  They will therefore want to rebalance their portfolios, for example by using the new deposits to buy higher-yielding assets such as bonds and shares issued by companies — leading to the ‘hot potato’ effect discussed earlier.  This will raise the value of those assets and lower the cost to companies of raising funds in these markets.  That, in turn, should lead to higher spending in the economy.(1)  The way in which QE works therefore differs from two common misconceptions about central bank asset purchases:  that QE involves giving banks ‘free money’;  and that the key aim of QE is to increase bank lending by providing more reserves to the banking system, as might be described by the money multiplier theory.  This section explains the relationship between money and QE and dispels these misconceptions.  The link between QE and quantities of money QE has a direct effect on the quantities of both base and broad money because of the way in which the Bank carries out its asset purchases.  The policy aims to buy assets, government bonds, mainly from non-bank financial companies, such as pension funds or insurance companies.  Consider, for example, the purchase of £1 billion of government bonds from a pension fund.  One way in which the Bank could carry out the purchase would be to print £1 billion of banknotes and swap these directly with the pension fund.  But transacting in such large quantities of banknotes is impractical.  These sorts of transactions are therefore carried out using electronic forms of money.  As the pension fund does not hold a reserves account with the Bank of England, the commercial bank with whom they hold a bank account is used as an intermediary.  The pension fund’s bank credits the pension fund’s account with £1 billion of deposits in exchange for the government bonds.  This is shown in the first panel of Figure 3.  The Bank of England finances its purchase by crediting reserves to the pension fund’s bank — it gives the commercial bank an IOU (second row).  The commercial bank’s balance sheet expands:  new deposit liabilities are matched with an asset in the form of new reserves (third row).  Figure 3  Impact of QE on balance sheets(a)  Before asset purchase  After asset purchase  Pension fund  Assets  Liabilities  Assets  Liabilities  Government debt  Other  Deposits  Other  Central bank(b)  Assets  Liabilities  Assets  Liabilities  Government debt  Reserves  Other assets  Reserves  Other assets  Commercial bank  Assets  Liabilities  Assets  Liabilities  Reserves  Deposits Reserves  Deposits (a)  Balance sheets are highly stylised for ease of exposition:  quantities of assets and liabilities shown do not correspond to the quantities actually held by those sectors.  The figure only shows assets and liabilities relevant to the transaction.  (b)  Government debt is actually purchased by the Bank of England’s Asset Purchase Facility using a loan from the Bank of England, so does not actually appear directly on the Bank’s official consolidated balance sheet.  Two misconceptions about how QE works Why the extra reserves are not ‘free money’ for banks While the central bank’s asset purchases involve — and affect — commercial banks’ balance sheets, the primary role of those banks is as an intermediary to facilitate the transaction between the central bank and the pension fund.  The additional reserves shown in Figure 3 are simply a by-product of this transaction.  It is sometimes argued that, because they are assets held by commercial banks that earn interest, these reserves represent ‘free money’ for banks.  While banks do earn interest on the newly created reserves, QE also creates an accompanying liability for the bank in the form of the pension fund’s deposit, which the bank will itself typically have to pay interest on.  In other words, QE leaves banks with both a new IOU from the central bank but also a new, equally sized IOU to consumers (in this case, the pension fund), and the interest rates on both of these depend on Bank Rate.  Why the extra reserves are not multiplied up into new loans and broad money As discussed earlier, the transmission mechanism of QE relies on the effects of the newly created broad — rather than base — money.  The start of that transmission is the creation of  (1)  The ways in which QE affects the economy are covered in more detail in Benford et al (2009), Joyce, Tong and Woods (2011) and Bowdler and Radia (2012).  The role of money more specifically is described in Bridges, Rossiter and Thomas (2011), Bridges and Thomas (2012) and Butt et al (2012).  Topical articles  Money creation in the modern economy  25  bank deposits on the asset holder’s balance sheet in the place of government debt (Figure 3, first row).  Importantly, the reserves created in the banking sector (Figure 3, third row) do not play a central role.  This is because, as explained earlier, banks cannot directly lend out reserves.  Reserves are an IOU from the central bank to commercial banks.  Those banks can use them to make payments to each other, but they cannot ‘lend’ them on to consumers in the economy, who do not hold reserves accounts.  When banks make additional loans they are matched by extra deposits — the amount of reserves does not change.  Moreover, the new reserves are not mechanically multiplied up into new loans and new deposits as predicted by the money multiplier theory.  QE boosts broad money without directly leading to, or requiring, an increase in lending.  While the first leg of the money multiplier theory does hold during QE — the monetary stance mechanically determines the quantity of reserves — the newly created reserves do not, by themselves, meaningfully change the incentives for the banks to create new broad money by lending.  It is possible that QE might indirectly affect the incentives facing banks to make new loans, for example by reducing their funding costs, or by increasing the quantity of credit by boosting activity.(1)  But equally, QE could lead to companies repaying bank credit, if they were to issue more bonds or equity and use those funds  to repay bank loans.  On balance, it is therefore possible for QE to increase or to reduce the amount of bank lending in the economy.  However these channels were not expected to be key parts of its transmission:  instead, QE works by circumventing the banking sector, aiming to increase private sector spending directly.(2)  Conclusion  This article has discussed how money is created in the modern economy.  Most of the money in circulation is created, not by the printing presses of the Bank of England, but by the commercial banks themselves:  banks create money whenever they lend to someone in the economy or buy an asset from consumers.  And in contrast to descriptions found in some textbooks, the Bank of England does not directly control the quantity of either base or broad money.  The Bank of England is nevertheless still able to influence the amount of money in the economy.  It does so in normal times by setting monetary policy — through the interest rate that it pays on reserves held by commercial banks with the Bank of England.  More recently, though, with Bank Rate constrained by the effective lower bound, the Bank of England’s asset purchase programme has sought to raise the quantity of broad money in circulation. This in turn affects the prices and quantities of a range of assets in the economy, including money.  (1)  A similar mechanism whereby QE could increase bank lending by enabling banks to  attract more stable funding is discussed in Miles (2012).  (2)  These channels, along with the effect of QE on bank lending more broadly, are  discussed in detail in a box in Butt et al (2012).  26  Quarterly Bulletin  2014 Q1  References  Astley, M and Haldane, A (1995), ‘Money as an indicator’, Bank of England Working Paper No. 35.  Bank of England (1999), ‘The transmission mechanism of monetary policy’, available at www.bankofengland.co.uk/publications/ Documents/other/monetary/montrans.pdf.  Benes, J and Kumhof, M (2012), ‘The Chicago Plan revisited’, IMF Working Paper No. 12/202.  Benford, J, Berry, S, Nikolov, K, Robson, M and Young, C (2009), ‘Quantitative easing’, Bank of England Quarterly Bulletin, Vol. 49, No. 2, pages 90–100.  Bernanke, B (2007), ‘The financial accelerator and the credit channel’, speech at a conference on The Credit Channel of Monetary Policy in the Twenty-first Century, Federal Reserve Bank of Atlanta.  Bindseil, U (2004), ‘The operational target of monetary policy and the rise and fall of the reserve position doctrine’, ECB Working Paper No. 372.  Bowdler, C and Radia, A (2012), ‘Unconventional monetary policy: the assessment’, Oxford Review of Economic Policy, Vol. 28, No. 4, pages 603–21.  Clews, R, Salmon, C and Weeken, O (2010), ‘The Bank’s money market framework’, Bank of England Quarterly Bulletin, Vol. 50, No. 4, pages 292–301.  Congdon, T (1992), Reflections on monetarism, Clarendon Press.  Congdon, T (2005), ‘Money and asset prices in boom and bust’, Institute of Economic Affairs, Hobart Paper No. 152.  Disyatat, P (2008), ‘Monetary policy implementation: misconceptions and their consequences’, BIS Working Paper No. 269.  Farag, M, Harland, D and Nixon, D (2013), ‘Bank capital and liquidity’, Bank of England Quarterly Bulletin, Vol. 53, No. 3, pages 201–15.  Friedman, M (1963), Inflation:  causes and consequences, Asia Publishing House.  Haldane, A (2009), ‘Why banks failed the stress test’, available at www.bankofengland.co.uk/archive/documents/historicpubs/ speeches/2009/speech374.pdf.  Hancock, M (2005), ‘Divisia money’, Bank of England Quarterly Bulletin, Spring, pages 39–46.  Bridges, J, Rossiter, N and Thomas, R (2011), ‘Understanding the recent weakness in broad money growth’, Bank of England Quarterly Bulletin, Vol. 51, No. 1, pages 22–35.  Hills, S, Thomas, R and Dimsdale, N (2010), ‘The UK recession in context — what do three centuries of data tell us?’, Bank of England Quarterly Bulletin, Vol. 50, No. 4, pages 277–91.  Bridges, J and Thomas, R (2012), ‘The impact of QE on the UK economy — some supportive monetarist arithmetic’, Bank of England Working Paper No. 442.  Brigden, A and Mizen, P (2004), ‘Money, credit and investment in the UK industrial and commercial companies sector’, The Manchester School, Vol. 72, No. 1, pages 72–79.  Burgess, S and Janssen, N (2007), ‘Proposals to modify the measurement of broad money in the United Kingdom:  a user consultation’, Bank of England Quarterly Bulletin, Vol. 47, No. 3, pages 402–14.  Butt, N, Domit, S, Kirkham, L, McLeay, M and Thomas, R (2012), ‘What can the money data tell us about the impact of QE?’, Bank of England Quarterly Bulletin, Vol. 52, No. 4, pages 321–31.  Button, R, Pezzini, S and Rossiter, N (2010), ‘Understanding the price of new lending to households’, Bank of England Quarterly Bulletin, Vol. 50, No. 3, pages 172–82.  Capie, F and Webber, A (1985), A monetary history of the United Kingdom, 1870–1982, Vol. 1, Routledge.  Carpenter, S and Demiralp, S (2012), ‘Money, reserves, and the transmission of monetary policy:  does the money multiplier exist?’, Journal of Macroeconomics, Vol. 34, No. 1, pages 59–75.  Howells, P (1995), ‘The demand for endogenous money’, Journal of Post Keynesian Economics, Vol. 18, No. 1, pages 89–106.  Jackson, C and Sim, M (2013), ‘Recent developments in the sterling overnight money market’, Bank of England Quarterly Bulletin, Vol. 53, No. 3, pages 223–32.  Joyce, M, Tong, M and Woods, R (2011), ‘The United Kingdom’s quantitative easing policy:  design, operation and impact’, Bank of England Quarterly Bulletin, Vol. 51, No. 3, pages 200–12.  Kaldor, N and Trevithick, J (1981), ‘A Keynesian perspective on money’, Lloyds Bank Review, January, pages 1–19.  Kamath, K, Reinold, K, Nielsen, M and Radia, A (2011), ‘The financial position of British households:  evidence from the 2011 NMG Consulting survey’, Bank of England Quarterly Bulletin, Vol. 51, No. 4, pages 305–18.  Kydland, F and Prescott, E (1990), ‘Business cycles:  real facts and a monetary myth’, Federal Reserve Bank of Minneapolis Quarterly Review, Vol. 14, No. 2, pages 3–18.  Laidler, D (1984), ‘The buffer stock notion in monetary economics’, The Economic Journal, Vol. 94, Supplement:  Conference Papers, pages 17–34.  Topical articles  Money creation in the modern economy  27  Laidler, D and Robson, W (1995), ‘Endogenous buffer-stock money’, Credit, interest rate spreads and the monetary policy transmission mechanism, Session 3, conference on The Transmission of Monetary Policy held at the Bank of Canada in November 1994.  Miles, D (2012), ‘Asset prices, saving and the wider effects of monetary policy’, available at www.bankofengland.co.uk/ publications/Documents/speeches/2012/speech549.pdf.  Mitchell, B R (1988), British historical statistics, Cambridge University Press.  Moore, B (1988), Horizontalists and verticalists:  the macroeconomics of credit money, Cambridge University Press.  Palley, T (1996), Post Keynesian economics:  debt, distribution and the macro economy, Macmillan.  Sefton, J and Weale, M (1995), Reconciliation of National Income and Expenditure:  balanced estimates of national income for the United Kingdom, 1920–1990, Cambridge University Press.  Singh, M (2013), ‘Collateral and monetary policy’, IMF Working Paper No. 13/186.  Solomou, S N and Weale, M (1991), ‘Balanced estimates of UK GDP 1870–1913’, Explorations in Economic History, Vol. 28, No. 1, pages 54–63.  Thomas, R (1997a), ‘The demand for M4:  a sectoral analysis, Part 1 — the personal sector’, Bank of England Working Paper No. 61.  Thomas, R (1997b), ‘The demand for M4:  a sectoral analysis, Part 2 — the corporate sector’, Bank of England Working Paper No. 62.  Tobin, J (1963), ‘Commercial banks as creators of ‘money’’, Cowles Foundation Discussion Papers No. 159.  Tucker, P (2009), ‘The debate on financial system resilience: macroprudential instruments’, available at www.bankofengland.co.uk/archive/Documents/historicpubs/ speeches/2009/speech407.pdf.  Tucker, P, Hall, S and Pattani, A (2013), ‘Macroprudential policy at the Bank of England’, Bank of England Quarterly Bulletin, Vol. 53, No. 3, pages 192–200.  "
https://news.ycombinator.com/rss,"‘Excuuuuse me, Princess ’: An oral history of The Legend of Zelda cartoon",https://www.polygon.com/zelda/23540526/legend-of-zelda-cartoon-oral-history-zeldathon,Comments,"

Share this story




Share this on Facebook





Share this on Twitter








Share
All sharing options






Share
All sharing options for:
‘Excuuuuse me, Princess!’: An oral history of The Legend of Zelda cartoon












Reddit







Pocket









Flipboard





Email









This story is part of a group of stories called 





    In 2023, Polygon is embarking on a Zeldathon. Join us on our journey through The Legend of Zelda series, from the original 1986 game to the release of The Legend of Zelda: Tears of the Kingdom, and beyond.
  


The world knows The Legend of Zelda’s Link as the brave hero of Hyrule — a young warrior of few words. Link is a master with his bow and an excellent swordsman. But back in 1989, when The Legend of Zelda cartoon first aired, all Link wanted was a smooch. A kiss from Zelda, to be exact — but he’s not exactly picky, and unlike the laconic hero of the games, he would not shut up about it. The hero of Hyrule is still tasked with defending the Triforce of Wisdom from Ganon’s grasp on the TV show, but that’s secondary to his insistence on a little kiss. The show’s bizarre portrayal of Link — especially his constant begging of “Excuse me, Princess!” — has made The Legend of Zelda cartoon a hilarious head-scratcher to this day. 




In 2023, Polygon is embarking on a Zeldathon. Join us on our journey through The Legend of Zelda series, from the original 1986 game to the release of The Legend of Zelda: Tears of the Kingdom, and beyond.



Back in 1989, The Legend of Zelda aired in 15-minute episodes every Friday during The Super Mario Bros. Super Show!, a mix of live-action and animated segments based on Nintendo games. Once a week, The Legend of Zelda replaced the Super Mario Bros. show, which featured animated segments of Mario and Luigi but, more memorably, the wacky, iconic live-action performances of WWF wrestler Lou Albano as Mario and The Jeffersons’ Danny Wells as Luigi, who welcomed fans of the show with the catchphrase, “Hey there, paisanos.”
Clearly, Super Mario Bros. was the main event for the Nintendo-themed TV block. It ran for 52 episodes compared to The Legend of Zelda’s 13. But for the writers of the Zelda cartoon, that was a boon: They had very little oversight and direction beyond character designs, a franchise “bible” provided by Nintendo, and the original game, also called The Legend of Zelda, and its sequel, Zelda 2: The Adventure of Link. As they were not video game players themselves, the writers did their research and decided to go in a different direction — one that’s more focused on story than gameplay. There were elements of the games, like sound effects and visuals, but the show mostly has Zelda and Link posted up in Hyrule castle defending the Triforce of Wisdom from Ganon while trying to acquire the Triforce of Power from the evil wizard himself. (The Triforces talk, by the way.)
Between the mischief that Zelda, Link, and fairy friend Spryte get into, The Legend of Zelda relied heavily on the relationship between Zelda and Link. Zelda, donning pink pants and purple thigh-high boots, more often plays the hero to Link’s bumbling teenage angst.
What we get from the short-lived ordeal is a charming and absurd rendition of a beloved (and often quiet and unvoiced) franchise.
 









Image: DiC Entertainment/Nintendo



From pixels to the small screen
Most of the Super Mario Bros. Super Show’s budget was tied up in the main part of the show — the Super Mario Bros. show that led the time slot. When Super Mario. Bros Super Show was canceled, The Legend of Zelda was shut down alongside it. But for the show’s short run, writers said they had little interference from Nintendo, which just wanted more eyes on its game properties — especially a new one like The Legend of Zelda. It was the first time — and still one of the rare times — that Link and Zelda got their own voice actor performances, and probably not the ones fans expected. 
Rather than simply recreating the video game, The Legend of Zelda’s writers positioned the show more as a mix of action, comedy, and drama, taking specific inspiration from Cybill Shepherd’s and Bruce Willis’ ’80s show Moonlighting. Writers wanted Zelda and Link’s relationship to mirror Shepherd’s and Willis’ rapport as Maddie and David on the detective show — the same angry sexual tension, but goofier and lighter for the kid-friendly cartoon TV show. 
 











Bob Forward Story editor and writer, The Legend of Zelda
The Legend of Zelda was going to be a small addendum to the Super Mario Bros. Super Show, which was the actual star of the time slot. DiC needed somebody who could handle it on their own without a lot of supervision. After we had the initial discussion, they supplied me with a VHS tape of [a playthrough of] the game itself, since I wasn’t actually a person who played video games — not that I had any objection. I just hadn’t really done it. They had a playthrough of the game that my sons were fascinated by. That was my research for it.
I don’t know if anyone cares about this, but the playthrough VHS tape that they supplied me with I guess had been played by one of the new Charlie’s Angels. I think it was Tonya Roberts. I guess she was a gamer when she was younger.

 









Image: DiC Entertainment/Nintendo



 











Reed Shelly Story editor and writer, Super Mario Bros. Super Show
The project originated as a concept by Andy Heyward as Super Mario Bros. Power Hour, a one hour-long animation block that would have featured series based on a number of intellectual properties. Concept art was produced for adaptations of Super Mario Bros., The Legend of Zelda, Metroid, Castlevania, Double Dragon, and California Games. With the exception of Mario and Zelda, none of these additional adaptations were ultimately produced.

 











John Grusd Director, The Legend of Zelda
Nintendo wanted us to base the show on the new game [Zelda 2: The Adventure of Link], because, you know, it’s great marketing. What they did was give me the Japanese version of the game, because it wasn’t out here yet. I didn’t know anything about the game when I started. I’d never played them. I wasn’t a gamer or anything. That’s how I learned how the characters move, the sound effects, the music. I got to be able to do the games all the way through pretty quickly, as a matter of fact, because I knew all the shortcuts. I could get through both of them in less than two minutes, probably. It’s pretty fast.

 











Phil Harnage Writer, The Legend of Zelda
It was a fun little show. And I say little, because they tacked it on to Super Mario. It really should have been a stand-alone show. It was very limiting for what the writers could do. I worked on the bible and wrote a couple of episodes. When you write the bible, you hand it off to somebody else, but occasionally you get to write a script. That’s the fun part. It was a fun show to write for because of the tension between Link and the princess. We modeled it after Moonlighting. We tried to capture that, and I think we did. Maybe over the top a little bit, but that’s what we were shooting for. We could have come up with a lot more shows. That was the sad part, that we only got to do one season. 


 









Image: Eve Forward



 











Eve Forward Writer, The Legend of Zelda
My brother somehow ended up suggesting I try writing an episode, and I was able to turn out a couple of scripts that, with his editing, ended up getting used. I was about 16-17 at the time. The only direction I had was the show bible, which outlined the basic characters and sorts of stories they were looking for. I didn’t have a Nintendo, so I rented one, and the game, and tried to play it, but I didn’t get very far. But the basic relationships were all established in the show bible; Ganon bad guy, Zelda tough girl, Link charming scamp, Triforce MacGuffin, etc.
I did play Dungeons & Dragons though, at the time, and some of that feel made it into the show. [The seventh episode] “Doppelganger” was based on a cursed mirror in D&D. Well, the monsters in Zelda were all based on things from the Nintendo game; same with the weapons, like Link’s boomerang. But in D&D of course you’re always fighting monsters and imagining how cool your character looks doing it, so a lot of the various swashbuckling stuff I liked to put in was based on things that had happened in our D&D games. I always thought of Link as more of a rogue than a fighter.

 











Bob Forward
We had a schedule we had to put the scripts through, and I think it was two a week. That wasn’t hard — I worked on shows we had to do five a week, so two a week was just fine. Eve and I were just writing them on our own. We even had my mom pitch a story. She wrote something that we ended up having to do a lot of work on, but it wasn’t a bad initial concept. [Bob and Eve’s mom, Marsha Forward, had her script adapted as The Legend of Zelda’s 11th episode, “Fairies in the Spring.”]
I wrote a bible for my own purposes, something that just outlined who all the characters were and what they wanted. Robby London [DiC executive] wanted to have some signature lines, and Moonlighting had just come out, or was very popular. Robby London came up with the idea of the line, “Excuuuuse me, Princess,” which is inspired by the Moonlighting relationship and a snarky line from a Steve Martin routine. I’ll be honest, what I liked about Robby is that he would make quick decisions. As much as I was giving him a hard time about it, I put [that line] into the show way more than it was really necessary. But it turned out to be OK, even though people made fun of it. People remembered it, so I guess he was right. I have to admit, it caught on.

 









Image: DiC Entertainment/Nintendo




No one had ever heard Link or Zelda speak
People were certainly familiar with Link and Zelda by the time The Legend of Zelda cartoon was released — The Legend of Zelda and Zelda 2: The Adventure of Link had been out for some time and already were popular. But characters were composed of just a few dozen pixels, and they weren’t voiced. It gave the TV writers lots of room to mess around; the show existed outside of the games, with Link just hanging around Zelda and her father’s castle, defending the Triforce from Ganon every once in a while. 
The show had to be largely carried by Link and Zelda’s personalities, plus the few other characters who appeared: Spryte, a fairy, and the two talking Triforce pieces (Wisdom and Power). So, the writers made those few characters big. Ganon is merely an annoyance to Link, whose more pressing problem is convincing Zelda to give him a kiss.
 











Bob Forward
We very much made it up as we went along. The other nice thing was that everybody was so concentrated on the Mario brothers that they completely left us alone, which is always my favorite way of working. You know, as long as we hit the page count and got the scripts in on time, nobody was looking. 
Link always wanted a kiss. That was one of Robby’s inventions. I thought it worked out. I was down for it. I kept expecting people to tell us we couldn’t do it. But apparently it worked.

 











Jonathan Potts Voice actor, Link
I pictured Link as being a teenager who was like the ultimate teenage boy, who was like a puppy. If you can imagine what a puppy would be [like] — running around, peeing on the carpet, and overreacting — everything was dramatic. I remember wanting to do that. I wasn’t a teenager then; I was well into my 20s when I did the part. I had to be that youthful, goofy teenage boy who acts before he thinks.

 











Cynthia Preston Voice actor, Zelda
You start reading something and you just have instincts — all of your experience, and all of the movies you’ve done, and all of the classes you’ve taken, and that feeds into how to start molding a character. What does this character want? What do they want from this character? I don’t think I was playing Zelda as a teenager. She was an independent woman — a young woman, but she was independent. She didn’t need a hero to save her, and that was so cool.
The show certainly wasn’t ahead of its time, but nonetheless it was a cool aspect that it wasn’t playing a damsel in distress.

 











Phil Harnage
We didn’t want a Disney princess. We’re not going to be selling princess dresses to six-year-olds. So yeah, she was an action hero in her own right, and that was kind of unique. But the writers didn’t come up with [Zelda wearing pants] — that was something the artists came up with, and Nintendo loved it.
It was ahead of its time in some ways, but wasn’t always. Zelda was a good role model for girls. She was confident and took charge. She did want what she wanted, but was also very responsible. And Link was irresponsible. He was out there conniving: “How am I gonna get her to kiss me?” There’s fun in that. That’s where the Moonlighting model really worked.

 











Jonathan Potts
The scripts weren’t complex. There weren’t a lot of deep things going on. It was all right there, sort of obvious. So [direction] usually came down to technical things — more energy. 

 











Cynthia Preston
There was this time a director wanted me to laugh more as Zelda. I was trying, but laughing is harder than crying to do naturally. Shockingly, he mooned me and I fell over laughing. I really have the feeling I didn’t get the right laugh, but it was damn funny.

 









Image: DiC Entertainment/Nintendo



A talking Triforce?
Writers said Nintendo didn’t want them coming up with new characters and backstories, so they worked with what they had. That’s where the Triforce pieces came in — the show couldn’t only be Zelda, Link, Ganon, and Spryte. There was the Triforce. Why not make it talk? Successful or not, the Wisdom piece of the Triforce did have a role in the show: Moving the story forward and explaining the situation.
 











Bob Forward
Link and Zelda wanted the Triforce of Power, and Ganon wanted the Triforce of Wisdom, so [in] half the shows Link and Zelda would be the ones to instigate the action as opposed to just hanging around and waiting for Ganon to start something and trying to reestablish the status quo.

 











Phil Harnage
The whole Triforce thing, it came out of the game and everything, but I don’t know — it was hard to figure out. What does that mean? The Triforce? What do you have to do with it to make it work? I wasn’t really happy with that. I thought it would be much more fun to have them fighting over who’s going to control the land. But [the Triforce] was from the game, and you had to do it for the gamers. 
The more things talk, the more explanatory it can be. You’re like, Why did this happen? And the Triforce can tell you, you know? It’s magic. In a magical world, you have to set the rules, of course. But you set the rules yourself.

 









Image: DiC Entertainment/Nintendo



A sword fighter in a show without fighting
For a TV show about a game with a hero who hits things with swords, The Legend of Zelda has surprisingly little sword fighting. The Legend of Zelda was a kids’ TV show, and that meant it had to  follow the network’s standards — so characters couldn’t die. Link and Zelda still have weapons, of course, but they don’t seem deadly. Link’s sword shoots out magic bullets that stun enemies, and Zelda often uses a magic bow that uses magic instead of arrows.
 











John Grusd
Link has a sword, but can he actually use it to chop somebody’s head off? He can’t do what he does in the game. Nintendo wants us to do what they do in the game, but the standards and practices at the network say no. We can’t kill someone on children’s TV.

 











Phil Harnage
Magic brings a whole different ambiance to a cartoon, because it’s something you can do that’s not repeatable by kids. You can shoot a lightning bolt and turn someone into toast. And the toast gets up and walks away. You just have to be careful — you can’t do everything you want to do. You can’t do anything that could be copied by a child. You don’t want kids sword fighting.

 











Bob Forward
Link’s sword could fire like a ranged weapon. Actually hitting people with swords was questionable. It wasn’t something they wanted to do back then. It was easier to just shoot zaps from the sword. We also had to establish that nobody was dying, so there was the jar of evil or something, where everyone hit by zaps were sent to and got put into storage for a while. We had to downplay a lot of things.

 









Image: DiC Entertainment/Nintendo



One and done
While Super Mario Bros. Super Show had tons of episodes, The Legend of Zelda has only one season. That’s the way of TV cartoons — things get canceled and people move on. The Legend of Zelda itself has gone on to be one of Nintendo’s most successful properties, but the TV show is still a small part of that legacy.
 











Reed Shelly
The show feels like a time capsule to me. It’s such a different world now and so different for kids. The shows were made for a different era.
It was an incredible creative playground. We had to deliver 52 episodes at a rate of four a week [for the Super Mario. Bros Super Show]. We had live action, animation, and an action sequence set to a well known song. It was an amazing production circus to be a part of.
With Andy Heyward and Haim Saban executive producing and running the shows, we were allowed to have a ton of fun. All we had to do was make millions of kids laugh.

 











Eve Forward
I’ve no idea what the reception to the show was. This was in the days before internet; you couldn’t just log in and see your work torn apart in real time. My own feeling is that the Super Mario Bros. show wasn’t very good, especially the live-action bits, and that Zelda was the best part of it, but y’know, it was a cartoon, for kids. We weren’t trying to make Citizen Kane or something. But of course it was a huge thrill for me to see my work on television!

 











Phil Harnage
Part of the reason [the show was canceled] is that it wasn’t its own show — it was part of the Mario Bros. show. It was tied to it, and they didn’t want to renew The Mario Bros., and Zelda got shuffled off. History, at that point. I wish I had done more. We could have come up with a lot more shows. That was the sad part, that we only got to do one season.
I think the show holds up pretty well after all these years. They’re all on YouTube. [Ed. note: And Amazon Prime Video!] I don’t know if you know this, but we don’t get residuals.
Everybody wishes that Link and Zelda had gone on to bigger and better things [with the TV show], but they didn’t. You have these regrets about every show you do. Sometimes you wish you could have done more, that you could do more, but there were certain things you had to do to please the network.
We got a lot of good feedback from kids, and even older kids who knew the video game. They would watch the show out of curiosity and get sucked in. We had a few letters saying, “Oh, please don’t cancel it!” But getting a few letters isn’t enough to convince the network. They’re the boss, because they funded the things. DiC, the studio I worked for at the time — they were known for finding the current properties they could exploit. They were purely in the business to make money, like all the studios.

 











Jonathan Potts
I’m always surprised at how much notoriety it has. I don’t think it was a hit at the time, because then we would have done more. We did it years ago, and it was one season with 13 episodes. It was a one-and-done sort of thing. It had its time, and it just keeps growing. I get letters from all over the world. 
I was teaching voice classes at Second City, and the class would be people in their 30s, and the engineer would look at me like, Go ahead, tell them. And I’d say, “You know, I was the voice of Link in The Legend of Zelda,” and inevitably, three or four people would be like… I became a celebrity. I can’t believe that. It was just a gig years ago.
They would be so starstruck, which is a joke, because I’m not a star. But they’d get … [imitates expression of amazement]

 











Cynthia Preston
It came up [at a party] that I was the voice of Zelda in the cartoon, and [people] were so stunned. They rolled up their shirt sleeves, and they both have the Triforce tattooed on their arms. I’ve been at pitch sessions and somebody will find out that I’m the voice of Zelda, and the reaction is astounding. People love it so much. 

 











Reed Shelly
I remember on my first trip to Redmond and the Nintendo headquarters, they had a couple of hundred “game counselors” in a call center at computers giving tips to gamers calling in. It cost, as I remember, something like 99 cents a minute for players to get game tips. When a group got to go on their lunch break, they raced each other to play the newest arcade console game in the cafeteria. I remember thinking, “This computer gaming thing is gonna be big...” 




"
https://news.ycombinator.com/rss,Twitter API Page,https://developer.twitter.com/apitools,Comments,"




Twitter / Error















This page is down
I scream. You scream. We all scream... for us to fix this page. We’ll stop making jokes and get things up and running soon.
Retry




Home
Status
Terms of Service
Privacy Policy
Cookie Policy
Imprint
Ads info
© Twitter     






"
https://news.ycombinator.com/rss,1991: A server-side web framework written in Forth,https://www.1-9-9-1.com/,Comments,"


World Wild Web

                        The year is 1991. The World Wide Web has just seen public release. 1991 looks to ease your interactions with the new web using cutting edge programming techniques in Forth (well, Gforth).
                    


Logging In

                        Getting started in 1991 is easy.
                    

                        All you need to do is include 1991.fs into your Forth source file. Next, you can define your public routes using the /1991 word. Once your routes are all layed out, start the server using 1991:.
                    

\ app.fs
\ Load 1991.
include 1991.fs

\ Define our route handlers.
: handle-/ ( -- addr u )
    \ Any string returned by the handler
    \ will be output to the browser.
    s"" Hello, 1991."" ;

\ Set up our routes.
/1991 / handle-/

\ Start the server on port 8080.
8080 1991:

You can run the server using gforth app.fs.
Logging In II: Logging In, Deeper
Route Wildcards (Fuzzy Routing / URL Mapping)

                        If you want to specify that some part of a route is a wildcard (accepts any value), then you can wrap some named value in <chevrons>. 1991 will accept any URL that matches your wildcard pattern, setting the internal value of whatever you place between the chevrons to whatever is actually requested.
                    

                        In the example below, <uid> specifies that we're willing to accept any (non-empty) value in its place which we'd like to access using the name uid.
                    

\ wildcards.fs
\ Load 1991.
include 1991.fs

\ Define our route handler.
: handle-wildcard-route ( -- addr u )
    s"" contents of the route request: "" get-query-string s+ ;

\ Set up our route.
/1991 /users/<uid> handle-wildcard-route

\ We can set up multiple wildcards too (must be slash-separated).
/1991 /users/<uid>/posts/<pid> handle-wildcard-route

\ Start server on port 8080.
8080 1991:


                         All wildcards are treated similar to query string arguments. As such, wildcards can be retrieved using get-query-string.
                    

                        In the example above, visiting http://localhost:8080/users/urlysses will result in the following query string: uid=urlysses.
                    File Serving

                        Use a public/ directory to act as a basic fileserver.
                        Whenever a requested URL doesn't resolve through the registered routes, 1991 will attempt to find the requested route within your specified public directory.
                    

\ public.fs
\ Load 1991.
include 1991.fs

\ Specify the location of our public directory.
\ Anything in the public/ directory within the
\ same dir as this source file will resolve.
\ You can change ""public"" to anything you want
\ as long as it matches your directory name.
sourcedir s"" public"" s+ set-public-path

\ We can set mimetypes using the `filetype:` word.
\ In the case below, we want .mp4 files to be served
\ with the content-type video/mp4.
s"" video/mp4"" filetype: mp4

\ Start the server on port 8080.
8080 1991:


                        In the above example, If we have a file public/my-video.mp4, then it will be available through http://localhost:8080/my-video.mp4.
                    
Views

1991 offers basic templating through views.
                    

                        In order to get started, you should specify the views/ path. Notice the trailing slash, which differs from how we define public.
                    

                        Once you've specified your views/ directory, you can write views/ files to it. This can be any kind of file, honestly. The benefit offered by views/ is the ability to use basic templating. You can write any valid Forth code within opening (<$ ) and closing ( $>) tags. Additionally, you can use the import word to import other views into your view.
                    

\ views.fs
\ Load 1991.
include 1991.fs

\ Specify the location of our views directory.
sourcedir s"" views/"" s+ set-view-path

\ Define some words we'll use within
\ our view.
: page-title ( -- addr u )
    s"" Dynamic page title"" ;
: ten-lines ( -- )
    10 0 do
        s"" line "" i s>d <# #s #> s+
        s"" <br>"" s+
        $type
    loop ;

\ Use render-view to output the contents
\ of a file in the views/ directory.
: handle-/
    s"" v-index.html"" render-view ;

/1991 / handle-/

\ Start the server on port 8080.
8080 1991:


\ views/index.html
<!DOCTYPE html>
<html>
    <head>
        <meta charset=""utf-8"">
        <title><$ page-title $type $></title>
    </head>
    <body>
        <$ ten-lines $>
        <$ s"" imported-view.html"" import $>
    </body>
</html>


\ views/imported-view.html
It's possible to import view files from within other view files. This is from <code>views/imported-view.html</code>



Wait, what?
Why is 1991: post-fix when /1991 is pre-fix?

                        Forth is a (mostly) post-fix notation language. So, for example, you'd write two plus two as 2 2 +. This is the language's natural and immediate notation. Along those lines, 1991: is an immediate word——running it results in immediate action. As such, we use Forth's post-fix notation to set the port and start the server immediately. Alternately, /1991 doesn't exactly have immediate effect per se. All it does is tell 1991 that any request to /path should be handled by path-handler. As such, we opt to write non-immediate code using pre-fix notation.
                    
You're using Gforth, which came out in 1992. Also, it's 2017.
Okay. But Fredric Jameson establishes that in postmodernism we have experienced a weakening sense of historisity such that what is, what was, and what will be all exist as presents in time. 1970, 1991, 1992, and 2017 all happen simultaneously. Hence developers working on new projects while still coding in decades-old text editors. They write the future in the past and are made present in so doing.


"
https://news.ycombinator.com/rss,NASA’s Double Asteroid Redirection Test Is a Smashing Success,https://eos.org/articles/nasas-double-asteroid-redirection-test-is-a-smashing-success,Comments,"

Posted inNews 
			NASA’s Double Asteroid Redirection Test Is a Smashing Success		

			The mission, focused on the Didymos-Dimorphos binary asteroid system, proved that an asteroid’s orbit can be altered by kinetic impactor technology.		




by
Katherine Kornei 
12 January 202312 January 2023 
Share this:Print 



 This illustration of NASA’s Double Asteroid Redirection Test (DART) spacecraft and the Italian Space Agency’s LICIACube depicts them just prior to impact at the Didymos binary system on 26 September 2022. Credit: NASA/Johns Hopkins APL/Steve Gribben





Rocks from space have walloped Earth for eons, and it’s only a matter of time until our planet lands yet again in the crosshairs of a very large asteroid. But unlike other forms of life—here’s looking at you, dinosaurs—humans have a fighting chance of altering our cosmic destiny. At AGU’s Fall Meeting 2022 held in December, researchers presented a slate of new results from NASA’s Double Asteroid Redirection Test (DART) mission, the first demonstration of asteroid deflection.
Peering at an Orbit
DART’s target, the Didymos-Dimorphos asteroid system, was first discovered in the mid-1990s. But astronomers back then spotted only its larger member, Didymos, which is roughly 800 meters (half a mile) in diameter. It wasn’t until 2003 that scientists realized that a much smaller body, dubbed Dimorphos, was also present. Dimorphos is about one fifth the size of Didymos, and its orbit takes it in front of and behind Didymos as seen from Earth. That’s serendipitous, because by monitoring how the brightness of the Didymos-Dimorphos asteroid system varies over time, scientists were able to precisely determine how long it took Dimorphos to complete an orbit: 11 hours and 55 minutes.
“We needed to understand the Didymos-Dimorphos system before we changed it.”
“We needed to understand the Didymos-Dimorphos system before we changed it,” said Cristina Thomas, a planetary scientist at Northern Arizona University in Flagstaff, at AGU’s Fall Meeting 2022.
 



This newsletter rocks.
Get the most fascinating science news stories of the week in your inbox every Friday.

Sign up now



The primary goals of the DART mission were simple, at least in concept: Hit Dimorphos with the roughly 570-kilogram (half-ton) DART spacecraft to alter the orbital period of Dimorphos around Didymos significantly and measure that change and characterize the physics of the impact. If successful, it would be the first demonstration of deflecting an asteroid using so-called kinetic impactor technology. (In 2005, another NASA mission, Deep Impact, tested kinetic impactor technology with a comet.)
On 23 November 2021, a Falcon 9 rocket lifted off from California’s Vandenberg Space Force Base. By then, the SpaceX-designed rocket had notched more than 100 successful launches, but for members of the DART mission, the event was anything but ordinary: Nestled within the rocket’s nose cone was the spacecraft they’d spent well over a decade designing, building, and testing.
The launch went smoothly, and DART soon entered into orbit around the Sun. For roughly 10 months, the spacecraft largely tracked the orbit of Earth, essentially waiting to catch up to the Didymos-Dimorphos asteroid system, which orbits the Sun between Earth and Mars. “We stayed close to Earth the entire time and just caught up with the Didymos system at its closest approach to Earth,” said Elena Adams, DART mission systems engineer at the Johns Hopkins University Applied Physics Laboratory in Laurel, Md.
Approaching the Unknown
It was only around July of 2022 that DART’s onboard camera—the Didymos Reconnaissance and Asteroid Camera for Optical navigation (DRACO)—caught its first glimpse of Didymos. But Dimorphos wouldn’t come into view until much, much later: Just an hour before impact, at a distance of roughly 25,000 kilometers, the tiny moonlet was still a mere two pixels across in DRACO images.
“We didn’t see Dimorphos until late in the game,” said Adams. To prepare for the uncertainties of impacting a body they knew virtually nothing about, DART team members ran thousands of Monte Carlo simulations beforehand in which they varied the moonlet’s size, shape, albedo, and a slew of other parameters.
The DART spacecraft successfully impacted Dimorphos on 26 September 2022. The event was recorded by a cadre of Earth-based telescopes and also the Light Italian Cubesat for Imaging of Asteroids (LICIACube), a briefcase-sized spacecraft carrying two cameras that launched with DART and was released from the spacecraft 15 days prior to impact.
A Serendipitous Boost
Researchers had calculated that the impact, which occurred roughly head-on, would shorten Dimorphos’s orbital period by just under 10 minutes. That was assuming the simplest case of no ejecta being produced, said Andy Cheng, DART investigation team lead at the Johns Hopkins University Applied Physics Laboratory, at a press conference.
“The amount of momentum that you put in the target is exactly equal to the momentum that the spacecraft came in with.” But if ejecta flies off the asteroid after impact, physics dictates that the asteroid can get an extra boost, said Cheng. “You end up with a bigger deflection.”
“If you’re trying to save the Earth, that makes a big difference.”
That’s good news when it comes to pushing a potentially harmful space rock out of the way, said Cheng. “If you’re trying to save the Earth, that makes a big difference.”
And ejecta there was, in spades—on the basis of detailed follow-up observations of the Didymos-Dimorphos system, scientists discovered that Dimorphos is now traveling around Didymos once every 11 hours and 22 minutes. That’s a full 33 minutes shorter than its original orbital period, a finding that implied that a substantial amount of ejecta was produced. Imagery obtained from ground- and space-based telescopes has borne that out—a plume of debris tens of thousands of kilometers long currently stretches out from Dimorphos. Researchers have estimated that at least a million kilograms (1,100 U.S. tons) of material were blasted off the asteroid by the impact. That’s enough debris to fill several rail cars, said Andy Rivkin, DART investigation team lead at the Johns Hopkins University Applied Physics Laboratory, at a press conference at the Fall Meeting.
Follow the Debris
Interestingly, the ejecta shed by Dimorphos has remained in distinctly more plumelike configurations than the debris shed by comet 9P/Tempel 1 when NASA’s Deep Impact spacecraft intentionally crashed into it in 2005. “The Dimorphos ejecta has a lot of morphological features,” said Jian-Yang Li, a planetary scientist at the Planetary Science Institute in Fairfax County, Virginia, and a member of the DART team, at the Fall Meeting.
The reason is probably the different compositions and surface features of the two bodies, he said. Tempel 1 is rich in volatiles and fine-grained dust; Dimorphos’s surface, on the other hand, is littered with boulders. Scientists plan to continue to monitor Dimorphos’s debris plume through at least March.
The DART mission has also enabled scientists to investigate a fundamental question about the Didymos-Dimorphos asteroid system: Do the two asteroids have the same composition? It’s a common assumption when it comes to binary asteroids, but it’s never been confirmed. Thomas, leader of the DART Observations Working Group, presented new results on the subject at a press conference at the Fall Meeting. She shared near-infrared spectra of the binary asteroid system that astronomers had collected both before and after impact using a NASA telescope in Hawaii.
Observations obtained prior to impact (when the overwhelming majority of the sunlight reflected off the asteroid system came from Didymos) and after impact (when the debris shed by Dimorphos was responsible for more than two thirds of the reflected light) revealed very similar spectra, with characteristic dips at wavelengths of 1 and 2 micrometers in both cases. That’s strong evidence that the two asteroids have similar compositions, said Thomas.
Scientists aren’t yet finished with Didymos and Dimorphos: In 2024, researchers involved in the European Space Agency’s Hera mission plan to launch a spacecraft to the system to further characterize the asteroids—including accurately measuring the mass of Dimorphos—and to study the crater created by the DART impact.
—Katherine Kornei (@KatherineKornei), Science Writer
Citation: Kornei, K. (2023), NASA’s Double Asteroid Redirection Test is a smashing success, Eos, 104, https://doi.org/10.1029/2023EO230010. Published on 12 January 2023.
Text © 2023. The authors. CC BY-NC-ND 3.0Except where otherwise noted, images are subject to copyright. Any reuse without express permission from the copyright owner is prohibited.
RelatedAre We Prepared for an Asteroid Headed Straight to Earth?NASA's New Asteroid Sampler Will Illuminate Solar System's HistoryExploring Planetary Breadcrumbs One Asteroid at a Time 

Tagged: #AGU22, asteroids, NASA, orbits & rotations, solar system, Space & Planets 









"
https://news.ycombinator.com/rss,Janet Malcolm on the Stand,https://www.nplusonemag.com/online-only/book-review/malcolm-on-the-stand/,Comments,"         Malcolm on the Stand | Online Only | n+1 | Max Abelson                                      Sign InYour AccountHomeMagazineOnline OnlyBookstoreEventsDonateSubscribeFull
 NavigationSign In to n+1    Forgot Password Subscribe NowCloseSearch   Online Only February 3, 2023A discussion of The Feeling SonnetsEventsEugene Ostashevsky and Genya Turovskaya in conversation  January 6, 2023Film ReviewAn Entire Society Exists Within MeMark Krotov  December 24, 2022Online OnlyCouscous and ChickenNicholas Hamburger   December 23, 2022Online OnlyA Strike DiaryKyle McCarthy   Book ReviewMax AbelsonMalcolm on the StandShe is cutting, wary, funny, and wise. Her style is what I wish I had instead of the chipper inner voice I’m stuck with. Nothing in Malcolm’s writing is dull or amiss unless she’s quoting somebody else. Her lines put me in mind of the painter Agnes Martin—everything so even and tight.To read Malcolm is to be moved by the clarity of her journalism—and warned, again and again, that the form is no good    January 10, 2023  Tags Reading, Writing, and Publishing Reviews  Share and Save Twitter Facebook Google
 Plus    Instapaper Email NewsletterGet n+1 in your inbox. Email Address    Janet Malcolm: The Last Interview and Other Conversations. Introduction by Katie Roiphe. Melville House, 2022.In the few times Janet Malcolm let other reporters interview her, she did what she could to keep herself safe.“Doing this interview by email gives me a chance to think of answers to your questions,” Malcolm wrote to the Believer in 2004. “If we did it in person, I might just look at you in blank helplessness.” She invited a Paris Review interviewer over to her Gramercy Park apartment seven years later, but didn’t answer the questions until typing on her desktop Mac.Malcolm, who died in 2021 at 86, was as attuned as anyone to the dangers—malice, betrayal, misunderstanding—of a tape recorder clicking on. The monster in her masterpiece, The Journalist and the Murderer, isn’t the man convicted of killing his family but the bestselling author he took to court for publishing a tell-all; what haunts The Silent Woman, her book about Sylvia Plath, isn’t the poet’s suicide or Ted Hughes but the couple’s biographers. To read Malcolm’s decades of work for the New Yorker is to be moved by the clarity of her journalism—and warned, again and again, that the form is no good.There are few reporters you’d rather see on the other side—the wrong end—of a Q&A. That’s where we find her in Janet Malcolm: The Last Interview and Other Conversations, a compilation of her exchanges with critic Nan Goldberg for Salon in 2001, novelist Daphne Beal in the Believer, Canadian radio host Eleanor Wachtel in 2008, writer Katie Roiphe in the Paris Review, and the New York Times Book Review in 2019. At their best, the transcripts channel and help explain Malcolm’s mesmerizing journalism, only the tables are turned. Reading the interviews has the perverse quality of seeing a judge on trial or your analyst in therapy.Often, though, it’s a polite book—one in Melville House’s series of “last” interviews with interesting people—that chooses the wrong times to go soft. Malcolm, who knew the subjects of journalism are always “astonished when they see the flash of the knife,” doesn’t even get nicked. The friction that’s missing here is what electrifies not just Malcolm’s writing but the record of what happened when she was actually put on the hot seat. When, decades ago, the star of one of her books sued her for libel, taking her to the Supreme Court and then to two trials, Malcolm—in front of a jury—gave the moral accounting these interviews avoid.What makes Malcolm’s reporting unusual, besides the trouble it caused her, is how much fun it is to be in her company on the page. She is cutting, wary, funny, and wise. Her style is what I wish I had instead of the chipper inner voice I’m stuck with. Nothing in Malcolm’s writing is dull or amiss unless she’s quoting somebody else. Her lines put me in mind of the painter Agnes Martin—everything so even and tight.I like the way Beal puts it in the introduction to the Believer interview: “What grabs and re-grabs the reader in her writing is its deft commingling of sleuthing and contemplation,” she writes. “Reading Malcolm, one has the sensation of being in the presence of a mind constantly in action on several levels, mediating between external reality (one most often consisting of facts that are at odds with one another) and her own consciousness.”For reporters, Malcolm offers even more than just a guidebook to craft. She’s a tuning fork whose pitch tells the rest of us when we’ve fallen flat or drifted sharp. That’s because of the clarity of her writing—“vanity, hypocrisy, pomposity, inanity, mediocrity” is in The Journalist and the Murderer with “tenderness, sensitivity, judgment, warmth” as well as “ambiguity, obscurity, doubt, disappointment, compromise, and accommodation”—and how in touch it is with what’s going on, how it comes together, and why it sometimes falls apart. When I read Malcolm I’m like Roiphe greeting her for their Paris Review interview: “Around her it is hard not to feel large, flashy, blowsy, theatrical, reckless.”Malcolm was born Jana Klara Wienerová in Prague in July 1934. Five years later, after Hitler had marched through the city, her family escaped on one of the last civilian ships to America from Europe before the war. Her father became a successful New York psychiatrist—in these interviews she swears she “paid little attention to my father’s work” and that psychoanalysis “has had curiously little influence” on her style, but one can’t quite believe her. At the University of Michigan, she wrote for the newspaper, edited its humor magazine, and met her first husband. They both went on to write for the New Yorker, and after he died she married her editor, whose stepfather’s yeast fortune helped fund the magazine. It wasn’t until she gave up cigarettes in the late ’70s that Malcolm did her first long piece of journalism: “I figured that by the time I finished the reporting I would be ready to try writing without smoking.”If you want Malcolm at her most rabbinical, there’s Reading Chekhov, her underrated and atmospheric meditation on the Russian genius from 2001. For momentum that crime writers would kill for, she has Iphigenia in Forest Hills, a 2011 thriller about an Orthodox Jewish woman on trial for murder. Her stories about psychotherapy have the sweet swing of sportswriting, and her work about the law, too, is riveting and deep. Malcolm’s ear and eye—and unusual sense of structure—are most dazzling in her writing about art, especially “Forty-one False Starts,” a portrait of the painter David Salle that begins again and again, and “A Girl of the Zeitgeist,” a profile of the editor Ingrid Sischy that waits and waits to find her. The shapes of the pieces convey so much about their subjects that a reader can’t help but feel both are also about the machinery of journalism.There are different forms of Malcolm on the page. The observant and sometimes cold journalist who asks her subjects unsettlingly short questions isn’t quite the same figure as the ingenious narrator whose essayistic contemplation radiates generosity. Then there’s the small and sometimes anxious woman who emerges to play key plot roles, especially in Iphigenia.The Malcolm we encounter in The Last Interview shares the self-awareness, briskness, clarity, and humor she possesses elsewhere: “I walk fast and am impatient. I get bored easily,” Malcolm tells the Paris Review. “I often get stuck. Then I get sleepy and have to lie down. Or I make myself leave the house—walking sometimes produces a solution. The problem is usually one of logic or point of view. I keep regular morning hours. The first hour is the most productive one.” Worrywart writers will find much to love in Malcolm’s description of herself: “The machinery works slowly and erratically and I am always a little nervous about it, though by now I’m pretty used to it,” she emails. “I guess I trust it more.”But where Malcolm the journalist is unsparing and direct, Malcolm the interviewee is ultimately vague and evasive. “What’s true? Is it possible to know what’s true?” Goldberg asks in the Salon interview. Malcolm answers: “I’d love to hear you talk about it rather than me.”“Why do you think the subject of betrayal is something that’s your subject?” Wachtel asks her on the radio. “That’s very interesting,” Malcolm says. “You’re kind of putting me on the spot.”Interviews aren’t the same as sworn testimony, but they rhyme. They use questions to flatter, badger, and trap witnesses who, in turn, evade when they can and admit things they don’t want to. Reporting and the law both rely on evidence and discovery, asking for honesty and promising fairness in exchange. They offer just about the best systems we have for hearing arguments, measuring doubt, rendering judgment, and appealing verdicts—except, maybe, for psychoanalysis. All three approaches use confrontation to turn ambiguity into clarity, but only one can punish an outburst or lie by locking the speaker away.The long and famous case against Malcolm began in 1984, after she published a New Yorker profile about a lawsuit from a star scholar named Jeffrey Moussaieff Masson against the ​​Sigmund Freud Archives. It was also a vibrant portrait of a charming and ambitious heel. Masson accused Malcolm in the Washington Post of misquoting him “any number of times.” (The newspaper reporter, a young David Remnick, wrote that Malcolm was away on vacation in Italy and couldn’t be reached for comment.) When Malcolm expanded the piece into a book, In the Freud Archives, Masson complained again, in a letter to the New York Times. The response she sent offered to play the tapes of their conversations for Times editors “whenever they have 40 or 50 short hours to spare.” Masson sued her for libel soon after.His case began terribly. Many of the quotes he had denied saying turned out to be on her tape. Others really were missing, though, and Malcolm had an unusual story: she had tripped over her recorder’s cord before a morning interview, took notes instead, typed them up, and misplaced the originals. But a judge decided Malcolm had reasonably interpreted whatever Masson had actually said and threw out the suit. Just after Malcolm published The Journalist and the Murderer, the Supreme Court agreed to hear Masson’s appeal. Anthony Kennedy, joined by six other justices, wrote that a misquote has to hit the reader’s mind differently than the right one would to cross the line into libel. It would be up to a jury to decide if Malcolm’s writing and Masson’s words shared “the substance, the gist, the sting.” (When the Times asked for comment, Malcolm was back in Italy.)Inside a federal court in California in May 1993, Malcolm took the stand. Masson’s lawyer, a former quarterback and Air Force navigator named Charles O. Morgan Jr., asked Malcolm about a key monologue at the Berkeley restaurant Chez Panisse. “You reported in the article that the entire statement was made by Mr. Masson at lunch,” Morgan asked.1“Yes,” Malcolm said.“And that is not true.”“That’s right,” Malcolm said. She had compressed conversations over seven months into one monologue, she told the jury, using them like “sketches incorporated into one painting.” Anything else, she testified, would be foolish: “I do not want to write the exact words, I do not want to write a transcript,” she went on. “This thing called speech is sloppy, redundant, repetitious, full of uhs and ahs.” When that line surfaces in the introduction to The Last Interview, Roiphe cites it admiringly, praising Malcolm for improving on the “casualness and mediocrity of expression” by “trimming and shaping.” Morgan, the attorney, argued otherwise.“Do you call that rearranging events?” he asked.“I don’t know what that means.”“Do you call it creating a conversation?”“I wouldn’t put it that way, no.”Before the trial ended, Malcolm testified that the chaos and contradiction of speech had forced her hand: “He’s trying to tell too many things at the same time. You had to work hard to get the story straight because he was all over the place.”Jurors decided for Masson. But they couldn’t agree on damages, so the judge announced they’d start the whole thing over. In one of her most gripping pieces, published in the New York Review of Books months before her death, Malcolm recalls the makeover she gave herself with a speech coach before returning to the stand. Back in the courthouse, she gave “a long speech about the monologue technique that Morgan kept interrupting but was unable to stop,” Malcolm writes. “I went relentlessly on and on. I talked about the difference between the full and compelling account of his rise and fall in the Freud Archives that Masson gives in the article and his wandering incomplete speech in the restaurant. I spoke of the months of interviews out of which, bit by bit, the monologue was formed. I concluded by saying, ‘I have taken this round-about way of answering your question, Mr. Morgan, because I wanted the jury to know how I work, and what we’re talking about here in talking about this monologue.’”After one kind of Malcolm monologue about another, the jury dismissed concerns about three of the five quotes in question. But jurors decided the two others—about the sterility of psychoanalysis and Masson’s bosses—were false, and that the latter qualified as defamation. They also decided, though, that Malcolm hadn’t been reckless enough to cross the legal line of libel. She won and wept.What happened a year later gets more attention in the book of her interviews than her testimony. “I was in my country house, and there was something red on the floor, and I picked it up, this red notebook. My granddaughter had seen something red in the bookcase and pulled it out, and there were the notes”—the ones she took after tripping over the recorder’s cord. “I felt like I was going to faint.” If only she had found them earlier, she tells Wachtel, “the whole thing could’ve been avoided.” She emails the story to the Believer, too: “The jury had decided to believe me anyway. But if the notebook hadn’t got misplaced, there would have been no lawsuit.”The fact is that Malcolm’s accounts, as she liked to write about other people, “don’t add up.” The missing red notebook had three of the quotes in question, but not the two that had bothered the jury. “Do we ourselves add up?” Goldberg asks Malcolm in the Salon interview.“No,” Malcolm answers. “Of course we don’t.”That someone so thoughtful about the moral perils of journalism could be messy enough to collage scenes together—then blithe enough to testify it was all for the best and deluded enough to say a missing notebook explained it all—is a paradox that we Malcolm fans have to live with. It’s also, of course, a version of what her work was warning about. Whether her shortcomings bring her journalism to life or do something more like undermine it is the kind of thing you would want to ask her, right before running out of the room.Roiphe’s introduction to The Last Interview avoids Malcolm’s mysteries and messes. It’s less about the morals or machinery of her journalism than what it was like to be her bud. “When a friend texted me that Janet Malcolm had died, I experienced more than the usual amount of disbelief,” it begins. “This is one of the conversations I wish I’d had with Janet herself at Choshi,” Roiphe writes later, “the now-closed sushi place she favored around the corner from her house. I know she would have had thoughts on it.” If you’re in a generous mood, you can read the intro as a kind of pun on Malcolm’s interest in the “I” who narrates literary nonfiction: “I wonder,” “I confessed,” “I suggested,” “I fixate,” I understand,” “I hear,” Roiphe writes, and then “I love” three times in a row.It all comes to a crescendo with a sort of Freudian slip. “I have always used her writing to teach confidence,” she writes, meaning to refer to Malcolm’s authoritativeness but channeling the line that follows The Journalist and the Murderer’s famous opening: “He is a kind of confidence man, preying on people’s vanity, ignorance, or loneliness, gaining their trust and betraying them without remorse.”The first of the interviews, with Salon, opens with the kind of reportorial antagonism I found myself missing as the book went on: If journalists are murderers, Goldberg asks, why was Malcolm speaking to one? Malcolm twists away by complimenting the question and pointing out she used to avoid reporters entirely. “When the book came out and people wanted to ask me questions, I said, ‘Well, read the book.”“I did,” Goldberg answers, standing her ground. “That’s why I’m asking.” Instead of saying why she agreed to the interview, Malcolm explains why she shouldn’t have: “I’m just not very good at it. I often have no answers to the questions; I think of the answers later.”It’s a moment when the tension and confrontation that the book mostly suppresses manages to leak out, but not the only one. In the Believer, when Beal dares to ask if getting sued changed her approach with subjects, Malcolm shows her teeth: “Until this moment you were the first interviewer who did not bring Jeffrey Masson into the discussion. I guess that isn’t possible after all.” You again get the sense that the nastiness of journalism was only fun for Malcolm to consider when it was someone else’s.Beal backs off: “Sorry to be so tiresome,” she writes back. “Just like the rest.” The exchange ends soon after, but, following a line break, an italicized note says Malcolm read the transcript and sent this in an email: “I read the interview in the way one looks at photographs of oneself, and, except for one place, I thought I came out looking okay. But the exception may be the most interesting part of the interview.” It’s Masson. “Until that moment the atmosphere of the interview is friendly and collegial, almost conspiratorial. Now it turns icy.” Malcolm goes on: “What is most interesting about this moment in our interview is the illustration it offers of a subject’s feeling of betrayal when he or she realizes that the journalist is writing his or her own story.”If a goal of Malcolm’s journalism was to measure the distance between the stories that reporters shape and the ones subjects tell about themselves, this book makes the mistake of letting her maintain almost complete control of the tape—or, in the case of Roiphe’s Paris Review interview, the notebook. “I want to talk about that moment in our meeting at my apartment last week, when I left the room to find a book and suggested that while I was away you might want to take notes about the living room for the descriptive opening of this interview,” Malcolm writes in an email that’s quoted in the interview’s introduction. “You obediently took out a notebook, and gave me a rather stricken look, as if I had asked you to do something faintly embarrassing.” Malcolm fills it in for her in the interview: “My living room has an oakwood floor, Persian carpets, floor-to-ceiling bookcases, a large ficus and large fern, a fireplace with a group of photographs and drawings over it, a glass-top coffee table with a bowl of dried pomegranates on it, and sofas and chairs covered in off-white linen.”That’s not to say deference is so terrible. Even though the “By the Book” feature in the Times only asks Malcolm about reading, she looks around her bookshelves and spots “petulant desperation,” “wild terrain,” “the eye of eternity,” and on top of her night table “a box of Kleenex, a two-year-old Garnet Hill catalog and a cough drop on it.”But you find yourself yearning for confrontation and catharsis. Did she, in her later writing, stick with the collage method she defended on the stand, or was it sublimated into the delicate collages she started making—and exhibiting at art galleries—from handwriting, typewritten papers and book pages?Salon asks this: “Do you see any relationship between your collages and your writing?”“I think so,” Malcolm answers. “I like to think about my work as kind of collage-like.” There isn’t a followup, because their interview, right there, is over.Roiphe ends her essay with herself, so I get to end mine with me.When I was hired as a newspaper reporter in 2006, taking over the New York Observer’s weekly column on the city’s most expensive real estate, I was introduced to business journalism at the peak of what turned out to be a bubble. I wrote about buyers and sellers who played a role in inflating and then bursting it: “Money just doesn’t mean anything,” one of the city’s high-end brokers told me not long before Bear Stearns collapsed and the era came to an end. I switched beats in 2009 to report on the landscape of Wall Street’s culture, covering some of the same people, only how they made their money instead of where they spent it.I fell in love with Malcolm’s journalism because it seemed to me at the time to be a map to the crises I would have to navigate. I imagined that the tensions her books describe only multiply if interview subjects are rich and powerful. Higher stakes, I figured, only complicate access and relationships. Reading these interviews makes me think I was wrong. They are reminders that even the subjects who best understand what’s happening can’t fully explain themselves to outsiders, and that no journalist knows how to get them to reveal it all.What is there to do about it? “Perhaps the way to minimize one’s feeling that one has not been as straightforward with the subject as one should have been,” she tells the Believer, “is to be a little more straightforward.” Sometimes I talk to my subjects about the trouble Malcolm picked up on, and the trouble she got into. And sometimes I can feel the acknowledgement make us both relax, at least for a little bit.But Malcolm wouldn’t want anybody to get too comfortable. When the Believer asks if subjects occupy a different place in her mind at the end of writing than they had during interviews, she writes back that she isn’t sure she understands the question. Beal tries again: “How do you make the switch from supplicant or equal interviewer to authority writer?”“Yes, it is a problem,” Malcolm answers, “and no, it can’t be resolved.”Quotes from the trials come from coverage in the New York Times, Village Voice and Washington Post. ↩ If you like this article, please subscribe or leave a tax-deductible tip below to support n+1.    The Burglaries Were Never the Story Related Articles  Issue 2 HappinessKeith Gessen replies: The genital flag?Issue 2LettersThe Editors   Issue 5 Decivilizing ProcessIt has lately become clear that nothing burdens a life like an email account.Issue 5Against EmailThe Editors   Issue 39 Take CareI can tell you only what I found helpful.Issue 39Baby YeahAnthony Veasna So   Issue 6 MainstreamCanons in daily life just demarcate the books you can count on other people feeling comfortable about in conversation.Issue 6The Spirit of RevivalThe Editors More by this Author August 20, 2021“Listen to the last half-hour of ‘Dark Star’ in a darkened room and see if you feel remotely secure.”Online OnlyIn the Dead ArchivesMax Abelson  n+1n+1 is a print and digital magazine of literature, culture, and politics published three times a year. We also post new online-only work several times each week and publish books expanding on the interests of the magazine.MagazineCurrent IssueRenew SubscriptionSubscribeGift SubscriptionsYour AccountBack IssuesBookstoresLibrariesAdvertiseRSSAbout n+1AboutFoundationEventsContactBack to TopCopyright © 2023 n+1 Foundation, Inc.Terms & Conditions | Privacy Policy                                        "
https://news.ycombinator.com/rss,CircleCI says hackers stole encryption keys and customers’ source code,https://techcrunch.com/2023/01/14/circleci-hackers-stole-customer-source-code/,Comments,"










 


CircleCI says hackers stole encryption keys and customers' secrets • TechCrunch















































































 
 



 













CircleCI says hackers stole encryption keys and customers’ secrets




			Zack Whittaker		

@zackwhittaker
 / 
						23 hours		







CircleCi, a software company whose products are popular with developers and software engineers, confirmed that some customers’ data was stolen in a data breach last month.
The company said in a detailed blog post on Friday that it identified the intruder’s initial point of access as an employee’s laptop that was compromised with malware, allowing the theft of session tokens used to keep the employee logged in to certain applications, even though their access was protected with two-factor authentication.
The company took the blame for the compromise, calling it a “systems failure,” adding that its antivirus software failed to detect the token-stealing malware on the employee’s laptop.
Session tokens allow a user to stay logged in without having to keep re-entering their password or re-authorizing using two-factor authentication each time. But a stolen session token allows an intruder to gain the same access as the account holder without needing their password or two-factor code. As such, it can be difficult to differentiate between a session token of the account owner, or a hacker who stole the token.
CircleCi said the theft of the session token allowed the cybercriminals to impersonate the employee and gain access to some of the company’s production systems, which store customer data.
“Because the targeted employee had privileges to generate production access tokens as part of the employee’s regular duties, the unauthorized third party was able to access and exfiltrate data from a subset of databases and stores, including customer environment variables, tokens, and keys,” said Rob Zuber, the company’s chief technology officer. Zuber said the intruders had access from December 16 through January 4.
Zuber said that while customer data was encrypted, the cybercriminals also obtained the encryption keys able to decrypt customer data. “We encourage customers who have yet to take action to do so in order to prevent unauthorized access to third-party systems and stores,” Zuber added.
Several customers have already informed CircleCi of unauthorized access to their systems, Zuber said.
The post-mortem comes days after the company warned customers to rotate “any and all secrets” stored in its platform, fearing that hackers had stolen its customers’ code and other sensitive secrets used for access to other applications and services.
Zuber said that CircleCi employees who retain access to production systems “have added additional step-up authentication steps and controls,” which should prevent a repeat-incident, likely by way of using hardware security keys.
The initial point of access — the token-stealing on an employee’s laptop — bears some resemblance to how the password manager giant LastPass was hacked, which also involved an intruder targeting an employee’s device, though it’s not known if the two incidents are linked. LastPass confirmed in December that its customers’ encrypted password vaults were stolen in an earlier breach. LastPass said the intruders had initially compromised an employee’s device and account access, allowing them to break into LastPass’ internal developer environment.
Updated headline to better reflect the customer data that was taken.















 
 


"
https://news.ycombinator.com/rss,America’s trustbusters plan to curtail the use of non-compete clauses. Good,https://www.economist.com/leaders/2023/01/12/americas-trustbusters-plan-to-curtail-the-use-of-non-compete-clauses-good,Comments,"LeadersAmerica’s trustbusters plan to curtail the use of non-compete clauses. GoodThe clue is in the name Jan 12th 2023ShareThree-quarters of Americans who work, do so for a firm. They have contracts setting out their pay, holiday, benefits and sometimes the appropriate way to dress (although not in journalism). A lot of contracts also say whether employees may work for a competitor if they leave the company. It is hard to know what share of American workers are restricted by these non-compete clauses, but the available evidence suggests it may be as high as one in five. More worrying, these clauses are as likely to apply to workers operating deep-fat fryers in fast-food kitchens as they are to workers operating in the conference rooms of white-shoe law firms. The Federal Trade Commission (FTC) has these clauses in its sights, on the grounds that they are anticompetitive and suppress wages. Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the <audio> element.Listen to this storySave time by listening to our audio articles as you multitaskOKFans of non-compete clauses argue that scrapping them by decree will invite the state into the realm of private contracts, a symptom of regulatory excess. They have a point, but the FTC’s case is stronger.It is easy to see why firms like non-compete clauses, which are designed to suppress competition. It suits companies to be able to prevent a star employee from joining a rival, or starting out on their own and wooing clients. But there are also some arguments that non-competes could serve the public interest. Companies sometimes say that the clauses incentivise them to think about talent in a longer-term way. Why bother to spend time and money on training employees if they then join rival firms that go on to reap the benefit of the investment? Some companies also have legitimate worries about trade secrets walking off to a competitor when an employee leaves.However, a non-compete clause is a heavy-handed way to achieve those ends. Intellectual-property law and non-disclosure agreements exist to preserve true trade secrets, and lots of firms find ways to keep valuable employees without shackling them to their jobs using non-compete clauses. The public interest also conflicts with the firm’s interest: innovation and productivity spread when better ideas about how to do things become widely adopted. Hiring people with specific knowledge and experience can speed this process up, which is one reason why the engineers fired by tech giants like Meta and Twitter are sought after by firms in older industries eager to learn.If the theory points in one direction, the evidence from how non-competes are used practically screams. In 2014 Jimmy John’s, a chain of sandwich shops, was found to have inserted a two-year non-compete clause in its employees’ contracts which barred them from seeking employment with any rival business that made money by “selling submarine, hero-type, deli-style, pita and/or wrapped or rolled sandwiches” within a three-mile radius of where they currently worked. After this egregious example came to light, the company ended the practice. But franchises often stop employees from going to work at other outlets of the same franchise, reducing their bargaining power.It strains credulity to argue that these workers are the guardians of trade secrets. Instead, the evidence is that firms use non-compete clauses to drive down wages by lowering the value of workers in the job market. About half of people with non-compete clauses in their contracts work in states where they cannot legally be enforced. They may get away with it because employees do not know their rights, especially those in the low-paid part of the labour market. The evidence that non-compete clauses really make companies more innovative and higher-skilled is scarcely more convincing. Washington state, home to Amazon and Microsoft, takes a middle path by restricting non-compete clauses to the contracts of high-earners. California, the global hq of disruptive innovation, goes a step further and bans non-competes altogether. The FTC should do the same, on the grounds that they are anticompetitive. ■This article appeared in the Leaders section of the print edition under the headline ""The clue is in the name""Leaders January 14th 2023The destructive new logic that threatens globalisationThe West should supply tanks to UkraineHow Brazil should deal with the bolsonarista insurrectionAmerica’s trustbusters plan to curtail the use of non-compete clauses. GoodFixing Britain’s health service means fixing its family doctorsFrom the January 14th 2023 editionDiscover stories from this section and more in the list of contents Explore the editionShareReuse this contentMore from LeadersThe glory of grandparentsWhy the soaring number of grandmas and grandpas is a good thingHow Brazil should deal with the bolsonarista insurrectionPunish those who broke the law, but govern inclusivelyFixing Britain’s health service means fixing its family doctorsDon’t change the partnership model. Do change the targets"
https://news.ycombinator.com/rss,Wobbly clock,https://somethingorotherwhatever.com/wobble-clock/,Comments,"


Wobbly clock!













"
https://news.ycombinator.com/rss,Godot for AA/AAA game development – What's missing?,https://godotengine.org/article/whats-missing-in-godot-for-aaa/,Comments,"














					Godot for AA/AAA game development - What's missing?
				

					By: Juan Linietsky
					 16 January 2023



News



Godot 4.0 is coming out soon. It includes major improvements all across the board in features, performance, and usability. Still, one of the biggest questions the community has is: How does it compare with mainstream commercial offerings?
Godot 4.0 improvements
Rendering
Godot 4.0 has an entirely new rendering architecture, which is divided into modern and compatibility backends.
The modern one does rendering via RenderingDevice (which is implemented in drivers such as Vulkan, Direct3D 12, and more in the future). Additionally, the modern backend can implement rendering methods, such as forward clustered, mobile, and more in the future (such as deferred clustered, cinematic, etc.).
The compatibility backend is based on OpenGL ES 3.0 / OpenGL 3.3 / WebGL 2.0 and is intended to run on very old PC hardware as well as most older (still working) mobile phones.
Rendering is significantly more efficient in Godot 4.0, using data oriented algorithms to process the culling of objects and both secondary command buffers and automatic batching to efficiently submit the draw primitives.
The features offered are also a lot more reminiscent of AAA games, such as far more material options and advanced visual effects (including circle DOF, volumetric fog, AMD FSR, etc.). Additionally, Godot 4.0 supports advanced global illumination techniques such as lightmapping (including SH lightmapping), Voxel GI (which is fully real-time) and SDFGI (which is a single click, open world GI solution). Screen space GI can be used to enhance the realism even more.
Physics
After an unsatisfactory attempt at using Bullet, Godot 4.0 returns to its own physics engine which, despite not being a high end physics engine like PhysX, aims to offer a lot more flexibility and “just works” capabilities to users.
Several features were added to Godot Physics since 3.x, such as soft bodies and cylinder shape support, as well as several optimizations to make use of multiple threads.
The custom physics engine still has a considerable amount of issues remaining but we are working hard to ensure it is in a decent state for shipping when 4.0 reaches stability. It will continue seeing improvements afterwards, during the following 4.x release cycles.
That said, Godot 4.0 introduces the ability to bind custom physics engines at runtime (without recompiling Godot) via GDExtension, so it’s perfectly possible for the community to integrate other engines such as PhysX, Jolt, or Box2D if need to be.
Scripting
Godot 4.0 has a new version of GDScript, which is far more powerful and overcomes most shortcomings found in 3.x. Majorly, the addition of lambdas, first class functions/signals and a much reduced reliance on string identifiers (which are prone to errors). It also has more useful built-in data types such as integer vectors.
Core engine
The core engine has been significantly optimized, especially on the memory and data-oriented areas. Core and Variant have been massively cleaned up and made more extensible. Besides being faster and more modern, the core codebase is now significantly easier to maintain and extend.
GDExtension
It is now possible to extend Godot and add features to it practically in any language and without recompiling the engine, thanks to the new GDExtension system. Aside from Godot C++ (which makes it easy to extend Godot as easy as with modules but allowing pluggable, dynamic add-ons), there are other bindings in the work such as Python, Rust, Go, etc.
A lot more
Several other areas got improvements, like the editor (which has been vastly reworked), UI system, multiplayer, navigation, audio, animation, etc. This is a major release with major improvements all across the board.
So, what’s missing?
Do not be mistaken: A lot is still missing from Godot in order to be used comfortably for large projects and teams. That said, what remains is now much less work than it was for Godot 3.x.
First of all, most of the new features still have significant bugs and performance problems that will not be solved in time for the upcoming 4.0 release (there is just too much new code that needs to be tested throughly).
These problems will be fixed across the 4.x point releases (which we are now intending to do more often, allowing several releases per year). It may be an extra year or even two until everything feels as solid and fast as everyone expects. See this article about our plans for 4.0 and beyond.
But other than that, there are still some fundamental aspects missing in Godot. The following is an incomplete list of the most important ones:
Streaming
The traditional way to make games longer since the beginning of times is to divide them in stages. As soon as one stage is completed, it is unloaded while the new one is loaded.
Many games still use this approach nowadays (after all, if it’s not broken, don’t fix it) but, increasingly, game design has been moving from “individual stages” to “open” or “continuous” worlds where the boundaries between levels disappear. Creating games this way is, as a result, more challenging.
This is handled nowadays by a type of technology called “streaming”. It means that assets are pulled from disk on demand (loaded only at the time they are needed), rather than as a part of a larger stage. The most common types of streaming are:

Texture streaming: All textures are loaded in a tiny size by default. As textures get closer to the camera, higher resolution versions (or mip-maps) are streamed from disk. Textures which haven’t been used for some frames are freed instead. At any given time, the textures loaded (and their detail) closely reflect the place the player is in.
Mesh streaming: Models are loaded as low detail (few vertices). As they gradually approach the camera, higher resolution versions are streamed from disk. Models that were not used (displayed) since a while are often just freed and will be loaded again when needed.
Animation streaming: Modern games have long cinematics, which require a lot of animation data. Loading those animations require a lot of memory and loading them takes a lot of time. To avoid this, animations are streamed by generally keeping the first second or two in memory and then new sections are loaded on demand as the animation plays. Godot 4.0 supports strong animation compression and animation pages, so most of the work is already done.
Audio streaming: Similar to animation streaming, it requires storing the first second or two of audio and then streaming the rest directly from disk.

Of the above, most are relatively straightforward to implement. The most complex is mesh streaming, which generally needs to be implemented together with a GPU culling strategy to ensure that very large amounts of models can be drawn at no CPU cost. This is more or less what techniques like Nanite do in Unreal, although Godot does not need to implement something that complex to be of use in most cases.
Streaming is the most important feature missing for managing large scenes or open worlds. Without it, Godot users are subject to long loading times (as every texture, model and animation has to load before anything is shown). There is also a risk of running out of memory if too many assets are loaded in parallel instead of streaming them.
Low level rendering access
Despite the new renderer in Godot 4.0, there is no architecture that can be considered a one size fits all solution. Often developers need to implement rendering techniques, post processing effects, etc. that don’t come bundled with the engine.
The Godot philosophy has always been to cater to solving the most common use cases, and leave the door open for users to solve the less common on their own.
As such, this means that low level access to all the rendering server structures needs to be exposed via GDExtension. This will allow creating custom renderers or plugging custom code during the rendering steps, which is very useful for custom rendering techniques or post processes.
Scene job system
Most of the work done for the Godot 4.0 involved large feature and performance improvements to all the servers (rendering, physics, navigation, etc.). Servers are also now multithreaded and optimized. Even asset loading can now be done multithreaded (using multiple threads to load multiple assets).
Still, the scene system (which uses those servers), despite several usability improvements, has not seen significant optimization.
Scenes nodes in Godot are mostly intended to carry complex high level behaviors (such as animation trees, kinematic characters, IK, skeletons, etc.) for limited amounts of objects (in the hundreds at most). Currently, no threading happens at all and only a single CPU core is used. This makes it very inefficient.
This makes it an ideal target for optimizing with multithreading. There is an initial proposal on threaded processing for scene nodes, which should give complex scenes a very significant performance boost.
Swarms
Scenes, as mentioned before, are designed for complex high level behaviors in the hundreds of instances. Still, sometimes, some games require larger amounts of instances but less complex behaviors instead.
This is needed for some types of game mechanics such as:

Projectiles (bullet hell for example).
Units in some types of strategy games with thousands of entitites roaming across a map.
Cars/people in city simulators, where thousands appear all across the city.
Sandbox style simulations.
Complex custom particles that run on CPU.
Flocks, swarms, mobs, debris, etc.

More experienced programmers can use the servers directly or even plug C++ code to do the heavy lifting. ECS is often also proposed as a solution for this. Even GPU Compute (which is fully supported in Godot) can be easily used to solve this pattern.
But for the sake of keeping Godot accessible and easy to use, the idea is to create a swarm system that takes care of the rendering/physics/etc. in large amounts of those objects and the user only has to fill in the code logic.
Large team VCS support
Godot’s text file formats are very friendly to version control. They only write what is needed (no redundant information), keep the ordering of sections and are simple enough to understand changes by just looking at the diff. Few other technologies work as well in this area.
Despite that, this is far from enough to enable large team collaboration. To enable this, Godot VCS support has to improve in several areas:

Better integration with the filesystem dock.
Better real-time refresh of assets if they were modified externally (and checked out).
Support for permissions and file locking: Git does not support this out of the box, but Git LFS and Perforce do. This feature is essential for large teams to avoid conflicts and keep files protected from unintended modifications (e.g. a team member modifying code or a scene they don’t own by mistake).

Unless the support for this is solid, using Godot in large teams will remain difficult.
Commercial asset store
While for very large studios this is not an area of interest, medium-sized studios still rely on significant amounts of assets and pre-made functionality. The Asset Library currently existing in Godot only links to open source resources (e.g. hosted on GitHub or GitLab) and is unable to be used for commercial assets.
For the Godot project, a commercial asset store would be a great way to add an extra source of income, but it was not legally possible given our legal status until recently. With the move to the Godot Foundation, this is a new possibility that opens up.
Is solving these problems enough for Godot to become a top AA / AAA game engine?
The answer is “it depends”. Godot, at its core, is and will always be (by design) a very general purpose game engine. This mean that the tools provided, while certainly capable, are still game neutral. The goal for Godot is to provide a great set of building blocks that can be used and combined to create more specialized game functions and tools.
In contrast, other types of game engines already come with a lot of high level and ready to use components and behaviors.
I don’t meant to say that Godot should not support any of that in the future. If it does, though, it will most certainly be as official extensions.
So, what kind of features are we talking about? Well..
Game specific templates and behaviors
As an example, Unreal comes with a player controller, environment controller, and a lot of tools to manage the game pacing and flow. Most likely aimed at TPS/FPS games, which is the most popular game type made with the engine.
Some of these can be found as templates in Godot’s Asset Library but are nowhere close to that functionality. Eventually, official ones should be created that are more powerful and complete.
Visual scripting
While Godot had visual scripting in the past, we found that the form we had implemented didn’t really prove adequate for the needs of the community, so it was discontinued.
What we realized is that visual scripting really shines when combined together with the premade behaviors mentioned in the previous section. Without a significant amount of high level behaviors available, visual scripting is cumbersome to use as it requires a lot of work to achieve simple things by itself.
All this means that, if we produce a visual scripting solution again, it needs to go hand in hand with high level behaviors and, as such, it should be part of a set of extensions to the engine.
Specialized artist UIs
When doing tasks such as shader editing, VFX (particles) or animation, there is a large difference between Godot and engines such as Unreal.
The difference is not so much in features supported. In fact, the feature set is fairly similar! The main actual difference is in how they are presented to the user.
Godot is a very modular game engine: this means that you achieve results by combining what is there. As an example, editing a particle system in Godot means a lot of subsystems must be understood and used in combination:

GPUParticles node.
GPUParticlesMaterial resource (or even an optional dedicated shader).
Mesh resource for each pass of the particle.
Mesh material resource for each surface of the mesh (or even an optional dedicated shader).

As another example, the AnimationTree in Godot requires that AnimationNodes are laid out in a tree fashion. They can export parameters, sections can be reused (because they are resources), etc.
Or even more. Godot’s animation system is often praised because anything can be animated. Any property, other nodes, etc.
This makes Godot an extremely powerful engine that gives developers a lot of flexibility, but…
It also assumes that the user is knowledgable enough about Godot and all its inner workings in order to take advantage of it. To clarify, none of these systems are too technically complex and this is part of what makes Godot appealing and accessible, but it still requires a certain level of technical and engine knowledge.
In contrast, engines like Unreal have entirely dedicated and isolated interfaces for each of these tasks (materials, cinematic timeline, VFX, animation, etc.).
Sure, they are monolithic and hence less flexible, but for a large team with high amounts of specialization, an artist does not need to understand as much in-depth how the engine works in order to produce content with it.
This shows the fundamental difference of target user between engines. If Godot wants to appeal to larger studios, it needs to provide simpler and more monolithic interfaces for artists to be able to do their job without requiring significant more time investment in learning the technology.
This could, again, be supplied via official add-ons and, like the sections above, would require a significant amount of research to understand how to build it, since without actual feedback from artists we would only be guessing what is needed. But the question here is, is it worth it?
So, are we not even close?
While the goal of this article is to make clear how significant is the work remaining to make Godot an offering closer to the ones in the commercial segment, it is important to not forget one key detail:
Godot is Free and Open Source Software. And as such, it can be modified by anyone to fit any purpose.
Currently, many large studios have the ability to create their own in-house technology. Still, as hardware becomes more and more complex to develop for, they are giving up in favor of spending money on pre-existing commercial technology offerings.
Godot, on the other hand, serves as an excellent platform to build upon, as it solves the vast majority of problems already. As a result, more and more studios are using Godot as a base to derive their own technology from.
This is a win/win situation, as it allows them to keep their freedom to innovate and, at the same time, avoid paying expensive technology licensing costs.
Time will tell how Godot transitions from its current state to something more widely used by larger studios, but it will definitely need significantly more work from our side.
Future
I hope that this write up made more evident why Godot is such a key technology for the future of the game industry. We will continue working hard to ensure that more and more individuals and companies find Godot useful! But we need your help to happen, so please consider donating to the project.



"
https://news.ycombinator.com/rss,Reverse engineering a neural network's clever solution to binary addition,https://cprimozic.net/blog/reverse-engineering-a-small-neural-network/,Comments,"Reverse Engineering a Neural Network's Clever Solution to Binary Addition - Casey Primozic's Homepagecprimozic.net | @ameobea10cprimozic.net@ameobea10•Portfolio•Contact•Blog•Professional ExperienceReverse Engineering a Neural Network's Clever Solution to Binary AdditionSubscribe to Blog via RSS 

Training the Network


Unique Activation Functions


Dissecting the Model


The Network's Clever Solution

Summary



Epilogue

There's a ton of attention lately on massive neural networks with billions of parameters, and rightly so.  By combining huge parameter counts with powerful architectures like transformers and diffusion, neural networks are capable of accomplishing astounding feats.
However, even small networks can be surprisingly effective - especially when they're specifically designed for a specialized use-case.  As part of some previous work I did, I was training small (<1000 parameter) networks to generate sequence-to-sequence mappings and perform other simple logic tasks.  I wanted the models to be as small and simple as possible with the goal of building little interactive visualizations of their internal states.
After finding good success on very simple problems, I tried training neural networks to perform binary addition.  The networks would receive the bits for two 8-bit unsigned integers as input (converted the bits to floats as -1 for binary 0 and +1 for binary 1) and would be expected to produce properly-added output, including handling wrapping of overflows.
Training example in binary:

  01001011 + 11010110 -> 00100001

As input/output vectors for NN training:

  input:  [-1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1]
  output: [-1, -1, 1, -1, -1, -1, -1, 1]
What I hoped/imagined the network would learn internally is something akin to a binary adder circuit:










I expected that it would identify the relationships between different bits in the input and output, route them around as needed, and use the neurons as logic gates - which I'd seen happen in the past for other problems I tested.
Training the Network
To start out, I created a network with a pretty generous architecture that had 5 layers and several thousand parameters.  However, I wasn't sure even that was enough.  The logic circuit diagram above for the binary adder only handles a single bit; adding 8 bits to 8 bits would require a much larger number of gates, and the network would have to model all of them.
Additionally, I wasn't sure how the network would handle long chains of carries.  When adding 11111111 + 00000001, for example, it wraps and produces an output of 00000000.  In order for that to happen, the carry from the least-significant bit needs to propagate all the way through the adder to the most-significant bit.  I thought that there was a good chance the network would need at least 8 layers in order to facilitate this kind of behavior.
Even though I wasn't sure if it was going to be able to learn anything at all, I started off training the model.
I created training data by generating random 8-bit unsigned integers and adding them together with wrapping.  In addition to the loss computed during training of the network, I also added code to validate the network's accuracy on all 32,385 possible input combinations periodically during training to get a feel for how well it was doing overall.
After some tuning of hyperparameters like learning rate and batch size, I was surprised to see that the model was learning extremely well!  I was able to get it to the point where it was converging to perfect or nearly perfect solutions almost every training run.










I wanted to know what the network was doing internally to generate its solutions. The networks I was training were pretty severely overparameterized for the task at hand; it was very difficult to get a grasp of what they were doing through the tens of thousands of weights and biases. So, I started trimming the network down - removing layers and reducing the number of neurons in each layer.
To my continued surprise, it kept working! At some point perfect solutions became less common as networks become dependent on the luck of their starting parameters, but I was able to get it to learn perfect solutions with as few as 3 layers with neuron counts of 12, 10, and 8 respectively:
Layer (type)           Input Shape    Output shape  Param #
===========================================================
input1 (InputLayer)    [[null,16]]    [null,16]     0
___________________________________________________________
dense_Dense1 (Dense)   [[null,16]]    [null,12]     204
___________________________________________________________
dense_Dense2 (Dense)   [[null,12]]    [null,10]     130
___________________________________________________________
dense_Dense3 (Dense)   [[null,10]]    [null,8]      88
===========================================================
That's just 422 total parameters! I didn't expect that the network would be able to learn a complicated function like binary addition with that few.
It seemed too good to be true, to be honest, and I wanted to make sure I wasn't making some mistake with the way I was training the network or validating its outputs.  A review of my example generation code and training pipeline didn't reveal anything that looked off, so the next step was to actually take a look at the parameters after a successful training run.
Unique Activation Functions
One important thing to note at this point is the activation functions used for the different layers in the model.  Part of my previous work in this area consisted of designing and implementing a new activation function for use in neural networks with the goal of doing binary logic as efficiently as possible.  Among other things, it is capable of modeling any 2-input boolean function in a single neuron - meaning that it solves the XOR problem.
You can read about it in more detail in my other post, but here's what it looks like:

It looks a bit like a single period of a flattened sine wave, and it has a couple controllable parameters to configure how flat it is and how it handles out-of-range inputs.
For the models I was training for binary addition, all of them used this activation function (which I named Ameo) in the first layer and used tanh for all the other layers.
Dissecting the Model
Although the number of parameters was now pretty manageable, I couldn't discern what was going on just by looking at them. However, I did notice that there were lots of parameters that were very close to ""round"" values like 0, 1, 0.5, -0.25, etc.
Since lots of the logic gates I'd modeled previously were produced with parameters such as those, I figured that might be a good thing to focus on to find the signal in the noise.
I added some rounding and clamping that was applied to all network parameters closer than some threshold to those round values. I applied it periodically throughout training, giving the optimizer some time to adjust to the changes in between. After repeating several times and waiting for the network to converge to a perfect solution again, some clear patterns started to emerge:
layer 0 weights:
[[0         , 0         , 0.1942478 , 0.3666477, -0.0273195, 1         , 0.4076445 , 0.25     , 0.125    , -0.0775111, 0         , 0.0610434],
 [0         , 0         , 0.3904364 , 0.7304437, -0.0552268, -0.0209046, 0.8210054 , 0.5      , 0.25     , -0.1582894, -0.0270081, 0.125    ],
 [0         , 0         , 0.7264696 , 1.4563066, -0.1063093, -0.2293   , 1.6488117 , 1        , 0.4655252, -0.3091895, -0.051915 , 0.25     ],
 [0.0195805 , -0.1917275, 0.0501585 , 0.0484147, -0.25     , 0.1403822 , -0.0459261, 1.0557909, -1       , -0.5      , -0.125    , 0.5      ],
 [-0.1013674, -0.125    , 0         , 0        , -0.4704586, 0         , 0         , 0        , 0        , -1        , -0.25     , -1       ],
 [-0.25     , -0.25     , 0         , 0        , -1        , 0         , 0         , 0        , 0        , 0.2798074 , -0.5      , 0        ],
 [-0.5      , -0.5226266, 0         , 0        , 0         , 0         , 0         , 0        , 0        , 0.5       , -1        , 0        ],
 [1         , -0.9827325, 0         , 0        , 0         , 0         , 0         , 0        , 0        , -1        , 0         , 0        ],
 [0         , 0         , 0.1848682 , 0.3591821, -0.026541 , -1.0401837, 0.4050815 , 0.25     , 0.125    , -0.0777296, 0         , 0.0616584],
 [0         , 0         , 0.3899804 , 0.7313382, -0.0548765, -0.021433 , 0.8209481 , 0.5      , 0.25     , -0.156925 , -0.0267142, 0.125    ],
 [0         , 0         , 0.7257989 , 1.4584024, -0.1054092, -0.2270812, 1.6465081 , 1        , 0.4654536, -0.3099159, -0.0511372, 0.25     ],
 [-0.125    , 0.069297  , -0.0477796, 0.0764982, -0.2324274, -0.1522287, -0.0539475, -1       , 1        , -0.5      , -0.125    , 0.5      ],
 [-0.1006763, -0.125    , 0         , 0        , -0.4704363, 0         , 0         , 0        , 0        , -1        , -0.25     , 1        ],
 [-0.25     , -0.25     , 0         , 0        , -1        , 0         , 0         , 0        , 0        , 0.2754751 , -0.5      , 0        ],
 [-0.5      , -0.520548 , 0         , 0        , 0         , 0         , 0         , 0        , 0        , 0.5       , 1         , 0        ],
 [-1        , -1        , 0         , 0        , 0         , 0         , 0         , 0        , 0        , -1        , 0         , 0        ]]

layer 0 biases:
[0          , 0         , -0.1824367,-0.3596431, 0.0269886 , 1.0454538 , -0.4033574, -0.25    , -0.125   , 0.0803178 , 0         , -0.0613749]
Above are the final weights generated for the first layer of the network after the clamping and rounding. Each column represents the parameters for a single neuron, meaning that the first 8 weights from top to bottom are applied to bits from the first input number and the next 8 are applied to bits from the second one.
All of these neurons have ended up in a very similar state. There is a pattern of doubling the weights as they move down the line and matching up weights between corresponding bits of both inputs. The bias was selected to match the lowest weight in magnitude. Different neurons had different bases for the multipliers and different offsets for starting digit.
The Network's Clever Solution
After puzzling over that for a while, I eventually started to understand how its solution worked.
Digital to analog converters (DACs) are electronic circuits that take digital signals split into multiple input bits and convert them into a single analog output signal.
DACs are used in applications like audio playback where a sound files are represented by numbers stored in memory. DACs take those binary values and convert them to an analog signal which is used to power the speakers, determining their position and vibrating the air to produce sound. For example, the Nintendo Game Boy had a 4-bit DAC for each of its two output audio channels.
Here's an example circuit diagram for a DAC:










If you look at the resistances of the resistors attached to each of the bits of the binary input, you can see that they double from one input to another from the least significant bit to the most significant.  This is extremely similar to what the network learned to do with the weights of the input layer.  The main difference is that the weights are duplicated between each of the two 8-bit inputs.
This allows the network to both sum the inputs as well as convert the sum to analog all within a single layer/neuron and do it all before any activation functions even come into play.
This was only part of the puzzle, though.  Once the digital inputs were converted to analog and summed together, they were immediately passed through the neuron's activation function.  To help track down what happened next, I plotted the post-activation outputs of a few of the neurons in the first layer as the inputs increased:

The neurons seemed to be generating sine wave-like outputs that changed smoothly as the sum of the binary inputs increased.  Different neurons had different periods; the ones pictured above have periods of 8, 4, and 32 respectively.  Other neurons had different periods or were offset by certain distances.
There's something very remarkable about this pattern: they map directly to the periods at which different binary digits switch between 0 and 1 when counting in binary.  The least significant digit switches between 0 and 1 with a period of 1, the second with a period of 2, and so on to 4, 8, 16, 32, etc.  This means that for at least some of the output bits, the network had learned to compute everything it needed in a single neuron.
Looking at the weights of neurons in the two later layers confirms this to be the case.  The later layers are mostly concerned with routing around the outputs from the first layer and combining them.  One additional benefit that those layers provide is ""saturating"" the signals and making them more square wave-like - pushing them closer to the target values of -1 and 1 for all values.  This is the exact same property which is used in digital signal processing for audio synthesis where tanh is used to add distortion to sound for things like guitar pedals.
While playing around with this setup, I tried re-training the network with the activation function for the first layer replaced with sin(x) and it ends up working pretty much the same way.  Interestingly, the weights learned in that case are fractions of π rather than 1.
For other output digits, the network learned to do some over clever things to generate the output signals it needed.  For example, it combined outputs from the first layer in such a way that it was able to produce a shifted version of the signal not present in any of the first-layer neurons by adding signals from other neurons with different periods together.  It worked out pretty well, more than accurate enough for the purpose of the network.
The sine-based version of the function learned by the network (blue) ends up being roughly equivalent to the function sin(1/2x + pi) (orange):










I have no idea if this is just another random mathematical coincidence or part of some infinite series or something, but it's very neat regardless.
Summary
So, in all, the network was accomplishing binary addition by:

Converting the binary inputs into ""analog"" using a version of a digital to audio converter implemented using the weights of the input layer
Mapping that internal analog signal into periodic sine wave-like signals using the Ameo activation function (even though that activation function isn't periodic)
Saturating the sine wave-like signal to make it more like a square wave so outputs are as close as possible to the expected values of -1 and 1 for all outputs

As I mentioned, before, I had imagined the network learning some fancy combination of logic gates to perform the whole addition process digitally, similarly to how a binary adder operates.  This trick is yet another example of neural networks finding unexpected ways to solve problems.
Epilogue
One thought that occurred to me after this investigation was the premise that the immense bleeding-edge models of today with billions of parameters might be able to be built using orders of magnitude fewer network resources by using more efficient or custom-designed architectures.
It's an exciting prospect to be sure, but my excitement is somewhat dulled because I was immediately reminded of The Bitter Lesson.  If you've not read it, you should read it now (it's very short); it really impacted the way I look at computing and programming.
Even if this particular solution was just a fluke of my network architecture or the system being modeled, it made me even more impressed by the power and versatility of gradient descent and similar optimization algorithms.  The fact that these very particular patterns can be brought into existence so consistently from pure randomness is really amazing to me.
I plan to continue my work with small neural networks and eventually create those visualizations I was talking about.  If you're interested, you can subscribe to my blog via RSS at the top of the page, follow me on Twitter @ameobea10, or Mastodon @ameo@mastodon.ameo.dev.

"
https://news.ycombinator.com/rss,GPT-3 Is the Best Journal I’ve Ever Used,https://every.to/superorganizers/gpt-3-is-the-best-journal-you-ve-ever-used,Comments,"


GPT-3 Is the Best Journal I've Ever Used - Superorganizers - Every






































Subscribe





≡


About
Founders‘ Letter
Publications
Collections

Contact Us
Become a Sponsor
Login











Superorganizers




          GPT-3 Is the Best Journal I’ve Ever Used
        
My slow and steady progression to living out the plot of the movie 'Her'

by Dan Shipper
January 13, 2023
♥ 192





Listen







This is a joke, but it's not entirely wrong either.





Sponsor Every

Do you run a software company looking to reach an audience of early-adopters? Consider sponsoring our smart long-form essays on tech, AI, and productivity:

﻿Sponsor Every﻿


Want to hide ads? Become a subscriber

For the past few weeks, I’ve been using GPT-3 to help me with personal development. I wanted to see if it could help me understand issues in my life better, pull out patterns in my thinking, help me bring more gratitude into my life, and clarify my values.I’ve been journaling for 10 years, and I can attest that using AI is journaling on steroids. To understand what it’s like, think of a continuum plotting levels of support you might get from different interactions:Talking to GPT-3 has a lot of the same benefits of journaling: it creates a written record, it never gets tired of listening to you talk, and it’s available day or night. If you know how to use it correctly and you want to use it for this purpose, GPT-3 is pretty close, in a lot of ways, to being at the level of an empathic friend:If you know how to use it right, you can even push it toward some of the support you’d get from a coach or therapist. It’s not a replacement for those things, but given its rate of improvement, I could see it being a highly effective adjunct to them over the next few years. People who have been using language models for much longer than I have seem to agree:
Nick@nickcammarata

Replying to @nickcammarata

@krismartens I'm afraid of seeming hyperbolic, but also don't want to lie or hide information. GPT-3 is really just an incredible therapist, and is able to uncover complex patterns in my thinking and distill clean narratives that helps me a lot. It's also a lot warmer than most therapists

July 17th 2020, 4:55am EST

6 Retweets36 Likes

(Nick is a researcher at OpenAI. He’s also into meditation and is generally a great follow on Twitter.)It sounds wild and weird, but I think language models can have a productive, supportive role in any personal development practice. Here’s why I think it works.Why chatbots are great for journalingJournaling is already an effective personal development practice. It can help you get your thoughts out of your head, rendering them less scary. It shows you patterns in your thinking, which increases your self-awareness and makes it easier for you to change.It creates a record of your journey through life, which can tell you who you are at crucial moments. It can help you create a new narrative or storyline for life events so that you can make meaning out of them.It can also guide your focus toward emotional states like gratitude, or directions you want your life to go in, rather than letting you get swept up in whatever is currently going on in your life. But journaling has a few problems. For one, it’s sometimes hard to sit down and do it. It can be difficult to stare at a blank page and know what to write. For another, sometimes it feels a little silly—is summarizing my day really worth something?Once you get over those hurdles, as a practice it tends to get stale. You don’t read through your old entries that often, so the act of writing down your thoughts and experiences doesn’t compound in the way that it should. The prompts you use often get old: one like, “What are you grateful for today?” might work for the first few weeks, but after a while you need something fresh in order for the question to feel genuine.You want your journal to feel like an intimate friend that you can confide in—someone who’s seen you in different situations and can reflect back to you what’s important in crucial moments. You want your journal to be personal to you, and the act of journaling to feel fresh and full of hope and possibility every time you do.Unfortunately, paper isn’t great at those things. But GPT-3 is. Journaling in GPT-3 feels more like a conversation, so you don’t have to stare at a blank page or feel silly because you don’t know what to say. The way it reacts to you depends on what you say to it, so it’s much less likely to get stale or old. (Sometimes it does repeat itself, which is annoying but I think long-term solvable.) It can summarize things you’ve said to it in new language that helps you look at yourself in a different light and reframe situations more effectively. In this way, GPT-3 is a mashup of journaling and more involved forms of support like talking to a friend. It becomes a guide through your mind—one that shows unconditional positive regard and acceptance for whatever you’re feeling. It asks thoughtful questions, and doesn’t judge. It’s around 24/7, it never gets tired or sick, and it’s not very expensive.Let me tell you about how I use it, what its limitations are, and where I think it might be going.How I started with GPT-3 journalingI didn’t think of using GPT-3 in this way myself. I saw Nick Cammarata’s tweets about it over the years first. My initial reaction was a lot of skepticism mixed with some curiosity. After we launched Lex and I got more interested in AI, I remembered those tweets and decided to play around for myself. I started in the OpenAI playground—a text box where you input a prompt that tells GPT-3 how you want it to behave, and then interact with it:I had a bunch of ideas to start. I tried one from a Facebook PM, Mina Fahmi, whom I met at the AI hackathon I wrote about a few weeks ago. He suggested telling GPT-3 to take on a persona, and told me that he’d had great results asking it to be Socrates.GPT-3 as famous compassionate figureI started experimenting with prompts like this:The green messages are responses from GPT-3. I tried Socrates, the Buddha, Jesus, and a few others, and found I liked Socrates the best (apologies to my Christian and Buddhist readers). The GPT-3 version of him is effective at driving toward the root of an issue and helping you figure out small steps to take to resolve it.There’s a long tradition in various religions of visualizing and interacting with a divine, compassionate figure as a way of getting support—and this was a surprisingly successful alternative route to a similar experience.After a while, though, I became a little bored of Socrates. I’m a verified therapy nerd, so the obvious next step was to try asking GPT-3 to do interactions based on various therapy modalities.GPT-3 as therapy modality expertI tried asking GPT-3 to become a bot that’s well-versed in Internal Family Systems—a style of therapy that emphasizes the idea that the self is composed of many different parts or sub-personalities, and that a lot of growth comes from learning to understand and integrate those parts. It turns out, GPT-3 isn’t bad at that:﻿I also tried asking it to be a psychoanalyst and a cognitive behavioral therapist, both of which were interesting and useful. I even asked it to do Jungian dream interpretation:I don’t know what to make of the efficacy of dream interpretation in general, nor do I know what an actual Jungian might say about this interpretation. But I have found that having dreams reflected back to me in this way can help me understand some of what I’ve been feeling day to day but haven’t been able to put into words. GPT-3 as gratitude journalAnother thing I tried is asking GPT-3 to help me increase my sense of gratitude and joy—like a better gratitude journal:You’ll notice it starts by acting like a normal gratitude journal, asking me to list three things I’m grateful for. But once I respond, it probes about details of what you’re grateful for to get you past your stock answers and into the emotional experience of gratitude. GPT-3 as values coachOne of my favorite therapy modalities is ACT—acceptance and commitment therapy—because I love its focus on values. ACT emphasizes helping people understand what’s most important to them and uses that knowledge to help them navigate difficult emotions and experiences in their lives.Values work is challenging because sometimes it’s hard to connect your day-to-day experiences to your values. So I wanted to see if GPT-3 could help. This is one of the experiments I tried:This works well, and one of the cool things about it is how the prompt works. I took a sample therapy dialog from an ACT-focused values book that I love, Values in Therapy, and asked GPT-3 to generalize from that dialog to learn how to talk to me about values.It worked—successfully guiding our conversation toward talking about what was most important to me. It’s not perfect, but it suggests interesting possibilities for things to try going forward.Problems and limitationsWhile I liked these early experiments, they had a few significant problems.First, the OpenAI playground isn’t designed to facilitate chats, so it’s hard to use. Second, it doesn’t record inputs between sessions, so I ended up having to re-explain myself every time I started a new session. Third, it sometimes gets repetitive and asks the same questions.These are solvable, though. I know because I built a solution: a web app with a chatbot interface that remembers what I say in every session so I never have to repeat myself.  The bot lets me select a persona—like Socrates or an Internal Family Systems therapist—which corresponds to the prompts above. Then I can have a conversation with it. It will help me work through something I’m dealing with, or set goals, or bring my attention to something I’m grateful for. It can even output and save a summary of the session to help me notice patterns in my thinking over time. It’s still early and there are a lot of problems to fix, but I find myself gravitating toward it every day. I feel like I’m building up a record of myself and my patterns over time, and the more I write in it, the more it compounds.I’ll be releasing the bot soon for paying Every members, so if you want access, make sure to subscribe. What’s nextHere’s what I’ve learned so far through all of these experiments with GPT-3 as a journaling tool.There is something innately appealing about  building a relationship with an empathetic friend that you can talk to any time. It’s comforting to know that it’s available, and it’s exciting to think about all of the different prompts you can experiment with to help it support you in the way you need.There is also something weird about all of this. Spilling your guts to a robot somehow cheapens the experience because it doesn’t cost much for a robot to tell you it understands you. This mix of feelings is reflected in this Twitter thread by Rob Morris, the founder of a peer-to-peer support app called Koko:
Rob Morris@RobertRMorris

We provided mental health support to about 4,000 people — using GPT-3. Here’s what happened 👇

January 6th 2023, 2:50pm EST

1k Retweets6k Likes

When people were using GPT-3 to help them provide support to peers, their responses were rated significantly more highly than responses that were generated by humans alone:
Rob Morris@RobertRMorris

Replying to @RobertRMorris

Messages composed by AI (and supervised by humans) were rated significantly higher than those written by humans on their own (p < .001). Response times went down 50%, to well under a minute.

January 6th 2023, 2:50pm EST

66 Retweets785 Likes

But they had to stop using the GPT-3 integration because people felt like getting a response from GPT-3 wasn’t genuine and ruined the experience. Those feelings are understandable, but whether or not they ruin the experience depends on how the interaction is framed to you, and how familiar you are with these tools.I don’t think these objections will last over time for most people. It’s more likely a temporary result of contact with new technology. When you see a movie that you loved, does it cheapen the experience to know that you were touched by a set of pixels moving in the correct sequence over the course of a few hours? Obviously not, but if I had to bet, when movies were first introduced many people probably felt it was a cheaper version of a live performance experience.As these kinds of bots get more common, and we learn to interact with them and depend on them for different parts of our lives, we’ll be less likely to feel that our interactions with them are cheap or stilted.(None of this, by the way, means that in-person interactions aren’t valuable anymore—just that there’s probably more room for bot interactions in your life than you might realize.)If you're someone that's journaled for a long time, you'll find a lot of value in trying GPT-3 out as an alternative to your day-to-day practice. And if you've never journaled before this might be a good way to get started.I’ll be experimenting with this a lot more over the coming weeks and months, and I’ll be sharing everything I learn with you here. I’m excited for what’s next.




What did you think of this post?

Amazing
Good
Meh
Bad





Send Privately

      Your feedback has been saved anonymously. If you want it to be attributed to you, login or sign up.
    



Like this?Become a subscriber.
Subscribe →
Or, learn more.




Read this next:








Superorganizers


Managing Your Manager
How Helping Your Manager Succeed Will Help You Succeed

♥ 173

          Mar 9, 2022
          by Brie Wolfson











Superorganizers


The Four Kinds of Side Hustles
The CEO of Kettle and Fire breaks down how he thinks about side business opportunities

♥ 411

          Sep 16, 2020
          by Justin Mares











Superorganizers


The Fall of Roam
I don’t use Roam anymore. Why?

♥ 208

          Feb 12, 2022
          by Dan Shipper











Napkin Math


In Defense of the Unoptimized Life
Give yourself the space to be inspired

♥ 189

          Jan 12, 2023
          by Evan Armstrong











Every


Introducing: Thesis
Meet some of the internet’s best writers in person

♥ 1
🔒 
          Jan 12, 2023
      






Comments







Post









Post



    You need to login before you can comment.
    Don't have an account? Sign up!














@nattaliehartwig
about 2 hours ago


Loved this, are any of your projects available to try?



♡ 0

      ·
      Reply










✕
Thanks for reading Every!
Sign up for our daily email featuring the most interesting thinking (and thinkers) in tech.
Subscribe
Already a subscriber? Login




Contact Us ·
            Become a Sponsor ·
            Search ·
            Terms

©2023 Every Media, Inc





"
https://news.ycombinator.com/rss,The Cab Ride I'll Never Forget,https://kentnerburn.com/the-cab-ride-ill-never-forget/,Comments,"





















The Cab Ride I'll Never Forget | Kent Nerburn













































































 









		Skip to content













					Kent Nerburn
				


				wandering, wondering, writing
			
 





About

Menu Toggle





Interviews


Photo Gallery


Books
Speaking | Book Clubs
Musings
Shop
Contact
Home
 







 










					Kent Nerburn
				


				wandering, wondering, writing
			
 







Main Menu

 









About

Menu Toggle

InterviewsBook Review: Native EchoesBooksBooks-oldContactDan and Grover talk about Indian MascotsDancing with the Gods: Reflections on Life and ArtKent Nerburn

Menu Toggle

Join our mailing listMusingsPhoto GalleryPrivacyShopSouth Dakota TravelogueSpeaking

Menu Toggle

Presentation OptionsSubscribeThe Cab Ride I’ll Never Forget 

















 










The Cab Ride I'll Never Forget 




There was a time in my life twenty years ago when I was driving a cab for a living. It was a cowboy’s life, a gambler’s life, a life for someone who wanted no boss, constant movement and the thrill of a dice roll every time a new passenger got into the cab.What I didn’t count on when I took the job was that it was also a ministry. Because I drove the night shift, my cab became a rolling confessional. Passengers would climb in, sit behind me in total anonymity and tell me of their lives.We were like strangers on a train, the passengers and I, hurtling through the night, revealing intimacies we would never have dreamed of sharing during the brighter light of day. I encountered people whose lives amazed me, ennobled me, made me laugh and made me weep. And none of those lives touched me more than that of a woman I picked up late on a warm August night.I was responding to a call from a small brick fourplex in a quiet part of town. I assumed I was being sent to pick up some partiers, or someone who had just had a fight with a lover, or someone going off to an early shift at some factory for the industrial part of town.When I arrived at the address, the building was dark except for a single light in a ground-floor window. Under these circumstances, many drivers would just honk once or twice, wait a short minute, then drive away. Too many bad possibilities awaited a driver who went up to a darkened building at 2:30 in the morning.But I had seen too many people trapped in a life of poverty who depended on the cab as their only means of transportation. Unless a situation had a real whiff of danger, I always went to the door to find the passenger. It might, I reasoned, be someone who needs my assistance. Would I not want a driver to do the same if my mother or father had called for a cab?So I walked to the door and knocked.“Just a minute,” answered a frail and elderly voice. I could hear the sound of something being dragged across the floor. After a long pause, the door opened. A small woman somewhere in her 80s stood before me. She was wearing a print dress and a pillbox hat with a veil pinned on it, like you might see in a costume shop or a Goodwill store or in a 1940s movie. By her side was a small nylon suitcase. The sound had been her dragging it across the floor.The apartment looked as if no one had lived in it for years. All the furniture was covered with sheets. There were no clocks on the walls, no knickknacks or utensils on the counters. In the corner was a cardboard box filled with photos and glassware.“Would you carry my bag out to the car?” she said. “I’d like a few moments alone. Then, if you could come back and help me? I’m not very strong.”I took the suitcase to the cab, then returned to assist the woman. She took my arm, and we walked slowly toward the curb. She kept thanking me for my kindness.“It’s nothing,” I told her. “I just try to treat my passengers the way I would want my mother treated.”“Oh, you’re such a good boy,” she said. Her praise and appreciation were almost embarrassing.When we got in the cab, she gave me an address, then asked, “Could you drive through downtown?”“It’s not the shortest way,” I answered.“Oh, I don’t mind,” she said. “I’m in no hurry. I’m on my way to a hospice.”I looked in the rearview mirror. Her eyes were glistening. “I don’t have any family left,” she continued. “The doctor says I should go there. He says I don’t have very long.”I quietly reached over and shut off the meter. “What route would you like me to go?” I asked.For the next two hours we drove through the city. She showed me the building where she had once worked as an elevator operator. We drove through the neighborhood where she and her husband had lived when they had first been married. She had me pull up in front of a furniture warehouse that had once been a ballroom where she had gone dancing as a girl. Sometimes she would have me slow in front of a particular building or corner and would sit staring into the darkness, saying nothing.As the first hint of sun was creasing the horizon, she suddenly said, “I’m tired. Let’s go now.”We drove in silence to the address she had given me. It was a low building, like a small convalescent home, with a driveway that passed under a portico. Two orderlies came out to the cab as soon as we pulled up. Without waiting for me, they opened the door and began assisting the woman. They were solicitous and intent, watching her every move. They must have been expecting her; perhaps she had phoned them right before we left.I opened the trunk and took the small suitcase up to the door. The woman was already seated in a wheelchair.“How much do I owe you?” she asked, reaching into her purse.“Nothing,” I said.“You have to make a living,” she answered.“There are other passengers,” I responded.Almost without thinking, I bent and gave her a hug. She held on to me tightly. “You gave an old woman a little moment of joy,” she said. “Thank you.”There was nothing more to say. I squeezed her hand once, then walked out into the dim morning light. Behind me, I could hear the door shut. It was the sound of the closing of a life.I did not pick up any more passengers that shift. I drove aimlessly, lost in thought. For the remainder of that day, I could hardly talk. What if that woman had gotten an angry driver, or one who was impatient to end his shift? What if I had refused to take the run, or had honked once, then driven away? What if I had been in a foul mood and had refused to engage the woman in conversation? How many other moments like that had I missed or failed to grasp?We are so conditioned to think that our lives revolve around great moments. But great moments often catch us unawares. When that woman hugged me and said that I had brought her a moment of joy, it was possible to believe that I had been placed on earth for the sole purpose of providing her with that last ride.I do not think that I have ever done anything in my life that was any more important. 































 







Copyright © 2023 Kent Nerburn | Powered by kincaid-burrows
 










































"
https://news.ycombinator.com/rss,Show HN: Sketch – AI code-writing assistant that understands data content,https://github.com/approximatelabs/sketch,Comments,"








approximatelabs

/

sketch

Public




 

Notifications



 

Fork
    4




 


          Star
 163
  









        AI code-writing assistant that understands data content
      





163
          stars
 



4
          forks
 



 


          Star

  





 

Notifications












Code







Issues
0






Pull requests
0






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







approximatelabs/sketch









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





6
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




bluecoconut

update readme wording




        …
      




        9d567ec
      

Jan 16, 2023





update readme wording


9d567ec



Git stats







133

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github/workflows



remove python 3.7, add tests, remove uneeded code



Dec 15, 2022









sketch



some bug fix and copy addition



Jan 16, 2023









tests



remove python 3.7, add tests, remove uneeded code



Dec 15, 2022









.gitignore



update with edit and work on text2sql



Oct 13, 2022









README.md



update readme wording



Jan 16, 2023









dev-requirements.txt



starting to rebuild sketch



Dec 15, 2022









pyproject.toml



rename to pandas extension, add missing requirement, use base64 encoding



Jan 11, 2023









setup.py



starting to rebuild sketch



Dec 15, 2022




    View code
 


















sketch
Demo
How to use
.sketch.ask
.sketch.howto
.sketch.apply
Sketch currently uses prompts.approx.dev to help run with minimal setup
How it works





README.md




sketch
Sketch is an AI code-writing assistant for pandas users that understands the context of your data, greatly improving the relevance of suggestions. Sketch is usable in seconds and doesn't require adding a plugin to your IDE.
pip install sketch
Demo
Here we follow a ""standard"" (hypothetical) data-analysis workflow, showing a Natural Language interace that successfully navigates many tasks in the data stack landscape.

Data Catalogging:

General tagging (eg. PII identification)
Metadata generation (names and descriptions)


Data Engineering:

Data cleaning and masking (compliance)
Derived feature creation and extraction


Data Analysis:

Data questions
Data visualization








sketch-demo.mp4





Try it out in colab: 
How to use
It's as simple as importing sketch, and then using the .sketch extension on any pandas dataframe.
import sketch
Now, any pandas dataframe you have will have an extension registered to it. Access this new extension with your dataframes name .sketch
.sketch.ask
Ask is a basic question-answer system on sketch, this will return an answer in text that is based off of the summary statistics and description of the data.
Use ask to get an understanding of the data, get better column names, ask hypotheticals (how would I go about doing X with this data), and more.
df.sketch.ask(""Which columns are integer type?"")
.sketch.howto
Howto is the basic ""code-writing"" prompt in sketch. This will return a code-block you should be able to copy paste and use as a starting point (or possibly ending!) for any question you have to ask of the data. Ask this how to clean the data, normalize, create new features, plot, and even build models!
df.sketch.howto(""Plot the sales versus time"")
.sketch.apply
apply is a more advanced prompt that is more useful for data generation. Use it to parse fields, generate new features, and more. This is built directly on lambdaprompt. In order to use this, you will need to set up a free account with OpenAI, and set an environment variable with your API key. OPENAI_API_KEY=YOUR_API_KEY
df['review_keywords'] = df.sketch.apply(""Keywords for the review [{{ review_text }}] of product [{{ product_name }}] (comma separated):"")
df['capitol'] = pd.DataFrame({'State': ['Colorado', 'Kansas', 'California', 'New York']}).sketch.apply(""What is the capitol of [{{ State }}]?"")
Sketch currently uses prompts.approx.dev to help run with minimal setup
In the future, we plan to update the prompts at this endpoint with our own custom foundation model, built to answer questions more accurately than GPT-3 can with its minimal data context.
You can also directly call OpenAI directly (and not use our endpoint) by using your own API key. To do this, set 2 environment variables.
(1) SKETCH_USE_REMOTE_LAMBDAPROMPT=False
(2) OPENAI_API_KEY=YOUR_API_KEY
How it works
Sketch uses efficient approximation algorithms (data sketches) to quickly summarize your data, and feed that information into language models. Right now it does this by summarizing the columns and writing these summary statistics as additional context to be used by the code-writing prompt. In the future we hope to feed these sketches directly into custom made ""data + language"" foundation models to get more accurate results.









About

      AI code-writing assistant that understands data content
    
Topics



  python


  data-science


  data


  ai


  tabular-data


  pandas


  df


  sketches


  dataframe


  copilot


  codex


  ds


  datasketches


  gpt3


  lambdaprompt


  datasketch



Resources





      Readme
 


Stars





163
    stars

Watchers





2
    watching

Forks





4
    forks







    Releases





6
tags







    Packages 0


        No packages published 







        Used by 22
 




























            + 14
          







    Contributors 2








bluecoconut
Justin Waugh

 






jmbiven
Mike Biven

 





Languages










Python
100.0%











"
https://news.ycombinator.com/rss,EasyPost (YC S13) Is Hiring,https://www.easypost.com/careers,Comments,"Careers at EasyPost - EasyPostSolutionsexpand_lessProductsDiscounted ShippingCreate a LabelShipping APISmartRate APITracking APIAddress Verification APIShipping InsuranceCarbon Offset APIPartner White Label APIUse CasesstoreSmall Businessshopping_cartEcommercestorefrontMarketplacelayersPlatformlocal_shippingFulfillmentPartnersFind a PartnerBecome a PartnerDevelopersexpand_lessGetting StartedAPI DocsClient LibrariesAPI StatusEngineering BlogCarriersCompanyexpand_lessBlogCase StudiesNewsletterAbout UsCareersHelp CenterContact SalesPricingSolutionschevron_rightDeveloperschevron_rightCarriersCompanychevron_rightPricingSolutionsSolve complex shipping logistics problems with a single integration.View All Solutionschevron_rightProductsDiscounted ShippingCreate a LabelShipping APISmartRate APITracking APIAddress Verification APIShipping InsuranceCarbon Offset APIPartner White Label APIPartnersFind a PartnerBecome a PartnerUse CasesstoreSmall Businessshopping_cartEcommercestorefrontMarketplacelayersPlatformlocal_shippingFulfillmentDevelopersAccess our developer resources and learn how to easily integrate with the EasyPost API.Getting StartedAPI DocsClient LibrariesAPI StatusEngineering BlogCompanyExplore our company resources to learn more about EasyPost and the shipping industry.BlogCase StudiesNewsletterAbout UsCareersHelp CenterContact SalesSign up freeLog inCareers at EasyPostOur team of problem solvers brings a modern approach to shipping logistics. We collaborate across departments, ask challenging questions, explore new solutions, and take accountability for our wins and mistakes.See all job openingsThe future of youAs industry experts, we're working not only to help our customers make sense of the industry, but to define where it's headed. We are looking for candidates who are approachable, dynamic, inventive, intelligent, and reliable to join our team in unpacking the future of shipping.Join usThe future of shippingHow can modern, flexible technology improve the customer experience of shipping? What if every business was able to offer same-day shipping? How much waste would be removed from the environment if all our shipments were consolidated into one delivery per week? At EasyPost, we're figuring out the answer to these questions and more.Join usLife at EasyPostlooks_oneAdaptiveEmbrace new challenges to grow your skill set.looks_twoSimpleCreate efficient solutions that are easy to execute.looks_3InclusiveShare new ideas and work collaboratively across teams.Team and technologyWe're a fun group of passionate entrepreneurs who built our own revolutionary software designed to make shipping simple. EasyPost started as an Engineering first company and we are proud to have a pragmatic approach to software development. Our team has a wealth of diverse experience and different backgrounds ranging from startups to large technology companies.Be part of a leading technology company:CI/CD inspired workflows - we deploy dozens of times a daySmall services over monoliths - we've deployed hundreds of servicesStrong engineering tooling and developer supportTransparency and participation around architecture and technology decisionsCulture of blamelessness and improving today from yesterday's shortcomingsCheck out our engineering blogSee openingsBenefits and perksmedical_servicesMedical, dental, vision plansaccess_timeFlexible time-offauto_graphStock option opportunitiessavings401(k) matchsupervisor_accountCross-functional learningtodayMonthly virtual eventsIn the past 3 years, I've learned a staggering amount from our colleagues and have had the best experience of my career thus far.When I work in this type of environment, with such talented and knowledgeable teammates, I really thrive and am extremely motivated to help make EasyPost more successful.Kyle GravesEngineering @ EasyPostStart your adventure at EasyPost© Simpler Postage 2023SolutionsPricingCarriersDiscounted shippingCode-free label creationShipping APISmartRate APITracking APIAddress verificationShipping insuranceCarbon Offset APIPartner White Label APIPartnersDevelopersGuidesAPI DocsClient librariesEngineering blogStatusContact usTalk to supportContact salesCompanyAbout usBlogCareersCase studiesNewsletterPrivacy & termsSupport© Simpler Postage 2023Switch to Desktop Versioneasypost-web-202301132331-5e82544d2f-master"
https://news.ycombinator.com/rss,Speeding up the JavaScript ecosystem part 2 – Module Resolution,https://marvinh.dev/blog/speeding-up-javascript-ecosystem-part-2/,Comments,"


Speeding up the JavaScript ecosystem - module resolution

written by@marvinhagemeist15 January 2023


📖 tl;dr: Whether you’re building, testing and/or linting JavaScript, module resolution is always at the heart of everything. Despite its central place in our tools, not much time has been spent on making that aspect fast. With the changes discussed in this blog post tools can be sped up by as much as 30%.

In part 1 of this series we found a few ways to speed various libraries used in JavaScript tools. Whilst those low level patches moved the total build time number by a good chunk, I was wondering if there is something more fundamental in our tooling that can be improved. Something that has a greater impact on the total time of common JavaScript tasks like bundling, testing and linting.
So over the next couple of days I collected about a dozen CPU profiles from various tasks and tools that are commonly used in our industry. After a bit of inspection, I came across a repeating pattern that was present in every profile I looked at and affected the total runtime of these tasks by as much as 30%. It’s such a critical and influential part of our infrastructure that it deserves to have its own blog post.
That critical piece is called module resolution. And in all the traces I looked at it took more time in total than parsing source code.
The cost of capturing stack traces
It all started when I noticed that the most time consuming aspect in those traces was spent in captureLargerStackTrace an internal node function responsible for attaching stack traces to Error objects. That seemed a bit out of the ordinary, given that both tasks succeeded without showing any signs of errors being thrown.


							After clicking through a bunch of occurrences in the profiling data a clearer picture emerged as to what was happening. Nearly all of the error creations came from calling node’s native fs.statSync() function and that in turn was called inside a function called isFile. The documentation mentions that fs.statSync() is basically the equivalent to POSIX’s fstat command and commonly used to check if a path exists on disk, is a file or a directory. With that in mind we should only get an error here in the exceptional use case when the file doesn’t exist, we lack permissions to read it or something similar. It was time to take a peek at the source of isFile.
						
function isFile(file) {	try {		const stat = fs.statSync(file);		return stat.isFile() || stat.isFIFO();	} catch (err) {		if (err.code === ""ENOENT"" || err.code === ""ENOTDIR"") {			return false;		}		throw err;	}}
From a quick glance it’s an innocent looking function, but was showing up in traces nonetheless. Noticeably, we ignore certain error cases and return false instead of forwarding the error. Both the ENOENT and ENOTDIR error codes ultimately mean that the path doesn’t exist on disk. Maybe that’s the overhead we’re seeing? I mean we’re immediately ignoring those errors here. To test that theory I logged out all the errors that the try/catch-block caught. Low and behold every single error that was thrown was either a ENOENT code or an ENOTDIR code.

							A peek into node’s documentation of fs.statSync reveals that it supports passing a throwIfNoEntry option that prevents errors from being thrown when no file system entry exists. Instead it will return undefined in that case.
						
function isFile(file) {	const stat = fs.statSync(file, { throwIfNoEntry: false });	return stat !== undefined && (stat.isFile() || stat.isFIFO());}
Applying that option allows us to get rid of the if-statment in the catch block which in turn makes the try/catch redundant and allows us to simplify the function even further.
This single change reduced the time to lint the project by 7%. What’s even more awesome is that tests got a similar speedup from the same change too.
The file system is expensive
With the overhead of stack traces of that function being eliminated, I felt like there was still more to it. You know, throwing a couple of errors shouldn’t really show up at all in traces captured over the span of a couple of minutes. So I injected a simple counter into that function to get an idea how frequently it was called. It became apparent that it was called about 15k times, about 10x more than there were files in the project. That smells like an opportunity for improvement.
To module or not to module, that is the question
By default there are three kind of specifiers for a tool to know about:

Relative module imports: ./foo, ../bar/boof
Absolute module imports: /foo, /foo/bar/bob
Package imports foo, @foo/bar

The most interesting of the three from a performance perspective is the last one. Bare import specifiers, the ones that don’t start with a dot . or with a slash /, are a special kind of import that typically refer to npm packages. This algorithm is described in depth in node’s documentation. The gist of it is that it tries to parse the package name and then it will traverse upwards to check if a special node_modules directory is present that contains the module until it reaches the root of the file system. Let’s illustrate that with an example.
Let’s say that we have a file located at /Users/marvinh/my-project/src/features/DetailPage/components/Layout/index.js that tries to import a module foo. The algorithm will then check for the following locations.

/Users/marvinh/my-project/src/features/DetailPage/components/Layout/node_modules/foo/
/Users/marvinh/my-project/src/features/DetailPage/components/node_modules/foo/
/Users/marvinh/my-project/src/features/DetailPage/node_modules/foo/
/Users/marvinh/my-project/src/features/node_modules/foo/
/Users/marvinh/my-project/src/node_modules/foo/
/Users/marvinh/my-project/node_modules/foo/
/Users/marvinh/node_modules/foo/
/Users/node_modules/foo/

That’s a lot of file system calls. In a nutshell every directory will be checked if it contains a module directory. The amount of checks directly correlates to the number of directories the importing file is in. And the problem is that this happens for every file where foo is imported. Meaning if foo is imported in a file residing somewhere else, we’ll crawl the whole directory tree upwards again until we find a node_modules directory that contains the module. And that’s an aspect where caching the resolved module greatly helps.
But it gets even better! Lots of projects make use of path mapping aliases to save a little bit of typing, so that you can use the same import specifiers everywhere and avoid lots of dots ../../../. This is typically done via TypeScript’s paths compiler option or a resolve alias in a bundler. The problem with that is that these typically are indistinguishable from package imports. If I add a path mapping to the features directory at /Users/marvinh/my-project/src/features/ so that I can use an import declaration like import {...} from “features/DetailPage”, then every tool should know about this.
But what if it doesn’t? Since there is no centralized module resolution package that every JavaScript tool uses, they are multiple competing ones with various levels of features supported. In my case the project makes heavy use of path mappings and it included a linting plugin that wasn’t aware of the path mappings defined in TypeScript’s tsconfig.json. Naturally, it assumed that features/DetailPage was referring to a node module, which led it to do the whole recursive upwards traversal dance in hopes of finding the module. But it never did, so it threw an error.
Caching all the things
Next I enhanced the logging to see how many unique file paths the function was called with and if it always returned the same result. Only about 2.5k calls to isFile had a unique file path and there was a strong 1:1 mapping between the passed file argument and the returned value. It’s still more than the amount of files in the project, but it’s much lower than the total 15k times it was called. What if we added a cache around that to avoid reaching out to the file system?
const cache = new Map();function resolve(file) {	const cached = cache.get(file);	if (cached !== undefined) return cached;	// ...existing resolution logic here	const resolved = isFile(file);	cache.set(file, resolved);	return file;}
The addition of a cache sped up the total linting time by another 15%. Not bad! The risky bit about caching though is that they might become stale. There is a point in time where they usually have to be invalidated. Just to be on the safe side I ended up picking a more conservative approach that checks if the cached file still exists. This is not an uncommon thing to happen if you think of tooling often being run in watch mode where it’s expected to cache as much as possible and only invalidate the files that changed.
const cache = new Map();function resolve(file) {	const cached = cache.get(file);	// A bit conservative: Check if the cached file still exists on disk to avoid	// stale caches in watch mode where a file could be moved or be renamed.	if (cached !== undefined && isFile(file)) {		return cached;	}	// ...existing resolution logic here	for (const ext of extensions) {		const filePath = file + ext;		if (isFile(filePath)) {			cache.set(file, filePath);			return filePath;		}	}	throw new Error(`Could not resolve ${file}`);}
I was honestly expecting it to nullify the benefits of adding a cache in the first place since we’re reaching to the file system even in the cached scenario. But looking at the numbers this only worsened the total linting time only by 0.05%. That’s a very minor hit in comparison, but shouldn’t the additional file system call matter more?
The file extension guessing game
The thing with modules in JavaScript is that the language didn’t have a module system from the get go. When node.js came onto the scene it popularized the CommonJS module system. That system has several “cute” features like the ability to omit the extension of the file you’re loading. When you write a statement like require(""./foo"") it will automatically add the .js extension and try to read the file at ./foo.js. If that isn’t present it will check for json file ./foo.json and if that isn’t available either, it will check for an index file at ./foo/index.js.
Effectively we’re dealing with ambiguity here and the tooling has to make sense of what ./foo should resolve to. With that there is a high chance of doing wasted file system calls as there is no way of knowing where to resolve the file to, in advance. Tools literally have to try each combination until they find a match. This is worsened if we look at the total amount of possible extensions that exist today. Tools typically have an array of potential extensions to check for. If you include TypeScript the full list for a typical frontend project at the time of this writing is:
const extensions = [	"".js"",	"".jsx"",	"".cjs"",	"".mjs"",	"".ts"",	"".tsx"",	"".mts"",	"".cts"",];
That’s 8 potential extensions to check for. And that’s not all. You essentially have to double that list to account for index files which could resolve to all those extensions too! This means that our tools have no other option, other than looping through the list of extensions until we find one that exists on disk. When we want to resolve ./foo and the actual file is foo.ts, we’d need to check:

foo.js -> doesn’t exist
foo.jsx -> doesn’t exist
foo.cjs -> doesn’t exist
foo.mjs -> doesn’t exist
foo.ts -> bingo!

That’s four unnecessary file system calls. Sure you could change the order of the extensions and put the most common ones in your project at the start of the array. That would increase the chances of the correct extension to be found earlier, but it doesn’t eliminate the problem entirely.
As part of the ES2015 spec a new module system was proposed. All the details weren’t fleshed out in time, but the syntax was. Import statements quickly took over as they have very benefits over CommonJS for tooling. Due to its staticness it opened up the space for lots more tooling enhanced features like most famously tree-shaking where unused modules and or even functions in modules can be easily detected and dropped from the production build. Naturally, everyone jumped on the new import syntax.
There was one problem though: Only the syntax was finalized and not how the actual module loading or resolution should work. To fill that gap, tools re-used the existing semantics from CommonJS. This was good for adoption as porting most code bases only required syntactical changes and these could be automated via codemods. This was a fantastic aspect from an adoption point of view! But that also meant that we inherited the guessing game of which file extension the import specifier should resolve to.
The actual spec for module loading and resolution was finalized years later and it corrected this mistake by making extensions mandatory.
// Invalid ESM, missing extension in import specifierimport { doSomething } from ""./foo"";// Valid ESMimport { doSomething } from ""./foo.js"";
By removing this source of ambiguity and always adding an extension, we’re avoiding an entire class of problems. Tools get way faster too. But it will take time until the ecosystem moves forward on that or if at all, since tools have adapted to deal with the ambiguity.
Where to go from here?
Throughout this whole investigation I was a bit surprised to find that much room for improvement in regards to optimizing module resolution, given that it’s such a central in our tools. The few changes described in this article reduced the linting times by 30%!
The few optimizations we did here are not unique to JavaScript either. Those are the same optimizations that can be found in toolings for other programming languages. When it comes to module resolution the four main takeaways are:

Avoid calling out to the file system as much as possible
Cache as much as you can to avoid calling out to the file system
When you're using fs.stat or fs.statSync always set the throwIfNoEntry: false
Limit upwards traversal as much as possible

The slowness in our tooling wasn’t caused by JavaScript the language, but by things just not being optimized at all. The fragmentation of the JavaScript ecosystem doesn't help either as there isn’t a single standard package for module resolution. Instead, there are multiple and they all share a different subset of features. That’s no surprise though as the list of features to support has grown over the years and there is no single library out there that supports them all at the time of this writing. Having a single library that everyone uses would make solving this problem once and for all for everyone a lot easier.


"
https://news.ycombinator.com/rss,Servo to Advance in 2023,https://servo.org/blog/2023/01/16/servo-2023/,Comments,"














          How to start
        

          Contributing
        

          Blog
        

          Governance
        




GitHub





Twitter





Mastodon








Servo to Advance in 2023
(2023-01-16) A brief update on the Servo project's renewed activity in 2023.

We would like to share some exciting news about the Servo project. This year, thanks to new external funding, a team of developers will be actively working on Servo. The first task is to reactivate the project and the community around it, so we can attract new collaborators and sponsors for the project.
The focus for 2023 is to improve the situation of the layout system in Servo, with the initial goal of getting basic CSS2 layout working. Given the renewed activity in the project, we will keep you posted with more updates throughout the year. Stay tuned!
About Servo 


Created by Mozilla Labs in 2012, the Servo project is a Research & Development effort meant to create an independent, modular, embeddable web engine that allows developers to deliver content and applications using web standards.  Servo is an experimental browser engine written in Rust, taking advantage of the memory safety properties and concurrency features of the language.  Stewardship of Servo moved from Mozilla Labs to the Linux Foundation in 2020, where its mission remains unchanged.


Back



"
https://news.ycombinator.com/rss,"Show HN: Terra Firma, a playable erosion simulation",https://store.steampowered.com/app/1482770/Terra_Firma/,Comments,"





Terra Firma on Steam























































									Login								

		Store	

Home
Discovery Queue
Wishlist
Points Shop
News
Stats


			Community		

Home
Discussions
Workshop
Market
Broadcasts


		Support	


									Change language								

										View desktop website									





								© Valve Corporation. All rights reserved. All trademarks are property of their respective owners in the US and other countries.								
Privacy Policy
									 |  Legal
									 |  Steam Subscriber Agreement
									 |  Refunds








































		STORE	

Home
Discovery Queue
Wishlist
Points Shop
News
Stats


			COMMUNITY		

Home
Discussions
Workshop
Market
Broadcasts


				ABOUT			

		SUPPORT	






							Install Steam						

login
											 | 
						language


简体中文 (Simplified Chinese)
繁體中文 (Traditional Chinese)
日本語 (Japanese)
한국어 (Korean)
ไทย (Thai)
Български (Bulgarian)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Español - España (Spanish - Spain)
Español - Latinoamérica (Spanish - Latin America)
Ελληνικά (Greek)
Français (French)
Italiano (Italian)
Magyar (Hungarian)
Nederlands (Dutch)
Norsk (Norwegian)
Polski (Polish)
Português (Portuguese - Portugal)
Português - Brasil (Portuguese - Brazil)
Română (Romanian)
Русский (Russian)
Suomi (Finnish)
Svenska (Swedish)
Türkçe (Turkish)
Tiếng Việt (Vietnamese)
Українська (Ukrainian)
Report a translation problem


















								Cart								(0)
							









Your Store
Your Store






										Home									

                                            Community Recommendations                                        

										Recently Viewed									

                                            Steam Curators                                        





New & Noteworthy
New & Noteworthy






											Steam Replay 2022										

											Top Sellers										

											Most Played										

										New & Trending                                    

										Special Offers									

                                        Recently Updated                                    

                                        Popular Upcoming                                    





Categories
Categories






Special Sections

														Free to Play													

Demos


														Early Access													

Controller-Friendly


Remote Play


                                                Software											

												Soundtracks											

VR Titles


VR Hardware


Steam Deck


Great on Deck


													macOS												

													SteamOS + Linux												

For PC Cafés




													Genres
												

														Action													


															Action														


Arcade & Rhythm
Fighting & Martial Arts
First-Person Shooter
Hack & Slash
Platformer & Runner
Third-Person Shooter
shmup


														Adventure													


															Adventure														


Adventure RPG
Casual
Hidden Object
Metroidvania
Puzzle
Story-Rich
Visual Novel

 




														Role-Playing													


															Role-Playing														


Action RPG
Adventure RPG
JRPG
Party-Based
Rogue-Like
Strategy RPG
Turn-Based


														Simulation													


															Simulation														


Building & Automation
Dating
Farming & Crafting
Hobby & Job
Life & Immersive
Sandbox & Physics
Space & Flight

 




														Strategy													


															Strategy														


Card & Board
City & Settlement
Grand & 4X
Military
Real-Time Strategy
Tower Defense
Turn-Based Strategy


														Sports & Racing													


															Sports & Racing														


All Sports
Fishing & Hunting
Individual Sports
Racing
Racing Sim
Sports Sim
Team Sports

 

														Themes													

														Themes													

Adult Only
Anime
Horror
Mystery & Detective
Open World
Sci-Fi & Cyberpunk
Space
Survival
 

														Player Support													

														Player Support													

Co-Operative
LAN
Local & Party
MMO
Multiplayer
Online Competitive
Singleplayer

 


Points Shop


News


Labs







































All Games
																					> Simulation Games
																				> Terra Firma







Community Hub



Terra Firma
















Terra Firma

Developer

Working as Intended 
Publisher

Working as Intended 
Released

											Jun 27, 2021										



								Play god and create your own world in this complex simulation. Plate tectonics, wind and water erosion and plant life all develop and interact to produce a world that is viewable from a single tree to kilometres across.							



Recent Reviews:

Very Positive

													(15)
												

												- 100% of the 15 user reviews in the last 30 days are positive.											



All Reviews:

Very Positive

													(214)
												

												- 92% of the 214 user reviews for this game are positive.											









Release Date:
Jun 27, 2021


Developer:

Working as Intended 


Publisher:

Working as Intended 




Tags

Popular user-defined tags for this product:


												Simulation												
												Strategy												
												God Game												
												Life Sim												
												Sandbox												
												3D												
												Nature												
												Destruction												
												Early Access												
												Relaxing												
												Physics												
												Singleplayer												+


Reviews


All Reviews:

Very Positive

													(92% of 214) All Time




Recent Reviews:

Very Positive

													(100% of 15) Recent




























































































































Sign in to add this item to your wishlist, follow it, or mark it as ignored














Links & info




Is this game relevant to you?


									Sign in to see reasons why you may or may not like this based on your games, friends, and curators you follow.
								

Sign In

																			or										Open in Steam

Features


Single-player 

Profile Features Limited 
										







								Languages:



		English	






Interface
Full Audio
Subtitles



				English			

✔ 

✔ 










Title: Terra Firma
Genre: Simulation, Strategy, Early Access

Developer:
Working as Intended


Publisher:
Working as Intended

Release Date: Jun 27, 2021
Early Access Release Date: Jun 27, 2021




mehwoot on Twitter



			View update history		

			Read related news		

			View discussions		

            Find Community Groups        





Share
Embed
 








Early Access Game
Get instant access and start playing; get involved with this game as it develops.
Note: This Early Access game is not complete and may or may not change further. If you are not excited to play this game in its current state, then you
									should wait to see if the game progresses further in development. Learn more


What the developers have to say:

Why Early Access?
										“This game is in early access in order to give people a chance to play and give feedback on our initial, working version whilst we continue to develop it.”

										Approximately how long will this game be in Early Access?
										“3 years”

										How is the full version planned to differ from the Early Access version?
										“The full version of the game will feature a greatly expanded simulation that builds on what is currently available.  Extra simulation layers related to the weather, such as temperature, air pressure, wind, ocean currents will provide a much richer set of interactions in the simulated world.

We expect to greatly expand the plants and animals that inhabit the world to provide a more interesting outcomes of the user's choice of geography.

The full version will have vastly better graphics than the current version, which uses only a minimum of graphical assets in order to release a playable version of the game.”

										What is the current state of the Early Access version?
										“The current early access version is fully playable, featuringWater and plate tectonic simulationsAbility to change the terrain at willDifferent types of plant life developing and responding to the geography of the worldAbility to view the world from the scale of a single tree to dozens of square kilometres at once”

										Will the game be priced differently during and after Early Access?
										“We plan to gradually raise the price as we ship new content and features.”

										How are you planning on involving the Community in your development process?
										“We'll be closely monitoring feedback from the community both from reviews and community content to decide what features should be added and how the game should be improved.”

																	 
Read more







				Play Terra Firma			



							Free						


Play Game








View Community Hub

 






See all discussions

Report bugs and leave feedback for this game on the discussion boards





About This Game
							Terra Firma gives you the power to play god in a simulated world where the forces of nature interact in complex ways.  Currently in early access, right now you canWatch water flow through the landscape, eroding and depositing the land to form complex arrangements of tributary river systems, river deltas, lakes and oceans in an emergent fashion.Observe the ecosystem respond to the availability of water.  Plants automatically gross across the lanscape with different plants thriving in different environments depending on the distribution of water, nutrients and the geometry of the land.Create your landscape using simulated plate tectonics, watching mountains form, grow and erode as millions of years pass in the blink of an eye.  Customize the landscape to your will, raising, lowering and flattening it to see how the enviornment changes as a resultZoom all the way from looking at an individual tree or plant to view the entire map across hundreds of square kilometresIn future releases, the game willAllow saving and loading worldsHave a fully simulated enviornment with not just water but also climate factors such as temperature, weather, ice, snow, wind and ocean currents forming and changing depending on the geography as well as affecting each otherFeature a wide variety and animal and plant species that flourish across the map according to the geography as well as each other.  Animals will populate accroding to the presence of either plants or other animals which they consume 


System Requirements




Minimum:OS: Windows 7+Memory: 2048 MB RAMGraphics: Dedicated Graphics Card 



Recommended:OS: Windows 7+Memory: 8096 MB RAMGraphics: Dedicated Graphics Card 









See all

More like this







View all
What Curators Say

					1 Curator has reviewed this product. Click here to see them.				








Customer reviews













Overall Reviews:
Very Positive
(214 reviews)









Recent Reviews:
Very Positive
(15 reviews)










Review Type



All (214)

Positive (199)

Negative (15)




Purchase Type



All (214)

Steam Purchasers (0) 

Other (214) 




Language



All Languages (214)

Your Languages (181) 
Customize




Date Range



							To view reviews within a date range, please click and drag a selection on a graph above or click on a specific bar.							
Show graph


Lifetime

Only Specific Range (Select on graph above) 

Exclude Specific Range (Select on graph above) 




Playtime



Brought to you by Steam Labs


							Filter reviews by the user's playtime when the review was written:						

No Minimum

Over 1 hour

No minimum to No maximum








Display As: 

Summary
Most Helpful
Recent
Funny



Off-topic Review Activity



							When enabled, off-topic review activity will be filtered out.  This defaults to your Review Score Setting. Read more about it in the blog post.						
Enabled




Show graph  
Hide graph  





Filters

Filters				




Excluding Off-topic Review Activity
Playtime: 






			Loading reviews...		


			Loading reviews...		


			Loading reviews...		


			Loading reviews...		


			Loading reviews...		














There are no more reviews that match the filters set above
Adjust the filters above to see other reviews













Loading reviews...









Review Filters




















You can use this widget-maker to generate a bit of HTML that can be embedded in your website to easily allow customers to purchase this game on Steam.
Enter up to 375 characters to add a description to your widget:




Create widget




Copy and paste the HTML below into your website to make the above widget appear




Link to the game's store pagehttps://store.steampowered.com/app/1482770/Terra_Firma/





Popular user-defined tags for this product:(?)




Sign In
Sign in to add your own tags to this product.


Sign In











 







© 2023 Valve Corporation.  All rights reserved.  All trademarks are property of their respective owners in the US and other countries.
VAT included in all prices where applicable.  

            Privacy Policy
              |  
            Legal
              |  
            Steam Subscriber Agreement
              |  
            Refunds
              |  
            Cookies



View mobile website







About Valve
          |  Jobs
          |  Steamworks
          |  Steam Distribution
          |  Support
        		  |  Gift Cards
		  |   Steam
		  |   @steam



 
 

"
https://news.ycombinator.com/rss,"ASCII table and history – Or, why does Ctrl+i insert a Tab in my terminal?",https://bestasciitable.com,Comments,"




ASCII table and history (or, why does Ctrl+i insert a Tab in my terminal?)







ASCII table and history
Or, why does Ctrl+i insert a Tab in my terminal?



DecHexBinaryChar
00x0000 00000NUL
10x0100 00001SOH
20x0200 00010STX
30x0300 00011ETX
40x0400 00100EOT
50x0500 00101ENQ
60x0600 00110ACK
70x0700 00111BEL
80x0800 01000BS
90x0900 01001HT
100x0a00 01010LF
110x0b00 01011VT
120x0c00 01100FF
130x0d00 01101CR
140x0e00 01110SO
150x0f00 01111SI
160x1000 10000DLE
170x1100 10001DC1
180x1200 10010DC2
190x1300 10011DC3
200x1400 10100DC4
210x1500 10101NAK
220x1600 10110SYN
230x1700 10111ETB
240x1800 11000CAN
250x1900 11001EM
260x1a00 11010SUB
270x1b00 11011ESC
280x1c00 11100FS
290x1d00 11101GS
300x1e00 11110RS
310x1f00 11111US


DecHexBinaryChar
320x2001 00000SPACE
330x2101 00001!
340x2201 00010""
350x2301 00011#
360x2401 00100$
370x2501 00101%
380x2601 00110&
390x2701 00111'
400x2801 01000(
410x2901 01001)
420x2a01 01010*
430x2b01 01011+
440x2c01 01100,
450x2d01 01101-
460x2e01 01110.
470x2f01 01111/
480x3001 100000
490x3101 100011
500x3201 100102
510x3301 100113
520x3401 101004
530x3501 101015
540x3601 101106
550x3701 101117
560x3801 110008
570x3901 110019
580x3a01 11010:
590x3b01 11011;
600x3c01 11100<
610x3d01 11101=
620x3e01 11110>
630x3f01 11111?


DecHexBinaryChar
640x4010 00000@
650x4110 00001A
660x4210 00010B
670x4310 00011C
680x4410 00100D
690x4510 00101E
700x4610 00110F
710x4710 00111G
720x4810 01000H
730x4910 01001I
740x4a10 01010J
750x4b10 01011K
760x4c10 01100L
770x4d10 01101M
780x4e10 01110N
790x4f10 01111O
800x5010 10000P
810x5110 10001Q
820x5210 10010R
830x5310 10011S
840x5410 10100T
850x5510 10101U
860x5610 10110V
870x5710 10111W
880x5810 11000X
890x5910 11001Y
900x5a10 11010Z
910x5b10 11011[
920x5c10 11100\
930x5d10 11101]
940x5e10 11110^
950x5f10 11111_


DecHexBinaryChar
960x6011 00000`
970x6111 00001a
980x6211 00010b
990x6311 00011c
1000x6411 00100d
1010x6511 00101e
1020x6611 00110f
1030x6711 00111g
1040x6811 01000h
1050x6911 01001i
1060x6a11 01010j
1070x6b11 01011k
1080x6c11 01100l
1090x6d11 01101m
1100x6e11 01110n
1110x6f11 01111o
1120x7011 10000p
1130x7111 10001q
1140x7211 10010r
1150x7311 10011s
1160x7411 10100t
1170x7511 10101u
1180x7611 10110v
1190x7711 10111w
1200x7811 11000x
1210x7911 11001y
1220x7a11 11010z
1230x7b11 11011{
1240x7c11 11100|
1250x7d11 11101}
1260x7e11 11110~
1270x7f11 11111DEL



The binary representation has the most significant bit first
		(“big endian”).

		ASCII is 7-bit; because many have called encodings such as 
		CP437,
		ISO-8859-1,
		CP-1252,
		and others “extended ASCII” some are under the misapprehension that
		ASCII is 8-bit (1 byte).

Understanding ASCII (and terminals)


To understand why Control+i inserts a Tab in your terminal you need to understand
			ASCII, and to understand ASCII you need know a bit about its history and the world it
			was developed in. Please bear with me.
Teleprinters
Teleprinters evolved from the telegraph. Connect a printer and keyboard to a
			telegraph and you’ve got a teleprinter. Early versions were called “printing
			telegraphs”.
Most teleprinters communicated using the ITA2 protocol. For the most part this would
			just encode the alphabet, but there are a few control codes: WRU (“Who R U”) would cause
			the receiving teleprinter to send back its identification, BEL would ring a bell, and it
			had the familiar CR (Carriage Return) and LF (Line Feed).
This is all early 20th century stuff. There are no electronic computers; it’s all
			mechanical working with punched tape. ITA2 (and codes like it) were mechanically
			efficient; common letters such as “e” and “t” required only a single hole to be
			punched.
These 5-bit codes could only encode 32 characters, which is not even enough for just
			English. The solution was to add the FIGS and LTRS codes, which would switch between
			“figures” and “letters” mode. “FIGS R W” would produce “42”. This worked, but typo’ing a
			FIGS or LTRS (or losing one in line noise) would result in gibberish. Not ideal.
Terminals
In the 1950s teleprinters started to get connected to computers, rather than other
			teleprinters. ITA2 was designed for mechanical machines and was awkward to use. ASCII
			was designed specifically for computer use and published in 1962. Teleprinters used with
			computers were called terminals (as in “end of a connection”, like “train
			terminal”). Teleprinters were also called
			“TeleTYpewriter”, or TTY for short, and you can still
			find names like /dev/tty or /bin/stty on modern systems.
People really programmed computers using teleprinters. Here’s a
			video of a teleprinter in action,
			and here’s a somewhat cheesy (but interesting and cute) video which explains how they
			were used to program a PDP 11/10.
A terminal would connect to a computer with a serial port
			(RS-232),
			which simply transfers bytes back and forth. A terminal is more akin to a monitor with a
			keyboard, rather than a computer on its own. A modern monitor connected with HDMI is
			told “draw this pixel in this colour”, in the 1960s the computer merely said “here are a
			bunch of characters”.
If you’re wondering what a “shell” is: a shell is a program to interact with your
			computer. It provides a commandline, runs programs, and displays the result. The
			terminal just displays characters. It’s the difference between a TV and a DVD
			player.
Teleprinters needed some way to communicate events such as “stop sending me data” or
			“end of transmission”. This is what control characters are for. The exact meaning of
			control characters has varied greatly over the years (which is why extensive
			termcap databases
			are required). ASCII is more than just a character set; it’s a way to communicate
			between a terminal and a computer.
An additional method to communicate are
			escape sequences.
			This is a list of characters starting with the ESC control character (0x1b). For example
			F1 is <Esc>OP and the left arrow is <Esc>[OD.
			Computers can give instructions to terminals, too: <Esc>[2C is move
			the cursor 2 positions forward and <Esc>[4m underlines all subsequent
			text. This is also how the Alt key works: Alt+a is <Esc>a.
Modern systems and ASCII properties
All of this matters because modern terminals operate on the same principles as those
			of the 1960s. If you’re opening three xterm or iTerm2 windows then you’re emulating
			three terminals connecting to a “mainframe”.
If you look at the ASCII table above then there are some interesting properties: in
			the 1st column you can see how the left two bits are always set to zero, and
			that the other 5 bits count to 31 (32 characters in total; it starts at 0). The
			2nd column repeats this pattern but with the 6th bit set to 1
			(remember, read binary numbers from right-to-left, so that’s 6th counting
			from the right). The 3rd column repeats this pattern again with the
			7th bit set, and the final column has both bits set.
The interesting part here is that the letters A-Z and some punctuation map directly
			to the control characters in the 1st column. All that’s needed is removing
			one bit, and that’s exactly what the Control key did: clear the 7th bit.
			Lowercase and uppercase letters align in the 3rd and 4th columns,
			and this is what the Shift key did: clear the 6th bit.
Pressing Control+i (lowercase) would mean sending “)”, which is not very useful. So
			most terminals interpret this as Control+I (uppercase), which sends HT. DEL is last is
			so all bits are set to 1. This is how you “deleted” a character in punch tapes: punch
			all the holes!
This is kind of neat and well designed, but for us it means:

There is no way to see if the user pressed only Control or Shift, because from a
					terminal’s perspective all they do is modify a bit for the typed character.
There is no way to distinguish between the Tab key and Control+i. It’s not just
					‘the same’ as Tab, Control+i is Tab.
There is no way to distinguish between Control+a and Control+Shift+a.
Sending Control with a character from the 2nd column is useless.
					Control clears the 7th bit, but this is already 0, so Control+# will
					just send “#”.

The world has not completely stood still and there have been improvements since the
			1960s, but terminals are still fundamentally ASCII-based text interfaces, and programs
			running inside a terminal – like a shell or Vim – still have very limited facilities for
			modern key events. Non-terminal programs don’t have these problems as they’re not
			restricted to a 1960s text interface.
Note: for brevity’s sake many
			aspects have been omitted in the above: ITA2 was derived from Murray code, the 1967
			ASCII spec changed many aspects (1962 ASCII only had uppercase), there were other
			encodings (e.g. EBCDIC), graphical terminals such as the Tektronix 4014 (which xterm can
			emulate), ioctls, etc.
			References and further reading:
				An annotated history of some character codes,
				7-bit character sets,
				Control characters in ASCII and Unicode,
				The TTY demystified





Image 1, a printing telegraph produced in 1907. The
					alphabetically sorted piano keys are a great example of how the first generation
					of new innovations tends to resemble whatever already exists, and that it takes
					a few more innovations to really get the most out of it. This style of piano
					keyboards was introduced in the 1840s, and while the keyboard as we know it
					today was introduced in the 1870s, it took a while for it to replace all
					piano-style keyboards; this is probably among the last models that was
					made).



Image 2, the Teletype model 33 ASR, introduced in 1963. This is
					one first ASCII teleprinters. Note the machinery on the left; you could feed
					this with a punched tape to automatically type a program for you, similar to how
					you would now load a program from a disk.
					The Teletype model 33 was massively popular, and the brand name Teletype became
					synonymous with terminal.
				



Image 3, Ken Thompson working on the PDP-11 using a Teletype
					(model 33?). What always struck be about this image is the atrocious ergonomics
					of … everything. The keyboard, the chair, everything about the posture: it’s all
					terrible. Inventing Unix almost seems easy compared to dealing with
					that!



Image 4, DEC VT100, a kind of terminal that a terminal emulator
				such as xterm emulates. It has a visual display and supports the essential escape
				sequences still in use today. These were known as “visual terminals”, referring to
				the visual screen with characters, as opposed to printing them out.




Created by Martin Tournoij,
		because I’ve had to explain “Control+i is Tab” once too many
		times and figured an in-depth explanation would be helpful.
Source on GitHub;
		PRs and issues welcome.

			Image credits:
			
				Image 1 by Science Museum; CC BY-NC-SA |
			
				Image 2 by AlisonW; CC BY-SA |
			
				Image 3 by Peter Hamer; CC BY-SA |
			
				Image 4 by Jason Scott; CC BY







"
https://news.ycombinator.com/rss,Granian – a Rust HTTP server for Python applications,https://github.com/emmett-framework/granian,Comments,"








emmett-framework

/

granian

Public







 

Notifications



 

Fork
    9




 


          Star
 352
  









        A Rust HTTP server for Python applications
      
License





     BSD-3-Clause license
    






352
          stars
 



9
          forks
 



 


          Star

  





 

Notifications












Code







Issues
7






Pull requests
0






Discussions







Actions







Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Discussions
 


                  Actions
 


                  Security
 


                  Insights
 







emmett-framework/granian









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











master





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





13
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




gi0baro

Update CI release workflow




        …
      




        b843a90
      

Jan 13, 2023





Update CI release workflow


b843a90



Git stats







132

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github



Update CI release workflow



Jan 13, 2023









benchmarks



Update benchmarks



Jan 13, 2023









docs/spec



Fix typos (#14)



Nov 17, 2022









granian



Follow WSGI spec on response iterable (#29)



Jan 13, 2023









lib/pyo3-asyncio



Bump pyo3-asyncio to 0.17



Oct 25, 2022









src



Code cleanup



Jan 13, 2023









tests



Fix wsgi.input out of spec (close #24)



Jan 12, 2023









.gitignore



first implementation



Apr 15, 2022









Cargo.lock



Add PyPy support



Jan 3, 2023









Cargo.toml



Add PyPy support



Jan 3, 2023









LICENSE



first implementation



Apr 15, 2022









README.md



Update benchmarks results



Dec 24, 2022









build.rs



Add PyPy support



Jan 3, 2023









pyproject.toml



Add PyPy support



Jan 3, 2023









setup.py



review package meta



Apr 18, 2022




    View code
 















Granian
Rationale
Features
Quickstart
Project status
License





README.md




Granian
A Rust HTTP server for Python applications.
Rationale
The main reasons behind Granian design are:

Have a single, correct HTTP implementation, supporting versions 1, 2 (and eventually 3)
Provide a single package for several platforms
Avoid the usual Gunicorn + uvicorn + http-tools dependency composition on unix systems
Provide stable performance when compared to existing alternatives

Features

Supports ASGI/3, RSGI and WSGI interface applications
Implements HTTP/1 and HTTP/2 protocols
Supports HTTPS
Supports Websockets over HTTP/1 and HTTP/2

Quickstart
You can install Granian using pip:
$ pip install granian

Create an ASGI application in your main.py:
async def app(scope, receive, send):
    assert scope['type'] == 'http'

    await send({
        'type': 'http.response.start',
        'status': 200,
        'headers': [
            [b'content-type', b'text/plain'],
        ],
    })
    await send({
        'type': 'http.response.body',
        'body': b'Hello, world!',
    })
and serve it:
$ granian --interface asgi main:app

You can also create an app using the RSGI specification:
async def app(scope, proto):
    assert scope.proto == 'http'

    proto.response_str(
        status=200,
        headers=[
            ('content-type', 'text/plain')
        ],
        body=""Hello, world!""
    )
and serve it using:
$ granian --interface rsgi main:app

Project status
Granian is currently under active development.
Granian is compatible with Python 3.7 and above versions on unix platforms and 3.8 and above on Windows.
License
Granian is released under the BSD License.









About

      A Rust HTTP server for Python applications
    
Topics



  python


  rust


  http


  http-server


  asyncio


  asgi



Resources





      Readme
 
License





     BSD-3-Clause license
    



Stars





352
    stars

Watchers





6
    watching

Forks





9
    forks







    Releases
      13







Granian 0.2.1

          Latest
 
Jan 13, 2023

 

        + 12 releases





Sponsor this project



 

 

 Sponsor
 
Learn more about GitHub Sponsors







    Packages 0


        No packages published 







        Used by 6
 




























    Contributors 4





 



 



 



 







Languages












Rust
83.1%







Python
16.5%







Other
0.4%











"
https://news.ycombinator.com/rss,Wikipedia editors serving long sentences in Saudi Arabia since 2020,https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-01-16/Special_report,Comments,"



Wikipedia:Wikipedia Signpost/2023-01-16/Special report - Wikipedia


































Wikipedia:Wikipedia Signpost/2023-01-16/Special report

From Wikipedia, the free encyclopedia
< Wikipedia:Wikipedia Signpost‎ | 2023-01-16


Jump to navigation
Jump to search
Coverage of 2022 bans reveals editors serving long sentences in Saudi Arabia since 2020: Long-time contributors imprisoned for 32 and 8 years after ""swaying public opinion"" and ""violating public morals"".

← Back to ContentsView Latest Issue16 January 2023
Special report
Coverage of 2022 bans reveals editors serving long sentences in Saudi Arabia since 2020


Contribute  —  
Share this


 PDF download
 E-mail
 Facebook
 Twitter
 LinkedIn
 Reddit



By Andreas Kolbe and JPxG




Former Arabic Wikipedia administrators Osama Khalid (left) and Ziyad Alsufyani (right), both now in prison in Saudi Arabia.



Related articlesNation-state involvement

Missed and Dissed
28 November 2022
Editor given three-year sentence, big RfA makes news, Guy Standing takes it sitting down
26 June 2022
A net loss: Wikipedia attacked, closing off Russia? welcoming back Turkey?
30 September 2019
WMF staff turntable continues to spin; Endowment gets more cash; RfA continues to be a pit of steely knives
31 January 2019
Court-ordered article redaction, paid editing, and rock stars
1 December 2018





More articles





Wales in China; #Edit2015
16 December 2015
Russia temporarily blocks Wikipedia
26 August 2015
Turkish Wikipedia censorship; ""Can Wikipedia survive?""; PR editing
24 June 2015
China blocks secure version of Wikipedia
5 June 2013
French intelligence agents threaten Wikimedia volunteer
8 April 2013
Russian Wikipedia shuts down to fight censorship threat; E3 team and new tools; Wikitravel proposal bogged down
9 July 2012
Censorship, social media in schools, and more
30 March 2009







Wikipedians jailed for 32 and 8 years respectively
On January 5, 2023, we learnt that two Wikipedians, Osama Khalid (User:OsamaK) and Ziyad Alsufyani (User:Ziad), have been sitting in jail for more than two years, sentenced to serving 32 and eight years respectively in al-Ha'ir Prison, a Saudi Arabian maximum security facility. The offenses with which they were charged, according to the press release that broke the news, were ""swaying public opinion"" and ""violating public morals"".
The press release in question was published jointly by Democracy for the Arab World Now (DAWN, a human rights organisation co-founded by slain Saudi journalist Jamal Khashoggi) and Lebanese NGO Social Media Exchange (SMEX). It said that Osama and Ziyad had been arrested on the same day in 2020, and sentenced to 5 and 8 years respectively. In September 2022, Osama's sentence was increased to 32 years after an appeal was made; this reflects a recent trend in Saudi Arabia of imposing ever more draconian prison sentences for online criticism of the Saudi government, as reported by human rights organisation ALQST and The Washington Post. DAWN reports that in 2022, Saudi Arabia's Specialized Criminal Court sentenced women to 34 and 45 years of imprisonment for ""tweeting in support of reform"".
The DAWN/SMEX press release combined its report on Osama's and Ziyad's prison sentences with the news that the WMF had recently banned sixteen Wikipedians in the Middle East/North Africa region, including seven Arabic Wikipedia administrators, for alleged conflict-of-interest editing and advancing ""the aims of external parties"" (see Signpost coverage earlier this month).




DAWN (Democracy for the Arab World Now) was co-founded by murdered Saudi journalist Jamal Khashoggi


Internal Wikimedia Investigation Results in Termination of Entire Saudi-Based Team of Administrators
(January 5, 2023 – New York and Beirut): The Saudi Arabian government infiltrated Wikipedia by recruiting the organization's highest ranked administrators in the country to serve as government agents to control information about the country and prosecuting those who contributed critical information about political detainees, said SMEX and Democracy for the Arab World Now (DAWN) today.

Following an internal investigation in 2022, Wikimedia terminated all of its Wikipedia administrators in Saudi Arabia in December. DAWN and SMEX documented Wikipedia's infiltration by the Saudi government based on interviews with sources close to Wikipedia and the imprisoned administrators.
The authors of the press release added:

It's wildly irresponsible for international organizations and businesses to assume their affiliates can ever operate independently of, or safely from, Saudi government control.
The DAWN/SMEX press release was quickly picked up by AFP, resulting in a spate of media reports led by The Guardian and Middle East Eye, followed the next day by Ars Technica and many others.
While these press articles followed the pattern set by DAWN and SMEX, covering the sixteen WMF bans and the imprisonment of the two editors together, it is unclear what connection there is between these two sets of events, or indeed if there is any connection at all. Ars Technica hypothesizes that the prior arrest of Osama and Ziyad may have been related to Saudi infiltration efforts that led to the bans. The Wikimedia Foundation's Trust & Safety office has stated that the December 2022 bans were unrelated to the 2020 arrests.

Who are the jailed Wikimedians?



Osama organized this Wikipedia medical training and editing event at King Saud bin Abdulaziz University for Health Sciences in 2015


Both were longstanding Wikimedia contributors. Osama's first contributions to the English and Arabic Wikipedias date back to 2007. All in all, he made over 870,000 contributions to Wikidata, over 19,000 to the English Wikipedia, around 16,500 to the Arabic Wikipedia, over 16,000 to Commons, over 5,000 to the Arabic Wiktionary, and nearly 800 to Meta-Wiki. 
Ziyad started editing Arabic Wikipedia in 2009, making over 20,000 edits to Wikidata, around 7,500 to Commons, about 6,500 to Arabic Wikipedia, and exactly 100 to English Wikipedia. As medical students, both were particularly involved in editing and translating medical topics in Wikipedia. The Wiki Project Med Foundation, a Wikimedia affiliate specialising in improving Wikimedia projects' coverage of medical topics, issued the following statement to The Signpost: 

Wiki Project Med appreciates the medical editing which Osama Khalid and Ziyad Alsufyani contributed to Wikipedia. They are both Wikimedia editors in good standing who have organized medical editing, training of physicians to edit Wikipedia's medical topics, and good community discussions about improving Wikipedia's coverage of medical topics for Arabic language. The arrest is shocking to us and beyond our understanding. We know nothing about this except that these two are friendly Wikipedia editors who have been highly engaged in our Wikimedia community activities.



Ziyad uploaded this picture of himself to Commons in 2015, with the description ""Arabic Wikipedian"".


Both attended Wikimedia conferences. Osama joined multiple Wikimania events in person, and participated in the medical meetups there (see images on Wikimedia Commons); he also organized the Translation task force, importing Wikipedia medical articles from English to Arabic (and from Arabic to English).

Wikimedia responses to press coverage
Responding to the media coverage, Wikimedia Foundation spokespeople highlighted ""material inaccuracies"" in the press release. According to Ars Technica, for example:

A Wikimedia spokesperson told Ars that there are ""material inaccuracies in the statement released by SMEX/DAWN"" and in a Guardian report. ""There was no finding in our investigation that the Saudi government 'infiltrated' or penetrated Wikipedia's highest ranks,"" Wikimedia's spokesperson told Ars. ""And there are in fact no 'ranks' among Wikipedia admins. There was also no reference to Saudis acting under the influence of the Saudi government in our investigation. While we do not know where these volunteers actually reside, the bans of any volunteers who may have been Saudi were part of a much broader action globally banning 16 editors across the MENA region.""
The Wikimedia Foundation also published a longer statement on the Wikimedia-l mailing list on 6 January, titled ""Recent press around December Office Action"": 

Hello everyone,
Over the last couple of days, there have been several media reports about
the Foundation’s most recent office action, taken on December 6.
More are certain to follow. These media reports are based on a release from
SMEX and Democracy for the Arab World Now (DAWN) that contains many
material inaccuracies. Some of the errors will be obvious to our community
– for perhaps the most obvious, the report states that the 16 users are all
based in Saudi Arabia. This is unlikely to be the case. While we do not
know where these volunteers actually reside, the bans of any volunteers who
may have been Saudi were part of a much broader action globally banning 16
editors across the MENA region. Indeed, many of them are not active in the
Arabic language projects. These organizations did not share the statement
with the Foundation, and ""sources of knowledge"" as cited in their release
can get things wrong. In addition, we do not have staff in the country
named and never have, contrary to a message put out by the same groups on
social media.
As we noted in December in our statement, we are unable to discuss
Foundation office actions in detail. The Foundation always lists accounts banned as a result of its investigations.
It is our goal to be as transparent as we can be within essential
protection policies, which is why we do not ban in secret, but instead
disclose accounts impacted and (when large numbers are involved) have
disclosed the rationale.
The roots of our December action stretch back over several years. We were
initially contacted by outside experts who made us aware about concerns
they had about Farsi Wikipedia. We can’t comment on that report right now,
but it will be published by that organization soon. This report not only
contributed to our August 23, 2021 modification of our non-disclosure agreement to make it harder for rights-holders to be coerced, but led to further
evaluation of issues across MENA. The December bans were the culmination of
those evaluations.
Wikimedia is, as mentioned above, an open knowledge platform, and it
thrives on open participation. Investigations and global bans are not
things that any of us take lightly, but the Foundation is committed to
supporting the knowledge-sharing models that have created so many valuable
information resources in hundreds of languages across the world. Our first
line of defense of our Terms of Use are our volunteers themselves. Where issues present a credible threat of
harm to our users and to the security of Wikimedia platforms, we will do
the best we can to protect both.
We trust and hope that our communities understand that misinformation about
this action has the potential to cause harm to the individuals involved. We
believe in the incredible value produced by our volunteers across the
globe, but even so we recognize that being found in contravention of a
website’s Terms of Use — even in a manner that organization finds serious
enough to warrant a ban — is not the equivalent of being convicted of any
crime. Accordingly, we ask you to please be conscious of the real people
involved, in the spirit of our long established respect for living people on our sites. We realize that it is tempting to speculate, but we do ask you all to
recall that people’s employment options, their relationships, and even
their physical safety may be compromised by speculation.
If anyone feels unsafe on Wikimedia projects, please use the local
community processes or contact us. The Foundation and community will work
together or in parallel to enhance the safety of all volunteers. To contact
the Trust & Safety team please email ca(a)wikimedia.org .
Best regards,
WMF Office/Trust and Safety
Analysis



Sarah Leah Whitson, the Executive Director of DAWN, is a former director of the Middle East and North Africa division of Human Rights Watch.


Notably, this statement does not contain any reference to the two imprisoned Wikipedians. On the other hand, it does express consideration for the people behind the accounts banned last month, whose role in Wikipedia has suddenly become international news, in a way the Wikimedia Foundation clearly had not intended during their initial listing of the bans.
Democracy for the Arab World Now (DAWN) Executive Director Sarah Leah Whitson, a Human Rights Watch veteran, responded to the WMF statements in an update to the Ars Technica article, added a few hours after publication: 

Whitson told Ars that Wikimedia is ""playing technical word games"" in its statement and that ""it's really important for Wikimedia to be transparent about what they have described as a conflict of interest among its editors."" She said that Wikimedia should ""provide more transparency about the 16 users that they banned"" and ""the safety precautions they're going to take to avoid further endangering Wikipedia editors in totalitarian states, because there's no denying that two of them are now languishing in Saudi prisons"" and the problem goes ""well beyond Saudi Arabia."" Whitson urges Wikimedia to reconsider its global model of relying on Wikipedia editors based in totalitarian states, not just because it can endanger the editors, but also because Wikipedia ""loses its credibility"" when information edited in these states cannot be trusted.
These are important points. The WMF is now widely reported to have ""denied claims the Saudi government infiltrated its team in the Middle East"" – as a BBC article puts it – but this does create some inconsistencies. A month ago, on December 6, the WMF's Trust & Safety office issued a confident assertion that ""we were able to confirm that a number of users with close connections with external parties were editing the platform in a coordinated fashion to advance the aim of those parties"". The post stated that ""these connections are a source of serious concern for the safety of our users that go beyond the capacity of the local language project communities targeted to address"" and emphasised that the Foundation had issued these bans ""to keep our users and the projects safe"". But it has provided no information on who these parties threatening users' safety are, if they are indeed unrelated to the Saudi government.
The WMF statement does mention that the roots of the December 2022 bans lie in concerns expressed to the WMF about the Farsi Wikipedia some years ago. There is a public record of concerns about state interference in the Farsi Wikipedia being voiced by Open Democracy, for example, in a September 2019 article titled ""Persian Wikipedia: an independent source or a tool of the Iranian state?"", and by Justice for Iran in an October 2019 Radio Farda article titled ""Critics Say Some Persian Wikipedia Content Manipulated By Iran's Government"".




Radio Farda, the Iranian branch of the U.S. government-funded Radio Free Europe/Radio Liberty, reported on alleged manipulation of Farsi Wikipedia content by Iran's government in 2019. The WMF says concerns expressed about the Farsi Wikipedia a few years ago eventually led to its 2022 investigation that resulted in 16 global bans in December 2022, including bans of seven Arabic Wikipedia administrators


The DAWN/SMEX press release and the many press reports based on it did contain errors. The press release referred to ""16 Saudi administrators""; as reported earlier this month in The Signpost, only seven of the ten banned Arabic Wikipedia users were administrators, and six of the 16 banned users were contributors to the Farsi Wikipedia rather than the Arabic Wikipedia. Moreover, Osama and Ziyad, the two imprisoned Wikipedians, were not administrators at the time of their arrest – both had had their admin rights on Arabic Wikipedia withdrawn years before. The reason? They weren't using them, both having scaled down their Wikipedia activity considerably in recent years, presumably to focus on their medical studies. Ten years ago, however, Osama had uploaded pictures of a number of Saudi human rights activists to Commons; Ziyad uploaded Wikipedia's image of Loujain Alhathloul in 2016.
The headline of the article in The Guardian read: ""Saudi Arabia jails two Wikipedia staff in 'bid to control content'"". This will have left many readers once again with the false impression that Wikimedia Foundation staff administer Wikipedia's day-to-day content and community processes. (There is a reason headlines are not considered reliable sources in Wikipedia – the body of The Guardian's article referred correctly to ""volunteer administrators"".)
The WMF's claim that admins have ""no ranks"", however, is less persuasive. Two of the banned users, for example, had bureaucrat and checkuser rights in addition to administrator privileges (elevated rights that reqire users to sign non-disclosure agreements). Moreover, the entire Arabic Wikipedia – a project with 1.2 million articles – only had a grand total of 26 administrators prior to the global bans (it is now down to 20). To a person in the street, surely that makes any of the 26 people administering the project ""high-ranking"". 
Even more significant is the fact that the banned Arabic Wikipedia administrators include three of the four people who founded the Saudi Wikimedia User Group, the Wikimedia Foundation's official affiliate in Saudi Arabia – among them the affiliate's principal contact person. In total, seven of the ten banned Arabic users are listed as members of the Saudi user group. As for the other three, two, including one of the checkusers, say on their user pages that they are members of the Arabian Gulf Wikimedia User Group, which does not seem to be an officially recognised affiliate yet, and one (the other checkuser) says they're from Kuwait.
The Wikimedia Foundation made another statement on 8 January, saying, in part:

Our investigation and these bans are not connected to the arrest of these two users. The ban decision impacted 16 users, not all of whom were administrators, from Arabic and Farsi Wikipedia. As stated below, we have no reason to believe that these individuals are all residents of Saudi Arabia; on the contrary, this seems extremely unlikely. Further, we imagine you are all aware that editors are volunteers, not paid by the Foundation, and that the Foundation does not have offices or staff in Saudi Arabia.
While, as stated, the December office action is unrelated to the arrests of
two Wikimedians in Saudi Arabia, the safety of Wikimedia volunteers always
remains our utmost concern. We understand the desire to take action or
speak out. Know that we need to act in the interests of any volunteer whose
safety is under threat. As indicated in yesterday's message, additional
publicity around such cases can cause harm, as can speculation and
misinformation. We are confident that everyone values the safety of their
fellow volunteers and can understand the constraints this might create. 
Arabic Wikipedia community statement



The Arabic Wikipedia community has condemned the WMF action, arguing the bans are at odds with the model of decentralized governance that the Foundation always talks about.


The Arabic Wikipedia community has released a statement on the global bans, adopted with 38 in support, 2 opposed, and 0 neutral. What follows is an English translation of the community statement originally issued in Arabic:

Wikipedia: Statement regarding the events of December 6, 2022
This is a statement issued by the Arabic Wikipedia community to comment on the events of December 6, 2022, and the accompanying global ban that included ten user accounts on the Arabic Wikipedia, including seven administrators.
In the Arabic Wikipedia, we focus on a decentralized governance model in which all community members play roles in the decision-making process, oversight over the drafting of the encyclopedia's policies as well as guidelines, and their enforcement. This can be achieved through direct participation in the election of administrators, and in resolving conflicts and disagreements that occur in the encyclopedia. We do expect the Wikimedia Foundation, which has always supported this governance model, to follow it when dealing, not only with the Arabic community but with all other communities to ensure full transparency and mutual accountability.
We do condemn, in the strongest terms, the work model based on confidential complaints and non-public investigations, which creates a toxic work environment that is incompatible with the nature of volunteering and undermines the main Wikipedia principles of transparency and the assumption of good faith. At the same time, we call on the Foundation to adopt a transparent model in which it has no guardianship over communities, and where it accepts, without restrictions, mutual accountability from communities. The relationship should be based on the grounds that all parties, involved in a transparent governance process, are equal in all the stages of the process.
We also understand the existence of complications associated with attempts to manipulate the content of the Arabic Wikipedia, to polish or distort the image of certain parties; we condemn all these attempts without any reservations and stress the need for Wikipedia to be a platform that adopts a neutral point of view. At the same time, we call on the Foundation to involve local communities in the content protection process by sharing information with them in a way that does not harm the privacy of the users involved in the process and does not put them at risk.
If a user violates the policies, even if they hold administrator rights, they will be dealt with firmly in accordance with the local policies approved by our community. We do not tolerate the abuse of administrative powers nor the manipulation of encyclopedic content to serve third parties whatsoever, including directed editing, and we have policies governing these matters. They apply to all users equally without distinction. Therefore, we are surprised, in light of all this, that the institution imposes its supervision on our self-governing society without prior notice and issues irrevocable decisions without explanation.
We also point out the severe harm that the ban has done to our local community. We lost seven active administrators in one fell swoop! This represents 30% of the administrators in our community, including two bot operators. This has set our community back years and does not, surely, contribute to encyclopedia growth. Mainly, we have suffered the consequences of this ban at the technical level in the encyclopedia, and we appeal to the technical team in the Foundation and the open-source communities to provide the necessary technical assistance to maintain the continuity of the project as much as possible.
The Arabic community has chosen a committee of four people to follow up with the Wikimedia Foundation on the basis of mutual accountability on the issue of the above bans. We are waiting, and we hope, for the Wikimedia Foundation to cooperate with this committee, facilitate its work and share with it the information in its possession without harming the privacy of any user on the Arabic Wikipedia or its sister projects.
Wikimedia Foundation reply posted on the Arabic Wikipedia



Vinicius Siqueira, Osama Khalid, Netha Hussain, Emily Temple-Wood, Anthony Cole, Jake Orlowitz, Daniel Mietchen, Lane Rasberry, James Heilman and Peter Coti (clockwise starting front left) at a WikiProject Med meetup at the 2013 Hong Kong Wikimania conference


On 10 January, the Wikimedia Foundation replied to the Arabic Wikipedia community statement on the associated talk page. It is the first Foundation statement to actually use the imprisoned Wikipedians' names. The reply was posted in Arabic; a machine-aided (Google/Bing) translation follows below:

Update from the Wikimedia Foundation
Hello all
We know the past few weeks have been difficult for the community. We also realize that this situation remains confusing and worrying in light of the media reports that have emerged. As an organization, we regret the distress and concern this situation has caused the community. While we know we can't answer all of your questions, we want to make sure you understand our processes and the rationale behind them. We also want to ensure that our actions are in the best interests of the community to the best of our ability and with the tools available to us. As mentioned, the measures were not linked in any way to the recent media reports that are currently circulating, nor in any way to the arrests. The Foundation has learned of the arrest of Osama and Ziyad, and is actively following up on their situations.
As we know that not everyone will have read all of the data, we would like to reiterate that the process of reaching the decision to take action in December 2022 was not easy or rushed. The investigation into violations of the Terms of Use took a long time starting with the Persian Wikipedia and moving on as new information emerged, and the final decision was guided by multiple levels of review by several employees across different functions. After consideration, it was unanimously agreed that the action is necessary to keep the community and platforms safe. Proper implementation of this measure was equally important in keeping the community and platforms safe, and thus adhering to established policies and procedures.
We realize that media reports and recent actions in December 2022 make many of you skeptical and perhaps even apprehensive about participating in the projects. We want you to know that the projects are owned by everyone, and most of all, that you are the creators and curators of the content. Foundation interventions in content or management issues on the sites are rare and limited to exceptionally problematic circumstances. No one should fear that the Foundation will take action on unintentional mistakes made while participating as editors in good faith.
As many of you already know, the Foundation fully supports community autonomy and the principle of subsidiarity as part of our commitment to respecting and promoting community autonomy. Not only do we feel this is the right approach to our shared values, but it is the only approach that can make these amazing projects work. To ensure we maintain this commitment, we do not deal with general community or community member disputes that might otherwise be addressed through existing community actions, nor do we act as a means of appealing community policies and decisions. If such situations arise, we look forward to working to help the community members who need help, but most of the time, this assistance will consist of guiding the community members to find the right community avenue that will solve their problem.
On some occasions, the Foundation considers cases of abuse. This only occurs when it has been brought to our attention that the local community lacks the necessary processes to effectively address the situation, or when the organization has a legal obligation as a platform provider to act in the interests of the safety of users and the platform. When we intervene, we are limited in the course of action we can take. Our procedures are guided by the Office's work policies, which allow us to issue global bans, event bans, issue warnings, interaction bans, and advanced permission removal. While this responsibility rests with us, we do not take our interventions lightly; these investigations take a lot of time and effort and require multiple staff members across different departments to ensure that we provide a comprehensive understanding of the matter before we take any action. For the size of our communities, we have issued very few centralized global bans. Collective global bans like the one we issued in December 2022 are only put in place in the most exceptional circumstances, when the evidence strongly supports a serious threat to the organization's Terms of Use that all contributors must agree to abide by when editing the projects.
Our December 6 Office action was the result of the Foundation's multiple, long-term investigations undertaken as part of our duties as a platform provider. It was not related to the media reports currently circulating. While there are still limits to what we can disclose in order to protect the safety and privacy of our users, we truly understand and sympathize with the fact that this continues to be an upsetting situation and would like you to know that we would not have taken this action if it were not necessary.
We also want to acknowledge that the media reports have created significant doubt in people's minds about the safety of participating in Wikimedia projects, because of their direct linkage to cases of volunteers being arrested. It is unfortunate that many organizations relied on incomplete facts and indirect sources in their coverage, which directly contradicts our principles. Regardless of the current situation, the Foundation is well aware that such risks exist globally, and we want our community members to be aware too - and work with us to take precautions to stay safe. Six months ago, the United Nations published an article describing the rise of disinformation as a ""global disease"".
In late May 2020, the Board included protecting projects and communities from ""misinformation and bad actors"" in its Statement on Community Culture. On August 23, 2021, we amended our Non-Disclosure Agreement to make it more difficult to coerce rights holders, by restricting access in certain high-risk regions where individuals may be particularly vulnerable to threats to themselves and their families. We continue to work to secure the safety of those combating this ""global disease"" – disinformation – not just through Office actions but in terms of proactively encouraging safe practices, as in our recent blog post on protecting online anonymity. This assessment by external experts has identified a number of areas to support our approach, the Board has issued a policy symbolizing our commitment to this improvement, and our Human Rights Team continues to work to provide resources of information and support to users on the ground. We are also working on making additional digital security resources available to community members who feel unsafe online, which we will finalize soon.
We respect and realize that this action represents a major setback for the community and that is why we are open to providing the community with the support needed and what help we can provide. If there is anything we can do to help the community during this time, please do not hesitate to let us know via ca@wikimedia.org. As mentioned earlier, we are ready to provide you with the required support to the best of our ability.
Best Regards,
Wikimedia Foundation Office WMFOffice (talk) 09:09, 10 January 2023 (UTC)
Much to ponder
The WMF mentioned a change to the Non-Disclosure Agreement in the statements above. This concerns a document VRT volunteers, CheckUsers, Oversighters and Stewards are required to sign. The change, made on 23 August 2021, added the following words to the relevant page on Meta-Wiki:

The Foundation shall not grant Foundation volunteer NDA recognition to applicant(s) for volunteer roles if the applicants live in jurisdictions that block(ed) access to Wikimedia projects AND there is reason to believe that their domicile is known to others than the individual applicant(s) and the Foundation. Exemptions may be granted in individual cases following a request for review by the Legal department. Granting such NDAs would put the applicant(s) as well as other volunteers relying on the Foundation’s platform at undue risk. All NDA-based access rights granted to users fulfilling both criteria in the proposed adjustment shall be revoked at the point of policy adjustment.
This still seems weak, given the risk of decade-long prison sentences served in high-security facilities. Even if an editor's place of residence is only known to them and the Foundation today, there is no guarantee at all that others won't discover it at some point in the future. A checkuser whose identity becomes known to a present (or future) authoritarian government would not just be at risk personally, but could also be compelled – legally or otherwise – to collect user data and pass these on to state organs, putting other users at risk of prosecution.
There is much to ponder here about project governance, government influence on Wikimedia projects, and the vulnerability of editors and administrators to coercion and imprisonment. But the most pressing question is perhaps what we, as a movement, can do to help Osama and Ziyad. 
The Wikimedia Foundation, DAWN and SMEX clearly got off on the wrong foot – it would be good to see them engage in constructive dialogue now, and pool their resources, at least inasmuch as our fellow Wikimedians are concerned. According to DAWN Executive Director Sarah Leah Whitson, who discussed the case with The Signpost, campaigning for their release at this point, over two years into their sentences, is very unlikely to do them harm, and may do some good.

External links
""Foundation Trust & Safety action in the MENA Region"", Wikimedia-l mailing list thread, 6 December 2022 onwards
""Saudi Arabia: Government Agents Infiltrate Wikipedia, Sentence Independent Wikipedia Administrators to Prison"", DAWN press release, 5 January 2023, also published on the SMEX website
""Saudi Arabia jails two Wikipedia staff in 'bid to control content'"", The Guardian, 5 January 2023
""Saudi Arabia 'infiltrated' Wikipedia to control content, activists say"", Middle East Eye, 5 January 2023
""Wikipedia admin jailed for 32 years after alleged Saudi spy infiltration"", Ars Technica, 6 January 2023
""Recent press around December Office Action"", Wikimedia-l mailing list thread, 6 January 2023 onwards
""Wikipedia operator denies Saudi infiltration claim"", BBC, 7 January 2023, edited 10 January 2023
""Saudi Government Narrative Control Efforts Now Include The Jailing Of Wikipedia Administrators"", TechDirt, 10 January 2023






← Previous ""Special report""
In this issue16 January 2023From the team
Special report
News and notes
In the media
Technology report
In focus
Serendipity
Gallery
Humour
Opinion
Featured content
Traffic report
From the archives


+ Add a commentDiscuss this story
These comments are automatically transcluded from this article's talk page. To follow comments, add the page to your watchlist. If your comment has not appeared here, you can try purging the cache.

From what I remember reading on the Arabic Wikipedia discussion about the bans, there were a significant number of other editors there making blatantly pro-SA government statements and were angry at the editor accounts being banned in relation to that. I have concerns that the Arabic (and possibly Persian) language Wikipedia communities are entirely subsumed by blatantly biased pro-government accounts. Because the reason for the bans was never a mystery to anyone, not seriously. Even if the WMF has been trying to be vague about it all. Even this very Signpost article is quite clear and direct on the fact that we all know that the banned accounts were people working directly for the SA government in order to push their own personal views of events and to downplay the ongoing human rights atrocities that Saudi Arabia's administration is committing. With our unfortunate two editors discussed above being only a single example among many. SilverserenC 05:43, 16 January 2023 (UTC)Reply[reply]

That is an absolute monarchy for you.  scope_creepTalk 13:38, 16 January 2023 (UTC)Reply[reply]
I've said this before, but I believe that if there's any way for the WMF to use its considerable funds and influence to promote the spread of free knowledge in autocratic nations, then that should be one of its highest priorities. Free knowledge is why we're here. We as the Wikipedia communities, regardless of language, should be some of Khalid and Alsufyani's strongest advocates. Thebiguglyalien (talk) 17:38, 16 January 2023 (UTC)
Reply[reply]






The Signpost needs your help putting together the next issue.


Home
About
Archives
Newsroom
Subscribe
Suggestions





Retrieved from ""https://en.wikipedia.org/w/index.php?title=Wikipedia:Wikipedia_Signpost/2023-01-16/Special_report&oldid=1134032349""
Categories: Wikipedia Signpost archives 2023-01Wikipedia Signpost RSS feed



Navigation menu



Personal tools


Not logged inTalkContributionsCreate accountLog in





Namespaces


Project pageTalk





English









Views


ReadEditView history





More

























Navigation


Main pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate




Contribute


HelpLearn to editCommunity portalRecent changesUpload file




Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage information




Print/export


Download as PDFPrintable version




Languages



Add links






 This page was last edited on 16 January 2023, at 17:59 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License 3.0;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Mobile view
Developers
Statistics
Cookie statement








"
https://news.ycombinator.com/rss,Rendering like it's 1996 – Bitmap fonts and DOS,https://marioslab.io/posts/rendering-like-its-1996/dos-nostalgia/,Comments,"













Mario's Lab





Mario's Lab
Mastodon
Twitter
Github
RSS



Rendering like it's 1996 - Bitmap fonts and DOS
December 07, 2022




This screen has burned itself into my retina.



To follow along this blog post with running code, make sure you've installed the prerequisites. Then:

git clone https://github.com/badlogic/r96
cd r96
git checkout 04-dos-nostalgia
./tools/download-tools.sh
code .

Last time we learned about loading images and blitting. That was over 3 weeks ago, making me miss my target of posting one series entry a week. But there's a reason for it! I was rather busy in those two weeks.
After using Hopper to generate control flow graphs to discuss performance optimization, I got a little sick of the workflow and built my own assembly CFG viewer. Just paste some x86 or ARM assembly generated by MSVC, Clang, or GCC into the left panel, and view the control flow graph of each function on the right. I also made it a re-usable NPM package. Going forward, I can embed those fancy CFGs directly.
Then I drifted off into yet another rabbit hole. Spurred by a mean comment on Reddit about how the r96 code doesn't even run in DOS, I made the code of the series run in DOS.
First, I built a DOS backend for MiniFB. Then, I forked an old GDB version which is capable of remotely debugging 32-bit protected mode DOS programs as produced by DJGPP, the GCC fork I use to build C/C++ DOS programs. I also forked DOSBox-x to fix it up so my forked GDB can actually connect to DOS programs via the serial port/TCP emulation.
Finally, I took the barely functional GDB stub that comes with DJGPP, rewrote it and added a ton of functionality to it, so I can now debug DOS programs running in DOSBox-x from the comforts of Visual Studio Code.
All of that work culminated in a VS Code extension, which lets you go from 0 to debugging a simple DOS mode 13h demo app in VS code in about 80 seconds:

With all of that out of my system, I built some shell scripts that will help you install (almost) all the tools to compile, run, and debug the r96 project for desktop, web, and DOS. And I added some VS Code magic so you can comfortably start debugging sessions on each platform.
And to round it all off, I cleaned up the Git repo, so each blog post maps to exactly one commit. And I rewrote the first 3 blog posts in the series. So yeah.
I can now happily continue writing the series. Promise. Unless I'll add Android and iOS support in the future. I currently don't feel that specific masochism piling up inside of me.
Today, we're looking at DOS support, and then load and draw some bitmap fonts.
Demo: Hello DOS
Alright, go get the latest and greatest from the r96 repository. Follow the README.md to install the tools, including the new DOS tools. The README.md will also get you up to speed on how to build and debug everything in VS Code or on the command line. Or, if you want a detailed run-down of the project and its build and IDE support, read the first entry of the series.
To celebrate DOS support, I've added a new demo called 12_hello_dos.c:

#include <MiniFB.h>
#include <stdio.h>
#include ""r96/r96.h""
#include ""stdlib.h""
#include <math.h>

#define GDB_IMPLEMENTATION
#include ""dos/gdbstub.h""

#define num_grunts 100

typedef struct grunt {
	int x, y, vx, vy;
} grunt;

int main(void) {
	gdb_start();
	r96_image image;
	if (!r96_image_init_from_file(ℑ, ""assets/grunt.png"")) {
		printf(""Couldn't load file 'assets/grunt.png'\n"");
		return -1;
	}

	r96_image output;
	r96_image_init(&output, 320, 240);
	struct mfb_window *window = mfb_open(""12_hello_dos"", output.width, output.height);

	grunt grunts[num_grunts];
	for (int i = 0; i < num_grunts; i++) {
		grunt *grunt = &grunts[i];
		grunt->x = rand() % 320;
		grunt->y = rand() % 200;
		grunt->vx = 1;
		grunt->vy = 1;
	}
	do {
		r96_clear_with_color(&output, 0xff222222);
		for (int i = 0; i < num_grunts; i++) {
			grunt *grunt = &grunts[i];
			if (grunt->x < 0) {
				grunt->x = 0;
				grunt->vx = -grunt->vx;
			}
			if (grunt->x > 320 - 64) {
				grunt->x = 320 - 64;
				grunt->vx = -grunt->vx;
			}
			if (grunt->y < 0) {
				grunt->y = 0;
				grunt->vy = -grunt->vy;
			}
			if (grunt->y > 240 - 64) {
				grunt->y = 240 - 64;
				grunt->vy = -grunt->vy;
			}
			grunt->x += grunt->vx;
			grunt->y += grunt->vy;
			r96_blit_keyed(&output, ℑ, grunt->x, grunt->y, 0x00000000);
		}
		if (mfb_update_ex(window, output.pixels, output.width, output.height) != STATE_OK) break;
		gdb_checkpoint();
	} while (mfb_wait_sync(window));

	r96_image_dispose(ℑ);
	r96_image_dispose(&output);
	return 0;
}

This is our first animated demo!
The demo draws 100 moving grunts, that bounce off of the screen boundaries. Each grunt is stored in a simple grunt struct, which in turn stores the grunt's position (x, y) and velocity on the x- and y-axis (vx, vy) in pixels per frame. During initialization, we give each grunt a random position within the screen boundaries and set its velocity on each axis to 1 (lines 29-35).
What's a frame you may ask? A frame can be many things, but in our case, a frame is simply one iteration of the main loop of your program (lines 36-62). In each frame, we check whether each grunt is still inside the screen boundaries. If a grunt is outside the screen boundaries on the x- or y-axis (or both), we move them back inside the bounds and negate their velocity on the axis they left the screen on.
E.g. a grunt moving to the right (vx = 1), leaving the screen on the x-axis (x > 320 - 64), will be moved back inside the screen boundaries (x = 320 - 64), and its velocity on the x-axis will become -1. Starting in the next frame, the grunt will then move to the left, until it exits the screen boundaries on the left side of the screen. The same happens on the y-axis.
Once all the checks are complete, we add the grunt's velocity to its position. Each frame, the grunt's position thus changes by vx pixels on the x-axis, and vy pixels on the y-axis. Hence why vx and vy are given as pixels per frame.

Note: This is a very basic form of explicit Euler integration. It's much less scary than it sounds! Go learn your fundamentals.

Now, there's one big problem with this type of moving objects: it depends on the speed of execution.
We call mfb_wait_sync(), which waits for a vertical refresh, effectively limiting the number of frames per second to the screen refresh rate, so 60Hz, 90Hz, 120Hz, or whatever other wonky screen refresh rate the display has.
On a 60Hz screen a grunt will thus move 60 pixels per second, on a 120Hz it will move 120 pixels.
For a game, that's not great: different players will experience the game at different speeds, depending on their hardware. We'll look into this issue in a future series entry.

Note: Many old DOS games actually did have this problem: they would not take into account how much time has passed since the last frame, but instead update game object positions at a fixed rate each frame. There's a reason Wikipedia has an entry on the notorious PC turbo button.

Here's the little demo on the web:





And here it is running in DOSBox-x, telling DOSBox-x to go full speed.

DOSBox-x on my system syncs to 60Hz in windowed mode, while Chrome runs the web demo at the full 120Hz of my display. In the video above, there is some smearing and artifacts. That's mostly due to the MP4 encoding and doesn't look like that when actually running the demo in DOSBox-x on your system.
Is the DOSBox-x performance indicative of performance on old systems? No. DOSBox-x is going full speed, which is way faster than what my old 486 could do. However, you can modify the emulation speed via the DOSBox-x menu CPU > Emulated CPU speed. In the following video, I've set the emulated CPU speed to be equivalent to a 486DX2 with 66Mhz:

While that's more accurate, it's still not quite the same as real hardware. To get a more accurate sense of how the program would perform on a real 486, we can use 86Box. 86Box is as cycle accurate emulator for various old x86 systems.

Looks like DOSBox-x isn't far off with its emulation. So why is it so slow?

Note: Setting up virtual machine images for 86Box is a bit terrible. I've created 2 images you can download, a 486 image and a Pentium image, pre-installed with MS-DOS 6.22, a mouse driver, and a CD-ROM driver. You can run them via 86box -c 486/86box.conf and 86box -c pentium/86box.conf. The images also include QBasic 1.1. And NIBBLES.BAS and GORILLA.BAS. Just saying.

Why is it so slow on a 486?
The MiniFB DOS backend sets up a video mode with either 24-bit or 32-bit color depth through VESA. MiniFB assumes 32-bit color depth, so we have to abide by that and go VESA.
This works pretty well from Pentium class machines onwards, if the (emulated) video card supports VESA. Here's the demo on Pentium class hardware in 86Box:

A 486 may support 24-bit and 32-bit color depth video modes, depending on the graphics card. Mine did. However, that doesn't mean the system is fast enough to actually deal with that amount of data. A run of the mill 486 would have memory throughput somewhere in the range of 10-25MB/s. You read that right.
In our demo above, we render to a 320x240 output r96_image. The call to r96_clear_with_color() has to touch 0.3MB worth of pixels. Rendering a single grunt means reading 64x64x4 bytes from the grunt image and writing them to a 64x64x4 bytes big region in the output r96_image. For 100 grunts, that's reading 1.6MB and writing 1.6MB. Finally, the output r96_image is transferred to the VESA linear buffer, a memory mapped region from which the graphics card will read what it should output to the display. That's another 320x240x4 bytes, or 0.3MB. Each frame we thus touch 0.3 + 1.6 + 1.6 + 0.3 = 3.8MB of memory. And while this simple analysis doesn't account for memory caches, it does align with what we experience when running the demo on a (emulated) 486. We do indeed only get something like 3-5 frames per second, which is 11.4-19MB of data pushed by the demo per second.
That's one of the reasons pretty much all older DOS games targeting 386 or 486 would use mode 13h or derivatives like Mode X. Both of these video modes use 8 bits to encode a pixel's color. But instead of directly encoding the color's red, green and blue component, the 8-bit value is an index into a palette with a total of 256 colors. That cuts down on memory and bandwidth needs considerably.
If we went mode 13h in our demo, we'd go from 3.8MB to 0.95MB of data per frame. That translates to 12-20 frames per second, which is still not great, but often playable enough. That's about the frame rate I got when playing MicroProse's Formula One Grand Prix on my 486.
So what's the solution? Draw less each frame! DOOM and Quake relied on various techniques like binary space partitioning to avoid drawing things that are invisible or occluded. Drawing less means touching less memory. Consider that 100 grunts are about 5.3 screens worth of pixels. That's a lot of overdraw.
Yes, we could probably squeeze a lot of cycles out of the blitting functions if we handcrafted some 32-bit x86 assembly. But DJGPP actually does a pretty good job at producing fast machine code. And I don't want to drop down into assembly land.

Note: modern hardware won't save you from these issues either sometimes. When NVIDIA sent me a prototype Tegra board in the early 2010s, I soon found out that you could only render about 2 full-screen alpha blended rectangles through OpenGL ES before the frame-rate takes a heavy hit.

Excursion: DOS debugging support
When we debug the demo on the desktop, the debugger will spawn the demo process and use system APIs to stop, resume, inspect, and otherwise manipulate the process.
For DOS applications running in DOSBox-x or on a real machine, we do not have the luxury of a debugger. Instead, we use a piece of code called GDB stub that we integrate in our program. Here's how that works in 12_hello_dos.c.
Of note are 3 pieces of code in the demo above, which do nothing on any platform other than DOS. In lines 7-8 we have:
#define GDB_IMPLEMENTATION
#include ""dos/gdbstub.h""

This pulls in my GDB stub implementation for DJGPP/DOS, which is a single header file library.
The stub's task is it to communicate with the debugger over the serial port, and tell it when the program has stopped due to a breakpoint, or segfault, or other reason. The stub then waits for commands from the debugger to execute, like setting breakpoints, inspecting memory and CPU registers, stepping, continuing, etc.
This GDB stub type of debugging is a cooperative debugging approach. The stub needs to be integrated with the program itself. This explains the other two GDB related lines of code in the demo.
The gdb_start() function is called at the beginning of main(). It waits for the debugger to connect on the serial port. When the debugger tells the stub to continue execution of the program, the stub stops communicating with the debugger for the time being, and gives back control to the program.
The stub then waits for a system level signal to be raised, like a breakpoint or segfault, for which the stub has registered handlers. If such a signal happens, the stub takes over control from the program again, tells the debugger about the program being stopped, and waits for debugger commands to execute.
The final GDB related line is gdb_checkpoint() in line 61. It is placed at the end of our main loop. This is required so the stub can check if the debugger asked to interrupt the program, in which case the stub will take control of the program again and talk to the debugger.
The GDB stub expects all communication to happen through serial port COM1. Some emulators and virtual machines, like DOSBox-x or VirtualBox, can expose the emulated serial port as a TCP port to programs on the host OS. That's what's happening when we debug a demo in DOSBox-x. DOSBox-x exposes the serial port on TCP port 5123, to which GDB connects via TCP. DOSBox-x will then translate TCP packages to writes to the serial port, which the GDB stub reads from COM1. If the GDB stub writes to COM1, then DOSBox-x will forward the data through TCP to GDB.
In theory, the GDB stub should also work on real-hardware. Sadly, I do not have my 486 anymore, nor a serial cable or a serial port on my MacBook.
If you want to debug any of the demos in DOS, you'll have to add the 3 pieces of GDB stub related code to the demo's sources as outlined above. Only the 12_hello_dos.c demo is currently set-up for DOS debugging. Since our code is cross-platform, there won't be a need to debug in DOS a lot though.

Note: when debugging the demos compiled for DOS, we'll be using DOSBox-x instead of 86Box. Two reasons: getting data into and out of 86Box is very annoying. And there is no serial port over TCP emulation in 86Box, so the debugger couldn't even connect. It should be possible to hook the debugger up with a program running in MS-DOS or FreeDOS in VirtualBox though.

Bitmap fonts
Rendering text these days is really, really hard. When we go zooming around documents or web pages via mouse wheel or touch zoom, we expect text to scale seamlessly and stay crisp. If we want to get fancy, we add kerning and hinting to the mix.
It gets even harder when non-latin scripts like arabic script or CJK script need to get put on a screen. Now you have to deal with (more) ligatures, mixed left-to-right and right-to-left layouting, and various other complexities.
And to top it all off, what you get out of a font file is usually a vector representation of not a character, but a glyph, which can be a character, or a part of a character, and oh my, this is all very complicated.
Thankfully, there are various libraries that can help us draw text. For translating a text string to a set of glyphs, or shaping as it's usually called, you can use HarfBuzz. If you want to rasterize those glyphs, which are usually given in vector form, you can use FreeType. If you  want to use your GPU to do most of that, you can use Slug. Your operating system usually also comes with APIs to draw text.
We aren't going to do any of that though. We'll be going somewhat old school and draw inspiration from VGA text mode fonts, but with a 2022 spirit (aka being wasteful).
Before we can look at font pixels, we need to talk about how text is stored in the tubes of our computerers.
Character encodings
Text is composed of characters. When we store text digitally, those characters need to be stored as a sequence of (binary) numbers. When we read characters from a file to draw them to the screen, or translate key strokes to characters, we need to map numbers back to characters. Similarly, when the C compiler encounters a string literal like const char *text = ""Hello world"", it will convert the characters in the string to a sequence of numbers that gets embedded in the final executable.
Mapping those sequences of numbers to characters and vice versa is what character encodings are for.
One of the oldest character encodings is ASCII. Each character is encoded in 1 byte. Well, actually, ASCII only uses the first 7-bits, so it encodes a total of 128 characters. Well, that's not quite true either. Only 95 of these characters are printable. The other 33 ""characters"" are what's called control codes. Notable ones are \t or 9, which indicates a tab, and \n or 10, the line feed. See, it's already complicated!
Here are all the printable characters and non-printable control codes contained in ASCII with their (hexa-)decimal codes.

> ascii -d
Dec Hex    Dec Hex    Dec Hex  Dec Hex  Dec Hex  Dec Hex   Dec Hex   Dec Hex
  0 00 NUL  16 10 DLE  32 20    48 30 0  64 40 @  80 50 P   96 60 \`  112 70 p
  1 01 SOH  17 11 DC1  33 21 !  49 31 1  65 41 A  81 51 Q   97 61 a  113 71 q
  2 02 STX  18 12 DC2  34 22 ""  50 32 2  66 42 B  82 52 R   98 62 b  114 72 r
  3 03 ETX  19 13 DC3  35 23 #  51 33 3  67 43 C  83 53 S   99 63 c  115 73 s
  4 04 EOT  20 14 DC4  36 24 $  52 34 4  68 44 D  84 54 T  100 64 d  116 74 t
  5 05 ENQ  21 15 NAK  37 25 %  53 35 5  69 45 E  85 55 U  101 65 e  117 75 u
  6 06 ACK  22 16 SYN  38 26 &  54 36 6  70 46 F  86 56 V  102 66 f  118 76 v
  7 07 BEL  23 17 ETB  39 27 '  55 37 7  71 47 G  87 57 W  103 67 g  119 77 w
  8 08 BS   24 18 CAN  40 28 (  56 38 8  72 48 H  88 58 X  104 68 h  120 78 x
  9 09 HT   25 19 EM   41 29 )  57 39 9  73 49 I  89 59 Y  105 69 i  121 79 y
 10 0A LF   26 1A SUB  42 2A *  58 3A :  74 4A J  90 5A Z  106 6A j  122 7A z
 11 0B VT   27 1B ESC  43 2B +  59 3B ;  75 4B K  91 5B [  107 6B k  123 7B {
 12 0C FF   28 1C FS   44 2C ,  60 3C <  76 4C L  92 5C \  108 6C l  124 7C |
 13 0D CR   29 1D GS   45 2D -  61 3D =  77 4D M  93 5D ]  109 6D m  125 7D }
 14 0E SO   30 1E RS   46 2E .  62 3E >  78 4E N  94 5E ^  110 6E n  126 7E ~
 15 0F SI   31 1F US   47 2F /  63 3F ?  79 4F O  95 5F _  111 6F o  127 7F DEL

The codes 0-31 are control codes, including the \t (9) and \n (10) codes we discussed above. Printable characters start at code 32 (  or space) and go to code 126. The final code 127 is another control code.
ASCII is short for ""American Standard Code for Information Interchange"". Unsurprisingly, the ASCII encoding really only contains characters used in US English, and by coincidence, some other western scripts.
Now, I'm not 'merican. And based on my server logs, chances are good you aren't 'merican either. What about other fancy characters, like 'ö' or 'ê'? Or characters from the arabic or CJK scripts? Well, that's a lot more complicated and historically involves something called code pages, which was and still is an utter mess.
The alternative to code pages is Unicode. Unicode defines codes (or code points in Unicode parlance) for almost 150,000 characters used in scripts from all around the world, including historic ones. It also includes emojis, for better or worse. Your parents' brains have probably also switched to emoji only instant messaging communication. And they said computers would make us kids dumb. Thanks, Unicode.
Unicode has multiple encodings, like UTF-8, UTF-16, and so on. Thankfully, the world has now mostly standardized on UTF-8, for good reasons. UTF-8 is a multi-byte encoding. Depending on the character, we may need 1 to 4 bytes to store it.
For our demos, we'll store text either in C source code as literals ala const char *text = ""Hello world"", or in text files in the assets/ folder of the r96 project. Both the C sources and text files will be encoded using UTF-8. Anything else would be pain. This means we have to deal with UTF-8 when rendering text.
But as I said earlier, we do not want to go full Unicode text rendering, as that'd require us to integrate all the fancy libraries mentioned above. We want a simpler solution. Enter Unicode's first 256 code points. These code points are split up into 2 blocks.
The first block from code point 0-127 is called the Basic Latin Unicode block. The code points are the exact same codes as used in ASCII, including both non-printable control codes (0-31 and 127) and printable characters (32-126). When encoding text with UTF-8, the resulting sequence of bytes is backwards compatible with ASCII: the first 128 Unicode code points get encoded as a single byte in UTF-8.
The second block from code point 128-255 is called the Latin 1 Supplement block. It contains another set of non-printable control codes (128-159) called C1 controls, which we can safely ignore for the purpose of rendering text. The remaining code points in the block (160-255) include additional characters used in some western scripts. These Unicode code points are encoded with 2 bytes in UTF-8.
Surprise! Those first 256 Unicode code points map directly onto an old code page, namely, the  ISO-8859-1 character set. It is sometimes incorrectly referred to as extended ASCII. Here are the characters contained in the set.


The ISO-8859-1 character set Source: Wikipedia

E.g. ö is encoded as 0xF6 or 246 in decimal. The gray blocks are the control codes.
Alright, we've decided to use the first 2 Unicode blocks spanning code points 0-255. All our C source code containing string literals will be stored UTF-8 encoded. And any text files we put into assets/ to be read by our demos will also be UTF-8 encoded. There are two minor complications.
The first complication is how C compilers handle string literals. When the compiler encounters something like const char *text = ""Hello world"", it will use a character encoding to turn the literal ""Hello world"" into a sequence of bytes embedded in the executable. Which encoding is chosen, depends on the compiler. By default, Clang and GCC convert the string literal to UTF-8 and embed the corresponding byte sequence. Clang even assumes that the source file encoding is UTF-8 and refuses to compile anything else. MSVC is ... different. Luckily, we do not care for MSVC in this series. If you do care for some reason, just make sure to pass /utf8 as a compiler flag to ensure MSVC embeds string literals as UTF-8 as well.
The second complication is actually reading the code points of a UTF-8 encoded text string, whether it comes from a C string literal or a UTF-8 encoded file read from disk. We have to deal with the multi-byte nature of the UTF-8 encoding, as code points above 127 are encoded as two bytes. Luckily, I've taken care of that with the function r96_next_utf8_character():

uint32_t r96_next_utf8_code_point(const char *data, uint32_t *index, uint32_t end) {
	static const uint32_t utf8_offsets[6] = {
			0x00000000UL, 0x00003080UL, 0x000E2080UL,
			0x03C82080UL, 0xFA082080UL, 0x82082080UL};

	uint32_t character = 0;
	const unsigned char *bytes = (const unsigned char *) data;
	int num_bytes = 0;
	do {
		character <<= 6;
		character += bytes[(*index)++];
		num_bytes++;
	} while (*index != end && ((bytes[*index]) & 0xC0) == 0x80);
	character -= utf8_offsets[num_bytes - 1];

	return character;
}

This function takes a sequence of bytes (data) encoding a UTF-8 string, an index into the byte sequence, and the last valid index (end). Both indices are byte offsets, not character offsets!
The function then reads the next UTF-8 character, which may be 1 to 4 bytes long, and returns its code point. Additionally, it increments the index accordingly, so we know at what byte offset the next character starts.

Note: I stole the original of this function many years ago from ... somewhere. I can not remember anymore. I've since modified it to my needs. To the original author: I'm deeply sorry I forgot who you are.

We can use this function to iterate all UTF-8 characters in a byte sequence and get their code points:

const char *utf8_text = ""¡ÄÖ$\n\t"";
uint32_t index = 0;
uint32_t end = strlen(utf8_text);
while (index != end) {
	uint32_t code_point = r96_next_utf8_code_point(utf8_text, &index, end);
	printf(""code point: %i/%x\n"", code_point, code_point);
}

Which prints the code point of each character in decimal and hexadecimal.
code point: 161/a1
code point: 196/c4
code point: 214/d6
code point: 36/24
code point: 10/a
code point: 9/9

As expected. Compare the output to the ISO-8859-1 chart above for validation.
This function can deal with any valid UTF-8 byte sequence and returns code points as a 32-bit unsigned integer. For our purposes, we are only interested in code points 0-255 and will ignore any other code points.
The glyph atlas
Alright, we have all our encoding bases covered. The next question is: how do we turn a code point like 64 (0x41) into the corresponding glyph image for the character A from a font, so we can blit it onto the screen?
To make things easy for us, we'll define some limits:

We'll only render the printable Unicode code points between 0-255 as described above.
We'll only use fixed-width or monospaced fonts. Each glyph in such a font has the same width. We can entirely ignore things like kerning this way.
The font size is fixed.

With these limits in place, the basic idea of a glyph atlas goes like this:

Pick a monospaced font, like the original IBM VGA 8x16 font.
Use a glyph rendering library like FreeType to load the font and render out a glyph image for each printable Unicode code point between 0-255.
Pack those glyph images into a single image called the glyph atlas in some order which makes mapping from a code point to the glyph image coordinates inside the glyph atlas trivial.

Here's an example of what such a glyph atlas could look like.




I've super-imposed a red grid de-marking each glyph's boundaries. An atlas we can use would not have that grid on it. The pixels of the glyph are fully opaque white (0xffffffff), while the background pixels are transparent (0x00000000);
The atlas above contains glyph images from the IBM VGA 8x16 font for the Unicode code points 0-255. Each glyph is 8x16 pixels in size. Each row consists of 16 glyphs. There are 16 rows in total, so 256 glyphs in total, one for each code point.
The glyphs in the first row map to code points 0-15, the glyphs in the second row map to code points 16-31, and so on. The first, second, ninth, and tenth row are empty, as these are the glyphs for non-printable control characters. The other rows contain the glyphs for all printable characters.
If you compare this glyph atlas with the ISO-8859-1 table above, you'll see that they are equivalent, except that the last glyph in the bottom right corner is missing from the atlas. The IBM VGA 8x16 font simply does not have a glyph for that code point.
So how do we generate this atlas? We don't. At least we won't write code for that as part of this series. I've already written a web tool based on FreeType that does exactly what we need. It's called Mario's (B)it(m)ap (F)ont (G)enerator (I'm a a dad, I'm allowed to name it like that) and you can run it in your browser here.
The tool lets you load a monospaced TrueType font, set the pixel height of the glyphs you want, and spits out a 16x14 grid of glyph images for the code points 32-255. It omits the code points 0-31 and thus the first two rows of the atlas as those are non-printable control codes anyways. The above atlas thus becomes this:




We're still wasting two rows in the middle for the second set of control codes. But keeping them around makes converting code points to glyph image coordinates easier.
We can store the generated glyph atlas as a .png file in the assets/ folder. I did just that using the file name assets/ibmvga.png. The generator also tells us that each glyph has a size of 8x16 pixels. We'll need to remember that for when we actually draw text later. Since the glyph atlas is a plain old image, we can load it via r96_image_init_from_file().
We're almost ready to render a text string. We need two more things:
* Being able to map a Unicode code point to a region in the glyph atlas image, where a region is defined by its top-left corner x- and y- pixel coordinates in the glyph atlas, and its width and height in pixels.
* Being able to not just blit an entire r96_image to another, but also blit regions of an r96_image to another r96_image.
Let's start with the mapping problem.

Note: We could put both the atlas and the glyph size information into some custom file format. I decided that's not worth it, so we'll go with a .png and some hard coded glyph sizes in the code.

Mapping code points to glyph atlas pixel coordinates
How can we map a code point to the pixel coordinates of the top left corner of a glyph image in the atlas?
Before we resolve pixel coordinates for a code point, it's actually easier to use a different coordinate system. Let's give each glyph in the atlas an x- and y-coordinate.


For our example glyph atlas in the last section above, each cell represents a glyph image of size 8x16 pixels. In the diagram, the cell shows both the glyph and its code point.
The top-left glyph image has coordinate (0, 0) and the bottom-right glyph image has coordinate (15, 13). We can define a simple equation that goes from glyph coordinates to code point, just like we did for pixel coordinates to pixel address:
code_point = glyph_x + glyph_y * glyphs_per_row + 32

Why the + 32? Because the first glyph has code point 32 (space). Without it, we'd get 0 for glyph_x = 0 and glyph_y = 0.
We can reverse this glyph coordinates to code point mapping as follows:
glyph_x = (code_point - 32) % glyphs_per_row;
glyph_y = (code_point - 32 - glyph_x) / glyphs_per_row;

The % glyphs_per_row basically strips the glyph_y * glyphs_per_row component from the original equation above, leaving us with the glyph x-coordinate.
To calculate glyph_y, we can then subtract the just calculated glyph_x, which gives us the code point of the first glyph in the row, and divide by glyphs_per_row to arrive at the glyph_y coordinate.
All that's left to get the pixel coordinate of the top left corner of a glyph is to multiply the glyph coordinates by the glyph pixel width and height of the font, 8 and 16 in the example above.
glyph_pixel_x = glyph_x * glyph_width;
glyph_pixel_y = glyph_x * glyph_height;

Blitting regions
Alright, we can generate glyph atlases for the first 255 Unicode code points, and we can calculate the pixel coordinates of a glyph image in the atlas corresponding to a code point. We also know the size of each glyph in pixels, as we specified that when generating the glyph atlas.
But we have one more problem: our current blitting functions can only blit an entire r96_image. What we need is blitting functions that blit just a region from a r96_image. Luckily, that's trivial, given our existing blitting functions! Here's a blitting function that blits a region from one r96_image to another.

void r96_blit_region(r96_image *dst, r96_image *src, int32_t dst_x, int32_t dst_y, int32_t src_x, int32_t src_y, int32_t src_width, int32_t src_height) {
	assert(src_x + src_width - 1 < src->width);
	assert(src_y + src_height - 1 < src->height);

	int32_t dst_x1 = dst_x;
	int32_t dst_y1 = dst_y;
	int32_t dst_x2 = dst_x + src_width - 1;
	int32_t dst_y2 = dst_y + src_height - 1;
	int32_t src_x1 = src_x;
	int32_t src_y1 = src_y;

	if (dst_x1 >= dst->width) return;
	if (dst_x2 < 0) return;
	if (dst_y1 >= dst->height) return;
	if (dst_y2 < 0) return;

	if (dst_x1 < 0) {
		src_x1 -= dst_x1;
		dst_x1 = 0;
	}
	if (dst_y1 < 0) {
		src_y1 -= dst_y1;
		dst_y1 = 0;
	}
	if (dst_x2 >= dst->width) dst_x2 = dst->width - 1;
	if (dst_y2 >= dst->height) dst_y2 = dst->height - 1;

	int32_t clipped_width = dst_x2 - dst_x1 + 1;
	int32_t dst_next_row = dst->width - clipped_width;
	int32_t src_next_row = src->width - clipped_width;
	uint32_t *dst_pixel = dst->pixels + dst_y1 * dst->width + dst_x1;
	uint32_t *src_pixel = src->pixels + src_y1 * src->width + src_x1;
	for (int32_t y = dst_y1; y <= dst_y2; y++) {
		for (int32_t i = 0; i < clipped_width; i++) {
			*dst_pixel++ = *src_pixel++;
		}
		dst_pixel += dst_next_row;
		src_pixel += src_next_row;
	}
}

This is basically our old r96_blit() function with additional arguments. We sepcify the destination (dst) and source (src) image as before. We also specify the coordinates (dst_x, dst_y) at which the source image should be blitted in the destination image. Those used to be called x and y. Finally, we specify the region from the source image we want to blit, given as its top-left corner (src_x, src_y) and width and height (src_width, src_height).
The implementation itself then only has three minor modifications compared to r96_blit().
The function starts with two asserts that ensure that the source region is valid. Next, dst_x2 and dst_y2 are calculated using the source region width and height instead of the source image width and height. Finally, src_x1 and src_y1 aren't initialized to 0, but to src_x and src_y.
That's it! The rest, including the clipping, is exactly the same as r96_blit(). We can already use this function to blit glyph images from the glyph atlas. And for some use cases, that'd be good enough.
However, if we only want to blit the white pixels of a glyph and ignore it's background pixels, we need color keying.
Easy, just copy r96_blit_keyed() and apply the same modifications.

void r96_blit_region_keyed(r96_image *dst, r96_image *src, int32_t dst_x, int32_t dst_y, int32_t src_x, int32_t src_y, int32_t src_width, int32_t src_height, uint32_t color_key) {
	assert(src_x + src_width - 1 < src->width);
	assert(src_y + src_height - 1 < src->height);

	int32_t dst_x1 = dst_x;
	int32_t dst_y1 = dst_y;
	int32_t dst_x2 = dst_x + src_width - 1;
	int32_t dst_y2 = dst_y + src_height - 1;
	int32_t src_x1 = src_x;
	int32_t src_y1 = src_y;

	if (dst_x1 >= dst->width) return;
	if (dst_x2 < 0) return;
	if (dst_y1 >= dst->height) return;
	if (dst_y2 < 0) return;

	if (dst_x1 < 0) {
		src_x1 -= dst_x1;
		dst_x1 = 0;
	}
	if (dst_y1 < 0) {
		src_y1 -= dst_y1;
		dst_y1 = 0;
	}
	if (dst_x2 >= dst->width) dst_x2 = dst->width - 1;
	if (dst_y2 >= dst->height) dst_y2 = dst->height - 1;

	int32_t clipped_width = dst_x2 - dst_x1 + 1;
	int32_t dst_next_row = dst->width - clipped_width;
	int32_t src_next_row = src->width - clipped_width;
	uint32_t *dst_pixel = dst->pixels + dst_y1 * dst->width + dst_x1;
	uint32_t *src_pixel = src->pixels + src_y1 * src->width + src_x1;
	for (dst_y = dst_y1; dst_y <= dst_y2; dst_y++) {
		for (int32_t i = 0; i < clipped_width; i++) {
			uint32_t src_color = *src_pixel;
			uint32_t dst_color = *dst_pixel;
			*dst_pixel = src_color != color_key ? src_color : dst_color;
			src_pixel++;
			dst_pixel++;
		}
		dst_pixel += dst_next_row;
		src_pixel += src_next_row;
	}
}

But we can do even better. No text rendering engine is complete without support for colored text! As is stands, we can only draw white text, as that's the color the glyph atlas generator spits out. On-top of color keying, we can also apply what's usually known as tinting.
We'll implement tinting in the simplest possible way: multiply the red, green, and blue color component of the source pixel with the red, green, and blue color component of the specified tinting color. That result of the multiplication is then normalized back to the 0-255 range for each component by dividing by 255. This effectively mixes the two colors.
tinted_red = ((source_red * tint_red) >> 8) & 0xff;
tinted_green = ((source_green * tint_green) >> 8) & 0xff;
tinted_blue = ((source_blue * tint_blue) >> 8) & 0xff;


Note: for the case of tinting glyphs images as generated by the generator, we could just write the tint color to the destination if the source pixel color doesn't match the color key. However, this approach above also works for tinting arbitrary source pixel colors. We'll see why that's useful in a later demo.

Here's the final region blitting routine, which takes both a color key and a tinting color:

void r96_blit_region_keyed_tinted(r96_image *dst, r96_image *src, int32_t dst_x, int32_t dst_y, int32_t src_x, int32_t src_y, int32_t src_width, int32_t src_height, uint32_t color_key, uint32_t tint) {
	assert(src_x + src_width - 1 < src->width);
	assert(src_y + src_height - 1 < src->height);

	int32_t dst_x1 = dst_x;
	int32_t dst_y1 = dst_y;
	int32_t dst_x2 = dst_x + src_width - 1;
	int32_t dst_y2 = dst_y + src_height - 1;
	int32_t src_x1 = src_x;
	int32_t src_y1 = src_y;

	if (dst_x1 >= dst->width) return;
	if (dst_x2 < 0) return;
	if (dst_y1 >= dst->height) return;
	if (dst_y2 < 0) return;

	if (dst_x1 < 0) {
		src_x1 -= dst_x1;
		dst_x1 = 0;
	}
	if (dst_y1 < 0) {
		src_y1 -= dst_y1;
		dst_y1 = 0;
	}
	if (dst_x2 >= dst->width) dst_x2 = dst->width - 1;
	if (dst_y2 >= dst->height) dst_y2 = dst->height - 1;

	uint32_t tint_r = R96_R(tint);
	uint32_t tint_g = R96_G(tint);
	uint32_t tint_b = R96_B(tint);

	int32_t clipped_width = dst_x2 - dst_x1 + 1;
	int32_t dst_next_row = dst->width - clipped_width;
	int32_t src_next_row = src->width - clipped_width;
	uint32_t *dst_pixel = dst->pixels + dst_y1 * dst->width + dst_x1;
	uint32_t *src_pixel = src->pixels + src_y1 * src->width + src_x1;
	for (dst_y = dst_y1; dst_y <= dst_y2; dst_y++) {
		for (int32_t i = 0; i < clipped_width; i++) {
			uint32_t src_color = *src_pixel;
			uint32_t dst_color = *dst_pixel;
			*dst_pixel = src_color != color_key ? R96_ARGB(
														  R96_A(src_color),
														  ((R96_R(src_color) * tint_r) >> 8) & 0xff,
														  ((R96_G(src_color) * tint_g) >> 8) & 0xff,
														  ((R96_B(src_color) * tint_b) >> 8) & 0xff)
												: dst_color;
			src_pixel++;
			dst_pixel++;
		}
		dst_pixel += dst_next_row;
		src_pixel += src_next_row;
	}
}

Since we've already extensively benchmarked and optimized the original blitter functions, and since these new functions only change some setup code, we have no need to do another optimization pass. Whew.
Alright, let's put everything we learned into a little demo.
Demo: Blitting regions
In this demo, we are going to blit the glyphs for the string ""Hello world!"" sourced from the glyph atlas in assets/ibmvga.png, which I generated via Mario's BMFG. We'll apply what we learned and created above, from iterating UTF-8 encoded characters, calculating pixel coordinates for glyphs from code points, to blitting regions in various ways.
Here's 13_blit_region.c:

#include <MiniFB.h>
#include <stdlib.h>
#include <string.h>
#include ""r96/r96.h""

int main(void) {
	const int window_width = 320, window_height = 240;
	struct mfb_window *window = mfb_open(""13_blit_region"", window_width, window_height);
	r96_image output;
	r96_image_init(&output, window_width, window_height);

	r96_image glyph_atlas;
	int32_t glyph_width = 8;
	int32_t glyph_height = 16;
	int32_t glyphs_per_row = 16;
	r96_image_init_from_file(&glyph_atlas, ""assets/ibmvga.png"");

	do {
		r96_clear_with_color(&output, R96_ARGB(0xff, 0x22, 0x22, 0x22));

		const char *text = ""Hello world!"";
		uint32_t text_length = strlen(text);
		uint32_t char_index = 0;
		uint32_t x_offset = 100;
		while (char_index < text_length) {
			uint32_t code_point = r96_next_utf8_code_point(text, &char_index, text_length);
			int32_t glyph_x = (code_point - 32) % glyphs_per_row;
			int32_t glyph_y = (code_point - 32 - glyph_x) / glyphs_per_row;
			int32_t glyph_pixel_x = glyph_x * glyph_width;
			int32_t glyph_pixel_y = glyph_y * glyph_height;

			r96_blit_region(&output, &glyph_atlas, x_offset, 50, glyph_pixel_x, glyph_pixel_y, glyph_width, glyph_height);
			r96_blit_region_keyed(&output, &glyph_atlas, x_offset, 100, glyph_pixel_x, glyph_pixel_y, glyph_width, glyph_height, 0x0);
			r96_blit_region_keyed_tinted(&output, &glyph_atlas, x_offset, 150, glyph_pixel_x, glyph_pixel_y, glyph_width, glyph_height, 0x0, 0xffff00ff);
			x_offset += glyph_width;
		}

		if (mfb_update_ex(window, output.pixels, window_width, window_height) != STATE_OK) break;
	} while (mfb_wait_sync(window));
	return 0;
}

As usual, we start out by creating a window and an output r96_image to which we draw, which gets later drawn to the window.
Next, we define the properties of our glyph atlas and the glyphs contained there-in, and load the glyph atlas image.
In the main loop, we clear the output image, then iterate through the characters in the text string via r96_next_utf8_code_point(). We then calculate the glyph pixel coordinates for the code point in the glyph atlas and use that information to blit the glyph to the screen three times, using the normal blit, keyed blit, and keyed and tinted blit functions.
Take special note of x_offset. It specifies at what x-coordinate the next glyph will be blitted in the output image. As our font is monospaced, we can easily advance the drawing position on the x-axis by glyph_width. All glyphs have the same width. Variable width fonts are quite a bit more complex to get right in that regard.
And here is the web version.





Let's pack all of this up into re-useable code.
r96_font
Looking at the last demo, we can almost see a struct for fonts plop out:

r96_image glyph_atlas;
int32_t glyph_width = 8;
int32_t glyph_height = 16;
int32_t glyphs_per_row = 16;

This is the minimum information we need to store for a font to draw text with it, which translates to the following struct:

typedef struct r96_font {
	r96_image glyph_atlas;
	int32_t glyph_width, glyph_height;
	int32_t glyphs_per_row;
	int32_t tab_size;
} r96_font;

We load the glyph_atlas from an image file. glyph_width and glyph_height are parameters we'll need to specify when initializing the r96_image font. glyphs_per_row we can actually automatically deduce from the glyph atlas width and the glyph width, reducing the amount of parameters we need to specify when initializing a font. tab_size will make sense in a minute! Here's r96_font_init():

bool r96_font_init(r96_font *font, const char *path, int32_t glyph_width, int32_t glyph_height) {
	if (!r96_image_init_from_file(&font->glyph_atlas, path)) return false;
	font->glyph_width = glyph_width;
	font->glyph_height = glyph_height;
	font->glyphs_per_row = font->glyph_atlas.width / glyph_width;
	font->tab_size = 3;
	return true;
}

Unremarkable. And the corresponding r96_font_dispose():

void r96_font_dispose(r96_font *font) {
	r96_image_dispose(&font->glyph_atlas);
}

The rendering logic from the last example can be directly translated to a re-usable function. But we'll add two more features. We'll interpret \n and \t and adjust the rendering position for the next glyph accordingly.

void r96_text(r96_image *image, r96_font *font, const char *text, int32_t x, int32_t y, uint32_t tint) {
	int32_t cursor_x = x;
	int32_t cursor_y = y;
	uint32_t text_length = strlen(text);
	uint32_t index = 0;
	while (index < text_length) {
		uint32_t c = r96_next_utf8_code_point(text, &index, text_length);
		if (c == '\t') {
			cursor_x += font->tab_size * font->glyph_width;
			continue;
		}
		if (c == '\n') {
			cursor_x = x;
			cursor_y += font->glyph_height;			
			continue;
		}
		if (c < 32 || c > 255) {
			cursor_x += font->glyph_width;
			continue;
		}

		int32_t glyph_index = c - 32;
		int32_t glyph_x = (glyph_index % font->glyphs_per_row);
		int32_t glyph_y = (glyph_index - glyph_x) / font->glyphs_per_row;
		glyph_x *= font->glyph_width;
		glyph_y *= font->glyph_height;

		r96_blit_region_keyed_tinted(image, &font->glyph_atlas, cursor_x, cursor_y, glyph_x, glyph_y, font->glyph_width, font->glyph_height, 0x0, tint);

		cursor_x += font->glyph_width;
	}
}

The function takes the image we want to render the text to, the font to render with, the text as a null-terminated UTF-8 string, and the x and y position to start rendering the first glyph at in the image. It's final parameter is the tint color.
Inside the function, we keep track of the position to render the next glyph at in cursor_x and cursor_y. We also keep track of the text length in bytes and the byte index from which we'll read the next Unicode code point from the text.
The loop then iterates over all code points in the text via r96_next_utf8_code_point(). In case we encounter \t, we advance the cursor position by font->tab_size * font->glyph_width and continue on to the next glyph. In case of \n, we reset cursor_x to the original x, essentially moving the cursor to the beginning of the text line. We then increase cursor_y by the glyph height to move it to the next line below. Yay, multi-line rendering!
Before we actually render the glyph for the current code point, we also check that the code point is within 32-255, so we don't try to draw a glyph that's not inside the glyph atlas.
The remainder of the function maps the code point to the glyph in the glyph atlas and uses r96_blit_region_keyed_tinted() to draw the glyph to the current cursor position. Finally, we advance the cursor by the glyph width.
Not counting the region blitting functions, the entire text rendering code code is about 70 LOC now. Let's add a few more lines of code.
In the previous demo, we positioned the glyphs at hard coded coordinates. If we wanted to center the text on the screen, or apply other alignments, we need to know the width and height of the text, also known as its bounds.
Let's write a little function that calculates exactly that.

void r96_font_get_text_bounds(r96_font *font, const char *text, int32_t *width, int32_t *height) {
	*width = 0;
	*height = font->glyph_height;
	int32_t current_line_width = 0;
	uint32_t text_length = strlen(text);
	uint32_t index = 0;
	while (index < text_length) {
		uint32_t c = r96_next_utf8_code_point(text, &index, text_length);
		if (c == '\t') {
			current_line_width += font->tab_size * font->glyph_width;
			continue;
		}
		if (c == '\n') {
			*width = current_line_width > *width ? current_line_width : *width;
			*height += font->glyph_height;
			current_line_width = 0;
			continue;
		}
		current_line_width += font->glyph_width;
	}
	*width = current_line_width > *width ? current_line_width : *width;
}

The function takes the font that the text will be rendered with, as well as pointers width and height to which we write the calculated bounds.
The function then mirrors parts of the rendering logic in r96_text(), calculating the maximum line width, as well as how many lines there actually are.
Alright, let's use all this in a little demo.
Demo: using r96_font and friends
Here's 14_fonts.c, our cute font demo:

#include <MiniFB.h>
#include <stdlib.h>
#include ""r96/r96.h""

int main(void) {
	const int window_width = 320, window_height = 240;
	struct mfb_window *window = mfb_open(""14_fonts"", window_width, window_height);
	r96_image output;
	r96_image_init(&output, window_width, window_height);
	r96_font font;
	r96_font_init(&font, ""assets/ibmvga.png"", 8, 16);

	do {
		r96_clear_with_color(&output, R96_ARGB(0xff, 0x22, 0x22, 0x22));

		const char *text = ""The quick brown fox jumps\nover the lazy dog\n""
						   ""¡¢£¤¥¦§¨©ª«¬\n""
						   ""ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏ"";

		int32_t text_x, text_y, text_width, text_height;
		r96_font_get_text_bounds(&font, text, &text_width, &text_height);
		text_x = window_width / 2 - text_width / 2;
		text_y = window_height / 2 - text_height / 2;

		r96_rect(&output, text_x, text_y, text_width, text_height, R96_ARGB(0xff, 0xff, 0x0, 0xff));
		r96_text(&output, &font, text, text_x + 1, text_y + 1, 0x00000000);
		r96_text(&output, &font, text, text_x, text_y, 0xffffffff);

		if (mfb_update_ex(window, output.pixels, window_width, window_height) != STATE_OK) break;
	} while (mfb_wait_sync(window));
	return 0;
}

We start off by loading the font in line 11, specifying the glyph atlas image path, the glyph width, and the glyph height. r96_font_init loads the glyph atlas image and sets up all the fields of the font as we saw earlier.
In the main loop, we clear the output image, then define the text we want to render. The text consists of 3 lines, using characters from the code point range we support.
The next block of code calculates the bounds of the text via r96_font_get_text_bounds(), which we use to calculate the text's top-left corner position in such a way, that the text is centered in the middle of the screen.
In the final block, we render a background rectangle using the text bounds, followed by rendering the text offset by 1 pixel on both axes with a black tint. Finally, we render the text at the calculated position with a white tint. Rendering the text twice this way gives us a simple shadow effect. Here's the demo running on the web.





Great success.
Demo: fun with fonts
While the original IBM VGA font is nice, it's also a bit of an outdated, and dare I say boring look.
I've added two more glyph atlases to the assets/ folder. The first one is derived from the awesome Tamzen font (assets/tamzen.png).




It has a lighter, more modern appearance and is well suited to display stats, like performance counters.
The other font was ripped from some old demo from the 90ies by Ian Hanschen. He's put up a GitHub repo with a gargantuan amount of ripped fonts. Most of them do not have attribution. This is the one I picked (assets/demofont.png).




Each font is basically just a glyph atlas. However, the atlas layout doesn't match the one generated by BMFG.
For the font I picked, we see that it only contains glyphs for the first few code points. Instead of 16 glyphs, it contains 20 glyphs per row. Thankfully, r96_init_font() can deal with this by calculating the number of glyphs per row based on the glyph atlas width and glyph width. The only thing we need to watch out for is to not use any code points that go above Z in our text strings.
This demo doesn't come with an explanation. Consider it to be a puzzle for your brain noggins! Can you figure out how it works? 15_font_fun.c:

#include <MiniFB.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>
#include ""r96/r96.h""

int main(void) {
	const int window_width = 320, window_height = 240;
	struct mfb_window *window = mfb_open(""15_font_fun"", window_width, window_height);
	r96_image output;
	r96_image_init(&output, window_width, window_height);
	r96_font font;
	r96_font_init(&font, ""assets/demofont.png"", 16, 16);
	float counter = 0;
	struct mfb_timer *timer = mfb_timer_create();
	do {
		r96_clear_with_color(&output, R96_ARGB(0xff, 0x22, 0x22, 0x22));

		const char *text = ""--(2022 DEMO CREW)--"";
		int32_t text_x = 0;
		uint32_t text_length = strlen(text);
		uint32_t char_index = 0;
		while (char_index < text_length) {
			char character[] = {0, 0};
			character[0] = (char) r96_next_utf8_code_point(text, &char_index, text_length);
			int32_t text_y = output.height / 2 - font.glyph_width / 2 + (int32_t) (sinf(counter + char_index / 10.0f) * output.height / 4);
			r96_text(&output, &font, character, text_x, text_y, 0xffffffff);
			text_x += font.glyph_width;
		}

		counter += M_1_PI * mfb_timer_delta(timer) * 12;
		mfb_timer_reset(timer);

		if (mfb_update_ex(window, output.pixels, window_width, window_height) != STATE_OK) break;
	} while (mfb_wait_sync(window));
	return 0;
}

And here it is in action.





Next time on ""Mario writes a lot of words""
Our little code base is shaping up to be kinda useful. Next time, we're going to look into drawing lines. Possibly with sub-pixel precision. Unless I can't figure that out.
Discuss this post on Twitter or Mastodon.




"
https://news.ycombinator.com/rss,The IAB loves tracking users. But it hates users tracking them,https://shkspr.mobi/blog/2023/01/the-iab-loves-tracking-users-but-it-hates-users-tracking-them/,Comments,"The IAB loves tracking users. But it hates users tracking them.By 
@edent

 on 
2023-01-16
advertising email privacy12 comments550 words
The Interactive Advertising Bureau (IAB) is a standards development group for the advertising industry. Their members love tracking users. They want to know where you are, who you're with, what you're buying, and what you think. All so they can convince you to spend slightly more on toothpaste.  Or change your political opinions. Either way, they are your adversaries.The IAB's tech lab is working on a system called UID2. It's a more advanced way to track you no matter what you do and no matter what steps you take to avoid it.
UID2 is a framework that enables deterministic identity for advertising opportunities on the open internet for many participants across the advertising ecosystem. The UID2 framework enables logged-in experiences from publisher websites, mobile apps, and Connected TV (CTV) apps to monetize through programmatic workflows.Basically, they tie your email address to everything you do. Signed in to watch a TV show? Better sell that info to the advertisers so when you sign in to a different site they can send you targetted messages. Yuck.One of the ways privacy conscious users normally avoid this is by subtly altering their email addresses for each service they use.  For example, GMail ignores any dots in your username. So if you are Han.Solo@gmail.com you can also use H.ansolo@gmail.com or ha.ns.ol.o@gmail.com.  A user might sign up to a service and use a specifically ""dotted"" email address.  If they later start receiving spam to that address, they know the service has leaked or sold their info.You can go one step further and use plus addressing.  For example han.solo+amazon@gmail.com and han.solo+github@gmail.com. They both will appear in your normal inbox, but are unique for every service you use. Again, this is great for making sure that someone hasn't sold your email address to spammers.The IAB hates this.As part of the UID2 API they specifically describe how an advertiser must ""normalise"" their users' email addresses.This means h.a.n.solo+iab@gmail.com becomes plain old hansolo@gmail.comI think this is pretty shitty behaviour. If someone has deliberately set their email address in this form it is because the user does not want their identities to be commingled.Last year, I asked them to respect users' privacy and reverse this change.  They finally responded:
Thank you for your input, we thought long about this update and ultimately as it stands today it is not a change we would like to add.So, there you have it. If you want to take even the smallest step to preserve your privacy - tough.
If you want to track which IAB members are using your data - tough.
If you want to track users even if they don't want to be tracked - the IAB is happy to help.If you want to opt out of this - and you trust the IAB to handle your data safely - you can submit your email address and phone number to https://transparentadvertising.org/.Personally, I recommend installing the uBlock advert blocker on all devices which support it.Share the love:MastodonTwitterFacebookLinkedInRedditHackerNewsLobstersEmailPocketWhatsAppTelegramMore posts from around the site:
12 thoughts on “The IAB loves tracking users. But it hates users tracking them.”

2023-01-16 12:38

 Ian Betteridge says:@Edent I’ve noticed several brands now blocking services like iCloud’s relay, which lets you sign up with a random email address that’s not related to yours. Firefox relays ducks around that by letting you use your own domain, which makes it much harder for them to block sign-ups, but that’s obviously only applicable to a few users.Reply

2023-01-16 12:55

 Gabor says:I've been loving fastmail's masked email functionality, which gives you a random email alias like ""salty.hotdog8233@fastmail.com"", plus it has 1password integration, so signing up to places is fairly straightforward if you use 1p.Reply

2023-01-16 13:00

 That Privacy Guy says:I just read this and the solution I use is my own instance of AnonAddy - and I create a new and unique email address for every site/service I use. If you don't want to run it yourself there is a SaaS version - plus it is FOSS.


Reply

2023-01-16 13:40

 HackerNewsTop10 says:The IAB loves tracking users. But it hates users tracking them
Link: shkspr.mobi/blog/2023/01/t…
Comments: news.ycombinator.com/item?id=344000…Reply

2023-01-16 13:42

 Kazaii says:@Edent wow, that's rather unsettling. Thanks for shedding light on this.Reply

2023-01-16 13:49

 That Privacy Guy says:I have been using my own installation of AnonAddy for a couple of years now. I used to just have a catchall in my mail server which would forward anything which was sent to a non existing email address to a delegated account



Reply

2023-01-16 13:55

 Fazal Majid says:The plus convention is not specific to GMail (Sendmail, MS Exchange, Postfix and other email software have it), but they only require stripping it for @gmail.com domains. I have my own dedicated domain for vendors so I won't be impacted, and Apple's email masking feature will do the same, along with competing offerings from DuckDuckGo et al.Hashing PII like an email is also PII and this proposal is a blatant violation of GDPR, of course.Reply

2023-01-16 13:55

 Nikki says:Personally my opinion of anyone involved in advertising is so poor that I'd probably not be allowed to express it here. I can easily imagine a world without advertising as the web allows you to find anything you want without having someone trying to force it down your throat. Also the idea that many parts of the web could not exist without advertising support is facile. It's a bit like saying that free and open parks cannot exist without employing pick pockets to gather funds to pay for maintenance. If there are any parts of the web that really can not exist without advertising, they must be so bankrupt of alternatives ideas that their services could not be trusted to be useful.Reply

2023-01-16 15:09

 Anonymous says:A link says uBlock but points to uBlock Origin. uBlock is different from uBlock Origin: https://github.com/gorhill/uBlock/wiki/uBlock-Origin-is-completely-unrelated-to-the-web-site-ublock.orgReply

2023-01-16 15:21

 Oli says:I’m a big fan of Fastmail’s masked addresses for this reason.Word dot word four digit number at my own domain, goes in the password manager, never thought about again!Reply

2023-01-16 15:44

 Privacy Matters says:Hi @IABTechLab  What is the legal basis relied on  to alter the email identities of individuals who will be targeted by those using UID2?Oh, & I note domain reg details for transparentadvertising.org are redacted for privacy reasons. Who owns the domain pls?

Reply

2023-01-16 15:56

 trinity says:I own my name dot [tld] so I can do slingshit@me.com. Looks like I'm still gonna be doing alright. Cloudflare's mail forwarding works well for this, before that I used ImprovMX. Both just point the proper DNS records from your site to someone's mail server for quick relay+disposal. I imagine having all mail filter through a magic box is technically A Bit Troublesome but it's still better than Google Mail!ReplyLeave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment *Name * Email * Website  Notify me of follow-up comments by email. Notify me of new posts by email. 
Δ
To respond on your own website, enter the URL of your response which should contain a link to this post's permalink URL. Your response will then appear (possibly after moderation) on this page. Want to update or remove your response? Update or delete your post and re-enter your post's URL again. (Learn More)



"
https://news.ycombinator.com/rss,Using paleogenomics to elucidate 10k years of immune system evolution,https://www.pasteur.fr/en/press-area/press-documents/using-paleogenomics-elucidate-10000-years-immune-system-evolution,Comments,"



You are hereHomePress areaPress documentsUsing paleogenomics to elucidate 10,000 years of immune system evolution








Using paleogenomics to elucidate 10,000 years of immune system evolution 



© Adobe Stock








  
      Press release  




2023.01.13




Print
|

Share
   





Scientists from the Institut Pasteur, Université Paris Cité, the CNRS and the Collège de France have used paleogenomics to trace 10,000 years of human immune system evolution. They analyzed the genomes of more than 2,800 individuals who lived in Europe over the past ten millennia. They were able to date the increase in frequency of most of the mutations that are advantageous in defending against pathogens to after the Bronze Age, 4,500 years ago. The scientists also observed that mutations conferring a higher risk of developing inflammatory disorders have become more frequent over the past 10,000 years. These enlightening results on the effects of natural selection on immunity genes were published in the journal Cell Genomics on January 13, 2023. 
In the 1950s, the geneticist J.B.S. Haldane attributed the maintenance or persistence of the mutation responsible for anomalies in red blood cells commonly observed in Africa to the protection these anomalies provided against malaria, an endemic infection that claims millions of lives. This theory suggested that pathogens are among the strongest selective pressures faced by humans. Several population genetics studies subsequently confirmed the theory. But major questions remained, especially regarding the specific epochs during which the selective pressures exerted by pathogens on human populations were strongest and their impact on the present-day risk of developing inflammatory or autoimmune disorders.
To address these questions, scientists from the Institut Pasteur, Université Paris Cité, the CNRS and the Collège de France, in collaboration with the Imagine Institute and The Rockefeller University (United States), adopted an approach based on paleogenomics. This discipline, which studies the DNA from fossil remains, has led to major discoveries about the history and evolution of humans and human diseases, as illustrated by the decision to award the 2022 Nobel Prize in Physiology or Medicine to the paleogeneticist Svante Pääbo. In the study led by the Institut Pasteur, published on January 13 in the journal Cell Genomics, the scientists analyzed the variability of the genomes of more than 2,800 individuals who lived in Europe over the past ten millennia – a period covering the Neolithic, the Bronze Age, the Iron Age, the Middle Ages and the present.
By reconstituting the evolution over time of hundreds of thousands of genetic mutations, the scientists initially identified mutations that rapidly increased in frequency in Europe, indicating that they were advantageous. These mutations that evolved under ""positive"" natural selection are mainly located in 89 genes enriched in functions relating to the innate immune response, including especially the OAS genes – which are responsible for antiviral activity – and the gene responsible for the ABO blood group system. Surprisingly, most of these positive selection events, which demonstrate a genetic adaptation to the pathogenic environment, began recently, from the start of the Bronze Age, around 4,500 years ago. The scientists explain this ""acceleration"" in adaptation by the growth in the human population during this period and/or by strong selective pressures exerted by pathogens in the Bronze Age, probably linked to the spread of severe infectious diseases such as plague.
At the same time, the scientists also looked at the opposite situation, in other words, mutations whose frequency fell significantly over the past ten millennia. These mutations are probably subject to ""negative"" selection because they increase the risk of disease. They noted that once again, these selection events mainly began in the Bronze Age. Many of these disadvantageous mutations were also located in genes associated with the innate immune response, such as TYK2, LPB, TLR3 and IL23R, and have been confirmed in experimental research to have a deleterious effect in terms of infectious disease risk. The results emphasize the value of adopting an evolutionary approach in research on genetic susceptibility to infectious diseases.
Finally, the scientists explored the theory that the selection exerted by pathogens in the past gave an advantage to alleles conferring resistance to infectious diseases, but that in turn these alleles have increased the present-day risk of autoimmune or inflammatory disorders. They investigated the few thousand mutations known to increase susceptibility firstly to tuberculosis, hepatitis, HIV or COVID-19, and secondly to rheumatoid arthritis, systemic lupus erythematosus or inflammatory bowel disease. By looking at the evolution of these mutations over time, they observed that those associated with an increased risk of inflammatory disorders – including Crohn's disease – became more frequent over the past 10,000 years, while the frequency of those associated with a risk of developing infectious diseases decreased. ""These results suggest that the risk of inflammatory disorders has increased in Europeans since the Neolithic period because of a positive selection of mutations improving resistance to infectious diseases,"" explains Lluis Quintana-Murci, director of the study and Head of the Human Evolutionary Genetics Unit (Institut Pasteur/CNRS Evolutionary Genomics, Modeling and Health Unit/Université Paris Cité).
The results of the study, which harnessed the huge potential of paleogenomics, show that natural selection has targeted human immunity genes over the past ten millennia in Europe, especially since the start of the Bronze Age, and contributed to present-day disparities in terms of the risk of infectious and inflammatory diseases.
As well as the institutions mentioned above, this research was supported by the French Foundation for Medical Research (FRM), the Allianz-Institut de France Foundation and the Fondation de France. 





Explanatory diagram. © Gaspard Kerner, Institut Pasteur
Source
Genetic adaptation to pathogens and increased risk of inflammatory disorders in post-Neolithic Europe, Cell Genomics, January 13, 2023
Gaspard Kerner,1* Anna-Lena Neehus,2,3 Quentin Philippot,2,3 Jonathan Bohlen,2,3 Darawan Rinchai,4 Nacim Kerrouche,4 Anne Puel,2,3 Shen-Ying Zhang,2,3,4 Stéphanie Boisson-Dupuis,2,3,4 Laurent Abel,2,3,4 Jean-Laurent Casanova,2,3,4,5,6 Etienne Patin,1,8 Guillaume Laval,1,8 and Lluis Quintana-Murci,1,7,8,9,*
1Institut Pasteur, Université Paris Cité, CNRS UMR2000, Human Evolutionary Genetics Unit, F-75015 Paris, France
2Laboratory of Human Genetics of Infectious Diseases, INSERM UMR 1163, Necker Hospital for Sick Children, 75015 Paris, France.
3University Paris Cité, Imagine Institute, 75015 Paris, France.
4St. Giles Laboratory of Human Genetics of Infectious Diseases, The Rockefeller University, New York, NY 10065, United States.5Howard Hughes Medical Institute, New York, NY 10065, United States
6Department of Pediatrics, Necker Hospital for Sick Children, 75015 Paris, France
7Collège de France, Chair of Human Genomics and Evolution, F-75005 Paris, France
8Senior author
9Lead contact
*Corresponding author
 









"
https://news.ycombinator.com/rss,Intro to GCC bootstrap in RISC-V,https://ekaitz.elenq.tech/bootstrapGcc0.html,Comments,"


Ekaitz's tech blog


















Home
Series



Feed
About

 





Ekaitz's tech blog:I make stuff at ElenQ Technology and I talk about it



Intro to GCC bootstrap in RISC-V



      Mon 14 February 2022
    

      By           Ekaitz Zárraga



You probably already know about how I spent more than a year having fun with
RISC-V and software bootstrapping from source.
As some may know from my FOSDEM talk, NLNet / NGI-Assure put the
funds to make me spend more time on this for this year and I decided
to work on GCC’s bootstrapping process for RISC-V.
Why GCC
GCC is probably the most used compiler collection, period.  With GCC we can
compile the world and have a proper distribution directly from source, but who
compiles the compiler?1
Well, someone has to.
The bootstrap
Bootstrapping a compiler with a long history like GCC for a new architecture
like RISC-V involves some complications, starting on the fact that the first
version of GCC that supports RISC-V needs a C++98 capable compiler in order to
build. C++98 is a really complex standard, so there’s no way we can bootstrap a
C++98 compiler at the moment for RISC-V. The easiest way we can think of at
this point is to use an older version of GCC for that, one of those that are
able to build C++98 programs but they only require a C compiler to build. Older
versions of GCC, of course, don’t have RISC-V support so… We need a
backport2.
So that’s what I’m doing right now. I’m taking an old version of GCC that only
depends on C89 and is able to compile C++98 code and I’m porting it to RISC-V
so we can build newer GCCs with it.
Only needing C to compile it’s a huge improvement because there are Tiny C
Compilers out there that can compile C to RISC-V, and those are written using
simple C that we can bootstrap with simpler tools of a more civilized world.
In summary:

C++98 is too complex, but C89 is fine.
GCC is the problem and also the solution.

What about GNU Mes?
When we3 started with this effort we wanted to prepare GNU Mes, a small C
compiler that is able to compile a Tiny C Compiler, to work with RISC-V so we
could start to work in this bootstrap process from the bottom.
Some random events, like someone else working on that part, made us rethink our
strategy so we decided to start from the top and try to combine both efforts at
the end. We share the same goal: full source bootstrap for RISC-V.
Tiny C Compilers?
There are many small C compilers out there that are written in simple C and are
able to compile an old GCC that is written in C. Our favorite is TinyCC (Tiny C Compiler).
GNU Mes is able to build a patched version of TinyCC, which already supports
RISC-V (RV64 only), and we can use that TinyCC to compile the GCC version I’m backporting.
We’d probably need to patch some things in both projects to make everything
work smoothly but that’s also included in the project plan.
Binutils
Binutils is also a problem mostly because GCC, as we will talk about in the
future, does not compile to binary directly. GCC generates assembly code and
coordinates calls to as and ld (the GNU Assembler and Linker) to generate
the final binaries. Thankfully, TinyCC can act as an assembler and a linker,
and there’s also the chance to compile a modern binutils version because it is
written in C.
In any case, the binary file generation and support must be taken in account,
because GCC is not the only actor in this film and RISC-V has some weird things
on the assembly and the binaries that have to be supported correctly.
Conclusion
This is a very interesting project, where I need to dig in BIG stuff, which
is cool, but also has a huge level of uncertainty, which scares the hell out of
me. I hope everything goes well…
In any case, I’ll share all I learn here in the blog and I keep you all posted
with the news we have.
That’s all for this time. If you have any question or comment or want to share
your thoughts and feelings with me5 you can find my
contact information here.


PS: Big up to NlNet / NGI-Assure for the money.










wHo wATcHes tHE wAtchMEN? ↩


Insert “Back to the Future” music here. ↩


“We” means I shared my thoughts and plans with other people who have a
  much better understanding of this than myself. ↩


But there are some others that are really interesting (see
cproc, for example) ↩


Or even hire me for some freelance IT stuff 🤓 ↩








          Supported by:
        











 



"
https://news.ycombinator.com/rss,"New Sony Walkman music players feature good looks, Android 12",https://arstechnica.com/gadgets/2023/01/new-sony-walkman-music-players-feature-stunning-good-looks-android-12/,Comments,"






      Where do you put the cassette tape?    —

New Sony Walkman music players feature stunning good looks, Android 12
Sony holds onto the beautiful dream of standalone portable audio players. 


Ron Amadeo
    -  Jan 13, 2023 7:53 pm UTC

 




reader comments
238
 with 0 posters participating


Share this story

Share on Facebook
Share on Twitter
Share on Reddit












                    The Sony Walkman NW-A300. It's a shame Sony never became a force in smartphones, because, wow, their product designs are still so good.                  


                                          Sony                                      









                    Yep, that's regular Android.                   


                                          Sony                                      









                    The bottom. From left to right we've got a headphone jack, lanyard hole, USB-C port, and a microSD slot.                   


                                          Sony                                      









                    Music buttons! So many music buttons!                  


                                          Sony                                      









                    The back has this nice scallop texture.                   


                                          Sony                                      









                    The frame is aluminum.                   


                                          Sony                                      









                    It comes in colors. That gray one really hits me in the nostalgic Sony sweet spot.                   


                                          Sony                                      









                    There are little folio cases! They are so cute.                  


                                          Sony                                      









                    The folio case lets it stand up!                  


                                          Sony                                      





Sony has a pair of new Android Walkmans out, the NW-A300 and NW-ZX700. Yes, that's right, Walkmans, Sony's legendary music player brand from the 1980s. Apple may have given up on the idea of a smartphone-adjacent music player when it killed the iPod Touch line recently, but Sony still makes Android-powered Walkmans and has for a while. The first was in 2012 with the Android 2.3 Gingerbread-powered NWZ-Z1000, which looked like Sony just stripped the modem out of an Xperia phone and shoved it onto the market as a music player. Since then, Sony has made designs with more purpose-built hardware, and today there are a whole series of Android-powered Walkman music players out there. Sadly these new ones seem to only be for sale in Japan, the UK, and Europe, for now.
We'll start with the most consumer-friendly of the two, the NW-A300. This basic design debuted in 2019 with the NW-A105, but that shipped with Android 9. This is an upgraded version of that device with a less-ancient version of Android, a new SoC, and a scalloped back design. In Sony's home of Japan, the 32GB version is 46,000 yen (about $360), while in Europe, it's 399 euro (about $430).
The NW-A300 is a tiny little device that measures 56.6×98.5×12 mm, so pretty close to a deck of playing cards. And really, just look at these pictures. Sony might not be the consumer electronics juggernaut it used to be, but it still has an incredible product design department. I have no use for a standalone music player, but both of these Walkmans are so pretty that I just want to hold one. 
Advertisement 


The front is dominated by a 3.6-inch, 60 Hz, 1280×720 touchscreen LCD. There's 32GB of storage, and the device supports Wi-Fi 802.11AC and Bluetooth 5. That's about all Sony wants to talk about for official specs. It touts ""longer battery life"" but won't say how big the battery is, promising only ""36 hours* of 44.1 KHz FLAC playback, up to 32 hours* of 96 KHz FLAC High-Resolution Audio playback."" Presumably, that's all with the screen off.
For more specs, we can visit The Walkman Blog, a wonderful site that is very serious about these little music players. In October, the site found documentation for the A300 listing a 1500 mAh battery. The system-on-a-chip in the older NW-A100 model was the NXP i.MX8M-Mini, a wildly slow 28 nm SoC that has just four Arm Cortex-A53 CPUs and 4GB of RAM. You can say, ""This is just a music player,"" but that's not really true since it still runs full Android with an app store and everything. Geekbench scores show this has a new quad-core Qualcomm chip of some kind with 4GB of RAM, but we can't be sure of the model number. A newer chip with smaller transistors would probably account for a lot of that ""better battery life"" promise.
This is a music player, so of course, there's a headphone jack on the bottom of the unit. You'll also find a spot for a lanyard, a speedy USB-C 3.2 Gen1 port for quick music transfers, and a MicroSD slot for storing all your music. Buttons along the side of the device also give you every music control you could want, like a hold switch, previous, play/pause, next, volume controls, and power.

Page: 1 2 Next →













reader comments
238
 with 0 posters participating


Share this story

Share on Facebook
Share on Twitter
Share on Reddit







Ron Amadeo
      Ron is the Reviews Editor at Ars Technica, where he specializes in Android OS and Google products. He is always on the hunt for a new gadget and loves to rip things apart to see how they work.    

Email ron@arstechnica.com
//
Twitter @RonAmadeo








Advertisement 





















Channel Ars Technica




← Previous story Next story →




Related Stories









Today on Ars







"
https://news.ycombinator.com/rss,Interactive Music Theory Cheat Sheet,https://muted.io/cheat-sheet/,Comments,"







                Music Theory Cheat Sheet: Keys, Scales, Chords, Notes & Intervals
              


💖 share it →






























  ✨🎵✨ An interactive music theory cheat sheet to get all you need at a
  glance:
  keys, scales, modes, notes, chords and
    intervals. 
  Just select a major or minor key and you'll get the notes of
  the scale, scale formula, the relative major or minor, modal scales for that
  key, scale degrees/intervals, the key signature, the diatonic chords, the
  diatonic 7th chords, the chord functions and the relationship with other keys
  on the Circle of Fifths (aka circle of
  fourth, when going counter-clockwise). 
  → For more reference on chords and scales, you may like this
  scale formula chart,
  chord formula chart and this
  list of chords.














      C
    

      C♯


      D♭


      D
    

      D♯


      E♭


      E
    

      F
    

      F♯


      G♭


      G
    

      G♯


      A♭


      A
    

      A♯


      B♭


      B
    



      Major
    

      Minor
    



key:


C
C♯
D♭
D
D♯
E♭
E
F
F♯
G♭
G
G♯
A♭
A
A♯
B♭
B




      major
    

      minor
    









C♯D♭


D


D♯E♭


E


F


F♯G♭


G


G♯A♭


A


A♯B♭


B





C♯D♭


D


D♯E♭


E


F


F♯G♭


G


G♯A♭


A


A♯B♭


B








C
Major




C
Major
      Scale
    







formula:
1 2 3 4 5 6 7
1 2 ♭3 4 5 ♭6 ♭7
steps:
whole whole half
        whole whole whole half

whole half whole whole half
        whole whole




        relative minor →
        A Minor












C
          Harmonic Minor Scale
        







formula:
1 2 ♭3 4 5 ♭6 7




C
          Melodic Minor Scale
        







formula:
1 2 ♭3 4 5 6 7





        relative major →
        A Major










Modes of the Major Scale






C
          Major Pentatonic Scale
        







formula:
1 2 3   5 6  




C
          Major Blues Scale
        







formula:
1 2 ♭3 3 5   6  






C
          Minor Pentatonic Scale
        







formula:
1   ♭3 4 5   ♭7




C
          Minor Blues Scale
        







formula:
1   ♭3 4 ♭5 5   ♭7





C
Major
      Scale Degrees & Intervals
    


        1- Tonic:
        
          C
        
        → Unison
      

        2- Supertonic:
        
          D
        
        → Major 2nd
      

        3- Mediant:
        
          E
        
        →
        Major 3rd
      

        4- Subdominant:
        
          F
        
        → Perfect 4th
      

        5- Dominant:
        
          G
        
        → Perfect 5th
      

        6- Submediant:
        
          A
        
        →
        Major 6th
      

        7-
        Leading Tone:
        
          B
        
        →
        Major 7th
      

        8/1- Octave/Tonic:
        
          C
        
        → Perfect 8th
      





C
Major
        Key Signature & Notation
      



Theoretical Scale

        The
        

 scale
        
        is a theoretical scale that contains double accidentals. For this
        reason, that scale is not used often. The enharmonic equivalent scale is
        used instead most of the time.
      



C
Major
      Diatonic Chords
    




I
ii
iii
IV
V
vi
vii°





C

C E G






Dm

D F A






Em

E G B






F








G








Am








B°












C
      Major Diatonic 7th Chords
    




IM7
iim7
iiim7
IVM7
V7
vim7
viiø7





C

C E G B






Dm7

D F A C






Em

E G B D






F

F A C E






G








Am7








Bø7












      In functional harmony for a major key:
      

          the tonic chords are chords
          I, iii & vi
        

          the subdominant chords are chords
          IV & ii
        

          the dominant chords are chords
          V & vii°
        






i
ii°
III
iv
v
VI
VII





C

C Eb G






D°

D F Ab






Eb

Eb G Bb






Fm








Gm








A








B












C
      Natural Minor Diatonic 7th Chords
    




im7
iiø7
IIIM7
ivm7
vm7
VIM7
vii7





C

C Eb G Bb






Dm7

D F Ab C






Em

E G B D






F

F A C E






G








AM7








B7












      In functional harmony for a minor key:
      

          the tonic chords are chords
          i & III
        

          the subdominant chords are chords
          iv, VI & ii°
        

          the dominant chords are chords
          V, v, VII & vii°
        


        The harmonic minor scale has a
        raised 7th scale degree compared to the natural minor
        scale, which makes that 7th scale degree into a
        leading tone and makes the V chord a
        major chord. In functional harmony, that V chord from
        the harmonic minor scale is used most often because the leading tone
        gives a stronger sense of wanting to be resolved to the tonic.
      



C
Major
      on the Circle of Fifths
    













      Common
      C
      Chords
    

click to hear the different chords









Order of sharps
F C G D A E B


Order of flats
B E A D G C F


Chromatic Scale

C-D♭-D-E♭-E-F-G♭-G-A♭-A-B♭-B
alternative enharmonic spelling:
C-C♯-D-D♯-E-F-F♯-G-G♯-A-A♯-B



Solfège syllables

Do Re Mi Fa So(l) La Ti Do



Accidentals

      Double Sharp: 𝄪
      Sharp: ♯
      Natural: ♮
      Flat: ♭
      Double Flat: ♭♭



Chord Symbols

      Major: M, maj, △
      Minor: m, min, -
      Dominant 7th: 7, dom7
      Diminished: dim, °
      Half-diminished: m7b5, ø
      Augmented: aug, +





I hope this music theory cheat sheet is useful! 😎
  
  You can get in touch with me here if you think I
  should add something else to this page.



Piano Samples

    The piano samples used for this music theory cheat sheet are from “
    Salamander Grand Piano V3
    ” by Alexander Holm, licensed under
    CC BY 3.0.
  












That's me, Seb, I'm creating muted.io. 😄

      Consider supporting this site by
      making a small donation via ko-fi here. Your support means so much and goes directly towards allowing me to
      spend more time creating fun and useful things for muted.io. 🙏
    
- Seb, ✌️ + ❤️




      Stay in the Loop
    



😎 Subscribe to be updated when I add something cool to the
            site:













"
https://news.ycombinator.com/rss,"Selfie – A tiny RISC-V C compiler, emulator and hypervisor",http://selfie.cs.uni-salzburg.at/,Comments,"






selfie | An educational software system of a tiny self-compiling C compiler, a tiny self-executing RISC-V emulator, and a tiny self-hosting RISC-V hypervisor.

























selfie
An educational software system of a tiny self-compiling C compiler, a tiny self-executing RISC-V emulator, and a tiny self-hosting RISC-V hypervisor.
View the Project on GitHub cksystemsteaching/selfie

Download ZIP File
Download TAR Ball
View On GitHub



Selfie is a project of the Computational Systems Group at the Department of Computer Sciences of the University of Salzburg in Austria.
The Selfie Project provides an educational platform for teaching undergraduate and graduate students the design and implementation of programming languages and runtime systems. The focus is on the construction of compilers, libraries, operating systems, and even virtual machine monitors. The common theme is to identify and resolve self-reference in systems code which is seen as the key challenge when teaching systems engineering, hence the name.
README for an overview of the system and all available resources.


This project is maintained by cksystemsteaching
Hosted on GitHub Pages — Theme by orderedlist





"
https://news.ycombinator.com/rss,Wasavi – Vi editor for any webpage,http://appsweets.net/wasavi/,Comments,"








wasavi - appsweets akahuku labs.











wasavi (VI editor for any web page)

Tweet 


#

wasavi is an extension for Chrome, Opera and Firefox. wasavi transforms TEXTAREA element of any page into a VI editor, so you can edit the text in VI.  wasavi supports almost all VI commands and some ex commands.
wasavi is under development. Any bug report or feature request is welcome.
And we also welcome a donation to continue development:


日本語版のREADME




A Quick Walkthrough

Here is a native TEXTAREA.  Focus the TEXTAREA, and press Ctrl+Enter to launch wasavi


Salient Features

wasavi supports some ex commands. This is the output of :set all

Vim's incremental search

wasavi online app. Open this link on a browser that has wasavi extension. wasavi will launch automatically. Then you can read and write files at your Dropbox/Google Drive/OneDrive account or local files.






How to install
Currently, wasavi is available for following browsers only. Select your browser and click the link. Standard extension installation procedure of your browser will follow. These extensions are hosted at the addons store of their respective browser.

Google Chrome extension
Opera addon
Firefox addon

Source code and latest development releases are hosted at Github:

Latest and unstable version of wasavi for Chrome
Latest and unstable version of wasavi for Blink Opera
Latest and unstable version of wasavi for Firefox

A note for Chrome users
Chrome has reserved some fundamental shortcuts, such as Ctrl+T, Ctrl+W and Ctrl+N. Although these keys cannot be used in wasavi, you can use Alt+T, Alt+W and Alt+N.




Frequently Asked Questions
How to launch wasavi
Focus TEXTAREA and press Ctrl+Enter.
How to quit wasavi
To quit wasavi press ZZ or :q or :wq or any other VI quit command.
Which options are accepted by the :set command?
See this table.
Note: there are also options which are accepted but don't have any effect yet.
How to modify initial settings:
Open preference wasavi extension (or enter :options on wasavi),
and edit ""exrc"" textbox.
How to control beep
Add set noerrorbells to your exrc to disable beep sound.  If you prefer a visual bell, add set visualbell instead.
Also, a chime at wasavi startup can be disabled with set nolaunchbell.
The volume of any beeps can be controlled with set bellvolume=N.  Range of value N is 1 to 100.
How to access local files
See document.
How to use wasavi with Vimperator/Keysnail/VimFx on Firefox
Vimperator
Put wasavi_mediator.js in your Vimperator plugin directory, for example,  ~/.vimperator/plugin or %HOME%\vimperator\plugin.
This plugin will control the pass-through mode of Vimperator according to the state of wasavi.
Keysnail
Put wasavi_mediator.ks.js in your Keysnail plugin directory.
This plugin will control suspend mode of Keysnail according to the state of wasavi.
VimFx
Latest VimFx recognizes wasavi as editable element.  While wasavi is running, VimFx suspends temporarily.
To use VimFx's key binding while wasavi is running, click outside area of wasavi or enter :set esctoblur and press <esc> in normal mode.  Then keyboard focus would be removed from wasavi, and you can use VimFx's key binding.
How to use wasavi as an independent text editor
Install the wasavi extension and open the link to wasavi online app. wasavi will start automatically. You can use ex commands :read, :write, :edit or :file to access your Dropbox/Google Drive/OneDrive files or local files. You will have to authorize wasavi via OAuth to access these storages.
About automatic setting override
The :set commands which you input while wasavi is running are stored to extension's persistent storage, and those are regenerated when you launch wasavi next time.
This setting override mechanism works each independent URLs (max 30). If you think this is unnecessary, put :set nooverride in your exrc. Then overriding will be skipped.
How to cooperate with Migemo
wasavi for Chrome can Migemo search.  Install Migemo Server, then input a special meta character \M in search query of / or ? command.  If \M included in search query, these search commands are executed via migemo.
I have noticed a bug
Please create an issue on wasavi issue tracker
Tips and Tricks

to maximize the wasavi: :set fullscreen or :set fs
to restore the wasavi: :set nofullscreen or :set nofs
to change a color theme: :set theme=blight or :set theme=charcoal or :set theme=matrix or :set theme=solarized or :set theme=solarized_dark
abbreviate syntax is


:abbreviate displays all the abbreviations currently registered.
:abbreviate [clear] clears all the abbreviations.
:abbreviate lhs displays the abbreviation corresponding to lhs.
:abbreviate lhs rhs registers a abbreviation which expands lhs to rhs.
:abbreviate [noremap] lhs rhs also registers, but it is not effected remap mechanism.

map syntax is


:map displays all the mappings currently registered.
:map [clear] clears all the mappings.
:map lhs rhs registers a rule which translates lhs to rhs. Its translation is recursive. About syntax of key stroke descriptor like <esc> in the lhs and rhs, see this page.
:map [noremap] lhs rhs also registers, but it is non-recursive.
:map targets the normal mode mappings. On the other hand,
:map! targets the insert mode. This is equivalent to vim's :imap.
For more detailed information, see Syntax of map command.

j k ^ $ moves cursor by physical row, on the other hand,
gj gk g^ g$ moves by wrapped row. To swap the behavior: :set jkdenotative
f F t T extension for Japanese: these commands recognizes reading (ro-ma ji
expression) of hiragana, katakana, and kanji. For example, fk will place
a cursor on 'か', 'カ', '漢' and so on.
f F t T extension for Latin script: these commands recognizes the base alphabet
of diacritical marked letter. For example, fa will place a cursor on
'å', 'ä', 'à', 'â', 'ā' and so on. Also see mapping table.
use a online storage as file system:


:filesystem status shows all file systems currently available.
:filesystem default shows default file system. You can set default file system
via :filesystem default dropbox or :filesystem default gdrive or :filesystem default onedrive.
:filesystem reset discards the access token for online storage.
You can place the file system name at the head of a file name explicitly:
for instance, :read dropbox:/hello.txt.

When you read from the register of A to Z, some registers returns special content:


B register: user agent string
D register: current date time string (formatted by using datetime option as template of strftime(3))
T register: title string
U register: URL string
W register: version string of wasavi

To return a setting to default state:


:set <option-name>& or :set <option-name>&default

To return all settings to default state:


:set all& or :set all&default

To return a setting to the state just after evaluation of exrc:


:set <option-name>&exrc

To return all settings to the state just after evaluation of exrc:


:set all&exrc

To submit a form automatically after writing text and closing wasavi:


:wqs
:submit (this can be shortened to :sub )


Commands implemented

[count] operation [count] motion
[count] operation [count] range-symbol
[count] surround-operation [count] motion surround-string
[count] surround-operation [count] range-symbol surround-string
[count] de-surround-operation [count] surround-identifier
[count] re-surround-operation [count] surround-identifier surround-string
[count] operation-alias
[count] surround-operation-alias surround-string
[count] motion
[count] scroll-command
[count] edit-command
[count] : ex-command

Operations
c y d > < gq gu gU
Operation Aliases
cc yy dd >> << C Y D gqq guu gUU yss ySS
A counter can be inserted in front of the last 1 character.
Surround Operations

to surround: ys yS
to remove a surround: ds
to change a surround: cs

Motions
- + ^ <home> $ <end> % | , ;
_ / ? ' ` ( ) { } [[ ]] <enter> 0
j k h l ^N ^P ^H
<down> <up> <left> <right> <space>
w W b B e E gg gj gk g^ g$ G H M L f F t T n N
Range symbols (Vim text objects)

a"" a' a` a[ a] a{ a} aB a< a> a( a) ab aw aW ap as at
i"" i' i` i[ i] i{ i} iB i< i> i( i) ib iw iW ip is it

Scroll commands
^U ^D ^Y ^E ^B ^F <pageup> <pagedown> z<enter> z. zz z-
Edit commands
x X <delete> p P J . u ^R ~ ^L ^G ga gv m @ q r R a A i I o O & s S v V ZZ gi ^A ^X
ex commands
abbreviate cd chdir copy delete edit file filesystem global join k map mark marks move options print put pwd quit read redo s & ~ set sort submit registers to unabbreviate undo unmap version v write wq wqs xit yank > < @ *
The addressing in ex command is fully supported:

whole buffer: %s/re/rep/
current line: .p
the last line of buffer: $p
absolute line number: 1,2p
relative line number: +1,+2p
regal expression: /re/p ?re?p
mark referencing: 'a,'bp

In addition to this wasavi also accepts offset, for example: /re/+1p.
Two addresses are usually connected by a ,, wasavi also supports ;.
Input mode commands

^@ input the most recently input text, and exit input mode. this key stroke is actually Ctrl+Space.
^D unshift. but if the last input character is 0 or ^, delete all indentation
^H delete a character
^R paste register's content
^T shift
^U delete all the characters entered in the current input session
^V literal input
^W delete a word

Line input mode commands

^A move cursor to top of line
^B back
^E move cursor to end of line
^F forward
^H delete a character
^N next history
^P previous history
^R paste register's content
^U delete whole line
^V literal input
^W delete a word
tab complete ex command name, set option name, file name argument of read/edit/write/file

Bound mode commands
Bound mode is similar to vim's visual mode.

c delete the bound, and switch to insert mode
d delete the bound
y yank the bound
< unshift the bound
> shift the bound
C delete the line-wise bound, and switch to insert mode
S surround the bound
R same as C
D delete the line-wise bound
X same as D
Y yank the line-wise bound
g prefix commands
a, i prefix range symbols
~ swap lower case and upper case in the bound
: switch to line input mode
J join the bound
p delete the bound, and paste a register's content
P same as p
r fill the bound up with inputted letter
s same as c
u lower-ize the bound
U upper-ize the bound
v character wise bound mode
V line wise bound mode
x same as d
^A add the counter to all numeric strings within the bound
^X subtract the counter to all numeric strings within the bound

Surrounding identifiers

quotations: one of !#$%&*+,\-.:;=?@^_|~""'`
brackets: one of abBrt[]{}()

Surrounding string

quotations: one of !#$%&*+,\-.:;=?@^_|~""'`
brackets: one of abBr[]{}()
tags: one of ^T ,<Tt

Vim features in wasavi

multiple level undo/redo
incremental search
range symbols (aka, Vim text objects)
following registers


"" unnamed register
: last executed ex command
* reading from and writing to the system clipboard
/ last searched string
= evaluate math expression. supported operators are: + - * / %. supported numeric expressions are: integer, float (including exponential form), binary (with leading 0b), octal (with leading 0), hex (with leading 0x)

auto-reformat in input mode, and reformat operator (gq command) on the state of textwidth > 0
bound mode (aka, Vim visual mode)
options: iskeyword, incsearch, smartcase, undolevels, quoteescape, relativenumber, textwidth, expandtab, cursorline, cursorcolumn, nrformats
writing to the register of A to Z
gu / gU + motion: lowerize or upperize a region
partial functionality of Surround.vim
partial functionality of :sort (regex pattern, r and i options)
^A to increase a numeric string. ^X to decrease a numeric string.







"
https://news.ycombinator.com/rss,The peculiar event sourced deadlock,https://jappie.me/the-peculiar-event-sourced-deadlock.html,Comments,"The peculiar event sourced deadlock / JappieJappie


<

About 📂
About me

>




<

Hire 🐧
Jappie for hire

>




<

Raster 🚀
Easy rosters for restaurants startup

>




<

Email ✉
Contact me

>


  The peculiar event sourced deadlock  published: 15日 01月 2023年 One thing that always surprises me is how casually serious problems are phrased by business people in their blissful ignorance. “Hey why am I seeing the down for maintenance screen?” “Oh try it now, the pack uploading has finished”, Said the QA engineer to the product manager. Once I saw this on slack, I grew really suspicious and started asking questions. After all, isn’t it a bit odd we’re seeing a down for maintenance screen in one part of the system, simply because another part is being used?Initially we thought this was caused by high CPU usage. The graphs showed high CPU load while processing packs, so maybe the rest of the system was being deprioritized somehow. Before assuming that was the cause however, I decided to reproduce the issue first. Here I noticed I could for example load the risk index easily (a read operation), but connecting a risk to a pack (a write operation), would hang forever. This made me suspect that the issue wasn’t CPU usage at all, so I asked Postgres to list it’s locks. Which showed several locks in progress. This lead me to the event source system. The event source system is at the core of all our business logic. In essence, it provides a ledger of all important business write activities that can happen. This is useful for auditing purposes for example.Welcome to an after action report of a complicated system level bug. It took me a week to find a satisfying solution. To start I need to sketch context. I’ll only use raw SQL because this entire story is related to the database and how we use it for event sourcing. So consider the tables of an event source system:CREATE TABLE event (
    id serial PRIMARY KEY NOT NULL,
    payload jsonb NOT NULL,
    type character varying NOT NULL,
    created timestamp with time zone NOT NULL
);

CREATE TABLE event_last_applied (
    id serial PRIMARY KEY NOT NULL,
    event_id bigint NOT NULL REFERENCES event (id)
);
In here the type and payload fields contains the information to (re)apply that event. The type will indicate what business logic or queries to execute, and the payload holds information for that logic. As we’ll see later, these queries will involve modifying other normal tables within a transaction. This application of events, or re-application through business logic or queries is called projecting. A type can for example be create-user and the payload would contain the data required for creating said user, for example {email:'hi@jappie.me'}. The id provides a unique global ordering, and the created field contains a timestamp of when the event was created, which is used for database administration purposes. Finally, the event_last_applied table is used to indicate whichever event was last applied, so the system can figure out if additional events need to be re-projected from the event table.Inserting an event works by projecting an event to normal Postgres tables in a transaction. Once this operation is not rejected by foreign keys, type errors or program exceptions, the event gets recorded in the ledger, also known as the event table. For example:begin;

/* left out projection code, insert user into tables here,
or do other projection stuff, as dictated by the event type*/

INSERT INTO event (payload, type, created)
    VALUES ('{""email"":""hi@jappie.me""}', 'create-user', now());
INSERT INTO event_last_applied (id, event_id)
SELECT 1, max(id) FROM event
ON CONFLICT (id)
    DO UPDATE SET
        event_id = lastval();
commit;
If the projection fails the entire event gets rejected, which means all changes within the transaction get rolled back by Postgres. This applies relational guarantees, to a non-relational system trough a transaction. We also weave this transaction trough business logic code, so that in case of an exception, we rollback. Quite an elegant solution, which I didnot invent.On system boot we figure out if we need to reproject or not, the query is rather simple:SELECT type, payload FROM event
WHERE
    id > (
        SELECT event_id FROM event_last_applied
        WHERE id = 1)
ORDER BY
    id ASC;
which returns something like this, telling the system what to do:    type     |          payload          
-------------+---------------------------
 create-user | {""email"": ""hi@jappie.me""}
With that, we can reproject, also known as replaying history. Replaying history involves truncating all tables that are event sourced. And then truncating the event_last_applied table, which in this case just removes the one row. Then the system will notice it needs to replay events on boot for example. This is a rather dangerous operation, because if any event fails, you may have potentially lost data. A lot of things can go wrong with a large history, foreign keys, exceptions, serialization mismatches, events out of order etc. Transactions can help here as well, and make this re-projection safe.DeadlockThere is one more important piece of context: An event maybe composed with other events into larger transactions. For example, if we create a user, we may also assign him to a company within the same transaction. In SQL that looks like this:BEGIN;

/* left out projection code, insert user into tables here */

INSERT INTO event (payload, type, created)
    VALUES (
        /* whatever event source data*/
        '{""email"":""hi@jappie.me""}', 'create-user', now());
INSERT INTO event_last_applied (id, event_id)
SELECT 1, max(id) FROM event
ON CONFLICT (id)
    DO UPDATE SET
        event_id = lastval();

/* left out projection code, connect user to company */

INSERT INTO event (payload, type, created)
    VALUES (
        /* whatever event source data*/
        '{""company-id"":2, ""user-id"": 1}', 'connect-company', now());
INSERT INTO event_last_applied (id, event_id)
SELECT 1, max(id) FROM event
ON CONFLICT (id)
    DO UPDATE SET
        event_id = lastval();
COMMIT;
Transactions form proper monoids, and they can grow arbitrarily large. This is good because even for large chuncks of business logic we always gaurantee our event log remains in a valid state. We’d expect our re-projections to always work, because only correct ones get recorded. Where does this go wrong then?The issue is concurrency, consider connection A and B:A opens a transaction and inserts a user, but has to do other projections and event insertions as wellB opens a transaction and wants to insert an event, B has to wait until A completes. This is because A made an update to the event_last_applied on row number 1, as part of the insert event logic. This row is locked until A completes, so B has to wait.A completes and releases the lock on row 1.B can now complete as well.This is not a deadlock as long as A completes. B can wait a long time because our transactions can grow arbitrarily large. For example when we’re inserting millions of rows of data, taking up half an hour. Which is far beyond the HTTP session length of 30 seconds, or whatever length a user finds acceptable. This was indeed the production bug encountered at supercede. One user was doing pack ingestion, which involves reading millions of excell file rows, and the rest of the system became unusable because of that.Now what?At first I started with the most obvious solution. I re-grouped how event sourcing took place. I put the event sourcing code at the end of the transaction in pack ingestion, so that the event source table remained available for other transactions up till that point. Because event sourcing is only a small part of normal transactions, this created a small locking window. Thus this worked! However it only worked for this transaction with pack ingestation, I didn’t know if there were any other transactions like this in our code base. Furthermore, I had to bypass parts of the event sourcing interface to make this work. For example, I had to project events by hand, and insert events by hand, rather then using the internal library. I decided this was a bad precedence to set. I was afraid other engineers would copy this approach when it wasn’t necessary. So I went looking for other solutions.Another idea is that instead of doing the large transaction, we could split it up into smaller ones. Allowing other events to clear while this bigger one was in progress. I didn’t like this either. For one this code was old, tried and tested, making a rather large modification like splitting the transaction could introduce many unintended bugs. For example when cleanup doesn’t happen correctly on failure. I thought this was likely because this transaction was large, and covered many tables. Also our normal tools such as types and integration tests wouldn’t help a lot with guaranteeing cleanup. So this would become difficult to maintain fast. Which is problematic for a piece of code which is the “money maker”, and needs to change often. Furthermore I had a much more simple but thorough solution in mind.I decided to redesign the event source tables. Naturally my colleagues exclaimed shouts of joy when I decided to modify an even older system. The event source system described above is almost as old as supercede. But I believed it was easier to modify, and more importantly, easier to test for correctness. Furthermore this would also solve the problem for other, possibly unknown, or future, large transactions. This change would keep our code easy to maintain and solve a bug. The new schema looks almost identical to the old one:CREATE TABLE event (
    id serial PRIMARY KEY NOT NULL,
    payload jsonb NOT NULL,
    type character varying NOT NULL,
    created timestamp with time zone NOT NULL
);

CREATE TABLE event_applied (
    id serial PRIMARY KEY NOT NULL,
    event_id bigint NOT NULL REFERENCES event (id),
    created timestamp with time zone NOT NULL
);
The big difference is that we renamed event_last_applied to event_applied and added a created field. With this change, inserting events is also quite similar to the initial system:BEGIN;
INSERT INTO event (payload, type, created)
    VALUES ('{""email"":""hi@jappie.me""}', 'create-user', now());
INSERT INTO event_applied (event_id, created)
SELECT last_value, now() FROM event_id_seq;
COMMIT;
The big difference is that instead of modifying always row number 1 to be the latest ID, we insert a new row into event_applied with the latest id. This avoids locking of row number 1. For re-projection we truncate the event_applied table, allowing the code to rerun all those events. The big difference is in figuring out which events haven’t been applied yet:SELECT type, payload FROM event AS e
WHERE
    NOT EXISTS (
        SELECT 1 FROM event_applied
        WHERE event_id = e.id)
ORDER BY
    id ASC;
We compare the event table to the event_applied table, and return any events that don’t exist in that. We’re still ordering by id to ensure the correct order. Is this correct? Let’s consider concurrency once more with connection A and B:A opens a transaction and inserts a user, but has to do other event source queries as well.B opens a transaction does it’s projection work and wants to insert an event, B creates a new row in the even_applied table and completes. There is no need to wait since there is no single row lock. So B finishes.A finishes it’s other event sourcing completes.This doesn’t deadlock. However it’s not completely correct in that A get’s id 1. and B get’s id 2, but A‘s transaction finishes after B by inserting another event with id 3. So on reprojection one of A‘s events get’s applied before B. But in the initial projection, all of A‘s event happened after B. So the first event of A is out of order. This may cause issues. This problem was also present in the original implementation, since an id is acquired before the lock waiting happens. I think a solution would be to group the events by transaction id, and then order by last created event. In this case all events created before B in A‘s transaction would be pushed behind it by an event happening after B finishes. If we do that, the event table gets an extra field:CREATE TABLE event (
    id serial PRIMARY KEY NOT NULL,
    payload jsonb NOT NULL,
    type character varying NOT NULL,
    created timestamp with time zone NOT NULL,
    transaction_id bigint NOT NULL
);
Our insert function retrieves the transaction id with txid_current:BEGIN;
INSERT INTO event (payload, type, created, transaction_id)
    VALUES ('{""email"":""hi@jappie.me""}'
           , 'create-user'
           , now()
           , txid_current());
INSERT INTO event_applied (event_id, created)
SELECT last_value, now() FROM event_id_seq;
COMMIT;
And our unnaplied events query now groups:SELECT
    array_agg(type) AS types,
    array_agg(payload) AS payloads
FROM event AS e
WHERE NOT EXISTS (
      SELECT 1 FROM event_applied WHERE event_id = e.id
    )
GROUP BY transaction_id
ORDER BY max(id) ASC;
If we run that unnaplied events query on an event table like this:id |        payload        |      type       | created    | transaction_id 
---+-----------------------+-----------------+------------+----------------
 6 | {email: hi@jappie.me} | delete-user     | 2023-01-15 | 77958
 7 | {email: hi@jappie.me} | create-user     | 2023-01-15 | 77959
 8 | {company-id: 2}       | delete-company  | 2023-01-15 | 77958
We’d get a result like:             types             |              payloads
-------------------------------+-----------------------------------------
 {create-user}                 | {{email: 'hi@jappie.me'}}
 {delete-user,delete-company}  | {{email: 'hi@jappie.me'},{company-id: 2}}
Which is what we want. Even though the create user event happened while the delete user event was happening, the delete user event was part of a larger transaction. So the create user even should come first when re-projecting. This allows arbitrary sized transactions to project alongside each-other and provides better ordering guarantees then the original implementation.Closing thoughtsPhew, that was a lot. I didn’t think this would become such a large post. Designing an event source system on Postgres transactions is rather hard. All I wanted to do is clear my thoughts on the matter, but that grouping issue is another bug I just found by writing about this 😅.I think the biggest lesson I’ve (re)learned from the deadlock bug itself is to make sure you reproduce an issue first before diving into solutions. Even nasty business threatening system level bugs like these can sometimes be solved with some minor modifications to the system. If we had skipped this small step of reproducing the issue, we may have focused on the CPU observation and moved pack ingestation to a separate machine, which would’ve taken weeks to implement and not solve anything.Furthermore, it’s humbling to see that even after having used relational databases for more then a decade, I still can learn new things about them. For example Postgres’ auto increment sidesteps the transaction, which was quite shocking to me. A rather important detail to keep in mind when reasoning about these systems.I made a github repository for playing around with the queries more easily. I hope you enjoyed this article, please leave a comment if you have any questions or suggestions below.Resources
The code in this blogpost
Postgres Lock monitoring
Blogs on event sourcing
Presentation on event sourcing


        Posted by Jappie J. T. Klooster
        in 

 reflection


  published: 
  15日 01月 2023年 


#postgres
#deadlock
#programming
#sql
#database

Recent stuff




                The peculiar event sourced deadlock
            





                Summerhouse Paradis Aruba
            





                Why do I still write this blog?
            





                Zurich hack 2022 Denotational Design
            





                Restoring mysql innodb on windows.
            





                Failing in Haskell
            





                Installing a NixOS desktop tracked with git
            





                A brief intro to MTL
            

Tagsaustraliabuild-toolscssdatabasedevopsfrphaskellindonesiajakartajoblinuxnixnixosopinionpainpragmatic-haskellprogrammingreflexservantstacktesttimetoolstraveltutorialvirtualizationwebsitework
Linkedin
Twitch
Youtube
Github
Penguin
Raster
Facebook
Twitter
Reddit
Discord
 Those who know do not speak. Those who speak do not know.  Powered by Pelican. Source code, licensed under GPLv3. "
https://news.ycombinator.com/rss,Ask HN: When to make the jump to freelance/consultant?,https://news.ycombinator.com/item?id=34400435,Comments,"

Ask HN: When to make the jump to freelance/consultant? | Hacker News

Hacker News
new | past | comments | ask | show | jobs | submit 
login




 Ask HN: When to make the jump to freelance/consultant?
67 points by mxmpawn 4 hours ago  | hide | past | favorite | 54 comments 

I'm working full time as a data engineer/scientist but I also have one ongoing customer (a previous employer).Another previous employer is launching a startup and has recently pinged me because they need to build a series of data pipelines and ML models for their product.I've to talk to them about specifics but I don't see myselft having enough available time to make it work.I've been thinking about starting my own data/ml services company for a while but I don't really know when to make the jump. I think (is a guess for the time being) that the income from this new job, in addition to the income of my current client, could be enough for my living expenses of this year, so I'm thinking if this is a good time to make the jump or not.The problem that I see is that this new lead is from a previous partner of my current client, so both jobs are related to my previous employer, I don't have a pipeline of possible prospects for my service, so I'm not sure if I'll be able to generate a pool of prospects while doing the work for this customers.My guess is that I should wait until there is a sign that I could probably get a stream of clients to keep the wheel going, and try to find the time to take this new job, maybe negotiating terms to make it possible.What do you think?Edit: I've savings already as I plan to buy a new home (I'm in Argentina, we buy it cash, no mortgage). I need three more months of salary to accomplish the home budget. 
 
  
 
wpietri 2 minutes ago  
             | next [–] 

My general answer for people asking this question is: when you think you can sustain yourself. And it sounds like you're almost there.It sounds like the question for you is whether you can find new clients at a rate fast enough to replace old ones. This is a question you'll never be able to answer fully as an employee, because employers generally don't like you putting out an ""open for business"" sign. So in your shoes I'd put together a marketing/sales plan, take the new customer, and devote some time over the next N months to seeing if you can build up enough interest and leads that you will have plenty of work down the road.It sounds like the worst case here is that you get to the end of your contracts and have to go get another job again. Which is not a terrible outcome as long as you don't burn your bridges. And it's worth noting that after a period of freelancing you may decide you'd rather have a job anyhow, so you can look at the next months/years as an experiment not just on prospects, but on whether or not you will be happy with it.
 
reply



  
 
dadro 2 hours ago  
             | prev | next [–] 

Every scenario is going to be different, I took the plunge as a software consultant and can provide some datapoints.I was laid off from my job as a software engineer about 5 years ago. I had previously worked in software consulting and knew the operating model and had a handful of potential clients. I had some savings and my wife works full time so I decided to give it a go for 6 months to see how it would go. I partnered with a colleague who was also laid off and had same appetite for risk as I did.  * The first year was a grind, especially doing the non-technical tasks like networking/business development/invoicing/etc. 
  * We ended up being profitable on year 1, I made less than my previous gig as a lead engineer. We grew from 2 of us to having 1 FTE and 3-4 dev contractors in year 1. 
  * Keep in touch with engineers you liked working with, hiring is WAY harder than I expected. If you decide to grow, you will need a team for larger contracts. 
  * Business development is largely a numbers game, you have to get out of your comfort zone and talk to a lot of people/companies and get on their radar. 
  * One of our first non-technical hires was business development. We did this when we hit $1mm/yr rev. 
  * Being in a niche can be helpful if you are able to explain your value prop AND to differentiate yourself.
  * Don't view other consulting firms as competition. We've formed some great relationships with other companies that align with our engineering process and refer work when we have too much and get work when they have too much.

Over the last 5 years we grew from 2 ""founders"" (along with some former colleagues as contractors) to about ~40 employees (80% FTE's/20% contractors) all remote, US based. In hindsight, I think my favorite size was when we were ~8 people. It was big enough to take on 1-2 large-ish contracts, but less stress in keeping pipeline full. The risk tolerance for having a 6 figure payroll every 2 weeks is not for everyone!(edit formatting)
 
reply



  
 
eddsh1994 1 hour ago  
             | parent | next [–] 

How many years experience did you have? :)
 
reply



  
 
dadro 9 minutes ago  
             | root | parent | next [–] 

I had about 15 years professional experience as a developer.
 
reply



  
 
PragmaticPulp 1 hour ago  
             | prev | next [–] 

> Edit: I've savings already as I plan to buy a new home (I'm in Argentina, we buy it cash, no mortgage). I need three more months of salary to accomplish the home budget.This is a significant last-minute edit. There's a lot of good advice throughout this thread, but I would suggest waiting until you've completed the home purchase and settled in before considering the jumping to freelance. It's only a few more months and you'll have a much better understanding of any issues and unexpected expenses of the new home.Beyond that, you need to make sure that you can do the process of networking, selling your services, cultivating a pipeline of clients, and collecting from clients who don't pay on time. Doing the work is only half of the battle when you're a freelancer. If you don't excel at the business side, you might not enjoy it after you run out of immediate contacts with work for you to do.
 
reply



  
 
ljm 44 minutes ago  
             | parent | next [–] 

This is true. I can't speak to the process of buying a home in Argentina but buying a house and moving is a significant change, and leaving your job to go self-employed is another significant change.Finish the home project before you commit to the self-employment project. Your full-time job is a safety net you won't have when you first go contract.No harm in putting feelers out meanwhile though, networking and making yourself known in relevant circles. Worst case is that you end up with more full-time job prospects and not freelance ones.
 
reply



  
 
ernestipark 10 minutes ago  
             | prev | next [–] 

How much work do you think the new work would take? Depending on the nature of the engagement, it doesn't have to be a 'jump' at all. If possible, an option would be to make it a more reduced hours engagement that you can moonlight. Then you can get the consulting experience, maintain the income of your full-time job, and see how it goes. Once you have one engagement under your belt, then it also becomes much easier to get more through WOM and very basic marketing/promotion through LinkedIn on what your skills are and what you can offer.As others have stated in the comments also, a lot of consultants never have to do marketing or sales, they just get it word of mouth through their networks as they continue to do good work. I think doing that first job well, but in a way that's not as scary as jumping away from your full-time job is a good choice if you have it.
 
reply



  
 
nocubicles 3 hours ago  
             | prev | next [–] 

I just started this today actually. Today is the first day where I am no longer employeed by any company and will need to find my own customers, invoice them etc. My field is ERP development and consultation.I have 10+ years experience in the field and I was working for a consultation company. But on the same time I would always get messages on Linkedin from recruiters and at some point I thought I should give it a go and try to work on the projects during the nights and weekends.Did that for like 6 months and felt I could do it full time and then I just kinda did it and will be doing it in the future.
 
reply



  
 
cableshaft 2 hours ago  
             | parent | next [–] 

Sounds like you got freelance work from recruiters contacting you for full-time job opportunities. How did you ask them about that, out of curiosity?
 
reply



  
 
NiagaraThistle 2 hours ago  
             | root | parent | next [–] 

I've done this - with Web Development, but still got projects this way. I just responded to the recruiter or directly to the company if I had their info, and stated my background, my rate, and asked if they were interested in working with my in a contractor capacity. It's gone very well, and I have several ongoing contracts from this process.Basically just state that you are interested in the work, but not the current agreement and see if the company is open to hiring you as a contractor. If your experience is good and viewable/provable, they probably will be. And Bonus points if you are a good communicator and timely with such and delivering deadlines - many contractors are not and companies/recruiters get burned by them and those who ARE dependable are worth their weight in gold to companies.
 
reply



  
 
nocubicles 2 hours ago  
             | root | parent | prev | next [–] 

I always told them that i'm not looking full time but only part time but that only did work once I think. Most success came from building a small persona online, participating in the discussions, networking on Discords/Twitter and then the opportunities came.
 
reply



  
 
MuffinFlavored 47 minutes ago  
             | parent | prev | next [–] 

how different are WMS and ERP?
 
reply



  
 
janetacarr 1 hour ago  
             | prev | next [–] 

Personally, as a freelancer/consultant, I never think there is a right time for anything in my life. Of course, you might feel differently (and it probably is different). I'm offering advice from my perspective only, and I don't have a home to make, or children to worry about. I've been freelance/consulting/whatever for about 2 years, so make of this what you will, but I'm still early stages having just been where you're at.First I want to say, the risk might not be as big as you might think it is. You've a pretty in-demand skill set right now, more so than most SWEs. If things don't pan out as you expect, you can always get another full-time position despite the 'looming recession'.That said, here is some unsolicited advice. Rebuttal as necessary :)Regarding leads/prospects, if you do decide to make the jump, chances are you'll be tapping your professional network for leads, or pitching strangers if you happen to get on a call with them via cold email. This can work for a while, maybe get your first recurring clients, but I don't recommend working with a matchmaker like Toptal, Fiverr, Upwork, or an agency.The platforms tend to have poor pay and bad clients. Agencies will limit your ability to build a direct relationship with the clients. I know people have built successful businesses using both of them, but for me having a direct relationship with high-value clients seems to have paid off doubly as I can set my own terms (fixed rate / value based) rather than the typical hourly model. If you can/want to build such a relationship, you should *get on the phone with a decision maker* (a huge unlock for closing work). Regular dev/engineer interview channels will yield regular dev work, pay, and circumstances (maybe that's a feature for you).After a few sales calls for clients, You may realize getting leads to come to you is best, or at minimum people should have a reason to answer your emails like having some kind of branding or marketing, so start writing, coding, tweeting, or whatever regularly to get attention. Keep at it. It's hard work.On top of everything, you will fuck up, and that's okay, so give yourself some breathing room financially and mentally.
 
reply



  
 
xeromal 1 hour ago  
             | prev | next [–] 

One element required is to have a network big enough to sustain itself off recommendations. I think that's absolutely necessary at least for 80% of your business revenue. If you're just hopping from one client to the next, you're not going to be in a strong decision to make good choices for your company when it comes to negotiations.
 
reply



  
 
sirsinsalot 1 hour ago  
             | parent | next [–] 

I've been a consultant through my own company for 15 years and never needed to do this.I simply apply for contract roles.My day rate is 150+% of the average for my role and area and my contract terms are ridiculously weighted in my favor.You can learn to negotiate going from one client to the next.And you mainly have to be able to negotiate better than the client. That's pretty easy usually.
 
reply



  
 
moneywoes 1 hour ago  
             | root | parent | next [–] 

Where do you find these roles
 
reply



  
 
Nextgrid 1 hour ago  
             | root | parent | next [–] 

Seconded.The vast majority of contract roles I see here in the UK are brokered by recruiters who will enforce their own standard contract terms & rates on a take-it-or-leave-it basis.This kind of approach seems like it has no chance of working as the recruiter would rather place someone at an ""okay"" rate rather than having to risk submitting a higher-rate candidate and having the client balk and go away altogether.
 
reply



  
 
sirsinsalot 13 minutes ago  
             | root | parent | next [–] 

Yes recruiters, and outside IR35.It isn't true that they set rates and dictate terms. Push back, negotiate with them.If you have recruiters who know you deliver, which drives more business for them and makes them look good, they'll listen to you and break their backs to place you even at rates higher than other people. They become your marketing team.They're salespeople. Make their job easy and make yourself a tool that generates more long term commission for them. They'll bank on you and you'll be first in line.All they care about is an easy win and their commission and sales targets. If you're their easy win, you're golden. The higher the rate they can place you at the more it works for them because the commission is percentile.I interview really well. I'm a salesman too. They know if they can get me in the door I'll get the gig and they can get their cut of 50% above market rate.I get to strike terms from contracts, set my notice period ... whatever I want really.Build the relationships.
 
reply



  
 
sirsinsalot 1 hour ago  
             | prev | next [–] 

15 year+ consultant through my own company with recurring clients and more work than I can handle (and no interest in delegating or growing)Never needed to network. Don't have to market myself. Never had or needed a full time position and never been short of work.Get to know good recruiters who have streams of contract work with goal based outcomes.Learn how the contracts are managed by middle men and understand why consultants and contractors are needed and what risk profiles they serve.I'm used when times are tough, deadlines are tight or goals absolutely have to be hit.My day rate is higher than my peers.I'd say do it, find your niche and be ome the goto person for a specific kind of work.
 
reply



  
 
aantix 1 hour ago  
             | parent | next [–] 

Why work with recruiters at all?They’ll charge 30-50% over your rate. They provide zero value besides the initial contact.Their overhead impacts your negotiation leverage for a higher rate.
 
reply



  
 
sirsinsalot 18 minutes ago  
             | root | parent | next [–] 

Recruiters here typically take 10-15% and I don't care as long as I'm getting what I want in revenue.Most recruiters are absolute snakes. They can be very useful and you don't have to let them negotiate for you.At the end of the day, as long as they get their cut they don't much care.Saves me endless hours at $X/hr searching myself. It's more cost effective for me to bill a client for those hours than market myself.If you know the industry and find good recruiters they'll break their back to place you.
 
reply



  
 
benjaminwootton 1 hour ago  
             | root | parent | prev | next [–] 

Recruiters typically charge approximately 10% in the markets I am familiar with.They also act as a free sales force and have supplier agreements with end clients.  These are very hard to secure directly for one man bands.I would argue that it's not really consulting or running your own business if you are working through a recruiter.  It's more akin to short term employment.  That may be fine for your aims however.
 
reply



  
 
sirsinsalot 10 minutes ago  
             | root | parent | next [–] 

Recruiters are also an insurance policy for clients. They mitigate some risk.
 
reply



  
 
sjducb 3 hours ago  
             | prev | next [–] 

It sounds like the perfect time to jump to freelance. You've got 2 clients already.However... What happens if both clients back out and you suddenly get no work for a year? Do you have 1 year of savings runway? Will your family and living situation survive no income for a year? You need at least a year of runway because it's perfectly normal to go for 2 months with no income and if you've only got 3 months of savings then you'll start panicking.Also getting a mortgage as a freelancer is more complicated. Have you bought all of the houses you want for the next 2 years? (Freelancer mortgages require 2 years of company accounts, so you won't be able to get a mortgage for the next 2 years)
 
reply



  
 
angarg12 41 minutes ago  
             | prev | next [–] 

I'm in a similar boat (worse actually, since I'm on a visa) so I can't give specific advice, but @patio11 [1] greatly encouraged me to consider freelance/consulting as a career path.I am going through the grind of prepping for the tech interview all over again, and it just occurred to me that I might be approaching this from the wrong angle. If instead of spending all these hours every day solving leetcode problems or trying to ""grok"" the system design interview, I spent them networking and building a portfolio, it might result in a much better ROI.But alas, my visa status doesn't allow me to do anything either way. Maybe one day.[1] https://www.kalzumeus.com/2012/09/17/ramit-sethi-and-patrick...
 
reply



  
 
indymike 1 hour ago  
             | prev | next [–] 

> The problem that I see is that this new lead is from a previous partner of my current client, so both jobs are related to my previous employer, I don't have a pipeline of possible prospects for my service, so I'm not sure if I'll be able to generate a pool of prospects while doing the work for this customers.Work on building your pipeline, and having customers from launch day on.
 
reply



  
 
comprev 3 hours ago  
             | prev | next [–] 

When you have enough savings to take the plunge and survive until you either pick up work or return to a perm position.Many people love the idea of being their own boss but the reality can be quite different and they return to perm roles.
 
reply



  
 
brownrw8888 2 hours ago  
             | prev | next [–] 

Toptal is what made it easy for me.  The most exhausting thing about running your own consultancy is networking and maintaining a funnel of new clients.  Unfortunately this kept me away from freelancing for years :(I was in a similar situation like you (FTE + consultant) where I was looking for prospects and not able to find enough.  It's a full-time job in itself...  I vastly expanded my opportunities by tapping into a bigger talent network with Toptal.Please reach out and I'd be happy to share morehttps://www.toptal.com/qal80m/worlds-top-talent
 
reply



  
 
bigmanwalter 2 hours ago  
             | parent | next [–] 

What kind of hourly rates can you expect to find on Top Tal?
 
reply



  
 
pastacacioepepe 1 hour ago  
             | root | parent | next [–] 

Definitely a lot higher than on Upwork, on average.
 
reply



  
 
Nextgrid 1 hour ago  
             | root | parent | next [–] 

That's not a high bar though.
 
reply



  
 
pxue 1 hour ago  
             | parent | prev | next [–] 

Who sets the rates on Toptal?I was going to use lemon.io but they cap out at $100usd an hour.Not worth my time.
 
reply



  
 
bckygldstn 23 minutes ago  
             | root | parent | next [–] 

You (the consultant) sets the rate you get paid. Toptal adds a secret margin on top of that which is what the client pays.I haven't done Toptal in a while, but my rate was always higher than that. I got pushback from from Toptal's internal recruiters whenever I asked to increase my rate and a few of their clients did turn me down due to the rate. But I never backed down and it never took more than a few weeks between starting applying to jobs on Toptal to signing a contract.I wouldn't want to depend on Toptal for my income. But it's great as a way to fill in between other engagements if you're firm with your rate and the kind of work you want.
 
reply



  
 
moneywoes 1 hour ago  
             | parent | prev | next [–] 

Aren’t these sites just a race to the bottom for devs especially with global reach
 
reply



  
 
thegeomaster 2 hours ago  
             | parent | prev | next [–] 

Looks like they are not accepting new devs right now.
 
reply



  
 
akmittal 1 hour ago  
             | parent | prev | next [–] 

In last 2 years developer pool has increased a lot on Toptal. There are not enough jobs there now.
 
reply



  
 
physcab 3 hours ago  
             | prev | next [–] 

If you have enough connections to get you through your first 6 months or so then you can make the jump. Often when consulting your rate is double your normal take home so it balances out the period of searching for more work. But you highlighted the key anxiety in going freelance: you will always have to be selling your services. Its just like any startup except you are the product. Having to get new gigs will always be part of the job whether its now, 6 months from now, or 3 years from now
 
reply



  
 
anon223345 24 minutes ago  
             | prev | next [–] 

Freelance is a very different thing than actual consulting by the wayWhat I mean is working for a consulting firm is substantially different than what you do as a freelancer.
 
reply



  
 
collyw 21 minutes ago  
             | parent | next [–] 

That depends, it's like saying a software engineer is different from a programmer. Can be true, or it can just be a different term for the same type of thing.
 
reply



  
 
tptacek 1 hour ago  
             | prev | next [–] 

I left full time work for consulting after I did a project and discovered my bill rate was so high I could replicate my salary income while being less than 40% utilized. I wasn't doing something especially specialized; that's just what the market clearing rate for that contracting work was. I would not have guessed that rate before discovering it.So my somewhat tangential advice is: figure out your rate, and start with a rate that doesn't require you to constantly hustle for clients to make your nut. It's easier to figure out the right (high) rate when you've still got a salary to fall back on, and much harder when you're grinding it out as a full time contractor.
 
reply



  
 
simne 2 hours ago  
             | prev | next [–] 

You should consider two most important things (unfortunately, each other opposite):1. Yes, better to start own business when favorable environment, for example, when see macroeconomic grow (now recession, and high probability of crisis), or when You have some money accumulation.2. But business is by definition, PRACTICE of make stakes and earn profits on success.- You will not practice if not try. And best practice at crisis.Exist good solution of this contradiction. If You could afford it, look out at nearest to You business community (offline), and find experienced mentor, to mentor You and You'll pay him from Your current payment.Any way, early start business is much better then late. Mostly, because when You younger, You have more health, and could do things faster.
 
reply



  
 
kdazzle 1 hour ago  
             | parent | next [–] 

> better to start own business when favorable environment, for example, when see macroeconomic grow (now recession, and high probability of crisis),I could actually see the opposite being true. Lots of companies might not want to hire FTEs right now due to a maybe recession, but there's still a lot of work to be done, so they just use contractors instead.
 
reply



  
 
choult 2 hours ago  
             | prev | next [–] 

I started on my own at the end of September; I'd resigned my job at Datto after an acquisition proved itself disastrous (never work for Kaseya, or any Insight Partners portfolio company), and the position I had lined up fell through for one reason or another.I figured - I'm 40, I've always wanted to do my own thing, I've got runway... Why not? So I formed a company[0], started planning out a B2B product and networking like crazy; I'm fortunate to have a wide diaspora of former colleagues.As I want to bootstrap my own SaaS app, I needed to get revenue coming in, so I began to offer my services on a consultancy basis. I got my first small customer at the end of November - a charity needing hosting and maintenance - and then landed my first major consulting gig at the end of December 2022.Who knows what I'll be doing in a month's time, but I know that I can build a track record of helping others, and it's so varied! Every business or organization I talk to gives me more ideas for my app, so it's also a great way to collect more knowledge about the wants and needs of my potential customers.If you have the opportunity and runway to give it a shot, I'd recommend trying it - put a plan together for how you'll achieve it, how long you can last without a gig and what you'll offer others. And then execute on it.You'll never know unless you try.[0] https://xarma.co
 
reply



  
 
fhd2 3 hours ago  
             | prev | next [–] 

I personally took the plunge, with enough savings to make it several months until the first invoice is paid. Some clients pay on time, some don't, some go bankrupt or just drag it out infinitely, from what I've seen.Without that kind of savings (and being comfortable with watching them meld away for a while, taking the risk that you might not recover them), I would suggest to try to find a safe way to experiment with this. For example reducing your hours, or taking on a new main job with less hours, and taking on part time freelance work. Maybe there are things you can do for your potential clients with less hours than what you/they think is needed.
 
reply



  
 
nicolas_t 2 hours ago  
             | parent | next [–] 

With regards to clients not paying on time or going bankrupt, one data point, after being freenlance for 18 years, I have lost about 3% to unpaid work, conflicts with clients etc... Besides this, I've had quite a few clients who have been late (up to 3 months)When I first started, I remember researching a lot and reading that to expect 5% loss as a rule of thumb and have enough runway to last a year without income, I think that's a good rule to keep in mind.
 
reply



  
 
chrisa 2 hours ago  
             | prev | next [–] 

I did the same thing several years ago (left my job once I had 2 clients). I was also worried, but it turned out just fine. It's true that it will take a bit of work to get clients, but it becomes easier and easier as you go. Your current clients may know others, etc.I have two big pieces of advice:1. What other comments here say about money and runway is definitely true. It can be common to go a bit without clients - and even once you find a new one it might take 30 days for the project to start, then maybe you bill after the first 30 days, then it takes them 30 days to pay - that's 3 months from the time you found them to when you get the first money coming in! So make sure you account for that.2. Have something you can point to which says ""this is what I do, and why I'm good at it"". In my case it was an ebook, but it could be a white paper, or sample projects, etc - but you need something that ""tells a story"" to the clients that you can do the job.And good luck! It's scary to take the jump, but also very rewarding :)
 
reply



  
 
andjelam990 1 hour ago  
             | prev | next [–] 

I would advise you to find a partner who is also a data engineer/scientist and team up, it would be much easier also to share the workload, ideally even someone with network will bring in some new clients.
 
reply



  
 
misiti3780 3 hours ago  
             | prev | next [–] 

I could be another client as I need help with this, ping me at joseph dot misiti at mathandpencil.com
 
reply



  
 
davidw 2 hours ago  
             | prev | next [–] 

I've done contracting and it's ok, but it's best if you have something more to sell than just your time. ""Do you earn money while you're asleep?""
 
reply



  
 
ushercakes 2 hours ago  
             | prev | next [–] 

I would really advise heavily against just separating freelance/full time as black and white, one at a time.Really, it should be a slow transition - you work full time, you take up 1-2 freelance clients on the side. As you start to get more clients on the side, and you see a clear path to replacing your income, then you can make that jump.If you just make the jump as a clear break, it totally can work, it does all the time. But it's just a bit riskier, and it adds a lot more weight and pressure to get deals, which can lead to a situation where you charge less than you are actually worth.Btw, in my profile, the site I run is for freelancers to basically share their hourly rates. It's essentially levels.fyi for for consultants. May or may not be helpful to you at this stage of your journeyTLDR: 
- Keep your job, start taking clients on the side 
- Once you have enough clients and you see a clear path to replacing your FT income, quit and take the leap
 
reply



  
 
atemerev 2 hours ago  
             | prev | next [–] 

As a consultant, urgently looking for a full time job: not now. Most probably not now.
 
reply



  
 
moneywoes 1 hour ago  
             | parent | next [–] 

Wouldn’t contracts be more in demand during a recession?
 
reply



  
 
hocuspocus 1 hour ago  
             | root | parent | next [–] 

In places where layoffs are complicated, contractors are obviously the first to go.
 
reply



  
 
justsomehnguy 2 hours ago  
             | prev [–] 

Yesterday.
 
reply







Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
Search:  


"
https://news.ycombinator.com/rss,Heat pumps of the 1800s are becoming the technology of the future,https://knowablemagazine.org/article/technology/2023/heat-pumps-becoming-technology-future,Comments,"








CREDIT: DANA SMITH


Heat pumps offer a green, effective way to heat homes using electricity, not fossil fuels. New designs are making them more efficient and suitable for more conditions.





Technology

How heat pumps of the 1800s are becoming the technology of the future

          Innovative thinking has done away with problems that long dogged the electric devices — and both scientists and environmentalists are excited about the possibilities
          

By Chris Baraniuk
01.11.2023

FacebookTwitterLinkedinRedditFlipboardEmailPrintRepublish




Support sound science and smart storiesHelp us make scientific knowledge accessible to all
Donate today





It was an engineering problem that had bugged Zhibin Yu for years — but now he had the perfect chance to fix it. Stuck at home during the first UK lockdown of the Covid-19 pandemic, the thermal engineer suddenly had all the time he needed to refine the efficiency of heat pumps: electrical devices that, as their name implies, move heat from the outdoors into people’s homes.The pumps are much more efficient than gas heaters, but standard models that absorb heat from the air are prone to icing up, which greatly reduces their effectiveness.

YOU MAY ALSO LIKE









Food & Environment


How cities can fight climate change












Food & Environment


Now is the time to prepare for the economic shocks of battling climate change












Technology


How smart windows save energy




Yu, who works at the University of Glasgow, UK, pondered the problem for weeks. He read paper after paper. And then he had an idea. Most heat pumps waste some of the heat that they generate — and if he could capture that waste heat and divert it, he realized, that could solve the defrosting issue and boost the pumps’ overall performance. “I suddenly found a solution to recover the heat,” he recalls. “That was really an amazing moment.”Yu’s idea is one of several recent innovations that aim to make 200-year-old heat pump technology even more efficient than it already is, potentially opening the door for much greater adoption of heat pumps worldwide. To date, only about 10 percent of space heating requirements around the world are met by heat pumps, according to the International Energy Agency (IEA). But due to the current energy crisis and growing pressure to reduce fossil fuel consumption in order to combat climate change, these devices are arguably more crucial than ever.Since his 2020 lockdown brainstorming, Yu and his colleagues have built a working prototype of a heat pump that stores leftover heat in a small water tank. In a paper published in the summer of 2022, they describe how their design helps the heat pump to use less energy. Plus, by separately rerouting some of this residual warmth to part of the heat pump exposed to cold air, the device can defrost itself when required, without having to pause heat supply to the house.The idea relies on the very principle by which heat pumps operate: If you can seize heat, you can use it. What makes heat pumps special is the fact that instead of just generating heat, they also capture heat from the environment and move it into your house — eventually transferring that heat to radiators or forced-air heating systems, for instance. This is possible thanks to the refrigerant that flows around inside a heat pump. When the refrigerant encounters heat — even a tiny amount in the air on a cold day — it absorbs that modicum of warmth.A compressor then forces the refrigerant to a higher pressure, which raises its temperature to the point where it can heat your house. It works because an increase of pressure pushes the refrigerant molecules closer together, increasing their motion. The refrigerant later expands again, cooling as it does so, and the cycle repeats. The entire cycle can run in reverse, too, allowing heat pumps to provide cooling when it’s hot in summer.





Air-source heat pumps, the most common design, capture heat from the outdoor air.


The magic of a heat pump is that it can move multiple kilowatt-hours of heat for each kWh of electricity it uses. Heat pump efficiencies are generally measured in terms of their coefficient of performance (COP). A COP of 3, for example, means 1 kWh of juice yields 3 kWh of warmth — that’s effectively 300 percent efficiency. The COP you get from your device can vary depending on the weather and other factors.It’s a powerful concept, but also an old one. The British mathematician, physicist and engineer Lord Kelvin proposed using heat pump systems for space heating way back in 1852. The first heat pump was designed and built a few years later and used industrially to heat brine in order to extract salt from the fluid. In the 1950s, members of the British Parliament discussed heat pumps when coal stocks were running low. And in the years following the 1973-74 oil crisis, heat pumps were touted as an alternative to fossil fuels for heating. “Hope rests with the future heat pump,” one commentator wrote in the 1977 Annual Review of Energy.Now the world faces yet another reckoning over energy supplies. When Russia, one of the world’s biggest sources of natural gas, invaded Ukraine in February 2022, the price of gas soared — which in turn shoved heat pumps into the spotlight because with few exceptions they run on electricity, not gas. The same month, environmentalist Bill McKibben wrote a widely shared blog post titled “Heat pumps for peace and freedom” in which, referring to the Russian president, he argued that the US could “peacefully punch Putin in the kidneys” by rolling out heat pumps on a massive scale while lowering Americans’ dependence on fossil fuels. Heat pumps can draw power from domestic solar panels, for instance, or a power grid supplied predominantly by renewables.Running the devices on green electricity can help to fight climate change, too, notes Karen Palmer, an economist and senior fellow at Resources for the Future, an independent research organization in Washington, DC, who coauthored an analysis of policies to enhance energy efficiency in the 2018 Annual Review of Resource Economics. “Moving towards greater use of electricity for energy needs in buildings is going to have to happen, absent a technology breakthrough in something else,” she says.




This video illustrates the principle behind heat pumps.
CREDIT: THIS OLD HOUSE

The IEA estimates that, globally, heat pumps have the potential to reduce carbon dioxide emissions by at least 500 million metric tons in 2030, equivalent to the annual CO2 emissions produced by all the cars in Europe today.Despite their long history and potential virtues, heat pumps have struggled to become commonplace in some countries. One reason is cost: The devices are substantially more expensive than gas heating units and, because natural gas has remained relatively cheap for decades, homeowners have had little incentive to switch.There has also long been a perception that heat pumps won’t work as well in cold climates, especially in poorly insulated houses that require a lot of heat. In the UK, for example, where houses tend to be rather drafty, some homeowners have long considered gas boilers a safer bet because they can supply hotter water (around 140 to 160 degrees Fahrenheit), to radiators, which makes it easier to heat up a room. By contrast, heat pumps tend to be most efficient when heating water to around 100 degrees Fahrenheit.The cold-climate problem is arguably less of an issue than some think, however, given that there are multiple modern air source devices on the market that work well even when outside temperatures drop as low as minus 10 degrees Fahrenheit. Norway, for example, is considered one of the world leaders in heat pump deployment. Palmer has a heat pump in her US home, along with a furnace as backup. “If it gets really cold, we can rely on the furnace,” she says.Innovations in heat pump design are leading to units that are even more efficient, better suited to houses with low levels of insulation and — potentially — cheaper, too. For example, Yu says his and his colleagues’ novel air source heat pump design could improve the COP by between 3 percent and 10 percent, while costing less than existing heat pump designs with comparable functionality. They are now looking to commercialize the technology.Yu’s work is innovative, says Rick Greenough, an energy systems engineer now retired from De Montfort University in the UK. “I must admit this is a method I hadn’t actually thought of,” he says.





This newer heat pump design, by thermal engineer Zhibin Yu of the University of Glasgow, stores residual heat that would otherwise be wasted and uses it to help heat the house or defrost part of the pump itself. This makes the system more efficient.


And there are plenty more ideas afoot. Greenough, for instance, has experimented with storing heat in the ground during warmer months, where it can be exploited by a heat pump when the weather turns cool. His design uses a circulating fluid to transfer excess heat from solar hot-water panels into shallow boreholes in the soil. That raises the temperature of the soil by around 22 degrees Fahrenheit, to a maximum of roughly 66 degrees Fahrenheit, he says. Then, in the winter, a heat pump can draw out some of this stored heat to run more efficiently when the air gets colder. This technology is already on the market, offered by some installers in the UK, notes Greenough.But most current heat pumps still only generate relatively low output temperatures, so owners of drafty homes may need to take on the added cost of insulation when installing a heat pump. Fortunately, a solution may be emerging: high-temperature heat pumps.“We said, ‘Hey, why not make a heat pump that can actually one-on-one replace a gas boiler without having to really, really thoroughly insulate your house?’” says Wouter Wolfswinkel, program manager for business development at Swedish energy firm Vattenfall, which manufactures heat pumps. Vattenfall and its Dutch subsidiary Feenstra have teamed up to develop a high-temperature heat pump, expected to debut in 2023.





Like air conditioners running in reverse, heat pumps such as the one being installed here use refrigerants to capture heat from the outdoors and move it indoors to heat the house.
CREDIT: PHYXTER HOME SERVICES


In their design, they use CO2 as a refrigerant. But because the heat-pump system’s hot, high-pressure operating conditions prevent the gas from condensing or otherwise cooling down very easily, they had to find a way of reducing the refrigerant’s temperature in order for it to be able to absorb enough heat from the air once again when it returns to the start of the heat pump loop. To this end, they added a “buffer” to the system: a water tank where a layer of cooler water rests beneath hotter water above. The heat pump uses the lower layer of cooler water from the tank to adjust the temperature of the refrigerant as required. But it can also send the hotter water at the top of the tank out to radiators, at temperatures up to 185 degrees Fahrenheit.The device is slightly less efficient than a conventional, lower temperature heat pump, Wolfswinkel acknowledges, offering a COP of around 265 percent versus 300 percent, depending on conditions. But that’s still better than a gas boiler (no more than 95 percent efficient), and as long as electricity prices aren’t significantly higher than gas prices, the high temperature heat pump could still be cheaper to run. Moreover, the higher temperature means that homeowners needn’t upgrade their insulation or upsize radiators right away, Wolfswinkel notes. This could help people make the transition to electrified heating more quickly.A key test was whether Dutch homeowners would go for it. As part of a pilot trial, Vattenfall and Feenstra installed the heat pump in 20 households of different sizes in the town of Heemskerk, not far from Amsterdam. After a few years of testing, in June 2022 they gave homeowners the option of taking back their old gas boiler, which they had kept in their homes, or of using the high temperature heat pump on a permanent basis. “All of them switched to the heat pump,” says Wolfswinkel.In some situations, home-by-home installations of heat pumps might be less efficient than building one large system to serve a whole neighborhood. For about a decade, Star Renewable Energy, based in Glasgow, has been building district systems that draw warmth from a nearby river or sea inlet, including a district heating system connected to a Norwegian fjord. A Scandinavian fjord might not be the first thing that comes to mind if you say the word “heat” — but the water deep in the fjord actually holds a fairly steady temperature of 46 degrees Fahrenheit, which heat pumps can exploit.





Ground-source and water-source heat pumps differ from air-source pumps by capturing heat from the ground or from bodies of water.


Via a very long pipe, the district heating system draws in this water and uses it to heat the refrigerant, in this case ammonia. A subsequent, serious increase of pressure for the refrigerant — to 50 atmospheres — raises its temperature to 250 degrees Fahrenheit. The hot refrigerant then passes its heat to water in the district heating loop, raising the temperature of that water to 195 degrees Fahrenheit. The sprawling system provides 85 percent of the hot water needed to heat buildings in the city of Drammen.“That type of thing is very exciting,” says Greenough.Not every home will be suitable for a heat pump. And not every budget can accommodate one, either. Yu himself says that the cost of replacing the gas boiler in his own home remains prohibitive. But it’s something he dreams of doing in the future. With ever-improving efficiencies, and rising sales in multiple countries, heat pumps are only getting harder for their detractors to dismiss. “Eventually,” says Yu, “I think everyone will switch to heat pumps.”



10.1146/knowable-011123-2


Chris Baraniuk is a freelance science journalist and nature lover who lives in Belfast, Northern Ireland. His work has been published by the BBC, the Guardian, New Scientist, Scientific American and Hakai Magazine, among other publications.

Republish This Article



        Technology
      



          Climate Change
        





Share this article
FacebookTwitterLinkedinRedditFlipboardEmailPrintRepublish



Support Knowable Magazine
Help us make scientific knowledge accessible to all
      Donate
 





CloseExplore MoreANNUAL REVIEW OF RESOURCE ECONOMICSAdvances in Evaluating Energy Efficiency Policies and ProgramsTAKE A DEEPER DIVE| Explore Related Scholarly Articles
                ANNUAL REVIEW OF RESOURCE ECONOMICSAdvances in Evaluating Energy Efficiency Policies and ProgramsThere are many possible energy efficiency interventions available, including heat pumps. Some of the financial savings yielded by such interventions are smaller than utilities suggest — but others are cost-effective.ANNUAL REVIEW OF ENVIRONMENT AND RESOURCESFrom Low- to Net-Zero Carbon Cities: The Next Global AgendaStudies suggest that cities can reach net-zero targets with the help of decarbonizing technologies, including heat pumps. Broad success requires systemic transformation of how cities and nearby areas consume energy and sequester carbon.ANNUAL REVIEW OF ENERGYThe Coming Age of ConservationIn 1977, greater understanding of global energy expenditure in various areas, from transportation to home heating, was leading to new thinking on energy consumption — and a drive to improve efficiency. Heat pumps were seen as a key part of that effort.





Stay in the Know
Subscribe to the Knowable Magazine newsletter.




* indicates required

Email Address  *




Country  *


United States of America
Aaland Islands
Afghanistan
Albania
Algeria
American Samoa
Andorra
Angola
Anguilla
Antarctica
Antigua And Barbuda
Argentina
Armenia
Aruba
Australia
Austria
Azerbaijan
Bahamas
Bahrain
Bangladesh
Barbados
Belarus
Belgium
Belize
Benin
Bermuda
Bhutan
Bolivia
Bonaire, Saint Eustatius and Saba
Bosnia and Herzegovina
Botswana
Bouvet Island
Brazil
British Indian Ocean Territory
Brunei Darussalam
Bulgaria
Burkina Faso
Burundi
Cambodia
Cameroon
Canada
Cape Verde
Cayman Islands
Central African Republic
Chad
Chile
China
Christmas Island
Cocos (Keeling) Islands
Colombia
Comoros
Congo
Cook Islands
Costa Rica
Cote D'Ivoire
Croatia
Cuba
Curacao
Cyprus
Czech Republic
Democratic Republic of the Congo
Denmark
Djibouti
Dominica
Dominican Republic
Ecuador
Egypt
El Salvador
Equatorial Guinea
Eritrea
Estonia
Ethiopia
Falkland Islands
Faroe Islands
Fiji
Finland
France
French Guiana
French Polynesia
French Southern Territories
Gabon
Gambia
Georgia
Germany
Ghana
Gibraltar
Greece
Greenland
Grenada
Guadeloupe
Guam
Guatemala
Guernsey
Guinea
Guinea-Bissau
Guyana
Haiti
Heard and Mc Donald Islands
Honduras
Hong Kong
Hungary
Iceland
India
Indonesia
Iran
Iraq
Ireland
Isle of Man
Israel
Italy
Jamaica
Japan
Jersey  (Channel Islands)
Jordan
Kazakhstan
Kenya
Kiribati
Kuwait
Kyrgyzstan
Lao People's Democratic Republic
Latvia
Lebanon
Lesotho
Liberia
Libya
Liechtenstein
Lithuania
Luxembourg
Macau
Macedonia
Madagascar
Malawi
Malaysia
Maldives
Mali
Malta
Marshall Islands
Martinique
Mauritania
Mauritius
Mayotte
Mexico
Micronesia, Federated States of
Moldova, Republic of
Monaco
Mongolia
Montenegro
Montserrat
Morocco
Mozambique
Myanmar
Namibia
Nauru
Nepal
Netherlands
Netherlands Antilles
New Caledonia
New Zealand
Nicaragua
Niger
Nigeria
Niue
Norfolk Island
North Korea
Northern Mariana Islands
Norway
Oman
Pakistan
Palau
Palestine
Panama
Papua New Guinea
Paraguay
Peru
Philippines
Pitcairn
Poland
Portugal
Puerto Rico
Qatar
Republic of Kosovo
Reunion
Romania
Russia
Rwanda
Saint Kitts and Nevis
Saint Lucia
Saint Martin
Saint Vincent and the Grenadines
Samoa (Independent)
San Marino
Sao Tome and Principe
Saudi Arabia
Senegal
Serbia
Seychelles
Sierra Leone
Singapore
Sint Maarten
Slovakia
Slovenia
Solomon Islands
Somalia
South Africa
South Georgia and the South Sandwich Islands
South Korea
South Sudan
Spain
Sri Lanka
St. Helena
St. Pierre and Miquelon
Sudan
Suriname
Svalbard and Jan Mayen Islands
Swaziland
Sweden
Switzerland
Syria
Taiwan
Tajikistan
Tanzania
Thailand
Timor-Leste
Togo
Tokelau
Tonga
Trinidad and Tobago
Tunisia
Turkey
Turkmenistan
Turks & Caicos Islands
Turks and Caicos Islands
Tuvalu
Uganda
Ukraine
United Arab Emirates
United Kingdom
Uruguay
USA Minor Outlying Islands
Uzbekistan
Vanuatu
Vatican City State (Holy See)
Venezuela
Vietnam
Virgin Islands (British)
Virgin Islands (U.S.)
Wallis and Futuna Islands
Western Sahara
Yemen
Zambia
Zimbabwe




Send me *
Newsletter: The latest from Knowable Magazine, along with other curated readings and our favorite science-inspired art delivered weekly.
Events: Invitations to our free online event series, featuring leading scientists, scholars and stakeholders discussing frontiers of knowledge and key societal issues.













Close







DONATE: Keep Knowable free to read and share


More FromRethinking air conditioning amid climate changeThe dazzling history of solar powerThe road to low-carbon concrete
RepublishThank you for your interest in republishing! This HTML is pre-formatted to adhere to our guidelines, which include: Crediting both the author and Knowable Magazine; preserving all hyperlinks; including the canonical link to the original article in the article metadata. Article text (including the headline) may not be edited without prior permission from Knowable Magazine staff. Photographs and illustrations are not included in this license. Please see our full guidelines for more information.
    
How heat pumps of the 1800s are becoming the technology of the future
Innovative thinking has done away with problems that long dogged the electric devices — and both scientists and environmentalists are excited about the possibilities

    By Chris Baraniuk 
    
  
1.11.2023

    It was an engineering problem that had bugged Zhibin Yu for years — but now he had the perfect chance to fix it. Stuck at home during the first UK lockdown of the Covid-19 pandemic, the thermal engineer suddenly had all the time he needed to refine the efficiency of heat pumps: electrical devices that, as their name implies, move heat from the outdoors into people’s homes.The pumps are much more efficient than gas heaters, but standard models that absorb heat from the air are prone to icing up, which greatly reduces their effectiveness.Yu, who works at the University of Glasgow, UK, pondered the problem for weeks. He read paper after paper. And then he had an idea. Most heat pumps waste some of the heat that they generate — and if he could capture that waste heat and divert it, he realized, that could solve the defrosting issue and boost the pumps’ overall performance. “I suddenly found a solution to recover the heat,” he recalls. “That was really an amazing moment.”Yu’s idea is one of several recent innovations that aim to make 200-year-old heat pump technology even more efficient than it already is, potentially opening the door for much greater adoption of heat pumps worldwide. To date, only about 10 percent of space heating requirements around the world are met by heat pumps, according to the International Energy Agency (IEA). But due to the current  energy crisis and growing pressure to reduce fossil fuel consumption in order to combat climate change, these devices are arguably more crucial than ever.Since his 2020 lockdown brainstorming, Yu and his colleagues have built a working prototype of a heat pump that stores leftover heat in a small water tank. In a paper published in the summer of 2022, they describe how their design helps the heat pump to use less energy. Plus, by separately rerouting some of this residual warmth to part of the heat pump exposed to cold air, the device can defrost itself when required, without having to pause heat supply to the house.The idea relies on the very principle by which heat pumps operate: If you can seize heat, you can use it. What makes heat pumps special is the fact that instead of just generating heat, they also capture heat from the environment and move it into your house — eventually transferring that heat to radiators or forced-air heating systems, for instance. This is possible thanks to the refrigerant that flows around inside a heat pump. When the refrigerant encounters heat — even a tiny amount in the air on a cold day — it absorbs that modicum of warmth.A compressor then forces the refrigerant to a higher pressure, which raises its temperature to the point where it can heat your house. It works because an increase of pressure pushes the refrigerant molecules closer together, increasing their motion. The refrigerant later expands again, cooling as it does so, and the cycle repeats. The entire cycle can run in reverse, too, allowing heat pumps to provide cooling when it’s hot in summer.The magic of a heat pump is that it can move multiple kilowatt-hours of heat for each kWh of electricity it uses. Heat pump efficiencies are generally measured in terms of their coefficient of performance (COP). A COP of 3, for example, means 1 kWh of juice yields 3 kWh of warmth — that’s effectively 300 percent efficiency. The COP you get from your device can vary depending on the weather and other factors.It’s a powerful concept, but also an old one. The British mathematician, physicist and engineer Lord Kelvin proposed using heat pump systems for space heating way back in 1852. The first heat pump was designed and  built a few years later and used industrially to heat brine in order to extract salt from the fluid. In the 1950s, members of  the British Parliament discussed heat pumps when coal stocks were running low. And in the years following  the 1973-74 oil crisis, heat pumps were touted as an alternative to fossil fuels for heating. “ Hope rests with the future heat pump,” one commentator wrote in the 1977  Annual Review of Energy.Now the world faces yet another reckoning over energy supplies. When Russia, one of the world’s biggest sources of natural gas, invaded Ukraine in February 2022, the price of gas soared — which in turn shoved heat pumps into the spotlight because with few exceptions they run on electricity, not gas. The same month, environmentalist Bill McKibben wrote a widely shared blog post titled “Heat pumps for peace and freedom” in which, referring to the Russian president, he argued that the US could “peacefully punch Putin in the kidneys” by rolling out heat pumps on a massive scale while lowering Americans’ dependence on fossil fuels. Heat pumps can draw power from domestic  solar panels, for instance, or a power grid supplied predominantly by renewables.Running the devices on green electricity can help to fight climate change, too, notes Karen Palmer, an economist and senior fellow at Resources for the Future, an independent research organization in Washington, DC, who coauthored  an analysis of policies to enhance energy efficiency in the 2018  Annual Review of Resource Economics. “Moving towards greater use of electricity for energy needs in buildings is going to have to happen, absent a technology breakthrough in something else,” she says.The IEA estimates that, globally, heat pumps have the potential to reduce carbon dioxide emissions by at least 500 million metric tons in 2030, equivalent to the annual CO 2 emissions produced by all the cars in Europe today.Despite their long history and potential virtues, heat pumps have struggled to become commonplace in some countries. One reason is cost: The devices are substantially more expensive than gas heating units and, because natural gas has remained relatively cheap for decades, homeowners have had little incentive to switch.There has also long been a perception that heat pumps won’t work as well in cold climates, especially in poorly insulated houses that require a lot of heat. In the UK, for example, where houses  tend to be rather drafty, some homeowners have long considered gas boilers a safer bet because they can supply hotter water ( around 140 to 160 degrees Fahrenheit), to radiators, which makes it easier to heat up a room. By contrast, heat pumps tend to be most efficient when heating water  to around 100 degrees Fahrenheit.The cold-climate problem is arguably less of an issue than some think, however, given that there are multiple modern air source devices on the market that work well even when outside temperatures drop as low as minus 10 degrees Fahrenheit. Norway, for example, is considered one of the world leaders in heat pump deployment. Palmer has a heat pump in her US home, along with a furnace as backup. “If it gets really cold, we can rely on the furnace,” she says.Innovations in heat pump design are leading to units that are even more efficient, better suited to houses with low levels of insulation and — potentially — cheaper, too. For example, Yu says his and his colleagues’ novel air source heat pump design could improve the COP by between 3 percent and 10 percent, while costing less than existing heat pump designs with comparable functionality. They are now looking to commercialize the technology.Yu’s work is innovative, says Rick Greenough, an energy systems engineer now retired from De Montfort University in the UK. “I must admit this is a method I hadn’t actually thought of,” he says.And there are plenty more ideas afoot. Greenough, for instance, has experimented with storing heat in the ground during warmer months, where it can be exploited by a heat pump when the weather turns cool. His design uses a circulating fluid to transfer excess heat from solar hot-water panels into shallow boreholes in the soil. That raises the temperature of the soil by around 22 degrees Fahrenheit, to a maximum of roughly 66 degrees Fahrenheit, he says. Then, in the winter, a heat pump can draw out some of this stored heat to run more efficiently when the air gets colder. This technology is already on the market, offered by some installers in the UK, notes Greenough.But most current heat pumps still only generate relatively low output temperatures, so owners of drafty homes may need to take on the added cost of insulation when installing a heat pump. Fortunately, a solution may be emerging: high-temperature heat pumps.“We said, ‘Hey, why not make a heat pump that can actually one-on-one replace a gas boiler without having to really, really thoroughly insulate your house?’” says Wouter Wolfswinkel, program manager for business development at Swedish energy firm Vattenfall, which manufactures heat pumps. Vattenfall and its Dutch subsidiary Feenstra have teamed up to develop a high-temperature heat pump, expected to debut in 2023.In their design, they use CO2 as a refrigerant. But because the heat-pump system’s hot, high-pressure operating conditions prevent the gas from condensing or otherwise cooling down very easily, they had to find a way of reducing the refrigerant’s temperature in order for it to be able to absorb enough heat from the air once again when it returns to the start of the heat pump loop. To this end, they added a “buffer” to the system: a water tank where a layer of cooler water rests beneath hotter water above. The heat pump uses the lower layer of cooler water from the tank to adjust the temperature of the refrigerant as required. But it can also send the hotter water at the top of the tank out to radiators, at temperatures up to 185 degrees Fahrenheit.The device is slightly less efficient than a conventional, lower temperature heat pump, Wolfswinkel acknowledges, offering a COP of around 265 percent versus 300 percent, depending on conditions. But that’s still better than a gas boiler (no more than 95 percent efficient), and as long as electricity prices aren’t significantly higher than gas prices, the high temperature heat pump could still be cheaper to run. Moreover, the higher temperature means that homeowners needn’t upgrade their insulation or upsize radiators right away, Wolfswinkel notes. This could help people make the transition to electrified heating more quickly.A key test was whether Dutch homeowners would go for it. As part of a pilot trial, Vattenfall and Feenstra installed the heat pump in 20 households of different sizes in the town of Heemskerk, not far from Amsterdam. After a few years of testing, in June 2022 they gave homeowners the option of taking back their old gas boiler, which they had kept in their homes, or of using the high temperature heat pump on a permanent basis. “All of them switched to the heat pump,” says Wolfswinkel.In some situations, home-by-home installations of heat pumps might be less efficient than building one large system to serve a whole neighborhood. For about a decade, Star Renewable Energy, based in Glasgow, has been building district systems that draw warmth from a nearby river or sea inlet, including a district heating system connected to a Norwegian fjord. A Scandinavian fjord might not be the first thing that comes to mind if you say the word “heat” — but the water deep in the fjord actually holds a fairly steady temperature of 46 degrees Fahrenheit, which heat pumps can exploit.Via a very long pipe, the district heating system draws in this water and uses it to heat the refrigerant, in this case ammonia. A subsequent, serious increase of pressure for the refrigerant — to 50 atmospheres — raises its temperature to 250 degrees Fahrenheit. The hot refrigerant then passes its heat to water in the district heating loop, raising the temperature of that water to 195 degrees Fahrenheit. The sprawling system provides 85 percent of the hot water needed to heat buildings in the city of Drammen.“That type of thing is very exciting,” says Greenough.Not every home will be suitable for a heat pump. And not every budget can accommodate one, either. Yu himself says that the cost of replacing the gas boiler in his own home remains prohibitive. But it’s something he dreams of doing in the future. With ever-improving efficiencies, and rising sales in multiple countries, heat pumps are only getting harder for their detractors to dismiss. “Eventually,” says Yu, “I think everyone will switch to heat pumps.”
    
    

10.1146/knowable-011123-2



Chris Baraniuk is a freelance science journalist and nature lover who lives in Belfast, Northern Ireland. His work has been published by the  BBC, the  Guardian,  New Scientist,  Scientific American and  Hakai Magazine, among other publications.

This article originally appeared in Knowable Magazine, an independent journalistic endeavor from Annual Reviews. Sign up for the newsletter.
  Copy htmlClose
"
https://news.ycombinator.com/rss,Rendering like it's 1996 – Baby's first pixel,https://marioslab.io/posts/rendering-like-its-1996/babys-first-pixel/,Comments,"













Mario's Lab





Mario's Lab
Mastodon
Twitter
Github
RSS



Rendering like it's 1996 - Baby's first pixel
December 02, 2022




There's absolutely no chance we'll get to this level of quality.



	In 1996, I was a teen without a gaming console. While my friends enjoyed their Crash Bandicoots, Tekens, and Turoks, I had a beige 486 DX 2 with a turbo button, 16Mb of RAM, a 256Mb hard disk, and a 2x CD-ROM drive running DOS. And then I got a copy of Quake. Did it run great? No. But it did run! And to my young eyes, it was the most beautiful thing I've ever seen on my computer screen. Ok, the most beautiful brown thing.


	3D accelerator graphics cards were in their infancy. Most DOS PC games around that time would render their glorious pixels via the CPU to a dedicated area in RAM, e.g. starting at segment address 0xa000. The (pretty dumb) graphics card would then read and display the contents of that memory area on your bulky CRT. This is known as software rendering or software rasterization.


	I did dabble in some graphics programming back then. I even managed to create a Wolfenstein style first person shooter in QBasic with some assembly before the end of the century.



Actually not a ray casting engine, but a polygonal 3D engine with terrible affine texture mapping.


	But I never really dove into the depths of contemporary graphics technology. And while my subsequent professional career featured plenty of graphics programming, it was mostly the GPU accelerated kind, not the ""worry about each cycle in your inner loops"" software rasterizer kind of type.

(Non-)Goals

	I want to explore the ins and outs of software rasterization, starting from first principles, i.e. getting a pixel on screen. From there, I want to delve into topics like simple demo effects, primitive rasterization, ray casting, voxel terrain, maybe even Quake-style 3D rendering, and whatever else comes to mind.


	Each blog post on a topic will lay out the theory the way I understand it in hopefully simple terms, discuss a naive practical implementation, and finally investigate ways to optimize the implementation until it is reasonably fast.


	The end product(s) should work on Windows, Linux, macOS, and common browsers. Ideally, a little software rasterizer library and demos will fall out at the end, that can serve both as an example implementation of common techniques, or as the basis for other demos or games with DOS game aesthetics.


	You'll be able to follow along both here, and by playing with the code on GitHub. For each blog post, there will be one tagged commit in the main branch you can check out. In addition to the render-y bits, I'll also demonstrate how I set up a cross-platform C project and show you how I structure, build, and debug C code in such a project. I love seeing and learning from other people's workflows. Maybe that's true for you too.


	What I do not want to do is dabble in things like assembly or SIMD optimizations. While that can be fun too, it is unlikely to be necessary on today's hardware, given that I'll target common DOS resolutions like 320x240, or 640x480. I might however inspect and discuss the compiler's assembly output to identify areas that can be improved performance wise in the higher level code.

Tools of the trade

	The weapon of choice will be C99 for aesthetic and practical reasons. I want all the code produced throughout this series to compile anywhere. It should also be easy to re-use the code in other languages through an FFI. C99 is a good choice for both objectives.


	In terms of ompilers, I'll be using Clang with MinGW headers and standard libraries on Windows, Clang through Xcode on macOS, and GCC on Linux. Why Clang on Windows? Because Visual Studio is a multi-gigabyte download, and setting up builds for it is a terrible experience. Clang also generates better code.


	I'll use CMake as the meta build tool, not because I love it, but because my favorite C/C++ IDE CLion has first class support for it. Other development environments understand CMake as well these days, including Visual Studio if that's your kink. For actually executing the builds, I'll use Ninja, which is wicked fast, especially compared to MSBuild and consorts.


	The pixels we'll generate need to be thrown up on the display somehow. On Windows, Linux, and macOS we'll use MiniFB. In a few lines of code, we can open a window, process keyboard and mouse input, and give it a bunch of pixels to draw to the window. It can even upscale our low resolution output if needed. Since MiniFB does not have browser support, I've written a web backend myself and submitted it as a pull request to the upstream repo. In the meantime, we'll use my MiniFB fork, which has web support baked in.


	To get the code running in the browser, we'll use Emscripten to compile the C code to WASM and a small .js file, which loads the .wasm file and exposes our C functions to JavaScript.


	In terms of IDE, you are free to use whatever you want. You'll most likely want something that can ingest CMake builds. For this series, I choose VS Code, not because I love it, but because it's free. The project contains a bunch of VS Code specific settings that make working on the project super simple for all supported platforms.

Getting the source code and tools

	That's a lot of tools! I've tried to make it as simple for you to follow along as possible. Here's what you need to install:


Visual Studio Code
Make sure code can be called on the command line! Open VS Code, press CTRL+SHIFT+P (or CMD+SHIFT+P on macOS), type Shell Command: Install 'code' command in PATH and hit enter.
Windows:

Git for Windows. Make sure its available on the command line via the system PATH.


Linux:

Git, GCC, GDB, Python, CMake, Curl, libx11-dev, libxkbcommon-dev, and libgl1-mesa-dev. On Ubuntu sudo apt install build-essential git gdb python3.11 cmake curl libx11-dev libxkbcommon-dev libgl1-mesa-dev


macOS:

Xcode. Make sure to also install the command line tools




	Once you've installed the above, clone the repository (on Windows, use Git Bash, which comes with Git for Windows):


git clone https://github.com/badlogic/r96
cd r96


	Next, checkout the tag for the blog post you want to follow along with, execute the tools/download-tools.sh script:


git checkout 01-babys-first-pixel
./tools/download-tools.sh


	The download-tools.sh script will download all remaining tools that are needed, like CMake, Ninja, Clang for Windows, Python, a small static file server, Emscripten, and Visual Studio Code extensions needed for C/C++ development. See the README.md for details.


Note: we may add new tools in future blog posts. After checking out a tag for a blog post, make sure to run tools/download-tools.sh again.

The r96 project

	These are the goals for the project scaffold:


Make building and debugging for the desktop and the web trivial.
Allow adding new demo apps that work without code modification on both the desktop and in the browser
Make creating re-usable code easy.


	Let's see how I tried to achieve the above. Open your clone of the r96 Git repository in VS Code and have a look what's inside.


Note: The first time you open the project in VS Code, you'll be asked to select a CMake configure preset.


Note: the first time you open a source file in VS Code, you will be asked if you want to install clangd. Click Yes.

File structure




Let's start in the root folder.

	The .gitignore, LICENSE, and README.md files are self-explanatory.


	The CMakeLists.txt and CMakePresets.json define our build. We'll look into these in a later section.


	The .clang-format file stores the formatting settings used to format the code via, you guessed it, clang-format. The VS Code C/C++ extension uses the settings in that file whenever you format a C/C++ source file. The file can also be used to format the entire code base from the command line.


	The src/ folder contains our code. Re-usable code goes into src/r96/. Demo apps go into the root of the src/ folder. There are two demo apps so far called 00_basic_window.c and 01_drawing_a_pixel.c. Any demo apps we write in subsequent blog posts will also go into src/ and start with a sequential number, so we immediately see in which order they were written.


	The src/web/ folder may be weird, even scary to seasoned C veterans. But we need it to run our demo apps on the web. A small price to pay. It contains one .html file per demo app. The purpose of that file is to:


Load the .js and .wasm files generated by Emscripten for the demo app executable target
Provide the demo app with a HTML5 canvas element to draw to
Kick off the demo apps execution by calling its main() function


	For any demo app we write in the future, we'll add a source file to the src/ folder, and a corresponding .html file to the src/web/ folder.


	The src/web/index.html file is just a plain listing linking to all the .html files of our demo apps. The src/web/r96.css file is a CSS style sheet used to make the elements in the demo app .html files a little prettier.


	The .vscode/ folder contains settings and launch configurations so working on the project is a nice experience in VS Code.


	Finally, the tools/ folder contains scripts to download the necessary tools as well as configuration files for a few of those tools. When executing the tools/download-tools.sh script, some of the tools actually get installed in the tools/ folder so they don't clog up your system. The folder also contains scripts and batch files used by the launch configurations to do their work.


	The details of the .vscode and tools folder are all gory and duct tape-y. You can have a look if you must. For the remained of the series, their content doesn't matter much. Just know that they are setup in a way to make our lives easy.

Building

The first time you open the project in VS Code, you're asked to select a configure preset.







	A configure preset defines for what platform the code should be build and with what compiler and build flags that should happen. The presets are defined in CMakePresets.json.


	For each platform the r96 project supports, there is a corresponding debug and release configure preset. To start, we'll select Desktop debug. You can also select the configure preset in the status bar at the bottom of VS Code.






	To build the project for the selected platform and build type (debug or release), click the Build button in the status bar.






	Alternatively, you can open the VS Code command palette (CTRL+SHIFT+P or CMD+SHIFT+P on macOS), type CMake: Build, and hit enter.


	In both cases, the CMake Tools extension, which was installed as part of tools/download-tools.sh, will configure the CMake build if necessary, then incrementally build the libraries and executables defined in CMakeLists.txt.


	The resulting build output consisting of executables and assets can be found in build/<os>-<build-type>. E.g. for Desktop debug, the build output will be located in build/windows-debug, build/linux-debug, or build/macos-debug depending on what operating system you are on. For Web release the output will be in build/web-output, and so on.


Note: To learn more about how to use VS Code CMake integration, check out the documentation.


	You can of course also build the project on the command line:


# Configure a Windows debug build and execute the build
cmake --preset windows-debug
cmake --build build/windows-debug

# Configure a web release build and execute the build
cmake --preset web-release
cmake --build build/web-release

Debugging

	The launch.json file in the .vscode/ folder defines launch configurations for each platform. Click the launch button in the status bar to select the launch configuration and start a debugging session. 






	After clicking this status bar entry, you'll be asked to select a launch configuration:






	When you first start a debugging session, you'll be asked to select a launch target, aka the executable you want to launch:






	You can also change the launch target in the status bar:






	After selecting the launch target, the code is incrementally rebuild, and the debugging session starts. 


	Instead of going through the status bar, you can also start a new debugging sessions by pressing `F5`. This will launch a session for the currently selected launch configuration, configure preset, and launch target.


Important: the launch configuration MUST match the preset you selected:


Desktop debug target: select the Desktop debug or Desktop release preset.
Web debug target: select the Web debug or Web release preset.


	Debugging a desktop build is the standard experience you are used to. Set breakpoints and watches, interrupt the program at any time, and so on.


	Debugging the C code compiled to WASM directly in VS Code is not possible. When you start a web debugging session, the respective launch configuration starts a local static file server (downloaded via tools/download-tools.sh) and opens the .html file corresponding to the selected launch target in a browser tab.


	When you are done ""debugging"" a web build, close the browser tab, and close the debugging session in VS Code by clicking the ""Stop"" button in the debugger controls.


 If you feel adventurous: it is possible to debug the C and JavaScript code in Chrome.. We'll look into that below.

Dissecting the CMakeLists.txt and CMakePresets.json files
To understand how the build is setup, we need to understand the CMakeLists.txt and CMakePresets.json files.

	We've already had a brief look at the CMakePresets.json file above. It defines a configure preset for each operating system and build type combination. Let's have a look at one of the presets, specifically, the one used for Windows debug builds.


{
	""name"": ""windows-debug"",
	""displayName"": ""Desktop debug"",
	""description"": """",
	""generator"": ""Ninja"",
	""binaryDir"": ""${sourceDir}/build/${presetName}"",
	""cacheVariables"": {
		""CMAKE_BUILD_TYPE"": ""Debug"",
		""CMAKE_MAKE_PROGRAM"": ""${sourceDir}/tools/desktop/ninja/ninja""
	},
	""toolchainFile"": ""${sourceDir}/tools/desktop/toolchain-clang-mingw.cmake"",
	""condition"": {
		""type"": ""equals"",
		""lhs"": ""${hostSystemName}"",
		""rhs"": ""Windows""
	}
},


The important bits are:


generator: we tell CMake to generate a Ninja build.
binaryDir: specifies the output directory for the build. In this case it maps to build/windows-debug through variable substitution.
CMAKE_BUILD_TYPE: tells CMake we want the binaries to include debugging information.
CMAKE_MAKE_PROGRAM: tells CMake were to find the Ninja executable. The tools/download-tools.sh script downloaded the executable to tools/desktop/ninja/
toolchainFile: where to find the compiler and linker. On Windows, we use Clang with MinGW headers and standard libraries., which the tools/download-tools.sh script downloads to tools/desktop/clang. The toolchain file references the compiler and linker in that location and sets up a few other CMake cache variables.
condition: tells CMake to only enable this configure preset if we're running on Windows.


	The other configure presets are pretty similar and only differ in what operating system they should be available on, as well as the toolchain being used. On macOS and Linux, the default toolchain is used (GCC or Xcode's Clang). For the web, the Emscripten toolchain is used through a toolchain file that ships with Emscripten.


	We could work without this presets file, but that would mean we'd have to specify all these parameters manually every time we configure a CMake build. With the presets, this becomes cmake --preset <preset-name>. Much nicer!


	The CMakeLists.txt file defines the actual build itself, i.e. which source files make up which libraries and executables, and what compiler flags to use. Definitions of libraries and executables are called targets in CMake. Let's go through it section by section. We start out with this:


cmake_minimum_required(VERSION 3.21.1)
project(r96)

set(CMAKE_C_STANDARD 99)
set(CMAKE_C_STANDARD_REQUIRED TRUE)
set(CMAKE_EXPORT_COMPILE_COMMANDS TRUE)


	We define the minimum CMake version and project name, and enable (and require) C99 support. The final line makes CMake generate a compile_commands.json file in the build folder. This is also known as a compilation database and used by many IDEs to understand a CMake build. In our case, the file is used by the clangd VS Code extension to provide us with code completion and other niceities.


include(FetchContent)
FetchContent_Declare(minifb GIT_REPOSITORY https://github.com/badlogic/minifb GIT_TAG dos-pr-master)
set(MINIFB_BUILD_EXAMPLES CACHE INTERNAL FALSE)
FetchContent_MakeAvailable(minifb)


	Next we pull in MiniFB via CMake's FetchContent mechanism. CMake veterans may sneer at this and rather use a Git submodule. But I like it that way, thank you very much. This magic incantation will clone my MiniFB fork with web support, disable the MiniFB example targets, and finally make the remaining MiniFB library target available to the targets defined in our own CMakeLists.txt. Nice.


add_compile_options(-Wall -Wextra -Wpedantic -Wno-implicit-fallthrough)


	This section sets the ""pedantic warnings are errors"" compiler flags. We want the code to be reasonably clean and fail if a warning is generated.


add_library(r96 ""src/r96/r96.c"")


	Next we add a library target called r96. It's compiled from the r96/r96.c source file. Any re-usable code we write during the course of this blog post series will go in there. Any of our demo app executable targets can then depend on the r96 library target to pull in its code.


add_executable(r96_00_basic_window ""src/00_basic_window.c"")
add_executable(r96_01_drawing_a_pixel ""src/01_drawing_a_pixel.c"")


	We define two executable targets for the demos of this blog post.


add_custom_target(r96_web_assets
    COMMAND ${CMAKE_COMMAND} -E copy_directory
    ${CMAKE_CURRENT_SOURCE_DIR}/src/web
    $<TARGET_FILE_DIR:r96_00_basic_window>
)


	We define a custom target that copies all the .html files from src/web/ to the output folder. This target is needed for web builds.


get_property(targets DIRECTORY ""${_dir}"" PROPERTY BUILDSYSTEM_TARGETS)
list(REMOVE_ITEM targets minifb r96 r96_assets r96_web_assets)
foreach(target IN LISTS targets)
    target_link_libraries(${target} LINK_PUBLIC minifb r96)    
    if(EMSCRIPTEN)
        add_dependencies(${target} r96_web_assets)
        target_link_options(${target} PRIVATE
                ""-sSTRICT=1""
                ""-sENVIRONMENT=web""
                ""-sLLD_REPORT_UNDEFINED""
                ""-sMODULARIZE=1""
                ""-sALLOW_MEMORY_GROWTH=1""
                ""-sALLOW_TABLE_GROWTH""
                ""-sMALLOC=emmalloc""
                ""-sEXPORT_ALL=1""
                ""-sEXPORTED_FUNCTIONS=[\""_malloc\"",\""_free\"",\""_main\""]""
                ""-sASYNCIFY""
                ""--no-entry""
                ""-sEXPORT_NAME=${target}""
        )
    endif()
endforeach()


	And then stuff gets crazy! The first two lines compiles a list of all demo executable targets. We then iterate through those executable targets and link the r96 library target to each of them. If we build for the web, we also add the custom r96_web_assets target to the executable as a dependency, so the .html files get copied over to the output folder.


	Finally, we add a few Emscripten specific linker options. These are settings I arrived at after working with WASM for the last 2 years. They are all Emscripten specific and do things like allowing heap memory to grow. You can check out all the options in Emscripten's settings.js file. There's a lot.

The purpose of this evil incantation is to reduce the amount of CMake spaghetti needed when adding a new demo. All we need to do is add a single add_executable_target() line, specifiyng the demo name and source files its composed of. The evil incantation will then link the new demo up with all the necessary bits automatically. Nice!
The first demo app: 00_basic_window

	Before we can get our hands dirty with programmatically creating the most beautiful pixels in the world, we need to understand how MiniFB works and how a demo app is structured in terms of code. With no further ado, here's src/00_basic_window.c:


#include <MiniFB.h>
#include <stdlib.h>

int main(void) {
	const int res_x = 320, res_y = 240;
	struct mfb_window *window = mfb_open(""00_basic_window"", res_x, res_y);
	uint32_t *pixels = (uint32_t *) malloc(sizeof(uint32_t) * res_x * res_y);
	do {
		mfb_update_ex(window, pixels, res_x, res_y);
	} while (mfb_wait_sync(window));
	return 0;
}


	This is a minimal MiniFB app that opens a window with a drawing area of 320x240 pixels (line 6). It then allocates a buffer of 320x240 unit32_t elements (line 7). Each uint32_t element encodes the color of a pixel. Next, we keep drawing the contents of the buffer to the window via mfb_update_ex() (line 9) until mfb_wait_sync() returns false (line 10), e.g. because the user pressed the ESC key to quit the app. It can't get any simpler.


	MiniFB has a super minimal API. You can learn more about it here.


Note: The mfb_update_ex() function also returns a status code which can be used to decide if the app should be exited. We're not using this above for brevity's sake.

Running the demo app on the desktop

	To compile and run (or debug) our little demo app on the desktop, select the Desktop debug configure preset in the VS Code status bar, select the r96_00_basic_window target as the launch target, and the Desktop debug target launch configuration. Press F5 and you'll get this:



r96_00_basic_window on the desktop.


	Most impressive. You can make changes to the code and just hit F5 again to incrementally rebuild and restart the demo app. You can also set breakpoints, inspect variables and call stacks, and so on.


	Now how do we run the same demo app in the browser?

Running the demo app on the web

	Select the Web debug configure preset, and the Web debug target launch configuraton and press F5. You'll see this:



r96_00_basic_window running in the browser.


	The launch configuration starts a static file server (tools/web/static-server) which serves the files in build/web-debug/ on port 8123. The static server will also automatically open a browser tab with the URL corresponding to the demo's .html file.

When you're done being amazed by this, close the browser tab, and click the Stop button in the debugger controls in VS Code. If you want to be amazed again, just press F5
How the web version works

	It's kind of magic. Here's how the 00_basic_window.html file for the 00_basic_window.c demo app looks like:


<html>
<!DOCTYPE html>
<html lang='en'>
<head>
    <meta charset='utf-8'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1'>
    <link rel='stylesheet' href='r96.css'>
    <script src='r96_00_basic_window.js'></script>
</head>
<body class='r96_content'>
    <h2>Basic window</h2>
    <canvas id='00_basic_window'></canvas>
</body>
<script>
    async function init() {
        await r96_00_basic_window()
    }

    init();
</script>
</html>


	Ignoring the boring HTML boilerplate, we see that the r96_00_basic_window.js file is loaded via a <script> tag. This file was generated as part of the build by Emscripten. It contains JavaScript code that loads the WebAssembly file that stores our compiled C code and exports functions with which we can interact with the WebAssembly code.

	Next we define a <canvas> with the id 00_basic_window. Finally, a little JavaScript kicks of a call to r96_00_basic_window() (defined in r96_00_basic_window.js) in an asynchronous function. This call will load the r96_00_basic_window.wasm file and run its main() method.


	How does MiniFB know to render to the canvas? In our C code we have this line:


struct mfb_window *window = mfb_open(""00_basic_window"", res_x, res_y);


	Instead of opening a window with ""00_basic_window"" as the title, the MiniFB web backend uses the first argument passed to mfb_open() to search a canvas element with that string as its id. Any calls to mfb_update_ex() will then draw the contents of the provided buffer to this canvas.


	Also of note: We didn't have to modify our C code at all, it just ""works"". If you've ever done any front-end development, that may be very weird to you. The app basically has an infinite loop! If you do that in JavaScript, the browser tab (or the whole browser) will freeze, because the browser engine's event loop will never get a chance to run and process events. How does this magic work?


	The MiniFB web backend I wrote uses an Emscripten feature called Asyncify. In the implementation of mfb_wait_sync(), I call emscripten_sleep(0). This gives back control to the browser engine, so it can process any DOM events and not freeze. Our native C code will then resume again, without our C code ever knowing that it was actually put to sleep. The Asyncify feature rewrites our C code (or rather its WASM representation) to use continuations. That allows pausing and resuming the C code transparently. Super cool!

Can I debug the C code in the browser?

	Yes, we can in Chrome. When we build using the Web debug configure preset, Emscripten will emit DWARF information in the resulting .wasm file. Chrome can use that information to provide native code debugging right in the developer tools. To get that working:


Install Chrome
Install the C/C++ DevTools Support (DWARF) extension in Chrome
Open Chrome Developer Tools, click the gear (⚙) icon in the top right corner of dev tools pane, go to the experiments panel and tick WebAssembly Debugging: Enable DWARF support





Restart Chrome


	 Launch the demo app using the Web debug target launch config in VS code, then open the dev tools in the browser, and click on the Sources tab. You can find all the .c files that make up our little demo app under the file:// node, including the MiniFB sources. Open up 00_basic_window.c and set a breakpoint inside the loop:



C/C++ debugging in Chrome


	And there you have it: C/C++ debugging in Chrome! Since the C code runs the same on both the desktop and in the browser, we'll likely never need this functionality, unless we implement web specific features.


	Speaking of features, let's add a second demo app and draw our first pixel! But first, some very practical ""theory"".

Of colors, pixels, and rasters

	What's a pixel? Rumor has it that pixel is a stylized abbreviation of ""(pic)ture (el)ement"". A precise answer is actually quite involved and may even depend on the decade you are living in.


	Here, we lazily and imprecisely define a pixel as the smallest ""atomic"" area within a raster for which we can define a color. A raster is a rectangular area made up  of pixels. Each pixel in the raster is assumed to have the same size. The width of a raster equals the number of pixels in a row, the height equals the number of pixels in a column.


	The below raster has a width of 23 pixels and a height of 20 pixels. To locate a pixel inside the raster, we use an integer coordinate system, with the x-axis pointing to the right, and the y-axis pointing down. The top left pixel in the raster is at coordinate (0, 0), the top right pixel is at coordinate (22, 0) (or (width - 1, 0)), the bottom right pixel is at coordinate (22, 19) (or (width - 1, height -1)), and so on.



A fishy raster. Source: Wikipedia


	A raster can be a display device's output area, a piece of grid paper, etc. The rasters we'll work with are two-dimensional arrays in memory. Each array element stores the color of the pixel in some encoding.

Color encodings

	We encode the color of a pixel using the RGBA color model, where a color is represented as an additive mix of its red, green, and blue components, and an additional alpha component specifying the pixel's opacity. The opacity comes into play when we blend pixels of one raster with pixels from another raster. That's a topic for another blog post.


	More specifically, we use an ARGB8888 encoding that fits in a 32-bit unsigned integer (or uint32_t in C). Each color component is encoded as an 8-bit integer in the range 0 (no contribution) to 255 (highest contribution). For the alpha component, 0 means ""fully transparent"" and 255 means ""fully opaque"".


	Here's how the components are stored in a 32-bit unsigned integer. The most significant byte stores the alpha component, then come the red, green, and blue bytes.



Storage layout of an ARGB8888 color in a 32-bit unsigned integer. Source: Wikipedia



Here are a few colors in C:



uint32_t red = 0xffff0000;
uint32_t green = 0xff00ff00;
uint32_t blue = 0xff0000ff;
uint32_t pink = 0xffff00ff;
uint32_t fifty_percent_transparent_white = 0x80ffffff;


	More generally, we can compose a color by bit shifting and or'ing its individual components:


uint8_t alpha = 255; // fully opaque
uint8_t red = 20;    // a little red
uint8_t green = 200; // a lot of green
uint8_t blue = 0;    // no blue
uint32_t color = (alpha << 24) | (red << 16) | (green << 8) | blue;


	That looks like a great candidate for a re-usable macro! Why a macro? Because C99 support in Microsoft's C++ compiler is still meh and who knows how it does with inlined functions defined in a header. The macro guarantees that the code is inlined at the use site. Let's put the following in src/r96/r96.h


#include <stdint.h>

#define R96_ARGB(alpha, red, green, blue) (uint32_t)(((uint8_t) (alpha) << 24) | ((uint8_t) (red) << 16) | ((uint8_t) (green) << 8) | (uint8_t) (blue))


	Defining a color then becomes:


uint32_t color = R96_ARGB(255, 20, 200, 0);

Adressing a pixel in a raster

	We now can define colors easily. But how do we work with rasters in code and manipulate the colors of its pixels? We already did! Remember this line from our 00_basic_window demo app?


const int res_x = 320, res_y = 240;
...
uint32_t *pixels = (uint32_t *)malloc(res_x * res_y * sizeof(uint32_t))


	This allocates memory to store a 320x240 raster where each pixel is stored in a uint32_t. Each row of pixels is stored after the other. We can think of it as a one-dimensional array storing a two-dimensional raster.


This raster is passed to mfb_update_ex() to be drawn to the window. The reason the window content remains black is that the pixels all have the color 0x00000000 aka black (at least when building the debug variant or for Emscripten).


	We can set the pixel in the top left corner at coordinate (0, 0) to the color red like this:


pixels[0] = R96_ARGB(255, 255, 0, 0);


	OK, that was obvious. But how about a pixel at an arbitrary coordinate? Let's look at a smaller 4x3 pixel raster:




Our raster is a one dimensional block of memory. The pixel rows are stored one behind the other. The 4 pixels of the first pixel row with y=0 are stored in pixels[0] to pixels[3]. The index of a pixel in the first row is simply its x-coordinate. E.g. the pixel at coordinate (2, 0) is stored in pixels[2].

The pixels of the second row with y=1 are stored in pixels[4] to pixels[7]. The pixels of the third row with y=2 are stored in pixels[8] to pixels[11]. In general, the first pixel of a row at y-coordinate y is located at pixels[y * width]. And to address any pixel inside a row, we just add its x-coordinate! The general formula to go from a pixel's (x, y) coordinate to an index in the one dimensional array representing the raster is thus x + y * width!


Note: this is how C implements two-dimensional arrays under the hood as well. The principle also applies to higher dimensional arrays.


	If we want to set the color of the pixel at (160, 120) to red in our 320x240 pixel raster, we can do it like this:


const int res_x = 320, res_y = 240;
...
pixels[160 + 120 * res_x] = R96_ARGB(255, 255, 0, 0);

Alright, time to draw some pixels!
Demo app: drawing a pixel

	Drawing a single pixel is a bit boring, so how about we flood the screen with a gazillion pixels instead?


#include <MiniFB.h>
#include <stdlib.h>
#include <string.h>
#include ""r96/r96.h""

int main(void) {
	const int res_x = 320, res_y = 240;
	struct mfb_window *window = mfb_open(""01_drawing_a_pixel"", res_x, res_y);
	uint32_t *pixels = (uint32_t *) malloc(sizeof(uint32_t) * res_x * res_y);
	do {
		for (int i = 0; i < 200; i++) {
			int32_t x = rand() % res_x;
			int32_t y = rand() % res_y;
			uint32_t color = R96_ARGB(255, rand() % 255, rand() % 255, rand() % 255);
			pixels[x + y * res_x] = color;
		}

		if (mfb_get_mouse_button_buffer(window)[MOUSE_BTN_1]) {
			memset(pixels, 0, sizeof(uint32_t) * res_x * res_y);
		}

		mfb_update_ex(window, pixels, res_x, res_y);
	} while (mfb_wait_sync(window));
	return 0;
}


	The interesting bit happens in lines 11 to 15. Each frame, we generate 200 pixels at random coordinates with random colors. We ensure that the coordinates are within the raster bounds by % res_x and % res_y. We also clamp the color components to the range 0-255 via modulo.


	We also introduce some light input handling by checking if the left mouse button is pressed. If so, we set all pixels to the color black, giving the ""user"" a way to restart the glorious demo.


	Finally, we pass the pixels to mfb_update_ex(), which will draw them to the window.


	And here it is live in your browser, because why would we spend so much time getting the WASM build to work so nicely. Click/touch to start!







	I sure feel all this build up paid off, don't you?

Mario, WTF

	Yeah, I'm sorry. I sometimes just drift off. But we learned a lot! Next time I likely won't be so wordy. We'll have a looksy at how to draw rectangles. Exciting!


	Read the the next article in the series.


	Discuss this post on Twitter or Mastodon.





"
https://news.ycombinator.com/rss,Tesla Price Drop Angers Current Owners,https://www.bloomberg.com/news/articles/2023-01-13/tesla-price-drop-angers-current-owners-as-much-as-it-hits-profit-margins,Comments,"


Bloomberg - Are you a robot?









Bloomberg
Need help? Contact us


We've detected unusual activity from your computer network
To continue, please click the box below to let us know you're not a robot.




Why did this happen?
Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our Terms of
                Service and Cookie Policy.


Need Help?
For inquiries related to this message please contact
            our support team and provide the reference ID below.
Block reference ID:








"
https://news.ycombinator.com/rss,The art and science of spending money,https://collabfund.com/blog/the-art-and-science-of-spending-money/,Comments,"


























The Art and Science of Spending Money · Collab Fund



















Blog


About


Shared Future


SOS


Currency


Public


Blog












About


Shared Future


SOS


Currency


Public


Blog


Follow @collabfund








            
              The Art and Science of Spending Money
            
          







      Jan 12, 2023
    

SHARE ↓




        by
        Morgan Housel
@morganhousel

























          Copy Link
        












Former General Electric CEO Jack Welch once nearly died of a heart attack. Years later he was asked what went through his mind while he was being rushed to the hospital in what could have been his last moments alive.
“Damn it, I didn’t spend enough money,” was Welch’s response.
The interviewer, Stuart Varney, was puzzled, and asked why in the world that would go through his mind.
“We all are products of our background,” Welch said. “I didn’t have two nickels to rub together [when I was young], so I’m relatively cheap. I always bought cheap wine.”
After the heart attack Welch said he “swore to God I’d never buy a bottle of wine for less than a hundred dollars. That was absolutely one of the takeaways from that experience.”
“Is that it?” Varney asks, stunned.
“That’s about it,” says Welch.
Money is so complicated. There’s a human element that can defy logic – it’s personal, it’s messy, it’s emotional.
Behavioral finance is now well documented. But most of the attention goes to how people invest. Welch’s story shows how much deeper the psychology of money can go. How you spend money can reveal an existential struggle of what you find valuable in life, who you want to spend time with, why you chose your career, and the kind of attention you want from other people.
There is a science to spending money – how to find a bargain, how to make a budget, things like that.
But there’s also an art to spending. A part that can’t be quantified and varies person to person.
In my book I called money “the greatest show on earth” because of its ability to reveal things about people’s character and values. How people invest their money tends to be hidden from view. But how they spend is far more visible, so what it shows about who you are can be even more insightful.
Everyone’s different, which is part of what makes this topic fascinating. There are no black-and-white rules.
But here are a few things I’ve noticed about the art of spending money.
1. Your family background and past experiences heavily influences your spending preferences.
I love this Washington Post headline from June, 1927 – the Roaring ‘20s, the last hurrah before the Great Depression:

This is timeless, and explains so much.
After Covid lockdowns there was the concept of “revenge spending” – a furious blast of conspicuous consumption, letting out everything that had been pent up and held back in 2020.
Revenge spending happens at a broad level, too. The most stunning examples I’ve seen of this are wealthy adults who grew up poor – and were heckled, bullied, and teased for being poor as kids. Their revenge spending mentality can become permanent.
If you dig into it, I think you’ll see that a disproportionate share of those with the biggest homes, the fastest cars, and the shiniest jewelry, grew up “snubbed” in some way. Part of their current spending isn’t about getting value out of flashy material goods; it’s about healing a social wound inflicted when they were younger.
Even when “wound” is the wrong word, the desire to show the world that you’ve made it increases if you grew up snubbed out of what you wanted. To someone who grew up in an old-money affluent family, a Lamborghini might be a symbol of gaudy egotism; to those who grew up with nothing, the car might serve as the ultimate symbol that you’ve made it.
A lot of spending is done to fulfill a deep-seated psychological need.
2. Entrapped by spending: Rather than using money to build a life, your life is built around money.
George Vanderbilt spent six years building the 135,000-square-foot Biltmore house – with 40 master bedrooms and a full-time staff of nearly 400 – but allegedly spent little time there because it was “utterly unaddressed to any possible arrangement of life.” The house nevertheless cost so much to maintain it nearly ruined Vanderbilt. Ninety percent of the land was sold off to pay tax debts, and the house was turned into a tourist attraction.
In 1875 an op-ed said socialites “devote themselves to pleasure regardless of expense.” A Vanderbilt heir responded that actually they “devote themselves to expense regardless of pleasure.”
The Vanderbilt’s are obviously extreme, but that is a common trait among more ordinary people.
The devotion to expense regardless of pleasure.
Part of this is the belief that spending money will make you happier. When it doesn’t – either because it never will or because you haven’t discovered purchases that bring joy – your reaction is that you must not be spending enough, so you double down, again and again.
I’ve often wondered how many personal bankruptcies and financial troubles were caused by spending that brought no joy to begin with. It must be enormous. And it’s a double loss: not only are you in trouble, but you didn’t even have fun getting there.
I have an old friend who buried himself in credit card debt to go skiing in Europe and loved every second of it. I can wrap my head around that decision, even if I wouldn’t recommend it. He’s in control of his finances.
But what about those whose spending is driven by the belief that money is to be spent, regardless of what pleasure it brings? Money has them by the neck; they are held in captivity by its influence.
3. Frugality inertia: a lifetime of good savings habits can’t be transitioned to a spending phase.
I think what many people really want from money is the ability to stop thinking about money. To have enough money that they can stop thinking about it and focus on other stuff.
But that ultimate goal can break down when your relationship with money becomes an ingrained part of your personality. You struggle to break away from focusing on money because the focus itself is a big part of who you are.
If you develop an early system of savings and living well below your means – congratulations, you’ve won. But if you can never break away from that system, and insist on a heavy savings regimen well into your retirement years … what is that? Is it still winning?
A lot of financial planners I’ve talked to say one of their biggest challenges is getting clients to spend money in retirement. Even an appropriate, conservative amount of money. Frugality and savings become such a big part of some people’s identity that they can’t ever switch gears.
I think for some people that’s actually fine. Watching money compound gives them more pleasure than they would get spending it.
But those whose ultimate goal is to stop thinking about money are stuck. Refusing to recognize that you’ve met your goal can be as bad as never meeting the goal to begin with.
4. An emotional attachment to large purchases, particularly a house.
My wife and I pride ourselves on making unemotional financial decisions. But a few years ago we were in the market for our first house. We found one online that we liked, and as we headed out for a tour we promised ourselves we wouldn’t do anything rash – this was just gathering information.
Then we pulled into the driveway and my wife gasped, “I love it!” I did too. We had an infant son – our first – and there was a kids’ tree swing in the front yard. Perfect.
And that was it. Emotion was involved and there was nothing we could do about it.
We have zero regrets – the house really was great. But no one should pretend that you can make life-changing decisions that will massively impact you and your family and treat it like a math problem.
Jason Zweig of The Wall Street Journal once wrote about his mom selling her longtime home:

“I have no emotional attachment to the house; I never liked it physically,” Mom told us. “But everything important that ever happened in our life as a family is here, and I can’t just leave all that behind.”

If I said, “How much are the memories with your kids worth?” you’d say it’s impossible to attach a dollar figure. But if I said, “How much is the home where you formed memories with your kids worth?” or “How much does staying in your local town impact your salary?” you could probably spit out a dollar figure with ease.
Understanding the difference between those two helps explain a lot of spending decisions.
5. The joy of spending can diminish as income rises because there’s less struggle, sacrifice, and sweat represented in purchases.
In his 1903 book The Quest for the Simple Life, William Dawson writes:

The thing that is least perceived about wealth is that all pleasure in money ends at the point where economy becomes unnecessary. The man who can buy anything he covets, without any consultation with his banker, values nothing that he buys.

Consider how you felt when you got your first paycheck from your first job. If you celebrated with as little as a milkshake from Denny’s you probably had a joyous feeling of, “I did this. I bought this. With my own money.” Going from not being able to buy anything to able to buy something is an amazing feeling. The gap between struggle and reward is a big part of what makes people happy.
Contrast that with later in your career, when (hopefully) savings have been built and paychecks have grown. It’s not that spending won’t make you happy – but it won’t be as thrilling and adrenaline-inducing as it was when there was more struggle behind each dollar.
I know a guy with a private chef. He’s served 5-star meals three times a day, an arrangement he’s enjoyed for years. It’s amazing; I’d lie if I said I wasn’t jealous. But I also wonder if the joy diminishes over time. He doesn’t have to struggle to get these meals – there’s no anticipation, no looking forward to a restaurant reservation, no contrasting gap between a “normal” meal and his daily delicacy.
There’s a saying that the best meal you’ll ever taste is a glass of water when you’re thirsty. All forms of spending have that equivalent.
Let me end with a wise quote from, of all people, Richard Nixon:

The unhappiest people of the world are those in the international watering places like the South Coast of France, and Newport, and Palm Springs, and Palm Beach. Going to parties every night. Playing golf every afternoon. Drinking too much. Talking too much. Thinking too little. Retired. No purpose.
So while there are those that would disagree with this and say “Gee, if I could just be a millionaire! That would be the most wonderful thing.” If I could just not have to work every day, if I could just be out fishing or hunting or playing golf or traveling, that would be the most wonderful life in the world – they don’t know life. Because what makes life mean something is purpose. A goal. The battle. The struggle – even if you don’t win it.

6. Asking $3 questions when $30,000 questions are all that matter.
There’s a saying: Save a little bit of money each month, and at the end of the year you’ll be surprised at how little you still have.
Author Ramit Sethi says too many people ask $3 questions (can I afford this latte?) when all that matters to financial success are $30,000 questions (what college should I go to?)
Historian Cyril Parkinson coined a thing called Parkinson’s Law of Triviality. It states: “The amount of attention a problem gets is the inverse of its importance.”
Parkinson described a fictional finance committee with three tasks: approval of a $10 million nuclear reactor, $400 for an employee bike shed, and $20 for employee refreshments in the break room.
The committee approves the $10 million nuclear reactor immediately, because the number is too big to contextualize, alternatives are too daunting to consider, and no one on the committee is an expert in nuclear power.
The bike shed gets considerably more debate. Committee members argue whether a bike rack would suffice and whether a shed should be wood or aluminum, because they have some experience working with those materials at home.
Employee refreshments take up two-thirds of the debate, because everyone has a strong opinion on what’s the best coffee, the best cookies, the best chips, etc.
Many households operate the same.
7. Social aspiration spending: Trickle-down consumption patterns from one socioeconomic group to the next.
Economist Joseph Stiglitz once wrote: “Trickle-down economics may be a chimera but trickle-down behaviorism is very real.”
There is no such thing as an objective level of wealth. Everything is relative to something else. People look around and say, “What’s that person driving, where are they living, what kind of clothes are they wearing?” Aspirations are calibrated accordingly.
I spoke with Wired magazine founding executive editor Kevin Kelly last week. He brought up an interesting point: If you want to know what lower-income groups will aspire to spend their money on in the future, look at what higher-income groups exclusively do today.
European vacations were once the exclusive playground of the rich. Then they trickled down.
Same with college. It was once reserved for the highest income groups. Then it spread.
Same with investing. In 1929 – the peak of the Roaring ‘20s bubble – five percent of Americans owned stocks, virtually all of them the very wealthy. Today, 58% of households own stocks in some form.
Same with two-car households, lawns, walk-in closets, granite countertops, six-burner stoves, jet travel, and even the entire concept of retirement.
Part of the reason these products spread to the masses is that they got cheaper. But the reason they got cheaper is because there was so much demand from the masses – hungered by their aspirations – that pushed companies to innovate new ways of mass production.
People like to mimic others, especially those who appear to be living better lives. Always been like that, always will be.
8. An underappreciation of the long-term cost of purchases, with too much emphasis on the initial price.
It’s common to find someone who bought their home in, say, 1974, for something like $60,000. Today it’s worth perhaps $350,000. The owners no doubt feel they have made the investment of their lives.
But those numbers above equate to an average annual return of 3.75%. Property taxes tend to average roughly 1%, so that brings our real return to 2.75% per year. Maintenance and repairs vary greatly, but spending 1% - 3% of your home’s value per year on upkeep should be expected.
Where does that leave our long-term returns? Ah, quite dim.
Price is easy to calculate. It’s just whatever you paid initially and sold for eventually.
Cost is harder to figure out. They tend to be a slow drip over time, which are easy to ignore but add up quickly.
Same for cars, boats, and hobbies. You can even say the cost of smoking cigarettes is the price of a pack plus the long-term cost of medical care associated with the habit. One is easy to calculate, the other is very difficult.
9. No one is impressed with your possessions as much as you are.
When you see someone driving a nice car, you rarely think, “Wow, the guy driving that car is cool.” Instead, you think, “Wow, if I had that car people would think I’m cool.” Subconsciously or not, this is how people think.
There is a paradox here: people tend to want wealth to signal to others that they should be liked and admired. But in reality those other people often bypass admiring you, not because they don’t think wealth is admirable, but because they use your wealth as a benchmark for their own desire to be liked and admired.
I wrote a letter to my son the day he was born. It says, in part:

You might think you want an expensive car, a fancy watch, and a huge house. But I’m telling you, you don’t. What you want is respect and admiration from other people, and you think having expensive stuff will bring it. It almost never does – especially from the people you want to respect and admire you.

Now, I like nice homes and nice cars as much as anyone. The point here is not to shoo you away from nice things.
It’s just a recognition that no one is as impressed with your stuff as much as you are. Or even that no one is thinking about you as much as you are. They’re busy thinking about themselves!
People generally aspire to be respected and admired by others, and using money to buy fancy things may bring less of it than you imagine. If respect and admiration are your goal, be careful how you seek it. Humility, kindness, and empathy will bring you more respect than horsepower ever will.
10. Not knowing what kind of spending will make you happy because you haven’t tried enough new and strange forms of spending.
Evolution is the most powerful force in the world, capable of transforming single-cell organisms into modern humans.
But evolution has no idea what it’s doing. There’s no guide, no manual, no rulebook. It’s not even necessarily good at selecting traits that work.
Its power is that it “tries” trillions upon trillions of different mutations and is ruthless about killing off the ones that don’t work. What’s left – the winners – stick around.
There’s a theory in evolutionary biology called Fisher’s Fundamental Theorem of Natural Selection. It’s the idea that variance equals strength, because the more diverse a population is the more chances it has to come up with new traits that can be selected for. No one can know what traits will be useful; that’s not how evolution works. But if you create a lot of traits, the useful one – whatever it is – will be in there somewhere.
There’s an important analogy here about spending money.
A lot of people have no idea what kind of spending will make them happy. What should you buy? Where should you travel? How much should you save? There is no single answer to these questions because everyone’s different. People default to what society tells them – whatever is most expensive will bring the most joy.
But that’s not how it works. You have to try spending money on tons of different oddball things before you find what works for you. For some people it’s travel; others can’t stand being away from home. For others it’s nice restaurants; others don’t get the hype and prefer cheap pizza. I know people who think spending money on first-class plane tickets is a borderline scam. Others would not dare sit behind row four. To each their own.
The more different kinds of spending you test out, the closer you’ll likely get to a system that works for you. The trials don’t have to be big: a $10 new food here, a $75 treat there, slightly nice shoes, etc.
Here’s Ramit Sethi again: “Frugality, quite simply, is about choosing the things you love enough to spend extravagantly on—and then cutting costs mercilessly on the things you don’t love.”
There is no guide on what will make you happy – you have to try a million different things and figure out what fits your personality.
11. The social signaling aspect of money, on both things you buy for yourself and charity given to others.
There’s a saying that if you get public recognition for donating money, it’s not charity – it’s philanthropy. And if you demand recognition, it’s not even charity – it’s a business deal. There’s a clear social benefit to you, the giver, in addition to the recipient. I don’t mean that in a negative way: Good donations to worthy causes would plunge if donors didn’t get recognition.
Most forms of spending have two purposes: To bring some sort of utility to the owner, and to signal something to other people.
Homes, cars, clothes, jewelry, obviously fit into that category. But even travel does as well – how many vacation destinations are picked at least in part by what you think will make a good Instagram picture, or just that it sounds cool. (My guess is most Bali vacations fall into that category).
Psychologist Jonathan Haidt says people don’t communicate on social media; they perform for one another. Spending money is like that, too.
It’s not always a bad thing. If you’ve merely thought about what clothes you’ll look best in before you leave in the morning, you’ve engaged in signaling. And it’s not always about looking the best: intentionally dressing casually to a formal meeting sends a powerful message about who holds the power. Before being caught as a sham, Sam Bankman-Fried said he intentionally didn’t wear pants to create a mystique.
The thing to recognize is that spending money “on yourself” is often done with the intent of influencing what other people think.
That should spark three questions: Whose opinion are you trying to influence, why, and are those people even paying attention?
12. The social hierarchy of spending, positioning you against your peers.
An old joke is about two hikers who come across a grizzly bear in the woods. One starts to run, and the other yells, “Are you crazy, you can’t outrun a bear!” The runner replies: “I don’t have to be faster than the bear. I only have to be faster than you.”
All success is simply relative to someone else – usually those around you.
That’s important for spending money, because for so many people the question of whether you’re buying nice things is actually, “are your things nicer than other peoples’ things?” The question of whether your home is big enough is actually, “is your home bigger than your neighbor’s?”
Not only is the urge to one-up your peers, but you may feel the need to continually surpass your own spending. Is this year’s vacation more expensive than last year’s? Is the next car fancier than the old one?
Money to some people is less of an asset and more of a social liability, indebting them to a status-chasing life that can leave them miserable.
It’s a dangerous trap if you don’t recognize the game and how it’s played. Montesquieu wrote 275 years ago, “If you only wished to be happy, this could be easily accomplished; but we wish to be happier than other people, and this is always difficult, for we believe others to be happier than they are.”
13. Spending can be a representation of how hard you’ve worked and how much stress went into earning your paycheck.
Someone who works 100 hours a week and hates their job may have an urge to spend frivolously in an attempt to compensate for the misery of how their paycheck was earned.
Never have I seen money burn a hole in someone’s pocket faster than an investment banker receiving their annual bonus. After 12 months of Excel modeling until 3am, you have an urge to prove to yourself that it was worth it, offsetting what you sacrificed. It’s like someone held underwater for a minute – they do not take a calm breath when they surface; they gasp.
The opposite can hold true. I can only back this up with anecdotal experiences, but those most capable of delayed gratification are often those who enjoy their work. The pay might be good, but the urge to compensate for your hard work with heavy spending isn’t there.
Spending money to make you happy is hard if you’re already happy.
More on this topic:


My book, The Psychology of Money


Lifestyles


Getting Wealthy vs. Staying Wealthy




SHARE Copy Link 















Sign up for more Collab Fund content

Email address












More from the blog…





  by
  
    
  
  — 
  














  by
  
    
  
  — 
  














  by
  
    
  
  — 
  

















Collab Fund

      Collaborative is a leading source of capital for big ideas pushing the world forward.
    



Newsletter
Sign up for updates ↗








Twitter
Follow @collabfund ↗








RSS
Subscribe to the blog ↗











About


Shared Future


SOS


Currency


Public


Blog


Site Credits







Collaborative Fund Management LLC, Collaborative Holdings Management LP and Collab+Currency Management, LLC are distinct investment advisory entities, are not a unitary enterprise and operate independently of one another.  From time to time Collaborative Fund Management LLC may draw on its relationship with Collaborative Holdings Management LP and/or Collab+Currency Management, LLC, but only to the extent consistent with its status as a separate investment adviser.






"
https://news.ycombinator.com/rss,TensorFlow for Python is dying?,https://thenextweb.com/news/why-tensorflow-for-python-is-dying-a-slow-death,Comments,"









                                    Story by
                                


                                        Ari Joury
                                    




Religious wars have been a cornerstone in tech. Whether it’s debating about the pros and cons of different operating systems, cloud providers, or deep learning frameworks — a few beers in, the facts slide aside and people start fighting for their technology like it’s the holy grail.
Just think about the endless talk about IDEs. Some people prefer VisualStudio, others use IntelliJ, again others use plain old editors like Vim. There’s a never-ending debate, half-ironic of course, about what your favorite text editor might say about your personality.
Similar wars seem to be flaring up around PyTorch and TensorFlow. Both camps have troves of supporters. And both camps have good arguments to suggest why their favorite deep learning framework might be the best.
Get your tickets for TNW Valencia in March!The heart of tech is coming to the heart of the Mediterranean
Join now
That being said, the data speaks a fairly simple truth. TensorFlow is, as of now, the most widespread deep learning framework. It gets almost twice as many questions on StackOverflow every month as PyTorch does.
On the other hand, TensorFlow hasn’t been growing since around 2018. PyTorch has been steadily gaining traction until the day this post got published.
For the sake of completeness, I’ve also included Keras in the figure below. It was released at around the same time as TensorFlow. But, as one can see, it’s tanked in recent years. The short explanation for this is that Keras is a bit simplistic and too slow for the demands that most deep learning practitioners have.
PyTorch is still growing, while TensorFlow’s growth has stalled. Graph from StackOverflow trends.
StackOverflow traffic for TensorFlow might not be declining at a rapid speed, but it’s declining nevertheless. And there are reasons to believe that this decline will become more pronounced in the next few years, particularly in the world of Python.
PyTorch feels more pythonic
Developed by Google, TensorFlow might have been one of the first frameworks to show up to the deep learning party in late 2015. However, the first version was rather cumbersome to use — as many first versions of any software tend to be.
That is why Meta started developing PyTorch as a means to offer pretty much the same functionalities as TensorFlow, but making it easier to use.
The people behind TensorFlow soon took note of this, and adopted many of PyTorch’s most popular features in TensorFlow 2.0.
A good rule of thumb is that you can do anything that PyTorch does in TensorFlow. It will just take you twice as much effort to write the code. It’s not so intuitive and feels quite un-pythonic, even today.
PyTorch, on the other hand, feels very natural to use if you enjoy using Python.
PyTorch has more available models
Many companies and academic institutions don’t have the massive computational power needed to build large models. Size is king, however, when it comes to machine learning; the larger the model the more impressive its performance is.
With HuggingFace, engineers can use large, trained and tuned models and incorporate them in their pipelines with just a few lines of code. However, a staggering 85% of these models can only be used with PyTorch. Only about 8% of HuggingFace models are exclusive to TensorFlow. The remainder is available for both frameworks.
This means that if you’re planning to use large models, you’d better stay away from TensorFlow or invest heavily in compute resources to train your own model.
PyTorch is better for students and research
PyTorch has a reputation for being appreciated more by academia. This is not unjustified; three out of four research papers use PyTorch. Even among those researchers who started out using TensorFlow — remember that it arrived earlier to the deep learning party — the majority have migrated to PyTorch now.
These trends are staggering and persist despite the fact that Google has quite a large footprint in AI research and mainly uses TensorFlow.
What’s perhaps more striking about this is that research influences teaching, and therefore defines what students might learn. A professor who has published the majority of their papers using PyTorch will be more inclined to use it in lectures. Not only are they more comfortable teaching and answering questions regarding PyTorch; they might also have stronger beliefs regarding its success.
College students therefore might get much more insights about PyTorch than TensorFlow. And, given that the college students of today are the workers of tomorrow, you can probably guess where this trend is going…
PyTorch’s ecosystem has grown faster
At the end of the day, software frameworks only matter insofar as they’re players in an ecosystem. Both PyTorch and TensorFlow have quite developed ecosystems, including repositories for trained models other than HuggingFace, data management systems, failure prevention mechanisms, and more.
It’s worth stating that, as of now, TensorFlow has a slightly more developed ecosystem than PyTorch. However, keep in mind that PyTorch has shown up later to the party and has had quite some user growth over the past few years. Therefore one can expect that PyTorch’s ecosystem might outgrow TensorFlow’s in due time.
TensorFlow has the better deployment infrastructure
As cumbersome as TensorFlow might be to code, once it’s written is a lot easier to deploy than PyTorch. Tools like TensorFlow Serving and TensorFlow Lite make deployment to cloud, servers, mobile, and IoT devices happen in a jiffy.
PyTorch, on the other hand, has been notoriously slow in releasing deployment tools. That being said, it has been closing the gap with TensorFlow quite rapidly as of late.
It’s hard to predict at this point in time, but it’s quite possible that PyTorch might match or even outgrow TensorFlow’s deployment infrastructure in the years to come.
TensorFlow code will probably stick around for a while because it’s costly to switch frameworks after deployment. However, it’s quite conceivable that newer deep learning applications will increasingly be written and deployed with PyTorch.
TensorFlow is not all about Python
TensorFlow isn’t dead. It’s just not as popular as it once was.
The core reason for this is that many people who use Python for machine learning are switching to PyTorch.
But Python is not the only language out there for machine learning. It’s the O.G. of machine learning, and that’s the only reason why the developers of TensorFlow centered its support around Python.
These days, one can use TensorFlow with JavaScript, Java, and C++. The community is also starting to develop support for other languages like Julia, Rust, Scala, and Haskell, among others.
PyTorch, on the other hand, is very centered around Python — that’s why it feels so pythonic after all. There is a C++ API, but there isn’t half the support for other languages that TensorFlow offers.
It’s quite conceivable that PyTorch will overtake TensorFlow within Python. On the other hand, TensorFlow, with its impressive ecosystem, deployment features, and support for other languages, will remain an important player in deep learning.
Whether you choose TensorFlow or PyTorch for your next project depends mostly on how much you love Python.
This article was written by Ari Joury and was originally published on Medium. You can read it here. 


Get the TNW newsletter
Get the most important tech news in your inbox each week.


                                    Follow @thenextweb
                                


Also tagged with



Python




                                Published January 13, 2023 - 2:47 pm UTC

Back to top





















"
https://news.ycombinator.com/rss,Need for speed: static analysis version,https://semgrep.dev/blog/2022/static-analysis-speed,Comments,"Need for speed: static analysis versionLog inSign up freeRegistryPlaygroundProductsSemgrep AppManage and enforce code standards across your organization. Get started for free.Semgrep Supply ChainFind dependency vulnerabilities in your code.Featured docsGetting started with Semgrep Supply ChainStart finding high-priority security issues in your dependenciesPricingResourcesDocsWant to read all the docs? Start hereTutorialLearn to write Semgrep rules in under 10 minutesBlogGet the latest news about SemgrepContact UsWant to talk to a human? Get in touch with us!Latest blog postsIntroducing Semgrep Supply Chain.Find reachable vulnerable dependencies in your codeGeneral availability support of PHPSemgrep adds PHP support including 40+ new rulesDemystifying taint modeA user-friendly guide to writing rules with Semgrep's taint modeAll blog postsLog inSign up freeBlogdevelopmentNeed for speed: static analysis versiondevelopmentNeed for speed: static analysis versionWhy speed is important in static analysis and how Semgrep achieves ludicrous speedBrandon WuNovember 29, 2022In this articleLudicrous graphsStatic analysis at scaleKnowing is half the battleA unique nicheFindings, fasterSemantics, speedilyConclusionSubscribe to our blogShareShareTL;DR: Semgrep has achieved remarkably fast scan times by prioritizing speed using methods like taint summaries and tree matching in OCaml. In addition, Semgrep’s design as a tool that searches for syntax makes it fast due to designs like purely textual single-file analysis, partial parsing, and optimizations like skipping files that cannot produce matches.Program analysis is an extremely interesting discipline that aims to combine an impossible task (finding undesirable parts of programs) with practical usage (being useful to developers to fix their code). Practical usage takes many forms ranging from convenience of information and quality of findings to the speed at which the analysis is carried out.At r2c, we have one motto which we stick to — “code analysis at ludicrous speed”. After almost 3 years of development, a question remains—what goes into making a code analysis product that can run at “ludicrous speed”, and have we achieved that goal with Semgrep?Ludicrous graphsHow do we qualify “ludicrous speed”? Some results for Semgrep’s speed can be seen here, in graphic form:Here is a graph of Semgrep’s scan time (in seconds) for the Django Python repository, over time. This data serves as a direct reflection of Semgrep’s growth over the past year, as various optimizations and engine upgrades have been carried out:Figure 1: Semgrep scan time (in seconds) for Django Python repository - sourceAnd for the lodash JavaScript repository:Figure 2: Semgrep scan time (in seconds) for lodash JavaScript repository - sourceHere’s the performance of Semgrep on all of its benchmarking repositories over time:Figure 3: Semgrep scan time for different repositoriesOver time, Semgrep has been making a consistent effort towards increased performance. In all benchmarked cases, that scan time using the latest Semgrep version takes place in less than 20 seconds, which is a significantly short enough period to run within a developer’s normal commit workflow.Here’s some data validating Semgrep’s run-time (in Python, on the Django repository) against some other open-source Python analysis tools. All data are averaged and sourced from tests run on an M1 Mac machine.Figure 4: Semgrep scan time as compared to other Python analysis tools on Django repositoryIn this graph, we see that Semgrep performs quite fast, beating out the other tools. It’s worth noting that pylint and flake8 are linting tools, which primarily work in the realm of style enforcement, which notably is not concerned with the behavior of the program, like Semgrep. With features like taint analysis, constant propagation, and dataflow analysis, it’s a fair description that Semgrep performs more computationally intensive analysis than the other options. More than just curiosity, Semgrep’s speed has made it feasible to be run by existing organizations in production to shift left.Static analysis at scaleWhat makes a static analysis (SAST) tool fast? Well, it’s useful to look at what may make a static analysis tool slow. SAST applications have to be able to process large amounts of source code, break it down into a format suitable for analysis, and then run detailed semantic scans on it.This analysis may also be done in a dynamic fashion, where programs are instrumented to detect certain faults during their runtime, but this has the disadvantage of adding extra overhead to the analyzed program, as well as operating closer to the hardware level, as opposed to the language level. In this article, we will focus on static analysis, and stay closer to the language level, which will yield dividends later on with Semgrep.Static analysis is inherently hard because it tries to find answers to questions about program behavior — about programs that may run for a very long time, if not forever. Given that it is static, this analysis must be done without running the program, so any actual evaluation is a non-starter. How can we make such a problem tractable?The way that this generally takes place is in approximation. While we cannot run the program itself to find out what is actually happening, standard techniques allow us to gain (possibly imprecise) knowledge of the state of the program. In particular, dataflow analysis, a classic technique, involves an iterative scan over all possible program points to find out properties that may be true at those points.In order to facilitate this dataflow analysis, static analysis tools need to know something about what paths the program may take during execution. This is achieved by computing a control-flow graph, which is a graph that connects the various parts of the code which may execute after each other. Given a project with many functions, conditionals, and program text in general, however, looping over the entire control-flow graph of a program is not a trivial task. How does this be done in a more optimal way?Knowing is half the battleThe general mantra that Semgrep follows to facilitate its success is that it only picks battles that it can win.Program analysis is a never-ending uphill slope because analyses can always be done more deeply, and more compute time can always be thrown into figuring out more things about the program's behavior. In practice, however, for the majority of applications, program analysis need not be particularly deep or theoretically based to be effective in general.From the beginning of Semgrep, speed has been a major focus. To make sure we stay in line with that, from the beginning, we make sure that we only support features when we know that Semgrep can win, or in other words, that it can be done in a fast way.A unique nicheIn a way, philosophically, Semgrep’s original purpose was in line with this way of thinking. Semgrep occupies a unique niche as a tool that straddles the line between syntactic and semantic, however, it used to be more towards the former. It started as sgrep at Facebook by r2c’s own Yoann Padioleau, an open source tool that was to match program text in a semantically-aware way, but which lacked some of the modern-day features Semgrep possesses, such as constant propagation and taint analysis.This original focus granted Semgrep a unique perspective on program analysis, as sgrep didn’t need to solve many of the problems that other SAST applications aimed to do. Since it was a matching tool, there was no need to be aware of code flow at all, which is the main bottleneck in terms of program analysis. Since it only focused on text, there was no need to have any kind of understanding of programs beyond single files—necessarily, there was no need even to require that analyzed code compiled. This also granted other advantages, such as partial parsing, where programs that are not syntactically correct can be parsed to trees that look “close enough”. This overall had the advantage of making Semgrep an extremely versatile and robust tool, from the get-go.In addition, the unique capabilities of Semgrep as a tool for code review automation, beyond just vulnerability-finding, necessitated that it be able to run quickly. In general, code analysis can occur on a nightly basis, with the goal of reporting any possible errors by the beginning of the next morning, so that engineers can triage those findings. This gives a sizeable 12-hour period for static analysis, which permits a large range of complex inquiries. Semgrep’s role as a customizable tool that catches vulnerabilities on each pull request means that it isn’t working in nearly the same time frame—developers need to be able to interface with it as a regular part of their workflow of merging commits and writing code. This gives an upper limit of minutes, as opposed to hours, for Semgrep. So not only does Semgrep only pick battles it can win—it must.Findings, fasterSpeed is an admirable goal for any static analysis tool. A more interesting question, however, is how this is achieved.In a similar sense, the fact that Semgrep lives so close to the syntax of a program helps again. One of the most helpful improvements made for Semgrep’s speed was in recognizing this. Whereas an arbitrary static analysis may not know specifically where a bug may occur and thus have to check all of a given program, Semgrep rules are typically written with some amount of the desired text to find—for instance, they may contain the name of a sensitive function or some particular primitive in a language.This characteristic made it possible to speed up Semgrep scans by only searching files that are known to contain literal text which matches that in the patterns. For instance, consider the following Semgrep rule which looks for a hardcoded tmp directory within an open:source: Semgrep ruleThe pattern which this rule looks for involves a call to open, which can only occur if the literal string open occurs anywhere in the given file. This can easily be tested in linear time, resulting in Semgrep being able to skip files that are known to be impossible to produce a match in, due to this property, which significantly improves scan times. In this case, it’s interesting how purely syntactic searches can supplement more semantic searches!Another significant speed benefit occurs from the inherent nature of the problem. Semgrep’s core engine is written in OCaml, a functional programming language because functional languages are ideal for the kind of structural decomposition on recursive data (the target program) that Semgrep’s main matching engine needs to do. This engine is used to provide raw matching data to the Python wrapper, which would then do the work of combining and analyzing the matches using the rule’s pattern combinators. This work is merely more structural decomposition on recursive data (the pattern), however, and another performance boost was gained upon porting that section of the logic to OCaml.Semantics, speedilyTree matching has a nearly negligible cost when compared to most deep program analysis techniques, such as pointer analysis or symbolic execution, so this was clearly a winning battle. As Semgrep grew more advanced, more features were added which caused it to err closer to the side of semantics, such as taint analysis and constant propagation.These analyses are not necessarily ones that can be done quickly. Taint analysis, in particular, requires running dataflow analysis over the entire control flow of a program, which can potentially be huge, when considering how large an entire codebase may be, with all of its function calls and tricky control flow logic. To do taint analysis in this way would be to pick a losing battle.Semgrep succeeds in that it only carries out single-file analysis, so the control flow graph never exceeds the size of a file. In addition, taint can be done incrementally. Functions have well-defined points where they begin and end, as well as generally well-defined entrances in terms of the data they accept (the function arguments). Thus, Semgrep collects taint summaries, which essentially (per function) encode the information about what taint may be possible, depending on the taint of the inputs that flow in.So for instance, given a function in Python:1def foo(a, b):
2
3    sink(a)
4
5    return NoneA taint summary for this function foo will note that, if the input a is tainted, then it will reach the tainted sink sink. Regardless of how the function foo is used, this is a fact about the function’s potential use. Then, at the call site to the function, if the input a is tainted, then we know to report a finding.This seems simple, but the end result is that we only ever need to run a dataflow analysis on the code of each function once. Never does a taint summary need to be collected for a given function more than once, meaning that we can simply stitch all these facts together at the end, making for speedy results. The core lesson is that, by collecting summaries and doing taint in an intelligent way, we pick the battle that we can win. It turns out that for our upcoming inter-file extension to Semgrep, by applying this approach in an inter-file manner, we can still reap the speed benefits. This lets us avoid running dataflow analysis on an entire control-flow graph, and instead do small, localized analyses.ConclusionAt the end of the day, solving an undecidable problem at a pace that is useful to security engineers is a hard task. Harder still is solving an undecidable problem at a pace that is useful to the developer, such that it can fit into the normal cycle of writing code and pushing commits. Semgrep’s unique philosophy as a tool has made it capable of bridging this gap, and over time, has proven it to have a speed that is nothing short of ludicrous.AboutSemgrep is a fast, open-source, static analysis tool for finding bugs, detecting vulnerabilities in third-party dependencies, and enforcing code standards.Learn more with Semgrep’s blogStart ScanningBrowse PostsAnnouncementJune 22, 20222 min readAnnouncing Semgrep's general availability support of PHPPablo EstradaAnnouncementMay 11, 20225 min readSemgrep's May 2022 updates: Introducing DeepSemgrep, plus new Playground, and self managed GitHub + GitLab support!Chinmay GaikwadBest practicesOctober 01, 20215 min readProtect Your GitHub Actions with SemgrepGrayson HardawayCode scanning at ludicrous speedFind Bugs and Enforce Code StandardsStart ScanningBook a DemoCode analysis at ludicrous speedStay up to dateTwitterSlackGitHubYouTubeResourcesDocsTutorialBlogAbout usContact usProductsSemgrep AppSemgrep Supply ChainPricingTwitterSlackGitHubYouTube© 2023 r2c. Semgrep is a registered trademark of r2c.Semgrep jobsTermsPrivacy"
https://news.ycombinator.com/rss,A cab ride I'll never forget (1999),https://kentnerburn.com/the-cab-ride-ill-never-forget/,Comments,"





















The Cab Ride I'll Never Forget | Kent Nerburn













































































 









		Skip to content













					Kent Nerburn
				


				wandering, wondering, writing
			
 





About

Menu Toggle





Interviews


Photo Gallery


Books
Speaking | Book Clubs
Musings
Shop
Contact
Home
 







 










					Kent Nerburn
				


				wandering, wondering, writing
			
 







Main Menu

 









About

Menu Toggle

InterviewsBook Review: Native EchoesBooksBooks-oldContactDan and Grover talk about Indian MascotsDancing with the Gods: Reflections on Life and ArtKent Nerburn

Menu Toggle

Join our mailing listMusingsPhoto GalleryPrivacyShopSouth Dakota TravelogueSpeaking

Menu Toggle

Presentation OptionsSubscribeThe Cab Ride I’ll Never Forget 

















 










The Cab Ride I'll Never Forget 




There was a time in my life twenty years ago when I was driving a cab for a living. It was a cowboy’s life, a gambler’s life, a life for someone who wanted no boss, constant movement and the thrill of a dice roll every time a new passenger got into the cab.What I didn’t count on when I took the job was that it was also a ministry. Because I drove the night shift, my cab became a rolling confessional. Passengers would climb in, sit behind me in total anonymity and tell me of their lives.We were like strangers on a train, the passengers and I, hurtling through the night, revealing intimacies we would never have dreamed of sharing during the brighter light of day. I encountered people whose lives amazed me, ennobled me, made me laugh and made me weep. And none of those lives touched me more than that of a woman I picked up late on a warm August night.I was responding to a call from a small brick fourplex in a quiet part of town. I assumed I was being sent to pick up some partiers, or someone who had just had a fight with a lover, or someone going off to an early shift at some factory for the industrial part of town.When I arrived at the address, the building was dark except for a single light in a ground-floor window. Under these circumstances, many drivers would just honk once or twice, wait a short minute, then drive away. Too many bad possibilities awaited a driver who went up to a darkened building at 2:30 in the morning.But I had seen too many people trapped in a life of poverty who depended on the cab as their only means of transportation. Unless a situation had a real whiff of danger, I always went to the door to find the passenger. It might, I reasoned, be someone who needs my assistance. Would I not want a driver to do the same if my mother or father had called for a cab?So I walked to the door and knocked.“Just a minute,” answered a frail and elderly voice. I could hear the sound of something being dragged across the floor. After a long pause, the door opened. A small woman somewhere in her 80s stood before me. She was wearing a print dress and a pillbox hat with a veil pinned on it, like you might see in a costume shop or a Goodwill store or in a 1940s movie. By her side was a small nylon suitcase. The sound had been her dragging it across the floor.The apartment looked as if no one had lived in it for years. All the furniture was covered with sheets. There were no clocks on the walls, no knickknacks or utensils on the counters. In the corner was a cardboard box filled with photos and glassware.“Would you carry my bag out to the car?” she said. “I’d like a few moments alone. Then, if you could come back and help me? I’m not very strong.”I took the suitcase to the cab, then returned to assist the woman. She took my arm, and we walked slowly toward the curb. She kept thanking me for my kindness.“It’s nothing,” I told her. “I just try to treat my passengers the way I would want my mother treated.”“Oh, you’re such a good boy,” she said. Her praise and appreciation were almost embarrassing.When we got in the cab, she gave me an address, then asked, “Could you drive through downtown?”“It’s not the shortest way,” I answered.“Oh, I don’t mind,” she said. “I’m in no hurry. I’m on my way to a hospice.”I looked in the rearview mirror. Her eyes were glistening. “I don’t have any family left,” she continued. “The doctor says I should go there. He says I don’t have very long.”I quietly reached over and shut off the meter. “What route would you like me to go?” I asked.For the next two hours we drove through the city. She showed me the building where she had once worked as an elevator operator. We drove through the neighborhood where she and her husband had lived when they had first been married. She had me pull up in front of a furniture warehouse that had once been a ballroom where she had gone dancing as a girl. Sometimes she would have me slow in front of a particular building or corner and would sit staring into the darkness, saying nothing.As the first hint of sun was creasing the horizon, she suddenly said, “I’m tired. Let’s go now.”We drove in silence to the address she had given me. It was a low building, like a small convalescent home, with a driveway that passed under a portico. Two orderlies came out to the cab as soon as we pulled up. Without waiting for me, they opened the door and began assisting the woman. They were solicitous and intent, watching her every move. They must have been expecting her; perhaps she had phoned them right before we left.I opened the trunk and took the small suitcase up to the door. The woman was already seated in a wheelchair.“How much do I owe you?” she asked, reaching into her purse.“Nothing,” I said.“You have to make a living,” she answered.“There are other passengers,” I responded.Almost without thinking, I bent and gave her a hug. She held on to me tightly. “You gave an old woman a little moment of joy,” she said. “Thank you.”There was nothing more to say. I squeezed her hand once, then walked out into the dim morning light. Behind me, I could hear the door shut. It was the sound of the closing of a life.I did not pick up any more passengers that shift. I drove aimlessly, lost in thought. For the remainder of that day, I could hardly talk. What if that woman had gotten an angry driver, or one who was impatient to end his shift? What if I had refused to take the run, or had honked once, then driven away? What if I had been in a foul mood and had refused to engage the woman in conversation? How many other moments like that had I missed or failed to grasp?We are so conditioned to think that our lives revolve around great moments. But great moments often catch us unawares. When that woman hugged me and said that I had brought her a moment of joy, it was possible to believe that I had been placed on earth for the sole purpose of providing her with that last ride.I do not think that I have ever done anything in my life that was any more important. 































 







Copyright © 2023 Kent Nerburn | Powered by kincaid-burrows
 










































"
https://news.ycombinator.com/rss,Intel Core i9-13900T CPU benchmarks show faster than 12900K 125W performance,https://wccftech.com/intel-core-i9-13900t-cpu-benchmarks-show-faster-than-12900k-125w-performance-at-35w/,Comments,"

HardwareReport
Intel Core i9-13900T CPU Benchmarks Show Faster Than 12900K 125W Performance at 35W

Hassan Mujtaba •
Jan 14, 2023 02:44 PM EST

•
Copy Shortlink
























Intel recently introduced brand new 13th Gen T-series chips which feature the Core i9-13900T that operates at a 35W TDP. The new chip has been benchmarked within Geekbench 5 and showcases impressive performance given its limited power budget.
Intel's 13th Gen Core i9-13900T 35W CPU Beats The 125W Core i9-12900K In Geekbench 5 Benchmark
Starting with the specifications, the Intel Core i9-13900T is a variation of the Core i9-13900 series that comes with a limited TDP design. While the standard chips boast 125W TDP in the unlocked and 65W TDP on the Non-K SKUs, the T-series chip is limited to a 35W TDP.  The Unlocked CPU is rated at up to 253W, the Non-K is rated at up to 219W while the T-series chip is rated at up to 106 Watts which is less than half the power budget of its higher-end siblings.
Related StoryHassan MujtabaIntel Core i9-13900KS, World’s First 6 GHz CPU, Now Available For $699 USThe Intel Core i9-13900T retains the same core configuration with 24 cores that are made up of 8 P-Cores and 16 E-Cores with 32 threads, a base clock of 1.10 GHz, a boost of up to 5.30 GHz & 68 MB of cache (L2+L3). The CPU also comes at a slightly lower price point of $549.00 US. Now the CPU is tested within the Geekbench 5 benchmark using an ASUS TUF Gaming B660M-PLUS WIFI board and coupled with 64 GB of DDR5 memory.

The CPU scored 2178 points in the single-core and 17339 points in the multi-core tests. We used the Intel Core i9-12900K for comparison which scores 1901 points in single-core and 17272 points in multi-core tests. This puts the Intel Core i9-13900T up to 15% faster in single-core and slightly faster in multi-threaded tests which is very impressive considering the Core i9-12900K also has a higher 125W base TDP (3.58x higher) and a peak TDP rating of 241W (2.27x higher).

Intel Core i9-13900KS Single-Thread CPU Benchmark (Geekbench 5)

Single-Core


050010001500200025003000




050010001500200025003000





Core i9-13900KS

2.3k


Core i9-13900K

2.2k


Ryzen 9 7900X

2.2k


Ryzen 9 7950X

2.2k


Ryzen 7 7700X

2.2k


Core i9-13900T

2.2k


Ryzen 5 7600X

2.2k


Ryzen 9 7900

2.1k


Core i9-13900

2.1k


Ryzen 7 7700

2.1k


Core i9-12900KS

2.1k


Core i9-13900HX

2k


Ryzen 5 7600

2k


Core i7-13700K

2k


Core i5-13600K

1.9k


Core i9-12900K

1.9k


Core i7-12700K

1.9k


M2 Max

1.9k


M1 Max

1.8k


Core i5-12600K

1.7k


Ryzen 9 5950X

1.7k


Ryzen 7 5800X

1.7k


Ryzen 9 5900X

1.7k


Ryzen 5 5600X

1.6k







Intel Core i9-13900KS Multi-Thread CPU Benchmark (Geekbench 5)

Multi-Core


050001000015000200002500030000




050001000015000200002500030000





Core i9-13900KS

26.8k


Core i9-13900K

24.3k


Ryzen 9 7950X

24.4k


Core i9-13900HX

20.9k


Core i9-13900

20.1k


Core i7-13700K

19.8k


Ryzen 9 7900X

19.3k


Core i9-12900KS

19k


Ryzen 9 7900

18.6k


Core i9-13900T

17.3k


Core i9-12900K

17.3k


Ryzen 9 5950X

16.5k


Core i5-13600K

16.1k


M2 Max

14.6k


Core i7-12700K

14.1k


Ryzen 7 7700X

14.1k


Ryzen 9 5900X

14k


Ryzen 7 7700

12.7k


M1 Max

12.3k


Core i5-12600K

11.6k


Ryzen 5 7600X

11.4k


Ryzen 5 7600

11.3k


Ryzen 7 5800X

10.3k


Ryzen 5 5600X

8.2k






This goes off to show the immense efficiency that Intel's 10nm ESF process node and the new hybrid architecture packs and we will also get to see some similar results with the mobility lineup, especially the 13th Gen HX parts which are going to ship in enthusiast-grade gaming laptops in the coming months. AMD also introduced its brand new 65W Ryzen 7000 Non-X CPUs which have been showcasing some impressive efficiency feats on their own with the Zen 4 core architecture.
News Source: Benchleaks
				
				Share this story
				 Facebook
 Twitter






Deal of the Day











Further Reading




 AMD Ryzen 9 7950X3D CPU Shown To Beat Intel Core i9-13900K In Games With Up To 24% Lead


 Intel Core i9-13980HX CPU Powered MSI Raider GE78HX Laptop Matches High-End Desktop CPUs In Performance


 Intel Core i9-13980HX Flagship Raptor Lake-HX CPU Spotted In ASUS’s Next-Gen ROG STRIX Laptop


 It’s Over 9000! Intel Core i9-13900KS Becomes The First CPU To Achieve 9 GHz Frequency World Record













Comments




Please enable JavaScript to view the comments.










Trending Stories


NASA Captures Star Eaten By Black Hole 300 Million Light Years Away



				91 Active Readers



Intel Core i9-13900T CPU Benchmarks Show Faster Than 12900K 125W Performance at 35W



				91 Active Readers



SpaceX’s Rockets Split Up In Mid Air For Rare & Stunning Views At 5,000 Km/h+



				41 Active Readers



PlayStation 5 Vertical Orientation Issue Clarified by Technician; Issue Happens on “Unopened” Consoles



				38 Active Readers



Apple M2 Max vs. M1 Max – Manufacturing Process, Specifications, And Upgrades Differences That You Should Know



				24 Active Readers








Popular Discussions


AMD Radeon RX 7900 XTX Failure Rates Reportedly At 11%, RMA’s Piling Up But Users Not Receiving Cards



				3116 Comments



AMD Radeon RX 6000 GPUs Mysteriously Start Dying, German Repair Shop Receives 48 Cards With Cracked Chips



				3020 Comments



Intel Lunar Lake To Feature A Brand New CPU Architecture Built From The Ground-Up, Perf/Watt Focused at Mobile



				2757 Comments



AMD To Give The Love of 3D V-Cache This Valentines With Its Ryzen 7000 X3D CPUs Launch



				2021 Comments



Intel Arc A770 Performs Above AMD & NVIDIA In DirectStorage 1.1 Performance Benchmark



				1797 Comments








	 







"
https://news.ycombinator.com/rss,Conditional CSS,https://ishadeed.com/article/conditional-css/,Comments,"







    Conditional CSS -
    Ahmad Shadeed
  























🎉I published a book about debugging
      CSS.
      Buy now



























Ahmad Shadeed






Home


Articles


Snippets


Journal


About


Hire me



















@shadeed9




Conditional CSS
09 Jan 2023

			

			
			Reading time: ~ 20 mins
		



I like to think of CSS as a conditional design language. Over the years, CSS was known as a way to style web pages. Now, however, CSS has evolved a lot to the point you can see conditional rules. The interesting bit is that those CSS rules aren’t direct (i.e: there is still no if/else in CSS), but the way features in CSS work is conditional.
Design tools like Figma, Sketch, and Adobe XD made a huge improvement for us designers, but they still lack a lot of the flexibility that CSS has.
In this article, I will go over a few CSS features that we use every day, and show you how conditional they are. In addition to that, I will compare a few examples where CSS is much more powerful than design tools.
What is conditional CSS?
In simple words, it’s about design that has certain conditions. When one or more conditions are met, the design is subject to change due to that.
For example, adding a new section to a design must push the other elements underneath it. In the following figure, we have a stack of items on the left. When adding a new one, the other items below it must move down.




Logically, that sounds expected and normal. In design tools, we got this a few years ago. In Figma, we have “Auto Layout” features that do the above. On the web, we have had that from day 1, even without CSS at all.
Conditional CSS
You might be thinking about what the heck conditional CSS is. Is that even a thing? No, there hasn’t been a direct “if” statement in CSS.
The main thing to distinguish is that some CSS properties work in specific conditions or scenarios. For example, when using the CSS :empty selector to check if an element is empty or not, it’s a conditional pseudo selector.
.alert p:empty {
  display: none;
}

If I want to explain the above to my 2 years old daughter, I will do it like this:

If there is nothing here, it will disappear.

Did you notice the if statement here? This is conditional design indirectly. In the following section, I’m going to explore a few CSS features which work similarly to an if/else statement.
The goal? To have a stronger idea and expectation about the CSS you wrote. I mean, you will be able to spot conditional CSS by just looking at the CSS for a component, a section, or a page.
CSS versus Figma
Why Figma? Well, I consider it as the standard for UX design these days, I thought it’s a good idea to do my comparison based on it. I want to share a simple example. There is list of tags that are displayed horizontally.




When you think deeply about it, you will spot some major differences. For example, the CSS version:

Can wrap into a new lines if there is no enough space.
Works with both LTR and RTL directions.
The gap will be used for rows when the items wrap.

Figma doesn’t have any of the above.
In CSS, there are three conditional rules happening:

If flex-wrap is set to wrap, then the items can wrap when there is no available space.
When the items wrap into a new line, the gap will work for the horizontal and vertical spaces.
If the page direction is RTL (right-to-left), the items will switch their order (e.g: design will be the first one from the right).

This is just one example, and I can write a book like that. Let’s explore a few cases where CSS can be conditional.
Conditional CSS examples
Media query
We can’t talk about conditional CSS without mentioning CSS media queries. The CSS spec is named CSS Conditional Rules Module. To be honest, this is the first time that I learn about that title.
When I did my research about who asks or mentions “Conditional CSS”, I found more than one time that media queries are the closest thing to an “if” statement in CSS.
.section {
  display: flex;
  flex-direction: column;
}

@media (min-width: 700px) {
  .section {
    flex-direction: row;
  }
}


If the viewport width is 700px or larger, change the flex-direction of .section to column. That’s explicit if statement, isn’t it?

The same thing can apply to media queries like @media (hover: hover). In the following CSS, the hover style will be applied only if the user is using a mouse or a trackpad.
@media (hover: hover) {
  .card:hover {
    /* Add hover styles.. */
  }
}

Size container query
With container queries, we can check if the parent of a component has a specific size and style the child component accordingly.




.card-wrapper {
  container-type: inline-size;
}

@container (min-width: 400px) {
  .card {
    display: flex;
    align-items: center;
  }
}

I have written about container queries multiple times, and have a place where I share demos about it.
Style container query
At the time of writing this article, this is behind a flag in Chrome Canary and is intended to ship in Chrome stable.
With a style query, we can check if a component is placed within a wrapper that has a specific CSS variable and if yes, we style it accordingly.
In the following figure, we have an article body that is coming from a CMS. We have a default style for the figure and another style that looks featured.




To implement that with style queries, we can style the default one, and then check if the figure has a special CSS variable to allow the custom styling.
figure {
  container-name: figure;
  --featured: true;
}

/* Featured figure style. */
@container figure style(--featured: true) {
  img {
    /* Custom styling */
  }

  figcaption {
    /* Custom styling */
  }
}

And if --featured: true isn’t there, we will default to the base figure design. We can use the not keyword to check when the figure doesn’t have that CSS variable.
/* Default figure style. */
@container figure not style(--featured: true) {
  figcaption {
    /* Custom styling */
  }
}

That’s an if statement, but it’s implicit.
Another example is having a component styled differently based on its parent. Consider the following figure:




The card style can switch to dark if it’s placed within a container that has the --theme: dark CSS variable.
.special-wrapper {
  --theme: dark;
  container-name: stats;
}

@container stats style(--theme: dark) {
  .stat {
    /* Add the dark styles. */
  }
}

If we read the above, it feels like:

If the container stats have the variable --theme: dark, add the following CSS.

CSS @supports
The @supports feature lets us test if a certain CSS feature is supported in a browser or not.
@supports (aspect-ratio: 1) {
  .card-thumb {
    aspect-ratio: 1;
  }
}

We can also test for the support of a selector, like :has.
@supports selector(:has(p)) {
  .card-thumb {
    aspect-ratio: 1;
  }
}

Flexbox wrapping
According to MDN:

The flex-wrap CSS property sets whether flex items are forced onto one line or can wrap onto multiple lines. If wrapping is allowed, it sets the direction in that lines are stacked.

The flex-wrap property allows flex items to wrap into a new line in case there is not enough space available.
Consider the following example. We have a card that contains a title and a link. When the space is small, each child item should wrap into a new line.
.card {
  display: flex;
  flex-wrap: wrap;
  align-items: center;
}

.card__title {
  margin-right: 12px;
}





That sounds like a conditional thing to me. If no available space, wrap into a new line(s).




When each flex item wraps into a line, how do I manage the spacing between the flex items, you asked? Currently, there is a margin-right on the heading, and when they are stacked, that should be replaced by margin-bottom. The problem is we don’t know when the items will wrap because it depends on the content.
The good thing is that the spacing can be conditional with the gap property. When they are in the same line, the spacing is horizontal, and with multiple, the spacing is vertical.
.card {
  display: flex;
  flex-wrap: wrap;
  align-items: center;
  gap: 1rem;
}

This is one of my favorite flexbox features. Here is a visual of how gap switches the spacing.




By the way, I consider flex-wrap as defensive CSS. I almost add it to any flex container to avoid any unexpected issues.
The flex property
Even more, the flex property can work conditionally, too. Considering the following example. I added flex: 1 to the card title to make it fill the available space.
.card__title {
  flex-grow: 1;
}





That works fine, but when the width of the card is too small, the card title will wrap into a new line.




Nothing too bad, but can we do better? For example, I want to tell the title: “Hey, if your width is less than X, then wrap into a new line”. We can do that by setting the flex-basis property.
In the following CSS, I set the maximum width of the title to 190px. If it’s less than that, it will wrap into a new line.
.card__title {
  flex-grow: 1;
  flex-basis: 190px;
}





To learn more about the flex property in CSS, I wrote a detailed article on that.
Take things further, and explain about adding flex-grow, string.. etc along the way.
The :has selector
For me, this is the closest thing to an “if” statement in CSS right now. It works in a way that mimics an if/else statement.
Changing a card style
In this example, we need to have two different styles, depending on if the card has an image or not.




If the card has an image:
.card:has(.card__image) {
  display: flex;
  align-items: center;
}

And if it doesn’t have an image:
.card:not(:has(.card__image)) {
  border-top: 3px solid #7c93e9;
}

That’s an if statement, and I strongly think so. Sorry, I got too excited.
Hiding or showing form items conditionally
In forms, it’s common to have an input field or a group of inputs hidden by default, and it will be shown once the user activates an option from a <select> menu.




With CSS :has, we can check if the other option is selected and if yes, show the input field.
.other-field {
  display: none;
}

form:has(option[value=""other""]:checked) .other-field {
  display: block;
}

Alerts
When there is an alert message on a page, like for example a major warning of something wrong in the system, we might need to make it even more obvious.




In this example, we have an alert within the page, and with CSS :has, we can check if the dashboard has an alert, and if yes, style accordingly.
.main:has(.alert) .header {
  border-top: 2px solid red;
  background-color: #fff4f4;
}

So useful.
Change grid columns based on the number of items
Have you ever needed to display and change the width of a column in a grid based on the number of child items?




CSS :has can do that, conditionally.
.wrapper {
  --item-size: 200px;
  display: grid;
  grid-template-columns: repeat(
    auto-fill,
    minmax(var(--item-size), 1fr)
  );
  gap: 1rem;
}

.wrapper:has(.item:nth-last-child(n + 5)) {
  --item-size: 120px;
}

In the example, it says that if the .wrapper has five items, then the --item-size variable will change to 120px.
To learn more about the CSS :has selector, I wrote an article on it with plenty of examples.
CSS grid minmax() function
The way minmax() works in CSS grid is conditional. When we use auto-fit keyword, we’re telling the browser: “if there is an available space, make the grid items fill the space”.




The adjacent sibling combinator
That combinator matches the second element that comes directly after an element.
In the following example, if an <h3> element is followed by a <p>, the <p> will get custom styles.
h3 + p {
  margin-top: 8px;
}





The <p> top margin has been modified conditionally.
The :focus-within pseudo-class
Another interesting feature in CSS is :focus-within. Say that you want to check whether an input is focused, and if yes, add a border to its parent.
Consider the following example:




We have a search component. When the input is focused, the whole wrapper should have an outline. With :focus-within, we can check if the input is focused, and style accordingly.
.hero-form:focus-within {
  box-shadow: 0 0 0 5px rgb(28 147 218 / 35%);
}





The :not selector
This pseudo-class excludes elements that don’t match a certain selector. For example, it can be useful to check if an item is the last one, and if yes, remove the border.
.item:not(:last-child) {
  border-bottom: 1px solid lightgrey;
}





Conditional border-radius
A while ago, I wrote about how I spotted an interesting conditional approach to add border-radius for a card on the Facebook website.




The idea is that when the card is equal to or larger than the viewport, the radius should be 8px, if not, then it’s 0px.
.card {
  border-radius: max(
    0px,
    min(8px, calc((100vw - 4px - 100%) * 9999))
  );
}

You can read the article here.
Conditional line separator
Another interesting use case where CSS works conditionally is having a line separator that switches its direction and size based on whether the items are wrapped or not.
In the following figure, notice the line separator between the two sections.




I want that line to switch horizontally when the flex items are stacked. By using flex-wrap and clamp comparison, we can achieve that.
.section {
  --: 400px;
  display: flex;
  flex-wrap: wrap;
  gap: 1rem;
}

.section:before {
  content: """";
  border: 2px solid lightgrey;
  width: clamp(0px, (var(--breakpoint) - 100%) * 999, 100%);
}

This has been written on my blog, and the clamp() solution is a suggestion by Temani Afif.




Intrinsic sizing: fit-content
The fit-content keyword is a combination of min-content and max-content. I know, it’s not clear. Let’s take a look at the following flowchart.




If we have an element with width: fit-content, it will work conditionally as per the flowchart above.
h2 {
  width: fit-content;
}

Here is a video of what’s happening on resize:



I wrote about intrinsic sizing on my blog if you’re interested.
Comparison functions
CSS comparison functions are min(), max(), and clamp(). One particular example that feels conditional for me is something that I stumbled upon in a recent article I wrote.




The idea is I have two different containers, one of the article header (title and date), and a container for the main content plus the aside.
I want to align the edge of the header content with the body content.
On mobile, I want the padding from the left to be 1rem, but on larger viewports, it will be dynamic as per the viewport width.




To do that, I can use the max() function to choose one of the two values (1rem or dynamic value) conditionally.
.prose {
  padding-left: max(1rem, (100vw - var(--wrapper-width)) / 2);
}

You can learn more about this technique in my article Inside the mind of a frontend developer: Article layout.
Pseudo-classes
There are a lot of pseudo-classes in CSS, but the ones that came to mind are :focused and :checked.
input:checked + label {
  /* Custom styling */
}

input:focus {
  outline: 2px solid #222;
}

If the input is checked, add those styles to the <label>. If the input is focused..and so on.
But.. CSS isn’t a programming language!
I know, thanks for letting me know. This is argument that I hear a lot. I personally don’t have a strong opinion on that, but CSS is conditional in many ways.
In fact, most of the examples above can’t be implemented in Javascript without using a conditional statement.
Conclusion
I enjoyed writing this article because it reminded me of why I love using CSS. To me, CSS is like a superpower because it allows me to make so many design decisions through its conditional features. Working with design tools can sometimes feel limiting because I feel like I’m constrained within certain walls. I think that the ability to create conditional rules with CSS is what sets it apart and makes it powerful for web design.
That doesn’t mean that I design in the browser. I consider design tools as an open canvas to try and experiment with design ideas, and building polished UI products.
I like to use the browser to tweak designs, instead of designing them completely.
And you, what do you think? I would love to hear your thoughts and ideas.
Thank you for reading.



			Do you like my content? You can support and buy me a coffee. Thank you so much!
		



More articles


Previous Article
 2022 Year In Review





Subscribe to my RSS feed






Subscribe to the newsletter
Get the latest CSS articles published by Ahmad Shadeed, a UX Designer and Front End Developer.


Email Address



First Name 




erorr
success














Ahmad Shadeed
UI, UX Designer & Front-End Developer. You can hire me. I Write about web accessibility on @weba11ymatters and share articles on my blog.







      The online studio of Ahmad Shadeed.
      © 2012–2023 Copyright Ahmad Shadeed. All rights
        reserved.


Find me online


Instagram


Twitter


Behance


CodePen


Github

RSS Feed












"
https://news.ycombinator.com/rss,Will Floating Point 8 Solve AI/ML Overhead?,https://semiengineering.com/will-floating-point-8-solve-ai-ml-overhead/,Comments,"












Will Floating Point 8 Solve AI/ML Overhead?

























































































 
 
 












 










 


Search for:



 Subscribe

中文 English 


















Home
Systems & Design
Low Power - High Performance
Manufacturing, Packaging & Materials
Test, Measurement & Analytics
Auto, Security & Pervasive Computing




Special Reports

Business & Startups
Jobs
Knowledge Center
Technical Papers 

Home';
				AI/ML/DLArchitecturesAutomotiveCommunication/Data MovementDesign & VerificationLithographyManufacturingMaterialsMemoryOptoelectronics / PhotonicsPackagingPower & PerformanceQuantumSecurityTest & AnalyticsTransistorsZ-End Applications


Events & Webinars 

Events
Webinars



Videos & Research

Videos
Industry Research



Newsletters





MENU 

Home
Special Reports
Systems & Design
Low Power-High Performance
Manufacturing, Packaging & Materials
Test, Measurement & Analytics
Auto, Security & Pervasive Computing
Knowledge Center
Videos
Startup Corner
Business & Startups
Jobs
Technical Papers 
Events
Webinars
Industry Research
Special Reports







































Home >
                                                                    Low Power-High Performance >
                                                                Will Floating Point 8 Solve AI/ML Overhead?                                                    

















Low Power-High Performance

Will Floating Point 8 Solve AI/ML Overhead?









Less precision equals lower power, but standards are required to make this work.





								January 12th, 2023 - 

								By: Karen Heyman








While the media buzzes about the Turing Test-busting results of ChatGPT, engineers are focused on the hardware challenges of running large language models and other deep learning networks. High on the ML punch list is how to run models more efficiently using less power, especially in critical applications like self-driving vehicles where latency becomes a matter of life or death.
AI already has led to a rethinking of computer architectures, in which the conventional von Neumann structure is replaced by near-compute and at-memory floorplans. But novel layouts aren’t enough to achieve the power reductions and speed increases required for deep learning networks. The industry also is updating the standards for floating-point (FP) arithmetic.
“There is a great deal of research and study on new data types in AI, as it is an area of rapid innovation,” said David Bell, product marketing director, Tensilica IP at Cadence. “Eight-bit floating-point (FP8) data types are being explored as a means to minimize hardware — both compute resources and memory — while preserving accuracy for network models as their complexities grow.”
As part of that effort, researchers at Arm, Intel, and Nvidia published a white paper proposing “FP8 Formats for Deep Learning.” [1]
“Bit precision has been a very active topic of debate in machine learning for several years,” said Steve Roddy, chief marketing officer at Quadric. “Six or eight years ago when models began to explode in size (parameter count), the sheer volume of shuffling weight data into and out of training compute (either CPU or GPU) became the performance limiting bottleneck in large training runs. Faced with a choice of ever more expensive memory interfaces, such as HBM, or cutting bit precision in training, a number of companies experimented successfully with lower-precision floats. Now that networks have continued to grow exponentially in size, the exploration of FP8 is the next logical step in reducing training bandwidth demands.”
How we got here
Floating-point arithmetic is a kind of scientific notation, which condenses the number of digits needed to represent a number. This trick is pulled off by an arithmetic expression first codified by IEEE working group 754 in 1986, when floating-point operations generally were performed on a co-processor.
IEEE 754 describes how the radix point (more commonly known in English as the “decimal” point) doesn’t have a fixed position, but rather “floats” where needed in the expression. It allows numbers with extremely long streams of digits (whether originally to the left or right of a fixed point) to fit into the limited bit-space of computers. It works in either base 10 or base 2, and it’s essential for computing, given that binary numbers extend to many more digits than decimal numbers (100 = 1100100).
 

Fig. 1: 12.345 as a base-10 floating-point number. Source: Wikipedia
 
While this is both an elegant solution and the bane of computer science students worldwide, its terms are key to understanding how precision is achieved in AI. The statement has three parts:

A sign bit, which determines whether the number is positive (0) or negative (1);
An exponent, which determines the position of the radix point, and
A mantissa, or significand, which represents the most significant digits of the number.


Fig. 2: IEEE 754 floating-point scheme. Source: WikiHow
As shown in figure 2, while the exponent gains 3 bits in a 64-bit representation, the mantissa jumps from 32 bits to 52 bits. Its length is key to precision.
IEEE 754, which defines FP32 bit and FP64, was designed for scientific computing, in which precision was the ultimate consideration. Currently, IEEE working group P3109 is developing a new standard for machine learning, aligned with the current (2019) version of 754. P3109 aims to create a floating-point 8 standard.
Precision tradeoffs
Machine learning often needs less precision than a 32-bit scheme. The white paper proposes two different flavors of FP8: E4M3 (4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa).
“Neural networks are a bit strange in that they are actually remarkably tolerant to relatively low precision,” said Richard Grisenthwaite, executive vice president and chief architect at Arm. “In our paper, we showed you don’t need 32 bits of mantissa for precision. You can use only two or three bits, and four or five bits of exponent will give you sufficient dynamic range. You really don’t need the massive precision that was defined in 754, which was designed for finite element analysis and other highly precise arithmetic tasks.”
Consider a real-world example: A weather forecast needs the extreme ranges of 754, but a self-driving car doesn’t need the fine-grained recognition of image search. The salient point is not whether it’s a boy or girl in the middle of the road. It’s just that the vehicle must immediately stop, with no time to waste on calculating additional details. So it’s fine to use a floating point with a smaller exponent and much smaller mantissa, especially for edge devices, which need to optimize energy usage.
“Energy is a fundamental quantity and no one’s going to make it go away as an issue,” said Martin Snelgrove, CTO of Untether AI. “And it’s also not a narrow one. Worrying about energy means you can’t afford to be sloppy in your software or your arithmetic. If doing a 32-bit floating point makes everything easier, but massively more power consuming, you just can’t do it. Throwing an extra 1,000 layers at something makes it slightly more accurate, but the value for power isn’t there. There’s an overall discipline about energy — the physics says you’re going to pay attention to this, whether you like it or not.”
In fact, to save energy and performance overhead, many deep learning networks had already shifted to an IEEE-approved 16-bit floating point and other formats, including mantissa-less integers. [2]
“Because compute energy and storage is at a premium in devices, nearly all high-performance device/edge deployments of ML always have been in INT8,” Quadric’s Roddy said. “Nearly all NPUs and accelerators are INT-8 optimized. An FP32 multiply-accumulate calculation takes nearly 10X the energy of an INT8 MAC, so the rationale is obvious.”
Why FP8 is necessary
The problem starts with the basic design of a deep learning network. In the early days of AI, there were simple, one-layer models that only operated in a feedforward manner. In 1986, David Rumelart, Geoffrey Hinton, and Ronald Williams published a breakthrough paper on back-propagation [3] that kicked off the modern era of AI. As their abstract describes, “The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units, which are not part of the input or output, come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units.”
In other words, they created a system in which better results could be achieved by adding more and more layers into a model, which would be improved by incorporating “learned” adjustments. Decades later, their ideas so vastly improved machine translation and transcription that college professors remain unsure whether undergraduates’ essays have been written by bots.
But additional layers require additional processing power. “Larger networks with more and more layers were found to be progressively more successful at neural networks tasks, but in certain applications this success came with an ultimately unmanageable increase in memory footprint, power consumption, and compute resources. It became imperative to reduce the size of the data elements (activations, weights, gradients) from 32 bits, and so the industry started using 16-bit formats, such as Bfloat16 and IEEE FP16,” according to the paper jointly written by Arm/Intel/Nvidia.
“The tradeoff fundamentally is with an 8-bit floating-point number compared to a 32-bit one,” said Grisenthwaite. “I can have four times the number of weights and activations in the same amount of memory, and I can get far more computational throughput as well. All of that means I can get much higher performance. I can make the models more involved. I can have more weights and activations at each of the layers. And that’s proved to be more useful than each of the individual points being hyper-accurate.”
Behind these issues are the two basic functions in machine learning, training and inference. Training is the first step in which, for example, the AI learns to classify features in an image by reviewing a dataset. With inference, the AI is given novel images outside of the training set and asked to classify them. If all goes as it should, the AI should distinguish that tails and wings are not human features, and at finer levels, that airplanes do not have feathers and a tube with a tail and wings is not a bird.
“If you’re doing training or inference, the math is identical,” said Ron Lowman, strategic marketing manager for IoT at Synopsys. “The difference is you do training over a known data set thousands of times, maybe even millions of times, to train what the results will be. Once that’s done, then you take an unknown picture and it will tell you what it should be. From a math perspective, a hardware perspective, that’s the big difference. So when you do training, you want to do that in parallel, rather than doing it in a single hardware implementation, because the time it takes to do training is very costly. It could take weeks or months, or even years in some cases, and that just costs too much.”
In industry, training and inference have become separate specialties, each with its own dedicated teams.
“Most companies that are deploying AI have a team of data scientists that create neural network architectures and train the networks using their datasets,” said Bob Beachler, vice president of product at Untether AI. “Most of the autonomous vehicle companies have their own data sets, and they use that as a differentiating factor. They train using their data sets on these novel network architectures that they come up with, which they feel gives them better accuracy. Then that gets taken to a different team, which does the actual implementation in the car. That is the inference portion of it.”
Training requires a wide dynamic range for the continual adjustment of coefficients that is the hallmark of backpropagation. The inference phase is computing on the inputs, rather than learning, so it needs much less dynamic range. “Once you’ve trained the network, you’re not tweaking the coefficients, and the dynamic range required is dramatically reduced,” explained Beachler.
For inference, continuing operations in FP32 or FP16 is just unnecessary overhead, so there’s a quantization step to shift the network down to FP8 or Integer 8 (Int8), which has become something of a de facto standard for inference, driven largely by TensorFlow.
“The idea of quantization is you’re taking all the floating point 32 bits of your model and you’re essentially cramming it into an eight-bit format,” said Gordon Cooper, product manager for Synopsys’ Vision and AI Processor IP. “We’ve done accuracy tests and for almost every neural network-based object detection. We can go from 32-bit floating point to Integer 8 with less than 1% accuracy loss.”
For quality/assurance, there’s often post-quantization retraining to see how converting the floating-point value has affected the network, which could iterate through several passes.
This is why training and inference can be performed using different hardware. “For example, a common pattern we’ve seen is accelerators using NVIDIA GPUs, which then end up running the inference on general purpose CPUs,” said Grisenthwaite.
The other approach is chips purpose-built for inference.
“We’re an inference accelerator. We don’t do training at all,” says Untether AI’s Beachler. “We place the entire neural network on our chip, every layer and every node, feed data at high bandwidth into our chip, resulting in each and every layer of the network computed inside our chip. It’s massively parallelized multiprocessing. Our chip has 511 processors, each of them with single instruction multiple data (SIMD) processing. The processing elements are essentially multiply/accumulate functions, directly attached to memory. We call this the Energy Centric AI computing architecture. This Energy Centric AI Computing architecture results in a very short distance for the coefficients of a matrix vector to travel, and the activations come in through each processing element in a row-based approach. So the activation comes in, we load the coefficients, do the matrix mathematics, do the multiply/accumulate, store the value, move the activation to the next row, and move on. Short distances of data movement equates to low power consumption.”
In broad outline, AI development started with CPUs, often with FP co-processors, then moved to GPUs, and now is splitting into a two-step process of GPUs (although some still use CPUs) for training and CPUs or dedicated chips for inference.
The creators of general-purpose CPU architectures and dedicated inference solutions may disagree on which approach will dominate. But they all agree that the key to a successful handoff between training and inference is a floating-point standard that minimizes the performance overhead and risk of errors during quantization and transferring operations between chips. Several companies, including NVIDIA, Intel, and Untether, have brought out FP8-based chips.
“It’s an interesting paper,” said Cooper. “8-bit floating point, or FP8, is more important on the training side. But the benefits they’re talking about with FP8 on the inference side is that you possibly can skip the quantization. And you get to match the format of what you’ve done between training and inference.”
Nevertheless, as always, there are still many challenges still to consider.
“The cost is one of model conversion — FP32 trained model converted to INT8. And that conversion cost is significant and labor intensive,” said Roddy. “But if FP8 becomes real, and if the popular training tools begin to develop ML models with FP8 as the native format, it could be a huge boon to embedded inference deployments. Eight-bit weights take the same storage space, whether they are INT8 or FP8. The energy cost of moving 8 bits (DDR to NPU, etc.) is the same, regardless of format. And a Float8 multiply-accumulate is not significantly more power consumptive than an INT8 MAC. FP8 would rapidly be adopted across the silicon landscape.  But the key is not whether processor licensors would rapidly adopt FP8. It’s whether the mathematicians building training tools can and will make the switch.”
Conclusion
As the quest for lower power continues, there’s debate about whether there might even be a FP4 standard, in which only 4 bits carry a sign, an exponent, and mantissa. People who follow a strict neuromorphic interpretation have even discussed binary neural networks, in which the input functions like an axon spike, just 0 or 1.
“Our sparsity level is going to go up,” said Untether’s Snelgrove. “There are hundreds of papers a day on new neural net techniques. Any one of them could completely revolutionize the field. If you talk to me in a year, all of these words could mean different things.”
At least at the moment, it’s hard to imagine that lower FPs or integer schemes could contain enough information for practical purposes. Right now, various flavors of FP8 are undergoing the slow grind towards standardization. For example, Graphcore, AMD, and Qualcomm have also brought a detailed FP8 proposal to the IEEE. [4]
“The advent of 8-bit floating point offers tremendous performance and efficiency benefits for AI compute,” said Simon Knowles, CTO and co-founder of Graphcore. “It is also an opportunity for the industry to settle on a single, open standard, rather than ushering in a confusing mix of competing formats.”
Indeed, everyone is optimistic there will be a standard — eventually. “We’re involved in IEEE P3109, as are many, many companies in this industry,” said Arm’s Grisenthwaite. “The committee has looked at all sorts of different formats. There are some really interesting ones out there. Some of them will stand the test of time, and some of them will fall by the wayside. We all want to make sure we’ve got complete compatibility and don’t just say, ‘Well, we’ve got six different competing formats and it’s all a mess, but we’ll call it a standard.”
References 

Micikevicius, P., et al. FP8 Formats for Deep Learning. Last revised Sep 29 2022 arXiv:2209.05433v2. https://doi.org/10.48550/arXiv.2209.05433
Sapunov, G. FP64, FP32, FP16, BFLOAT16, TF32, and other members of the ZOO. Medium. May 16, 2020. https://moocaholic.medium.com/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407
Rumelhart, D., Hinton, G. & Williams, R. Learning representations by back-propagating errors. Nature 323, 533–536 (1986). https://doi.org/10.1038/323533a0
Noune, B. 8-bit Numerical Formats for Deep Neural Networks. Submitted June 6 2022 arXiv:2206.02915 https://doi.org/10.48550/arXiv.2206.02915

Additional Reading:
How to convert a number from decimal to IEEE 754 Floating Point Representation.
Number Representation and Computer Arithmetic
https://web.ece.ucsb.edu/~parhami/pubs_folder/parh02-arith-encycl-infosys.pdf
Computer Representation of Numbers and Computer Arithmetic
https://people.cs.vt.edu/~asandu/Courses/CS3414/comp_arithm.pdf











 Tags: 8-bit floating point AI AI models AI/ML/DL AMD ARM base 10 base 2 BFLOAT16 Cadence Cadence Design Systems ChatGPT deep learning E4M3 E5M2 edge inference floating point FP16 FP32 FP64 FP8 Graphcore IEEE 754 inference INT8 Integer 8 Intel machine learning mantissa ML training neural network Nvidia P3109 Quadric Quadric.io Qualcomm Synopsys training Untether AI





Karen Heyman   (all posts)


							Karen Heyman is a technology editor at Semiconductor Engineering.
						





Leave a Reply Cancel replyComment * Name*(Note: This name will be displayed publicly)
Email*(This will not be displayed publicly) 
 

Δ 




 Knowledge Centers Blogs 
Spiking Neural Network (SNN)

Published on June 23, 2021



Recurrent Neural Network (RNN)

Published on June 25, 2019



Edge Computing

Published on April 4, 2019



Neural Networks

Published on July 25, 2017



Architectures

Published on 



Machine Learning (ML)

Published on May 10, 2017



Convolutional Neural Network (CNN)

Published on 



Artificial Intelligence (AI)

Published on 




Technical Papers
Hardware Trojan Detection Case Study Based on 4 Different ICs Manufactured in Progressively Smaller CMOS Process Technologies January 11, 2023 by Technical Paper LinkQuantum Computing Architecture Enabling  Communication Between Superconducting Quantum Processors (MIT) January 11, 2023 by Technical Paper LinkArbitrary Precision DNN Accelerator Controlled by a RISC-V CPU (Ecole Polytechnique Montreal, IBM, Mila, CMC) January 10, 2023 by Technical Paper LinkTechnique For Printing Electronic Circuits Onto Curved & Corrugated Surfaces Using Metal Nanowires (NC State) January 10, 2023 by Technical Paper LinkFPGA-Based Prototyping Framework For Processing In DRAM (ETH Zurich & TOBB Univ.) January 10, 2023 by Technical Paper Link 

  Trending Articles

RISC-V Pushes Into The Mainstream

Open-source processor cores are beginning to show up in heterogeneous SoCs and packages.


by Marie C. Baca and Ed Sperling



How Secure Are RISC-V Chips?

Open source by itself doesn’t guarantee security. It still comes down to the fundamentals of design.


by Jeff Goldman



Will Floating Point 8 Solve AI/ML Overhead?

Less precision equals lower power, but standards are required to make this work.


by Karen Heyman



RISC-V decoupled Vector Processing Unit (VPU) For HPC



by Technical Paper Link



Startup Funding: December 2022

Wafer manufacturing and GPUs draw investment; 106 companies raise $2.8B.


by Jesse Allen






Knowledge Centers Entities, people and technologies explored
Learn More



Related Articles

Foundational Changes In Chip Architectures

New memory approaches and challenges in scaling CMOS point to radical changes — and potentially huge improvements — in semiconductor designs. 


by Brian Bailey



How Memory Design Optimizes System Performance

Changes are steady in the memory hierarchy, but how and where that memory is accessed is having a big impact.


by John Koon



Startup Funding: October 2022

113 startups raise $3.5B; batteries, AI, and new architectures top the list.


by Jesse Allen



Startup Funding: November 2022

127 startups raise $2.6B; data center connectivity, quantum computing, and batteries draw big funding.


by Jesse Allen



IC Stresses Affect Reliability At Advanced Nodes

Thermal mismatch in heterogeneous designs, different use cases, can impact everything from accelerated aging to warpage and system failures.


by Ann Mutschler



Will Floating Point 8 Solve AI/ML Overhead?

Less precision equals lower power, but standards are required to make this work.


by Karen Heyman



3D-IC Reliability Degrades With Increasing Temperature

Electromigration and other aging factors become more complicated along the z axis.


by Ann Mutschler



On-Chip Power Distribution Modeling Becomes Essential Below 7nm

Why and when it’s needed, and what tools and technologies are required.


by Ann Mutschler











Sponsors






























Advertise with us





Advertise with us





Advertise with us





Newsletter Signup



Popular Tags2.5D
5G
7nm
advanced packaging
AI
ANSYS
Apple
Applied Materials
ARM
Atrenta
automotive
business
Cadence
EDA
eSilicon
EUV
finFETs
GlobalFoundries
Google
IBM
imec
Intel
IoT
IP
Lam Research
machine learning
memory
Mentor
Mentor Graphics
MIT
Moore's Law
Nvidia
NXP
Qualcomm
Rambus
Samsung
security
SEMI
Siemens
Siemens EDA
software
Sonics
Synopsys
TSMC
verification
Recent CommentsWZIS on Arbitrary Precision DNN Accelerator Controlled by a RISC-V CPU (Ecole Polytechnique Montreal, IBM, Mila, CMC)Rama Chaganti on Growing System Complexity Drives More IP ReuseTL on How Secure Are RISC-V Chips?Frank on The Good And Bad Of Bi-Directional ChargingSandeep Dixit on The Good And Bad Of Bi-Directional ChargingHertz on How Secure Are RISC-V Chips?Andrew on How Software Utilizes CoresAsaf Jivilik on Cybord: Electronic Component TraceabilitySantosh Kurinec on Where All The Semiconductor Investments Are Goingdick freebird on Designing And Securing Chips For Outer SpaceAkshay on Designing And Securing Chips For Outer SpaceRaj on Is UCIe Really Universal?Andrew TAM on How Software Utilizes CoresRiko R on Designing For Multiple DieDan Ganousis on RISC-V Pushes Into The MainstreamIvan Batinic on IC Stresses Affect Reliability At Advanced NodesGiovanni Lostumbo on A Power-First ApproachMohammed Zakir Hussain on Embracing the Challenges Of Cybersecurity In Automotive ApplicationsLaura Peters on Week In Review: Manufacturing, TestAiv on Week In Review: Manufacturing, TestRoss Youngblood on High Voltage Testing Races AheadMark Olivas on Cybord: Electronic Component TraceabilityKarl Stevens on The Drive Toward Virtual PrototypesRon Lavallee on The Politics Of StandardsDenis McCarthy on Hot Trends In Semiconductor Thermal ManagementTom Smith on Are We Too Hard On Artificial Intelligence For Autonomous Driving?Maya F on Where All The Semiconductor Investments Are GoingSaikatm on Balancing Power And Heat In Advanced Chip DesignsDoug L. on Holistic 3D-IC Interposer Analysis In Product DesignsAndy Deng on Post-Quantum And Pre-Quantum Security Issues GrowJohn Dunn on Post-Quantum And Pre-Quantum Security Issues Growmadmax2069 on Chip Design Shifts As Fundamental Laws Run Out Of SteamMatthew Slyman on Chip Design Shifts As Fundamental Laws Run Out Of SteamDouglas MacIntyre on Chip Design Shifts As Fundamental Laws Run Out Of SteamJose on Universal Verification Methodology Running Out Of SteamZhengji Lu on Moving From AMBA ACE to CHI For CoherencyJohn Bennice on A Power-First Approach[email protected] on Chip Design Shifts As Fundamental Laws Run Out Of SteamMatthew on Chip Design Shifts As Fundamental Laws Run Out Of SteamKarthik Krishnamoorthy on AI-Powered VerificationCPlusPlus4Ever on Chip Design Shifts As Fundamental Laws Run Out Of SteamDouglas on Chip Design Shifts As Fundamental Laws Run Out Of SteamBowie Poag on Chip Design Shifts As Fundamental Laws Run Out Of SteamEugene on Startup Funding: October 2022Wesley Sung on Fan-Out And Packaging ChallengesHong Xiao on Chip Design Shifts As Fundamental Laws Run Out Of SteamRobert Anderson on Chip Design Shifts As Fundamental Laws Run Out Of SteamMike Frank on A Power-First ApproachWilliam Ruby on A Power-First ApproachPeter C Salmon on A Power-First ApproachDr. Dev Gupta on Which Foundry Is In The Lead? It Depends.Steve Hoover on A Power-First ApproachDylanP on Which Foundry Is In The Lead? It Depends.Asaf Jivilik on Cybord: Electronic Component TraceabilityChris @ crossPORt on Foundational Changes In Chip ArchitecturesMark Olivas on Cybord: Electronic Component TraceabilityAri ben David on Constraints On The Electricity GridJeff Zika on Auto Safety Tech Adds New IC Design ChallengesJung Yoon on Foundational Changes In Chip ArchitecturesSchrodinger's Cat's Advocate on Foundational Changes In Chip ArchitecturesRigTig on Foundational Changes In Chip ArchitecturesSteve on Foundational Changes In Chip ArchitecturesPrashant Purwar on Why Mask Blanks Are CriticalMostafa Abdelgawwad on Radar For Automotive: How Far Can A Radar See?yieldWerx on Managing Wafer RetestJohn Horner on A Brief History of TestLakshm J on ESD Requirements Are ChangingDr. Dev Gupta on Improving Redistribution Layers for Fan-out Packages And SiPsAkarsh on Better PMIC Design Using Multi-Physics SimulationTodd Bermensolo on Reducing Schedule Slips With Automated Post-Route Verification Of SerDes High Speed Serial LinksLaur Rizzatti on Why Geofencing Will Enable L5Raj Raghuram on The Complex Art Of Handling S-ParametersStevo on CHIPS Act: U.S. Releases New Implementation StrategySantosh Kurinec on Quantum Research Bits: Sept. 12Lewis Sternberg on ML And UVM Share Same FlawsRoger Stierman on L5 Adoption Hinges on 5G/6GMarcel on MicroLEDs Move Toward CommercializationRagu Athreya on Is There A Limit To The Number of Layers In 3D-NAND?Brian Bailey on AI Power Consumption ExplodingDavid S on AI Power Consumption ExplodingMike Cormack on Cryogenic CMOS Becomes CoolLance Harvie on New Uses For AI In ChipsDoc R on Electronics And Its Role In Climate ChangeMagdy Abadir on Is Standardization Required For Security?guest on How Overlay Keeps Pace With EUV PatterningSantosh Kurinec on Week In Review, Manufacturing, Testsravani on Timing Library LVF Validation For Production Design FlowsDr. F on A Sputnik Moment For ChipsGary Dagastine on A Sputnik Moment For ChipsMike Sottak on A Sputnik Moment For ChipsRobert Pearson on A Sputnik Moment For ChipsRaye E. Ward on A Sputnik Moment For ChipsMichael Williams on A Look Inside RF DesignSURESHBABU CHILUGODU on Week In Review: Manufacturing, TestJC Bouzigues, Menta on Customizing ProcessorsSteve Swendrowski on IC Package Illustrations, From 2D To 3DEMV on Hybrid Bonding Moves Into The Fast LaneDr. Appo van der Wiel on Variation Making Trouble In Advanced Packageswang yu on Verification Of Functional SafetyFrederick Chen on High-NA EUV May Be Closer Than It AppearsFact Cheq on The Week In Review: DesignShiwen Huang on E-beam’s Role Grows For Detecting IC DefectsAdele Hars on Wafer Shortage Improvement In Sight For 300mm, But Not 200mmDavid A. Humphreys on IMS2022 Booth Tour: EDA And Measurement Science ConvergeMerritt on Can Analog Make A Comeback?subra ganesan on Meeting Processor Performance And Safety Requirements For New ADAS & Autonomous Vehicle SystemsGeorge on Building A More Secure SoCAmit Garg on A New Breed Of EDA RequiredKarl Stevens on A Minimal RISC-VKarl Stevens on EDA Gaps At The Leading EdgeMicah Forstein MS. on Risks Rise As Robotic Surgery Goes MainstreamDr. Punam Raskar on Who Does Processor Validation?Dr. Dev Gupta on Variation Making Trouble In Advanced PackagesCox on DRAM Thermal Issues Reach Crisis PointDavid Leary on DRAM Thermal Issues Reach Crisis PointGeeeeeee on DRAM Thermal Issues Reach Crisis PointPedro Ferro Laks on SOT-MRAM To Challenge SRAMObviously silly on DRAM Thermal Issues Reach Crisis PointSimon on DRAM Thermal Issues Reach Crisis PointGareth on Energy Harvesting Starting To Gain Traction

 





Selecting The Right RISC-V Core Brian BaileyWeek In Review: Semiconductor Ma... Karen Heyman 










  










About

About us
Contact us
Advertising on SemiEng
Newsletter SignUp



Navigation



Homepage
Special Reports
Systems & Design
Low Power-High Perf
Manufacturing, Packaging & Materials
Test, Measurement & Analytics
Auto, Security & Pervasive Computing




Videos
Jobs
Technical Papers
Events
Webinars
Knowledge Centers

Industry Research
Business & Startups
Newsletters





Connect With Us

Facebook
Twitter  @semiEngineering
LinkedIn
YouTube




Copyright ©2013-2023 SMG   |  Terms of Service  |  Privacy Policy





This site uses cookies. By continuing to use our website, you consent to our Cookies PolicyACCEPT Manage consent




Close






Privacy Overview 
This website uses cookies to improve your experience while you navigate through the website. The cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. We do not sell any personal information.

By continuing to use our website, you consent to our Privacy Policy. If you access other websites using the links provided, please be aware they may have their own privacy policies, and we do not accept any responsibility or liability for these policies or for any personal data which may be collected through these sites. Please check these policies before you submit any personal information to these sites.

 





								Necessary							


Necessary

Always Enabled




									Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.								






								Non-necessary							


Non-necessary





									Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website.								












SAVE & ACCEPT










 

 
 

 
























 








"
https://news.ycombinator.com/rss,A Supermarket Visit Brought Down the Soviet Union,http://beelineblogger.blogspot.com/2016/01/how-supermarket-visit-brought-down.html,Comments,"


















BeeLine: How A Supermarket Visit Brought Down The Soviet Union








































































BeeLine




The Shortest Route To What You Need To Know




















Scott Beeken





BeeLine



View my complete profile








































































Tuesday, January 5, 2016








How A Supermarket Visit Brought Down The Soviet Union





Many point to the fact that the Soviet Union collapse occurred as the Soviets were baited into trying to compete with the defense build-up instituted by Ronald Reagan.

The Soviets just did not have the financial resources to match the United States in defense spending while also tending to the needs of its citizenry. The Soviets spent money on guns rather than butter. Something had to give and the Soviet people were the ones that suffered.

However, a little known visit to a suburban Houston supermarket in 1989 by Boris Yeltsin appears to have been the catalyst that ended up bringing down the Soviet Union.

Yeltsin visited the Johnson Space Center in Houston in September, 1989 to tour mission control and to view a model of the planned International Space Station.

After visiting the Space Center, Yeltsin made an unplanned stop at a local Randall's grocery store that was close by before heading to the airport.

That visit changed the course of history.

At the time, Yeltsin was a newly elected member of the Soviet Parliament and the Supreme Soviet and had been a key ally of the General Secretary of the Communist Party Mikhail Gorbachev, who was initiating reforms but the pace of which was too slow for Yeltsin.

Houston Chronicle reporter Stephanie Asin was with Yeltsin on the visit to the grocery store that day.


Yeltsin, then 58, “roamed the aisles of Randall’s nodding his head in amazement,” wrote Asin. He told his fellow Russians in his entourage that if their people, who often must wait in line for most goods, saw the conditions of U.S. supermarkets, “there would be a revolution.”

“Even the Politburo doesn’t have this choice. Not even Mr. Gorbachev,” he said.

The fact that stores like these were on nearly every street corner in America amazed him. They even offered free cheese samples. According to Asin, Yeltsin didn’t leave empty-handed, as he was given a small bag of goodies to enjoy on his trip.

This is a picture of Yeltsin touring the grocery store.



Credit: Houston Chronicle


This Houston Chronicle story from 2014 fills in the rest of the story.


About a year after the Russian leader left office, a Yeltsin biographer later wrote that on the plane ride to Yeltsin’s next destination, Miami, he was despondent. He couldn’t stop thinking about the plentiful food at the grocery store and what his countrymen had to subsist on in Russia.

In Yeltsin’s own autobiography, he wrote about the experience at Randall’s, which shattered his view of communism, according to pundits. Two years later, he left the Communist Party and began making reforms to turn the economic tide in Russia. 

“When I saw those shelves crammed with hundreds, thousands of cans, cartons and goods of every possible sort, for the first time I felt quite frankly sick with despair for the Soviet people,” Yeltsin wrote. “That such a potentially super-rich country as ours has been brought to a state of such poverty! It is terrible to think of it.”

To give you some perspective on what was available in the Soviet Union at that time, here is a picture of a Russian store from that era.




Credit: Gennady Galperin/Reuters


An aide to Yeltsin later reported that in that visit to the grocery store in Houston “the last vestige of Bolshevism collapsed” inside his boss.

Two years later Yeltsin was elected to the newly created office of President of the Russian Federation after the collapse of the Soviet Union with Gorbachev.

Yeltsin immediately began dismantling the socialist economic system and introducing capitalism to the Russians. In the process he attempted to convert the world's largest command economy into a free-market one. 

The results of that transition were rocky in large part to cronyism in the break-up of many of the large state-owned businesses. In the process, many Russian oligarchs were created and Yeltsin eventually resigned his office in 1999 haunted by charges of corruption and incompetence.

His successor?

Vladimir Putin.

The falling price of oil has put a similar squeeze on Putin and the Russians today. Putin has been popular with the Russian people based on his macho style and nationalistic bombast. However, potential trouble lurks for Putin because of the Russian economy.

The Russian consumer is being squeezed with annual inflation of almost 20% and the average Russian spends about 50% of their income on food.

By comparison, the average American spends only 8% of income on groceries.

Will groceries once again determine the future of Russia?






Posted by



BeeLine




at

8:21 PM
















Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest














42 comments:




andreSeptember 14, 2017 at 4:52 AMobat viagraviagra asliReplyDeleteRepliesUnknownOctober 29, 2021 at 10:14 AMGreat Article Artificial Intelligence Projects Project Center in Chennai JavaScript Training in Chennai JavaScript Training in Chennai Project Centers in Chennai DeleteRepliesReplyReplychegekhanMarch 28, 2018 at 8:11 AMUseful Information, your blog is sharing unique information....Thanks for sharing!!! buy bakery products online south-cbuy branded food online in panganiReplyDeleteRepliesReplyUnknownSeptember 27, 2018 at 7:24 AMThank you for your post. This is excellent information. It is amazing and wonderful to visit your site.buy bakery products online south-c ReplyDeleteRepliesReplyYK AgencyDecember 25, 2018 at 11:30 AMSupermarket in Dubai Great article. Cool.ReplyDeleteRepliesReplyLuck CityFebruary 11, 2019 at 3:50 AMWithin this webpage, you'll see the page, you need to understand this data. https://digitalglobal.comReplyDeleteRepliesReplyjames brownNovember 22, 2019 at 6:45 AMAwesome blog. I enjoyed reading your articles. This is truly a read for me. I have bookmarked it and I am looking forward to reading new articles. Keep up the good work!Kroger customer surveyReplyDeleteRepliesReplyKroger experienceDecember 10, 2019 at 8:05 AMPlease share more like that.Kroger experienceReplyDeleteRepliesReplyAnonymousDecember 30, 2019 at 7:47 PMGreat article! Yeltsin revealed as a realist! I never knew this ...ReplyDeleteRepliesReplyDavid Grant Stewart, Sr., EgyptologistJanuary 1, 2020 at 3:17 PMWho paid you to write this drivel? You are either incredibly gullible or on the take from the USSR propaganda machine. You look and say the Soviet civilian economy is bad. There is no civilian economy in the USSR. The country does not have a war machine. The country is a war machine.ReplyDeleteRepliesReplyNeha UppalJanuary 9, 2020 at 3:33 AMThanks for sharing such beautiful information with us. I hope you will share some more information about best grocery shopping app. Please keep sharing.ReplyDeleteRepliesReplyjames brownFebruary 13, 2020 at 1:15 PMHey There. I found your blog using This is a very well written article. I’ll be sure to bookmark it and come back to read more of your useful info. Thanks for the post. I’ll definitely return.https://krogerexperiencee.com/ReplyDeleteRepliesReplyjames brownFebruary 18, 2020 at 7:24 AMGreat post I would like to thank you for the efforts you have made in writing this interesting and knowledgeable article.https://tellthebelll.usReplyDeleteRepliesReplysurvey monkey usaFebruary 19, 2020 at 7:45 AMGreat things you’ve always with us. Just keep writing this kind of posts.The time which was wasted in traveling for tuition now it can be used for studies.Thankssurvey monkey usaReplyDeleteRepliesReplydanielwilsonnFebruary 19, 2020 at 3:43 PM Thank you again for all the knowledge u distribute,Good post. I was very interested in the article, it's quite inspiring I should admit. I like visiting you site since I always come across interesting articles like this one.Great Job, I greatly appreciate that.Do Keep sharing! Regards,https://krogerfeeedback.us/ReplyDeleteRepliesReplydahliaApril 23, 2020 at 3:21 AM Thanks for sharing this information. I really like your post very much. You have really shared an informative and interesting post with people  TellTheBell ReplyDeleteRepliesReplypatronsurveysApril 24, 2020 at 8:23 AMAfter Kroger and Walmart, I prefer to go to Tesco supermarket. Do you know what? there is quality in products with an extraordinary service tescoviews com offers a platform to complete the Tesco customer satisfaction survey to win £1000 Gift Card & 25 Club Points. Survey site is giving a lot of store surveys at one place to complete.ReplyDeleteRepliesReplyjames brownJuly 14, 2020 at 3:24 AMThis comment has been removed by the author.ReplyDeleteRepliesReplyjames brownJuly 14, 2020 at 3:25 AMIts a great pleasure reading your post.Its full of information I am looking for and I love to post a comment that ""The content of your post is awesome"" Great work.https://krogerexperiencee.com/greatpeople-me-kroger-employee-login-portal/ReplyDeleteRepliesReplyNFL FanSeptember 20, 2020 at 4:46 PMThe official source for NFL news, video highlights, fantasy football, game-day coverage, schedules, stats, scores and more. Ravens FootballReplyDeleteRepliesReplytellthebellSeptember 22, 2020 at 11:42 AM Excellent website you have  so much cool information!..tellthebellReplyDeleteRepliesReplydgcustomerfirstJune 25, 2021 at 6:49 AMTo take an interest in the Dollar general super market survey, it is important to arrange a few things at one of its Branches initially. Visit the authority survey site of Dollar General survey at Dgcustomerfirst.Com and Win A $100 gift voucher. Then, at that point, you need to save the receipt of the store. Then, at that point, go to the authority survey site of dollar general. The Dg survey is accessible in both English or Spanish.ReplyDeleteRepliesReplymybkexperienceAugust 3, 2021 at 2:30 AMDollar general survey is an online platform that collects customers' most recent shopping experiences and overall customer satisfaction. Participate in the survey and be the lucky person to get enlisted in dg customer first winners. ReplyDeleteRepliesReplyarnavharperAugust 30, 2021 at 12:33 PMRemote for Fire TV is designed specifically to control Fire TV, Fire TV Cube and Fire TV Stick. Just connect mobile device and a TV or media player to the Firestick Remote.ReplyDeleteRepliesReplySEOOctober 12, 2021 at 3:37 AMdelta international recruitment agency in pakistanReplyDeleteRepliesReplyHealthandBeautyTipsNovember 6, 2021 at 5:24 PMEmployees can Perform Kroger E-Schedule Login at the Feed Kroger Login Portal once their Schedule Credentials are verified. If you are unable to sign in to Kroger Login then you need to contact the branch manager.Feed KrogerReplyDeleteRepliesReplyUnknownJanuary 27, 2022 at 5:48 AMThe NASA dark Brant IX sounding rocket conveyed the payload to an apogee of 177 miles prior to plunging by parachute and arriving at White Sands. Dgcustomerfirst.com Survey ReplyDeleteRepliesReplydgcustomerfirst.comMay 19, 2022 at 3:35 AMThe dgcustomerfirst.com criticism review permits customers to enter the Dollar General Sweepstakes of Cash $100 in the wake of finishing the overview.dgcustomerfirstwin.shop Survey  ReplyDeleteRepliesReplywww.DGCustomerFirst.comJune 6, 2022 at 10:50 AMDollar general survey is an online platform that collects customers' most recent shopping experiences and overall customer satisfaction. https://idgcustomerfirst.org/ReplyDeleteRepliesReplydgcustomerfirsts.shopJune 10, 2022 at 6:07 AMAlso, you have a superb opportunity to partake in the client criticism overview. DGCustomerFirst 2022 or Dollar General Survey is a study led by Dollar General's authorities for all United States inhabitants.dgcustomerfirsts executed a connected with the WWW review to take an arrangement about dgcustomerfirsts Helpline notwithstanding your support level thereafter visiting service  Click here dgcustomerfirsts ReplyDeleteRepliesReplySteveJune 16, 2022 at 9:58 AMGreat Article, it was very informative. That was such thought-provoking content. I enjoyed reading your content. Every week, I look forward to your column. In my opinion, this one is one of the best articles you have written so far.How to Change Instagram PasswordChange Windows 10 PasswordSubwaylistensHome Depot SurveyDQFanFeedback.com ReplyDeleteRepliesReplyUmairJune 30, 2022 at 6:44 AMirescopk.comReplyDeleteRepliesReplyDgcustomerfirstscom.shopJuly 9, 2022 at 6:51 AMFormerly referred to as J L Turner, Dollar General has numerous subsidiaries viz Dollar General Market, Dollar General Financial, Dollar General Global Sourcing, and lots extra.dgcustomerfirstscom executed a connected with the WWW review to take an arrangement about dgcustomerfirstscom Helpline notwithstanding your support level thereafter visiting service  Click here dgcustomerfirstscom ReplyDeleteRepliesReplyAnonymousJuly 19, 2022 at 1:35 PMLiveTheOrangeLife – Official Portal www.LiveTheOrangeLife.comWalmartOne Login - Walmartone.com Login Guidemyaccountaccess.comonevanillaJCP Associate KioskReplyDeleteRepliesReplySmith AdomJuly 30, 2022 at 12:03 PMTalkToWendys executed a connected with the WWW review to take an arrangement about TalkToWendys Helpline notwithstanding your support level thereafter visiting service  Click here TalkToWendys  It is mandatory to make a purchase at Wendy’s once before being a participant in this survey.ReplyDeleteRepliesReplyInformTarget.comAugust 10, 2022 at 3:38 AMRules are guidelines, and they are set to be accompanied. If you want to participate within the survey effectively, you need to adhere to the rules and policies set apart via the informtarget.Com remarks survey.informtargets executed a connected with the WWW review to take an arrangement about informtargets Helpline notwithstanding your support level thereafter visiting service  Click here informtargets.shop ReplyDeleteRepliesReplyTellBaskinRobbinsAugust 19, 2022 at 5:39 AMThere are some basic rules and requirements of this Baskin Robbins Customer Satisfaction Survey which I even have furnished in this newsletter. tellbaskinrobbins executed a connected with the WWW review to take an arrangement about tellbaskinrobbins Helpline notwithstanding your support level thereafter visiting service  Click here tellbaskinrobbins ReplyDeleteRepliesReplyTalktofoodlionAugust 20, 2022 at 2:37 AMTalktofoodlion The company's full name is general Dollar, and it is offering a $100 incentive to customers who take the time to participate in this little survey. visit here Talktofoodlion ReplyDeleteRepliesReplyFaiz IsrailiAugust 21, 2022 at 5:02 AMThe procedure is requesting information from individuals using a questionnaire, which may be completed offline or online. New technologies, however, are frequently disseminated via digital channels like social media, email, QR codes, or URLs. dgcustomerfirstReplyDeleteRepliesReplyTellBaskinRobbinsSeptember 8, 2022 at 7:11 AMTellBaskinRobbins After responding to the feedback questions, participants are expected to rate their experience shopping at Baskin Robbins.The feedback survey is sponsored by Baskin Robbins in order to better understand its service quality TellBaskinRobbinsReplyDeleteRepliesReplyNikithaOctober 8, 2022 at 1:31 AMmybkexperience customer satisfaction survey which is an online platform to get timely feedback from their customers about the food and services. This can improve their services according to customer’s needs and at the same time, rewards their customer for their time and loyalty towards the restaurant. So you can answer the questions and eat delicious food for free at the same time. Now you might be wondering about how to participate in the survey or what are the requirements and much more.ReplyDeleteRepliesReplySEOOctober 14, 2022 at 5:59 AMtop recruitment agencies in pakistan for saudi arabiaReplyDeleteRepliesReplyAdd commentLoad more...























Newer Post


Older Post

Home




Subscribe to:
Post Comments (Atom)















BeeLine Email Subscription

Get new posts by email:  Subscribe




Follow @BeeLineBlog




Followers











Blog Archive








        ► 
      



2023

(5)





        ► 
      



January

(5)









        ► 
      



2022

(139)





        ► 
      



December

(11)







        ► 
      



November

(10)







        ► 
      



October

(12)







        ► 
      



September

(12)







        ► 
      



August

(13)







        ► 
      



July

(11)







        ► 
      



June

(12)







        ► 
      



May

(12)







        ► 
      



April

(12)







        ► 
      



March

(12)







        ► 
      



February

(10)







        ► 
      



January

(12)









        ► 
      



2021

(131)





        ► 
      



December

(15)







        ► 
      



November

(12)







        ► 
      



October

(9)







        ► 
      



September

(13)







        ► 
      



August

(14)







        ► 
      



July

(11)







        ► 
      



June

(10)







        ► 
      



May

(6)







        ► 
      



April

(10)







        ► 
      



March

(11)







        ► 
      



February

(7)







        ► 
      



January

(13)









        ► 
      



2020

(154)





        ► 
      



December

(11)







        ► 
      



November

(12)







        ► 
      



October

(14)







        ► 
      



September

(11)







        ► 
      



August

(12)







        ► 
      



July

(13)







        ► 
      



June

(14)







        ► 
      



May

(12)







        ► 
      



April

(16)







        ► 
      



March

(16)







        ► 
      



February

(10)







        ► 
      



January

(13)









        ► 
      



2019

(145)





        ► 
      



December

(14)







        ► 
      



November

(11)







        ► 
      



October

(9)







        ► 
      



September

(12)







        ► 
      



August

(13)







        ► 
      



July

(12)







        ► 
      



June

(12)







        ► 
      



May

(14)







        ► 
      



April

(13)







        ► 
      



March

(12)







        ► 
      



February

(10)







        ► 
      



January

(13)









        ► 
      



2018

(139)





        ► 
      



December

(11)







        ► 
      



November

(10)







        ► 
      



October

(10)







        ► 
      



September

(10)







        ► 
      



August

(12)







        ► 
      



July

(13)







        ► 
      



June

(12)







        ► 
      



May

(12)







        ► 
      



April

(13)







        ► 
      



March

(12)







        ► 
      



February

(9)







        ► 
      



January

(15)









        ► 
      



2017

(132)





        ► 
      



December

(9)







        ► 
      



November

(10)







        ► 
      



October

(15)







        ► 
      



September

(9)







        ► 
      



August

(13)







        ► 
      



July

(12)







        ► 
      



June

(9)







        ► 
      



May

(13)







        ► 
      



April

(12)







        ► 
      



March

(10)







        ► 
      



February

(10)







        ► 
      



January

(10)









        ▼ 
      



2016

(119)





        ► 
      



December

(11)







        ► 
      



November

(15)







        ► 
      



October

(15)







        ► 
      



September

(10)







        ► 
      



August

(2)







        ► 
      



July

(9)







        ► 
      



June

(11)







        ► 
      



May

(6)







        ► 
      



April

(9)







        ► 
      



March

(11)







        ► 
      



February

(12)







        ▼ 
      



January

(8)

In the Middle of 5th Avenue
Risky Business
That's for the Birds
An Inconvenient Truth +10
Citizen Cruz
Context on Guns
How A Supermarket Visit Brought Down The Soviet Union
A Humble Servant?










        ► 
      



2015

(71)





        ► 
      



December

(9)







        ► 
      



November

(5)







        ► 
      



October

(2)







        ► 
      



September

(2)







        ► 
      



August

(9)







        ► 
      



July

(7)







        ► 
      



June

(6)







        ► 
      



May

(3)







        ► 
      



April

(10)







        ► 
      



March

(6)







        ► 
      



February

(4)







        ► 
      



January

(8)









        ► 
      



2014

(88)





        ► 
      



December

(5)







        ► 
      



November

(7)







        ► 
      



October

(10)







        ► 
      



September

(9)







        ► 
      



August

(5)







        ► 
      



July

(8)







        ► 
      



June

(7)







        ► 
      



May

(9)







        ► 
      



April

(7)







        ► 
      



March

(6)







        ► 
      



February

(5)







        ► 
      



January

(10)









        ► 
      



2013

(115)





        ► 
      



December

(8)







        ► 
      



November

(5)







        ► 
      



October

(13)







        ► 
      



September

(9)







        ► 
      



August

(7)







        ► 
      



July

(10)







        ► 
      



June

(10)







        ► 
      



May

(9)







        ► 
      



April

(13)







        ► 
      



March

(10)







        ► 
      



February

(7)







        ► 
      



January

(14)









        ► 
      



2012

(139)





        ► 
      



December

(6)







        ► 
      



November

(16)







        ► 
      



October

(16)







        ► 
      



September

(9)







        ► 
      



August

(14)







        ► 
      



July

(13)







        ► 
      



June

(8)







        ► 
      



May

(11)







        ► 
      



April

(9)







        ► 
      



March

(9)







        ► 
      



February

(11)







        ► 
      



January

(17)









        ► 
      



2011

(188)





        ► 
      



December

(12)







        ► 
      



November

(10)







        ► 
      



October

(10)







        ► 
      



September

(10)







        ► 
      



August

(17)







        ► 
      



July

(18)







        ► 
      



June

(11)







        ► 
      



May

(3)







        ► 
      



April

(16)







        ► 
      



March

(19)







        ► 
      



February

(25)







        ► 
      



January

(37)









About Me





BeeLine


Scott Beeken has practiced as an attorney, CPA and has been an officer with two Fortune 500 companies overseeing diverse functions such as Taxation, Employee Benefits, Human Resources, Real Estate Facilities, Risk Management, Corporate Communications, Marketing and Advertising. In addition to writing BeeLine, he is a Keynote Speaker, Author  and Strategic Consultant.

View my complete profile




























Total Pageviews

























Theme images by luoman. Powered by Blogger.

























"
https://news.ycombinator.com/rss,Show HN: Sketch – AI code-writing assistant that understands data content,https://github.com/approximatelabs/sketch,Comments,"








approximatelabs

/

sketch

Public




 

Notifications



 

Fork
    8




 


          Star
 281
  









        AI code-writing assistant that understands data content
      





281
          stars
 



8
          forks
 



 


          Star

  





 

Notifications












Code







Issues
0






Pull requests
0






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







approximatelabs/sketch









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





6
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




bluecoconut

update readme wording




        …
      




        9d567ec
      

Jan 16, 2023





update readme wording


9d567ec



Git stats







133

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github/workflows



remove python 3.7, add tests, remove uneeded code



Dec 15, 2022









sketch



some bug fix and copy addition



Jan 16, 2023









tests



remove python 3.7, add tests, remove uneeded code



Dec 15, 2022









.gitignore



update with edit and work on text2sql



Oct 13, 2022









README.md



update readme wording



Jan 16, 2023









dev-requirements.txt



starting to rebuild sketch



Dec 15, 2022









pyproject.toml



rename to pandas extension, add missing requirement, use base64 encoding



Jan 11, 2023









setup.py



starting to rebuild sketch



Dec 15, 2022




    View code
 


















sketch
Demo
How to use
.sketch.ask
.sketch.howto
.sketch.apply
Sketch currently uses prompts.approx.dev to help run with minimal setup
How it works





README.md




sketch
Sketch is an AI code-writing assistant for pandas users that understands the context of your data, greatly improving the relevance of suggestions. Sketch is usable in seconds and doesn't require adding a plugin to your IDE.
pip install sketch
Demo
Here we follow a ""standard"" (hypothetical) data-analysis workflow, showing a Natural Language interace that successfully navigates many tasks in the data stack landscape.

Data Catalogging:

General tagging (eg. PII identification)
Metadata generation (names and descriptions)


Data Engineering:

Data cleaning and masking (compliance)
Derived feature creation and extraction


Data Analysis:

Data questions
Data visualization








sketch-demo.mp4





Try it out in colab: 
How to use
It's as simple as importing sketch, and then using the .sketch extension on any pandas dataframe.
import sketch
Now, any pandas dataframe you have will have an extension registered to it. Access this new extension with your dataframes name .sketch
.sketch.ask
Ask is a basic question-answer system on sketch, this will return an answer in text that is based off of the summary statistics and description of the data.
Use ask to get an understanding of the data, get better column names, ask hypotheticals (how would I go about doing X with this data), and more.
df.sketch.ask(""Which columns are integer type?"")
.sketch.howto
Howto is the basic ""code-writing"" prompt in sketch. This will return a code-block you should be able to copy paste and use as a starting point (or possibly ending!) for any question you have to ask of the data. Ask this how to clean the data, normalize, create new features, plot, and even build models!
df.sketch.howto(""Plot the sales versus time"")
.sketch.apply
apply is a more advanced prompt that is more useful for data generation. Use it to parse fields, generate new features, and more. This is built directly on lambdaprompt. In order to use this, you will need to set up a free account with OpenAI, and set an environment variable with your API key. OPENAI_API_KEY=YOUR_API_KEY
df['review_keywords'] = df.sketch.apply(""Keywords for the review [{{ review_text }}] of product [{{ product_name }}] (comma separated):"")
df['capitol'] = pd.DataFrame({'State': ['Colorado', 'Kansas', 'California', 'New York']}).sketch.apply(""What is the capitol of [{{ State }}]?"")
Sketch currently uses prompts.approx.dev to help run with minimal setup
In the future, we plan to update the prompts at this endpoint with our own custom foundation model, built to answer questions more accurately than GPT-3 can with its minimal data context.
You can also directly call OpenAI directly (and not use our endpoint) by using your own API key. To do this, set 2 environment variables.
(1) SKETCH_USE_REMOTE_LAMBDAPROMPT=False
(2) OPENAI_API_KEY=YOUR_API_KEY
How it works
Sketch uses efficient approximation algorithms (data sketches) to quickly summarize your data, and feed that information into language models. Right now it does this by summarizing the columns and writing these summary statistics as additional context to be used by the code-writing prompt. In the future we hope to feed these sketches directly into custom made ""data + language"" foundation models to get more accurate results.









About

      AI code-writing assistant that understands data content
    
Topics



  python


  data-science


  data


  ai


  tabular-data


  pandas


  df


  sketches


  dataframe


  copilot


  codex


  ds


  datasketches


  gpt3


  lambdaprompt


  datasketch



Resources





      Readme
 


Stars





281
    stars

Watchers





2
    watching

Forks





8
    forks







    Releases





6
tags







    Packages 0


        No packages published 







        Used by 22
 




























            + 14
          







    Contributors 2








bluecoconut
Justin Waugh

 






jmbiven
Mike Biven

 





Languages










Python
100.0%











"
https://news.ycombinator.com/rss,Rust vs. C++ Formatting,https://brevzin.github.io/c++/2023/01/02/rust-cpp-format/,Comments," Rust vs C++ Formatting | Barry's C++ Blog      Barry's C++ BlogJust a blog about C++   HOME    TAGS    ARCHIVES    ABOUT                     Home   Rust vs C++ Formatting   Post     Cancel Rust vs C++ Formatting  Posted  Jan 2, 2023    Updated  Jan 9, 2023    By  Barry Revzin    35 min readIn Rust, if I want to print some 32-bit unsigned value in hex, with the leading 0x, padded out with zeros, I would write that as:println!(""{:#010x}"", value);
In C++23, if I want to do the same, that’s:std::println(""{:#010x}"", value);
The only difference is the spelling of the name of the thing we’re calling (which is a function template in C++ and a macro in Rust) - otherwise, identical.Nevertheless, there is a surprisingly vast gulf of difference between the two languages in how they handle formatting, at basically every level beneath the user-facing syntax. I thought the differences were pretty interesting and worth going over.Ergonomic FeaturesBefore I go over the differences in the two format libraries, it’s worth starting out by discussing the differences in ergonomic features that are available to users. By an ergonomic feature, what I mean is a feature that doesn’t necessarily add any functionality – it may solve a problem that may otherwise have been solvable – but rather that it makes it easier to get done, with less typing, and thus probably with fewer bugs.The canonical example in C++ of an ergonomic feature might be: lambdas. We could always write function objects by declaring a class or class template somewhere and overloading its operator() - but doing so is very verbose and, if the call operator needs to be a template, it can’t be written locally. Lambdas don’t add functionality, but they are tremendously more user-friendly 1.When it comes to formatting, Rust has two major ergonomic features that have tremendous impact on user experience: #[derive(Debug)] and f-strings.#[derive(Debug)]Rust has multiple different formatting traits (which I’ll get into later), but for now I’ll touch on the distinction between the two most important ones: fmt::Display and fmt::Debug. Debug is, as the name suggests, for debugging purposes and the Rust documentation states that:fmt::Debug implementations should be implemented for all public types. Output will typically represent the internal state as faithfully as possible. The purpose of the Debug trait is to facilitate debugging Rust code. In most cases, using #[derive(Debug)] is sufficient and recommended.Now, the notion of making your types printable for debug purposes is hardly unique to Rust. I do this in C++ all the time. But the key feature in Rust is how much code you have to write to accomplish this:#[derive(Debug)]
struct Point {
    x: i32,
    y: i32,
}

fn main() {
    println!(""p={:?}"", Point { x : 1, y : 2});
}
Which, when run, prints:p=Point { x: 1, y: 2 }
The fact that you have to write one line of code to achieve this is tremendous. And even calling this one line is a bit much, since you’ll typically be deriving many traits (like Eq or Ord or Clone, for a type like this), so we’re effectively just talking about a few characters.Of course, you could implement the Debug trait by hand - it’s not impossible without #[derive]. But over the long run, the ability to do this just adds so much value.f-stringsIn the intro, I showed that in Rust you could write this:println!(""{:#010x}"", value);
But recently (as of Rust 1.58), Rust also added the ability to use f-strings (also called interpolated literals or interpolated strings in some other languages), which allows you to just write:println!(""{value:#010x}"");
In Rust, this was originally implemented (as far as I’m aware) in the fstrings crate, modeled after the Python feature of the same name. This feature exists in a number of other languages (as noted in the Rust RFC): JavaScript, C#, VB, Swift, Ruby, Scala, Perl, and PHP. There are certainly more (like Kotlin).For a simple example like this, using string interpolation doesn’t really make a big difference. Sure, it’s shorter, but only by two characters, which is hardly significant. The value of the feature isn’t how much typing it saves - the value here is that it allows you to put the variables you’re formatting in the location that they’re being formatted. Compare the readability between these two lines:println!(""Point is at (x={}, y={}, z={})"", p.x, p.y, p.z);
println!(""Point is at (x={p.x}, y={p.y}, z={p.z})"");
It is much easier to understand what’s being formatted in the second line and, importantly, it’s easier to ensure that you format all of your arguments in the correct order - since seeing ""y={p.z}"" is clearly wrong.It’s been pointed to me by numerous people that Rust’s interpolation feature doesn’t actually support {p.x} like that. The fstrings crate does support that usage (you can see examples in their docs), but the RFC explicitly rejects this on the basis that once you allow arbitrary expressions, they become too hard to read. Not sure I agree with this line of reasoning - seems like the sort of thing you should leave up to the community:If any expressions beyond identifiers become accepted in format strings, then the RFC author expects that users will inevitably ask “why is my particular expression not accepted?”. This could lead to feature creep, and before long perhaps the following might become valid Rust:println!(""hello { if self.foo { &self.person } else { &self.other_person } }"");
This no longer seems easily readable to the RFC author.As with #[derive], f-strings don’t add any functionality - these two lines really do the same thing. But this is the kind of language feature that once you start using regularly in one language (in my case, Python), you really want to use it everywhere, all at once.See also David Sankel’s Rust Features That I Want In C++ from CppNow 2022.Format String BasicsNow that I got the ergonomics out of the way, let’s talk about the way format strings work - whether in Rust or Python or C++ or a number of other languages. I’m going to use the terms in the C++ grammar for format strings, since that’s what I’m most familiar with.Given a string like:""A string literal with x={} and y={:#08x} and name={:>25}""
What we have are alternating string pieces and replacement fields. Each replacement field consumes one or more trailing (ignoring interpolation) arguments that are passed into the formatting function (or macro). A replacement field is enclosed in braces - the {}, {:#08x} and {:>25} above are all replacement fields.A replacement field consists of an optional argument id followed by an optional format specifier. An argument id allows you to explicitly choose an argument by number ({} will replace with whatever the next argument is, while {0} will explicitly replace with the first argument). In C++ and Python, you cannot mix and match automatic and manual numbering. Manual numbering allows you to format the same argument multiple times, or to format the arguments out of order - which is particularly useful for translation purposes. The format specifier is what tells the library how to specifically format the chosen argument. The format specifier 2 must be introduced by a :.Types typically have a common set of format specifiers they can use:fill and alignmentsignwidthprecisionalternate type representation (e.g. hex for integers, or scientific for floating point)For more details on what these specifiers are and how to use them, check out the C++ fmt docs or the Rust docs or the Python docsThe good news here is that the way format strings work in Rust, Python, and C++ are mostly the same.One interesting distinction I do want to point out is how these languages differ in their handling of dynamic width. In all three, if I want to format a string, right-aligned, in a 25-character wide field, that’s something like this:format(""{:>25}"", s)
But if I want the width to come from a variable instead, I can use this in C++ (or the equivalent in Python):format(""{:>{}}"", s, 25)
But Rust doesn’t let you do {} (i.e. automatic numbering) for dynamic width , you can only provide an explicit index or a named variable – which then must be suffixed with $. Rust’s version is:println!(""{:>1$}"", ""hello"", 25);
Rust here allows a mix of automatic (for the string) and manual (for the width) indexing. Neither C++ nor Python allow this - you can write either {0:>{1}} or {:>{}} in those two languages, but not {:>{1}} or {0:>{}}.I’m not sure why Rust differs here - I think the visual distinction between {:>25} and {:>{}} is quite a bit larger than {:>25} and {:>1$}, since the latter seems like it could easily be misread to be a width of 1.C++ Formatting with {fmt}Let’s talk about the way the core C++ formatting library works – with {fmt} and now std::format. How do you implement formatting for your type?In C++, you have to specialize the type fmt::formatter (now std::formatter) and provide two functions for it: parse() and format().Using a two-dimensional Point example:struct Point {
    int x;
    int y;
};

template <>
struct std::formatter<Point> {
    constexpr auto parse(auto& ctx) {
        // ...
    }

    auto format(Point const& p, auto& ctx) const {
        // ...
    }
};
The job of parse() is to parse and validate the provided format specifier. It should throw an exception (format_error) if the provided format specifier is invalid. The ctx argument gives you access to the format string. What does it mean for a format specifier to be invalid for Point? Interesting question.The job of format() is to use the saved state from parse() (if any) to format the object (p) into the provided output iterator (which you get via ctx.out()).The simplest implementation would be to mandate that no format specifier is provided and then format the Point in some friendly, readable way:template <>
struct std::formatter<Point> {
    constexpr auto parse(auto& ctx) {
        return ctx.begin();
    }

    auto format(Point const& p, auto& ctx) const {
        return std::format_to(ctx.out(), ""(x={}, y={})"", p.x, p.y);
    }
};
Using the standard specifiersLet’s say we don’t want to just format p.x and p.y as regular integers, but also want to support whatever arbitrary format specifiers the ints do: padding, hex, etc. We can do that by deferring to formatter<int> for both the parsing and the formatting logic:template <>
struct fmt::formatter<Point> {
    fmt::formatter<int> f;

    constexpr auto parse(auto& ctx) {
        return f.parse(ctx);
    }

    auto format(Point const& p, auto& ctx) const {
        auto out = fmt::format_to(ctx.out(), ""(x="");
        ctx.advance_to(out);
        out = f.format(p.x, ctx);
        out = fmt::format_to(out, "", y="");
        ctx.advance_to(out);
        out = f.format(p.y, ctx);
        *out++ = ')';
        return out;
    }
};
And with that, we can get arbitrarily complex formatting:fmt::print(""{0}\n{0:#x}\n{0:*^7}\n"", Point{.x=100, .y=200});
which prints:(x=100, y=200)
(x=0x64, y=0xc8)
(x=**100**, y=**200**)
The implementation is mildly tedious because we have to do this two-step:ctx.advance_to(out);
out = f.format(p.x, ctx);
Perhaps a different way of writing it it that’s potentially less error prone would be to not even have a local variable for the output iterator:auto format(Point const& p, auto& ctx) const {
    ctx.advance_to(fmt::format_to(ctx.out(), ""(x=""));
    ctx.advance_to(f.format(p.x, ctx));
    ctx.advance_to(fmt::format_to(ctx.out(), "", y=""));
    ctx.advance_to(f.format(p.y, ctx));
    return fmt::format_to(ctx.out(), "")"");
}
This is because in the C++ model, the format context just has some arbitrary output iterator while the format() function on the formatter takes the format context - these two things need to be kept in sync. If we don’t remember to ctx.advance_to(out) and out happens to be something like char*, then we would just overwrite stuff that we’d already written.It’s hard to really ensure that you did this right because the default iterator in {fmt} is fmt::appender, with which you simply cannot run into this problem. It’s just a std::back_insert_iterator - the kind of output iterator where ++it doesn’t actually do anything since it doesn’t have a notion of position 3. Since all std::back_insert_iterator<Container>s into a given Container have the same state, forgetting to update with advance_to doesn’t matter.Because {fmt} (and std::format) type-erases the provided output iterator, even if you use fmt::format_to(out, ""{}"", p) where out is a char*, this still won’t break if you forget the ctx.advance_to(out). This issue will only surface in the library if you use both compile-time format strings and provide your own iterator:char buf[300] = {};
// without the calls to advance_to(), this will end up writing
// just ""2)"" instead of ""(x=1, y=2)"", but buf[2:3] will still be ""y=""
char* o = fmt::format_to(buf, FMT_COMPILE(""{}""), Point{1, 2});
You can see the impact of the missing advance_to call here.And that still isn’t even completely right. Output iterators in C++20 are allowed to be move-only, so ctx.advance_to(out) might not compile. Like I said, it’s hard to get this right.Outside of remembering this pitfall, this is pretty nifty. We get support for all of this logic in one go.Using custom specifiersWe’re not limited to just supporting the standard specifiers – we can also add our own. Let’s say that instead of supporting the standard integer specifiers, all we care about for our Point type is printing it either in cartesian coordinates, as (x={}, y={}), or in polar coordinates, as (r={}, theta={}).I go over this in more detail in my CppCon 2022 talk, “The Surprising Complexity of Formatting Ranges,” but say we wanted to use the c or r specifiers (for cartesian or rectangular) to format x/y and to use the p specifier (for polar) to format r/theta. We can implement that this way:template <>
struct fmt::formatter<Point> {
    // store additional state during parse()
    enum class Coord {
        cartesian,
        polar
    };
    Coord type = Coord::cartesian;

    constexpr auto parse(auto& ctx) {
        auto it = ctx.begin();
        // if we don't have any specifier, then we're done
        if (it == ctx.end() or *it == '}') {
            return it;
        }

        // otherwise consume the one character that we expect
        switch (*it++) {
        case 'r':
        case 'c':
            type = Coord::cartesian;
            break;
        case 'p':
            type = Coord::polar;
            break;
        default:
            throw fmt::format_error(""invalid specifier"");
        }
        return it;
    }

    auto format(Point const& p, auto& ctx) const {
        // our choice of output is based on our state
        if (type == Coord::cartesian) {
            return fmt::format_to(ctx.out(), ""(x={}, y={})"", p.x, p.y);
        } else {
            return fmt::format_to(ctx.out(), ""(r={:.4}, theta={:.4})"", p.r(), p.theta());
        }
    }
};
In my CppCon talk, I also go over some more complicated things that you can do with specifiers - like how to implement support for dynamic width that I mentioned earlier.Rust Formatting with std::fmtIn Rust, implementing manual formatting for Point looks quite different. There, you have to implement the trait Display, which only has one function for you to implement instead of two. The same, simplest-possible implementation for Point would look like this:impl fmt::Display for Point {
    fn fmt(&self, formatter : &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(formatter, ""(x={}, y={})"", self.x, self.y)
    }
}
&self here is the reference to Point, while formatter holds a similar role to the ctx arguments that were passed to parse() and format() in C++.While this implementation looks superficially similar to the C++ implementation, just being a little shorter as we only had to write one function instead of two, they’re actually quite different.The Formatter objectLet’s start by going over fmt::Formatter. You can find the docs here.In C++, parse() and format() are exposed to the user - to do with as they wish. In Rust, you only get format() (spelled fmt) - the library itself does the parsing for you, and gives you the state in the Formatter object.That state looks like so:pub struct Formatter<'a> {
    flags: u32,
    fill: char,
    align: rt::v1::Alignment,
    width: Option<usize>,
    precision: Option<usize>,

    buf: &'a mut (dyn Write + 'a),
}
The flags here includes the sign choice and whether we’re using the alternative formatting, the other fields are not surprising. buf is an arbitrary type-erased buffer, similar to the way that {fmt} type-erases the output iterator 4.Conceptually, you can think of Rust’s fmt::Display::fmt() for T and C++’s formatter<T>::format() as being fairly equivalent. They even get the same pieces of information:InformationC++Rustthe Tpassed as first parameter&selfthe parse statethe *this object, populated from parse()the Formatter object, populated by librarythe output bufferctx.out()formatter.bufBecause Display::fmt takes a Formatter, the right way to extend our formatter for Point to support the standard specifiers is to pass the Formatter we get into subsequent calls to fmt:impl fmt::Display for Point {
    fn fmt(&self, f : &mut fmt::Formatter<'_>) -> fmt::Result {
        f.write_str(""(x="")?;
        fmt::Display::fmt(&self.x, f)?;
        f.write_str("", y="")?;
        fmt::Display::fmt(&self.y, f)?;
        f.write_str("")"")
    }
}
Note the use of ? for error propagation, which I’d love to have in C++. I’m currently pursing this as P2561This has the same structure as the C++ implementation - we have one mechanism to write the string pieces (in this case f.write_str) and a different mechanism to format the underlying part (in this calling fmt::Display::fmt again). But that formatting is handled internally in a way that ends up being more convenient for the user. No manual buffer manipulation here.With that change, we have parity with our C++ implementation for the standard specifiers:let p = Point { x: 100, y: 200 };
println!(""{}"", p);      // (x=100, y=200)
println!(""{:*^7}"", p);  // (x=**100**, y=**200**)
println!(""{:#x}"", p);   // error
Well… we almost have parity.A Constellation of TraitsThe error we get from the above call is:error[E0277]: the trait bound `Point: LowerHex` is not satisfied
  --> src/main.rs:22:23
   |
22 |     println!(""{:#x}"", p);
   |                       ^ the trait `LowerHex` is not implemented for `Point`
   |
   = help: the following other types implement trait `LowerHex`:
             &T
             &mut T
             NonZeroI128
             NonZeroI16
             NonZeroI32
             NonZeroI64
             NonZeroI8
             NonZeroIsize
           and 21 others
note: required by a bound in `ArgumentV1::<'a>::new_lower_hex`
   = note: this error originates in the macro `$crate::format_args_nl` which comes from the expansion of the macro `arg_new` (in Nightly builds, run with -Z macro-backtrace for more info)

For more information about this error, try `rustc --explain E0277`.
error: could not compile `playground` due to previous error
The key really is just the first line. {} and {:*^7} require fmt::Display, but {:x} (and {:#x}, etc.) don’t go through fmt::Display. They instead go through an entirely different, unrelated trait: fmt::LowerHex.As the name implies, there’s not just fmt::LowerHex for x. There’s also fmt::UpperHex for X. And even that’s not all of them. In total, there are nine formatting traits:TraitKindBinarybDebug?DisplayotherLowerExpeLowerHexxOctaloPointerpUpperExpEUpperHexXWhat this means is that - if we wanted to support printing Point in all the different ways that you can print an i32, we need to implement nine traits. Which all would look exactly the same as what I showed for Display, just substituting the name Display for all the other names.The one odd exception is that implementing Debug doesn’t just give you {:?} but also {:x?} and {:X?} (but that’s it - so you can do debug hex, but not debug exponent?). I’m not sure why that’s the case.Using custom specifiersWith C++, implementing a formatter for T requires writing a parse() function. That parse() gets, basically, a std::string_view and can interpret its contents however it wants.With Rust, implementing any of the formatting traits (let’s just stick with Display) requires just writing fmt, and you get the parsed specifiers ready-made for use. That fmt::Formatter object is what you get. Full stop. Which is quite nice when that’s what you want, but there’s no way to get anything else.The Rust docs have this example:use std::fmt;

#[derive(Debug)]
struct Vector2D {
    x: isize,
    y: isize,
}

impl fmt::Display for Vector2D {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        // The `f` value implements the `Write` trait, which is what the
        // write! macro is expecting. Note that this formatting ignores the
        // various flags provided to format strings.
        write!(f, ""({}, {})"", self.x, self.y)
    }
}

// Different traits allow different forms of output of a type. The meaning
// of this format is to print the magnitude of a vector.
impl fmt::Binary for Vector2D {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        let magnitude = (self.x * self.x + self.y * self.y) as f64;
        let magnitude = magnitude.sqrt();

        // Respect the formatting flags by using the helper method
        // `pad_integral` on the Formatter object. See the method
        // documentation for details, and the function `pad` can be used
        // to pad strings.
        let decimals = f.precision().unwrap_or(3);
        let string = format!(""{:.*}"", decimals, magnitude);
        f.pad_integral(true, """", &string)
    }
}

fn main() {
    let myvector = Vector2D { x: 3, y: 4 };

    println!(""{myvector}"");       // => ""(3, 4)""
    println!(""{myvector:?}"");     // => ""Vector2D {x: 3, y:4}""
    println!(""{myvector:10.3b}""); // => ""     5.000""
}
The example demonstrates different format specifiers producing very different kinds of outputs:{} goes through Display, and just prints (x, y){:?} goes through Debug, which is #[derive]-ed, so you get the type name and then all the members{:b} goes through Binary which… prints the magnitude. Because b for… bagnitude?The example also ends up demonstrating the limitation of having only standard specifiers. That’s simply all that you have available to you, so you have to pick from what’s there.In C++, I showed how you can have a Point that’s printed either cartesian or polar, by providing that specifier, which is fairly straightforward to implement. In Rust, we can use ‘p’ (that’s fmt::Pointer) but you can’t use either c or r - those letters just aren’t available to you.There is technically the ability to use an arbitrary character, but only with alignment. That is:impl fmt::Display for Point {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match f.align().map(|_a| f.fill()).unwrap_or('c') {
            'c' | 'r' => {
                write!(f, ""(x={}, y={})"", self.x, self.y)
            }
            'p' => {
                write!(f, ""(r={:.4}, theta={:.4})"", self.r(), self.theta())
            }
            _ => Err(fmt::Error),
        }
    }
}

fn main() {
    let p = Point { x: 100, y: 200 };
    println!(""{:c^}"", p); // (x=100, y=200)
    println!(""{:p^}"", p); // (r=223.6068, theta=1.1071)
}
This… works, but just seems like an especially weird way to do this, and is highly limited anyway.I used the f.align().map(...) approach above with the goal of parity with the C++ implementation: by default, the format is cartesian. Otherwise, the only allowed characters are c, r, and p. If I just checked f.fill(), the default value is a space. But if I matched on space or c or r, I would allow {: ^}, which I don’t want to.Maybe that’s not a big deal, since I doubt anybody writes this sort of thing anyway.Comparing The Two ModelsNow that we’ve seen a brief introduction to the C++ and Rust formatting models, we can start to meaningfully compare the two.A simpler modelIn Rust, if we stick with the subset of standard specifiers supported by Formatter (basically: fill, align, width, precision, sign), then the Rust approach lets you do a lot of things easier. Because the buffer writing is hidden from you, you don’t have to worry about the keeping the writing in sync - the implementation of Display for Point supporting these specifiers is shorter than the C++ one.The other notable advantage of Rust’s fmt::Formatter is that it provides a lot of helper functions to get the various specifier state out and to use it. The Formatter will tell you the fill, the alignment, the width, the precision, the sign. And it has member functions to help you write things padded.In C++, whether in {fmt} or the standard library, there are no such helpers. You can use fmt::formatter<int>, as I did above, to be able to do more complex formatting - but the only members that type exposes are parse() and format(). So if you want to support these specifiers yourself, you kind of just have to implement all of that logic… yourself.ChronoOn the other hand, the custom specifier support allows for a lot more functionality to be handled by the formatting library itself. Let’s say I want to print today’s date in UTC. I can do that:std::println(""Today is {:%Y-%m-%d}"", std::chrono::system_clock::now());
Chrono basically has its own mini-language of placeholders that you can use to build up its format specifier. That is pretty cool, and very useful. You can’t do that in Rust - since there’s no notion of custom specifier at all, and something this complicated you can’t just hack into the fill character.Rust also has a chrono library, which gives you time points that are printable:let now = chrono::offset::Utc::now();
println!(""{}"", now);    // 2023-01-02 03:18:57.477157070 UTC
println!(""{:?}"", now);  // 2023-01-02T03:18:57.477157070Z
Rust’s chrono crate does support this sort of arbitrary placeholder logic that C++ does, and with the same placeholder syntax too. It’s just that you have to do it differently:let now = chrono::offset::Utc::now();
println!(""Today is {}"", now.format(""%Y-%m-%d"")); // Today is 2023-01-02
This works, and is fine for this simple example. But it doesn’t scale particularly well. Since what happens when we move up to more complex structures. Like…RangesRust has a very different philosophy for formatting ranges than C++ does.In C++, if I try to print a container and then an adapted version of it:auto v = std::vector{1, 2, 3, 4, 5};
std::println(""{}"", v);
std::println(""{}"",
    v | std::views::transform([](int i){
        return i * i;
    }));
That prints:[1, 2, 3, 4, 5]
[1, 4, 9, 16, 25]
Which, I think, is probably what most people want and expect. But Rust’s approach here is quite different:let v = vec![1, 2, 3, 4, 5];
println!(""{:?}"", v);
println!(""{:?}"", v.iter().map(|i| i * i));
That, instead, prints:[1, 2, 3, 4, 5]
Map { iter: Iter([1, 2, 3, 4, 5]) }
Neither Vec<T> nor any of the iterators implement fmt::Display, only fmt::Debug.I quoted from the Rust documentation earlier that, for fmt::Debug:Output will typically represent the internal state as faithfully as possible.So the goal of Debug for Map isn’t necessarily to print the elements that you get out - it’s to represent the internal state of the Map itself. That does make sense on some level.But Map doesn’t implement fmt::Display. None of these types do. So what do I do if I did want the [1, 4, 9, 16, 25] output that I get out of the box in C++? There’s actually no solution in the Rust standard library. We instead of have to turn to the itertools create to add some extensions for us:println!(""[{}]"", v.iter().map(|i| i * i).format("", ""));
The format function (on the Itertools trait) basically returns some new type that itself implements fmt::Display, using the specifier to print each element and the provided string as the delimiter.Now, if I want to format the elements of a range differently from just {}, I can do that in both Rust and C++:std::println(""{::*^5}"", v);
vsprintln!(""[{:*^5}]"", v.iter().format("", ""));
Both of these print:[**1**, **2**, **3**, **4**, **5**]
Though we get there in wildly different ways. In C++, the formatter for ranges uses the underlying type’s formatter to parse the element-specific format specifier (after the second colon), and we use that formatter to print every element. In Rust, the format() hook returns a new object which implements Display by using the provided Formatter to fmt each element, separated by the provided delimiter.Now here’s the question: what if I had a range of dates, and I wanted to print them all with the %Y-%m-%d format? In C++, that’s exactly the same idea as the previous example: we’re just providing the format specifier we want to use for each element:std::println(""{::%Y-%m-%d}"", dates);
But in Rust, for the chrono time point, this isn’t a format specifier. We had to do this whole other function call to get that behavior. So we need a new mechanism to solve this problem. The itertools crate provides this for us under the name format_with:println!(
    ""[{}]"",
    dates
        .iter()
        .format_with("", "", |elt, f| f(&elt.format(""%Y-%m-%d"")))
);
format_with is the most general API and probably lets you do anything you want. But it’s now fairly complicated, and differs quite a lot from the simple case. It may be helpful to place these examples back to back to make this more clear - in both cases, I’m formatting a range of some element type with some particular choice of specifier:// C++
std::println(""{::*^5}"", v);
std::println(""{::%Y-%m-%d}"", dates);
vs// Rust
println!(""[{:*^5}]"", v.iter().format("", ""));
println!(
    ""[{}]"",
    dates
        .iter()
        .format_with("", "", |elt, f| f(&elt.format(""%Y-%m-%d"")))
);
Unsupported SpecifiersOne other notable difference between the two is in their handling of unsupported specifiers. In the very first examples I showed for each language, I demonstrated how to format a Point as (x=1, y=2). The Rust implementation was shorter, but the C++ one wasn’t exactly a leviathan.But the two implementations weren’t exactly equivalent. The C++ approach only supported that case. Attempting to use any other specifier would have been a compile error:std::println(""{}"", p);     // (x=1, y=2)
std::println(""{:*^7}"", p); // error
std::println(""{:+}"", p);   // error
But the Rust approach would allow and simply ignore any specifier that wasn’t used in the implementation:println!(""{}"", p);     // (x=1, y=2)
println!(""{:*^7}"", p); // (x=1, y=2)
println!(""{:+}"", p);   // (x=1, y=2)
The specifiers still have to be valid - Rust would still reject {:7^*}, the thing that I seemingly always want to type instead. But we didn’t use any of specifiers the user provided in our implementation, so the output is the same.Is it better to reject unsupported specifiers, or ignore unsupported specifiers? Good question.Debug RepresentationIn Rust, debug formatting is a first-class citizen via fmt::Debug. And, as I mentioned at the very top of this blog, it’s extremely easy to opt into via #[derive(Debug)].In C++, debug formatting is also a thing, that notably surfaces in formatting ranges: formatting ""hello""s prints hello but formatting vector{""hello""s} prints [""hello""]. Notice the quotes: that comes from the debug representation. It’s not just quoting, the string will also be escaped (e.g. newlines will be formatted as the two-character sequence \n).But while in Rust, ? is just a first-class specifier that everybody can rely on, that simply isn’t the case in C++. Not all C++ types support debug formatting with ?, and whether a type does or not is entirely up to that type author. std::string supports it, but int does not, for instance. Even worse than that, a type could support a ? specifier whose meaning has nothing whatsoever to do with debugging (in the same way my Point example used p in a way that has nothing to do with pointers).That’s why in C++, I had to come up with a completely different mechanism for choosing the debug representation: an optional member function on formatter named set_debug_format(). And this design is still in flux, since originally this was a function that took no arguments, but it may need to change to take a bool (to enable or disable debug formatting). This would simply not be an issue we would have to think about if, in C++, we had any control over specifiers. But we don’t. That’s one of the downsides for allowing users to do anything they want: they can do anything they want.ConclusionOn the usage side, the C++ and Rust models of formatting look very similar. Nearly identical even. But they have a surprising amount of differences.In Rust, {:?} always works for every type and is always some debug-friendly formatting. In C++, ? isn’t special at all, so types support it or not as they see fit. It works for std::string, but not int. Instead, we have a completely different approach to debug representation, which is still in flux.In C++, each type can support whatever specifiers it wants. In Rust, there is one global fixed set of specifiers that is parsed by the implementation. This means that the Rust ecosystem is more consistent and coherent, since there’s only one way of doing things. But it also means that the many times that custom specifiers would prove useful, Rust needs some ad hoc escape hatch, which is just a different kind of inconsistency. It also means that Rust users will just abuse other formatting traits to solve their customization needs (such as the docs themselves demonstrating using Binary to display a magnitude), so you can end up with choices of specifiers that make no sense.In C++, writing wrappers that propagate all specifiers is mildly tedious due to needing to keep the context and output iterators in sync, but otherwise fairly straightforward. In Rust, this seems like it should be less tedious, since the formatter manages all of its state directly, except that you actually have to implement nine traits to do this, which seems… like an odd design decision to me.In C++, the library doesn’t provide any tools to help you parse things like fill, alignment, and width, so you have to implement them yourself. Which is not trivial. In Rust, you don’t need a tool to parse the specifiers since you just get the result of the parse.In C++, ranges are formattable. In Rust, iterators only implement fmt::Debug but in a way that logs their state, not the underlying elements. You need to include the itertools create to actually format ranges, but it ends up being a bit complicated due to the way that custom specifiers end up being handled.Which approach is better? I think different people could react very differently to those paragraphs.On the whole, I was surprised at how different Rust’s and C++’s approaches ended up being to solving the same problem, and I thought it was interesting to really consider the the implications of them.Implementing Rust’s approach with C++As is usually the case with C++ 5, you can implement the Rust model in C++, but you cannot implement the C++ model in Rust.To implement the Rust model, you can write a general formatter object:struct GeneralFormatter {
    uint32_t flags;
    char fill;
    Alignment align;
    std::optional<size_t> width;
    std::optional<size_t> precision;

    constexpr auto parse(auto& ctx) {
        // I am not even going to try
    }

    template <typename T>
    auto format(auto& object, auto& ctx) const {
        // I am not even going to try
    }
};
And then just use that to implement all of your parse() functions and as your helper for all of your format() functions.Improving C++’s approachThere are a few things that make Rust’s approach more user-friendly that I think we can, and should, pursue.The biggest two, overwhelmingly, are the ergonomic features I mentioned earlier. Static reflection will allow us to provide a #[derive(Debug)] equivalent and there is ongoing discussion about how to support interpolated literals in C++ for use with formatting.One of the difficulties (though not the only one) with supporting interpolated literals is precisely this issue of custom specifiers. For instance, how do you make this work:std::println(f""{name:>{width}}"");
Getting the name part is fine, but what do you do with width? If name happens to be a std::string, then >{} means right-aligned using the next format argument as the width. In which case {width} should be interpreted as interpolating width – doing name lookup on width and evaluating it.But if name is some other type, like an acme::Widget, then >{width} could mean anything. It could be a dynamic width like std::string and int and so forth. Or it could be a placeholder syntax similar to chrono, where >{width} is a request to print the Widget’s width, having nothing to do with alignment whatsoever, and certainly not a request for a variable named width. Would interpolation need to be something the user can hook into, so that if they do support dynamic arguments, they can interpolate, otherwise they don’t?The other problem is that, for an arbitrary user-defined type, the format specifier does not need to be balanced between { and }. I mean, it should be balanced, and it seems pretty hostile to try to come up with a reason to not be balanced. But this is C++, so since you can, somebody certainly will. But at least in this case, we can just say that if you play silly games, you win silly prizes: no string interpolation for you. Come back next year with sane bracing.Outside of these two language improvements, one of which really having nothing in particular to do with formatting, specifically, there are a few things that are probably worth thinking about:adding more member functions to format_context to make it more convenient to alternate between what kind of thing we’re formatting, which would also make it less error prone. Perhaps (using Rust’s names, as usual):  auto format(Point const& p, auto& ctx) const {
      return ctx.write_str(""(x="")
                .format(p.x, f)
                .write_str("", y="")
                .format(p.y, f)
                .write_str("")"")
                .out();
  }
adding something akin to the GenericFormatter type I showed earlier to make it easier for users to support common specifiers like padding.To be clear, both of these combined I think provide significantly less value than string interpolation which itself provides significantly less value than static reflection. But I do think these would provide some value, and I definitely think both are much easier problems to solve.Specifiers Are UsefulOutside of the significantly better ergonomics Rust provides, I do think C++’s general approach of allowing arbitrary specifiers is the better one. When used well (as in chrono or in range element formatting), it lets the user do a lot of complicated things quite concisely and consistently: the fact that formatting a range of dates and a range of integers looks the same is a big win.The specifier mini-language does have the potential to turn into line noise very quickly, so it’s certainly not a panacea. But on the whole I’d definitely rather have it than not.Even if I wish they were much terser. ↩Technically, in C++, a replacement-field is an optional arg-id followed by an optional format-specifier, where a format-specifier is a : followed by a format-spec. But while this makes sense grammatically, it’s a bit awkward in English to have “format specifier” and “format spec” be these subtly different things, so I’m going to (hopefully not super confusingly) use “format specifier” to refer to the stuff after the colon, which I think colloquially is how people actually think about this. ↩For those output iterators that aren’t already input iterators, I think the output iterator API is a bit lacking, and I go over this in more detail in my output iterators post ↩The fact that you can just write &dyn Write to get the language to give you a non-owning type-erased object (this is like std::function_ref, not std::function) in Rust is simply spectacular. ↩My CppNow 2021 and CPPP 2021 talks compared, in part, the C++ iterator model to the Rust iterator model. The talk showed how you can implement a Rust iterator with a C++ iterator pair pretty easily, but that you can’t do better than a C++ input iterator from a Rust iterator. ↩  c++ c++23 rust formatting This post is licensed under  CC BY 4.0  by the author. Share          Contents What's so hard about views::enumerate?-Trending Tags c++ c++20 concepts optional ranges c++17 spaceship span c++23 d © 2023 Barry Revzin. Some rights reserved. Powered by Jekyll with Chirpy theme.               
"
https://news.ycombinator.com/rss,Show HN: Cross-Platform GitHub Action,https://github.com/marketplace/actions/cross-platform-action,Comments,"





Marketplace
Actions
Cross Platform Action






play-circle





GitHub Action
Cross Platform Action




v0.9.0
Latest version







    Use latest version
 









play-circle






Cross Platform Action
Provides cross platform runner


Installation
Copy and paste the following snippet into your .yml file.












- name: Cross Platform Action
  uses: cross-platform-actions/action@v0.9.0



          Learn more about this action in cross-platform-actions/action












Choose a version







v0.9.0

                Cross Platform Action 0.9.0
              

 




v0.8.0

                Cross Platform Action 0.8.0
              

 




v0.7.0

                Cross Platform Action 0.7.0
              

 




v0.6.2

                Cross Platform Action 0.6.2
              

 




v0.6.1

                Cross Platform Action 0.6.1
              

 




v0.6.0

                Cross Platform Action 0.6.0
              

 




v0.5.0

                Cross Platform Action 0.5.0
              

 




v0.4.0

                Cross Platform Action 0.4.0
              

 




v0.3.1

                Cross Platform Action 0.3.1
              

 




v0.3.0

                Cross Platform Action 0.3.0
              

 








Cross-Platform GitHub Action
This project provides a GitHub action for running GitHub Action workflows on
multiple platforms. This includes platforms that GitHub Actions doesn't
currently natively support.
Features
Some of the features that are supported include:

Multiple operating system with one single action
Multiple versions of each operating system
Allows to use default shell or Bash shell
Low boot overhead
Fast execution

Usage
Here's a sample workflow file which will setup a matrix resulting in four jobs.
One which will run on FreeBSD 13.1, one which runs OpenBSD 7.2, one which runs
NetBSD 9.2 and one which runs OpenBSD 7.2 on ARM64.
name: CI

on: [push]

jobs:
  test:
    runs-on: ${{ matrix.os.host }}
    strategy:
      matrix:
        os:
          - name: freebsd
            architecture: x86-64
            version: '13.1'
            host: macos-12

          - name: openbsd
            architecture: x86-64
            version: '7.2'
            host: macos-12

          - name: openbsd
            architecture: arm64
            version: '7.2'
            host: ubuntu-latest

          - name: netbsd
            architecture: x86-64
            version: '9.2'
            host: ubuntu-latest

    steps:
      - uses: actions/checkout@v2

      - name: Test on ${{ matrix.os.name }}
        uses: cross-platform-actions/action@v0.9.0
        env:
          MY_ENV1: MY_ENV1
          MY_ENV2: MY_ENV2
        with:
          environment_variables: MY_ENV1 MY_ENV2
          operating_system: ${{ matrix.os.name }}
          architecture: ${{ matrix.os.architecture }}
          version: ${{ matrix.os.version }}
          shell: bash
          run: |
            uname -a
            echo $SHELL
            pwd
            ls -lah
            whoami
            env | sort
Different platforms need to run on different runners, see the
Runners section below.
Inputs
This section lists the available inputs for the action.



Input
Required
Default Value
Description




run
✓
✗
Runs command-line programs using the operating system's shell. This will be executed inside the virtual machine.


operating_system
✓
✗
The type of operating system to run the job on. See Supported Platforms.


version
✓
✗
The version of the operating system to use. See Supported Platforms.


shell
✗
default
The shell to use to execute the commands. Defaults to the default shell for the given operating system. Allowed values are: default, sh and bash


environment_variables
✗
""""
A list of environment variables to forward to the virtual machine. The list should be separated with spaces.



All inputs are expected to be strings. It's important that especially the
version is explicitly specified as a string, using single or double quotes.
Otherwise YAML might interpet the value as a numeric value instead of a string.
This might lead to some unexpected behavior. If the version is specified as
version: 13.0, YAML will interpet 13.0 as a floating point number, drop the
fraction part (because 13 and 13.0 are the same) and the GitHub action will
only see 13 instead of 13.0. The solution is to explicitly state that a
string is required by using quotes: version: '13.0'.
Supported Platforms
This sections lists the currently supported platforms by operating system. Each
operating system will list which versions are supported.
OpenBSD (openbsd)



Version
x86-64
arm64




7.2
✓
✓


7.1
✓
✓


6.9
✓
✓


6.8
✓
✗



FreeBSD (freebsd)



Version
x86-64




13.1
✓


13.0
✓


12.4
✓


12.2
✓



NetBSD (netbsd)



Version
x86-64




9.2
✓



Runners
This section list the different combinations of platforms and on which runners
they can run.



Runner
OpenBSD
FreeBSD
NetBSD




Linux
✓
✓
✓


macos-10.15, macos-11, macos-12
✓
✓
✗



Under the Hood
GitHub Actions currently only support the following platforms: macOS, Linux and
Windows. To be able to run other platforms, this GitHub action runs the
commands inside a virtual machine (VM). If the host platform is macOS the
hypervisor can take advantage of nested virtualization.
The FreeBSD and OpenBSD VMs run on the xhyve hypervisor (on a macOS
host), while the other platforms run on the QEMU hypervisor (on a Linux
host). xhyve is built on top of Apple's Hypervisor
framework. The Hypervisor framework allows to implement hypervisors with
support for hardware acceleration without the need for kernel extensions. xhyve
is a lightweight hypervisor that boots the guest operating systems quickly and
requires no dependencies outside of what's provided by the system. QEMU is a
more general purpose hypervisor that runs on most host platforms and supports
most guest systems. It's a bit slower than xhyve because it's general purpose
and it cannot use nested virtualization on the Linux hosts provided by GitHub.
The VM images running inside the hypervisor are built using Packer.
It's a tool for automatically creating VM images, installing the guest
operating system and doing any final provisioning.
The GitHub action uses SSH to communicate and execute commands inside the VM.
It uses rsync to share files between the guest VM and the host. xhyve
does not have any native support for sharing files. To authenticate the SSH
connection a unique key pair is used. This pair is generated each time the
action is run. The public key is added to the VM image and the private key is
stored on the host. Since xhyve does not support file sharing, a secondar hard
drive, which is backed by a file, is created. The public key is stored on this
hard drive, which is then mounted by the VM. At boot time, the secondary hard
drive will be identified and the public key will be copied to the appropriate
location.
To reduce the time it takes for the GitHub action to start executing the
commands specified by the user, it aims to boot the guest operating systems as
fast as possible. This is achieved in a couple of ways:


By downloading resources, like the hypervisor and a few other
tools, instead of installing them through a package manager


No compression is used for the resources that are downloaded. The size is
small enough anyway and it's faster to download the uncompressed data than
it is to download compressed data and then uncompress it.


It leverages async/await to perform tasks asynchronously. Like
downloading the VM image and other resources at the same time


It performs as much as possible of the setup ahead of time when the VM image
is provisioned


Local Development
Prerequisites

NodeJS
npm
git

Instructions


Install the above prerequisites


Clone the repository by running:
git clone https://github.com/cross-platform-actions/action



Navigate to the newly cloned repository: cd action


Install the dependencies by running: npm install


Run any of the below npm commands


npm Commands
The following npm commands are available:

build - Build the GitHub action
format - Reformat the code
lint - Lint the code
package - Package the GitHub action for distribution and end to end testing
test - Run unit tests
all - Will run all of the above commands

Running End to End Tests
The end to end tests can be run locally by running it through Act. By
default, resources and VM images will be downloaded from github.com. By running
a local HTTP server it's possible to point the GitHub action to local resources.
Prerequisites

Docker
Act

Instructions


Install the above prerequisites


Copy test/workflows/ci.yml.example to
test/workflows/ci.yml


Make any changes you like to test/workflows/ci.yml, this is file ignored by
Git


Build the GitHub action by running: npm run build


Package the GitHub action by running: npm run package


Run the GitHub action by running: act --privileged -W test/workflows


Providing Resources Locally
The GitHub action includes a development dependency on a HTTP server. The
test/http directory contains a skeleton of a directory structure
which matches the URLs that the GitHub action uses to download resources. All
files within the test/http are ignore by Git.


Add resources as necessary to the test/http directory


In one shell, run the following command to start the HTTP server:
./node_modules/http-server/bin/http-server test/http -a 127.0.0.1

The -a flag configures the HTTP server to only listen for incoming
connections from localhost, no external computers will be able to connect.


In another shell, run the GitHub action by running:
act --privileged -W test/workflows --env CPA_RESOURCE_URL=<url>

Where <url> is the URL inside Docker that points to localhost of the host
machine, for macOS, this is http://host.docker.internal:8080. By default,
the HTTP server is listening on port 8080.






Stars

 


          Star
 29
  





Contributors



 

 


Categories


  Continuous integration


  Testing




Links



cross-platform-actions/action
    



Open issues
        1




Pull requests
      1




Report abuse
 

Cross Platform Action is not certified by GitHub. It is provided by a third-party and is governed by separate terms of service, privacy policy, and support documentation.
    




"
https://news.ycombinator.com/rss,"Late Hokusai: Thought, technique, society",https://www.britishmuseum.org/research/projects/late-hokusai-thought-technique-society,Comments,"




405 Not allowed


Error 405 Not allowed
Not allowed
Error 54113
Details: cache-iad-kiad7000064-IAD 1673903211 1393961676

Varnish cache server


"
https://news.ycombinator.com/rss,TAB electronics books,https://worldradiohistory.com/BOOKSHELF-ARH/Bookshelf_TAB.htm,Comments,"




TAB BOOLS LIBRARY: TAB electronics books
























			|
















 






  TAB Books 



														Search:
														Select TAB from Menu 













     Books from the TAB Library 
					in the areas of Broadcasting and Electronics










							TAB Books was originally based in Blue Ridge Summit, 
							Pennsylvania, TAB was founded by Verne M. Ray and 
							Malcolm Parks Jr. in 1964 to publish technically 
							oriented magazines; TAB is an acronym for Technical 
							Author's Bureau. It became TAB Books Inc. in 1980 
							and published books in a wide variety of mostly 
							technical fields. It was acquired by McGraw-Hill in 
							1990, at which time it published books in 12 fields 
							including computing, electronics, aviation, 
							engineering, maritime, and several how-to subjects

















									Tab Books 






 















The Complete Broadcast Antenna Handbook  
									Cunningham 1977

									TAB-72-Oscilloscope-Techniques-1958 

									 

 

									 





 


 













Radio Control Manual
									Safford - 1979

									Handbook of Advanced Robotics
									Safford 1982 

									Electronic Circuit Design Handbook 1975 

Build a Personal Earth Station 1982

									Color TV Troubles and Solutions
									 1972



















 




199 Test & Alignment Procedures

									Principles and Practice of Impedance


									The Mater IC Cookbook

4 Channel Stereo Sessions 1973 

									The Complete Handbook of Robotics
									Safford 1978 





 


 


 



 


 





Audio Systems Handbook 

									Basic Color Television 

									Impedance
									1976

How to Troubleshoot & Repair Electronic 
									Circuits 

									Troubleshooting Microprocessors & Digital 
									Logic 




















									 





Basic 
									Transistor Course

									Oscilloscope Techniques

									How to Design & Build Your Own Custom TV 
									Games

Radio Communications Receivers

									Understanding Electronics
									1989 






 

									 


 



 

									 



Master Handbook of Electronic Tables 

									Giant Handbook of Electronic Circuits 

									Electronic Databook
									3rd Edition

Electronic Components

									Digital Interfacing with an Analog World  
















									Tab  Library courtesy of
									
									ncwalz





















							Service Manual 
	#2
	Supp. 1
















"
https://news.ycombinator.com/rss,"GPU Caching Compared Among AMD, Intel UHD, Apple M1",https://chipsandcheese.com/2022/05/21/igpu-cache-setups-compared-including-m1/,Comments,"




iGPU Cache Setups Compared, Including M1




May 21, 2022
clamchowder

1 Comment



Like CPUs, modern GPUs have evolved to use complex, multi level cache hierarchies. Integrated GPUs are no exception. In fact, they’re a special case because they share a memory bus with CPU cores. The iGPU has to contend with CPUs for limited memory bandwidth, making caching even more important than with dedicated GPUs. 
At the same time, the integrated nature of integrated GPUs provides a lot of interesting cache design options. We’re going to take a look at paths taken by AMD, Intel, and Apple.
Global Memory Latency
GPUs are given a lot of explicit parallelism, so memory latency isn’t as critical as it is for CPUs. Still, latency can play a role. GPUs often don’t run at full occupancy – that is, the amount of parallel work they’re tracking isn’t maximized. We have more on that in another article, so we’ll go right to the data.
Testing latency is also a good way of probing the cache setup. Doing so with bandwidth isn’t as straightforward because requests can be combined at various levels in the memory hierarchy, and defeating that to get clean breaks between cache levels can be surprisingly difficult.


The Ryzen 4800H’s cache hierarchy is exactly what you’d expect from AMD’s well known GCN graphics architecture. Each of the 4800H’s seven GCN-based CUs have a fast 16 KB L1 cache. Then, a larger 1 MB L2 is shared by all of the CUs. AMD’s strategy for dealing with memory bus constraints appears quite simple: use a higher L2 capacity to compute ratio than that of discrete GPUs. A fully enabled Renoir iGPU has 8 CUs, giving 128 KB per CU. Contrast this with AMD’s Vega 64, where 4 MB of L2 gives it 64 KB per CU.

GCN’s simple, two-level cache hierarchy with a 16 KB L1 is instantly recognizable across generations and form factors. Thanks to cha0s for contributing data
Apple’s cache setup is similar, with a fast L1 followed by a large 1 MB L2. Apple’s L1 is half of AMD’s size at 8 KB, but has similar latency. This low latency suggests it’s placed within iGPU cores, though we don’t have a test to directly verify this. Compared to AMD, Apple’s L2 is a bit lower latency, which should help make up for the smaller L1. We also expect to see a 8 MB SLC, but that doesn’t really show up in the latency test. It could be the somewhat lower latency area up to 32 MB.
Then, we have Intel. Compared to AMD and Apple, Intel tends to use a less conventional cache setup. Right off the bat, we’re hitting a large cache shared by all of the GPU’s cores. It’s at least 1.5 MB in size, making it bigger than AMD and Apple’s GPU-level caches. In terms of latency, it’s somewhere between AMD and Apple’s L2 caches. That’s not particularly good, because we don’t see a smaller, faster cache in front of it. But its large size should help Intel keep more memory traffic within the iGPU block. Intel should have smaller, presumably faster caches in front of the large shared iGPU-level cache. But we weren’t able to see them through testing.
Like Apple, Intel has a large, shared chip-level cache that’s very hard to spot on a latency plot. This is strange – our latency test clearly shows the shared L3 on prior generations of Intel integrated graphics.

Looks like the shared chip-level L3 doesn’t help latency much in certain Intel iGPU designs
From this first glance at latency, we can already get a good idea of how each manufacturer approaches caching. Let’s move on to bandwidth now.
Global Memory Bandwidth
Bandwidth is more important to GPUs than to CPUs. Usually, CPUs only see high bandwidth usage in heavily vectorized workloads. For GPUs though, all workloads are vectorized by nature. And bandwidth limitations can show up even when cache hitrates are high.


AMD and Apple’s iGPU private caches have roughly comparable bandwidth. Intel’s is much lower. Part of that is because Alder Lake’s integrated graphics have somewhat different goals. Comparing the GPU configurations makes this quite obvious:
FP32 ALUsClock SpeedFP32 FMA Vector ThroughputAMD Ryzen 4800H, Vega 7448 (out of 512 possible)1.6 GHz1433.6 GFLOPsIntel Core i5-12600K, Xe GT12561.45 GHz742.4 GFLOPsApple M1, 7 Core iGPU896 (out of 1024 possible)1.278 GHz?2290.2 GFLOPs
AMD’s Renoir and Apple’s M1 are designed to provide low end gaming capability to thin and light laptops, where a separate GPU can be hard to fit. But desktop Alder Lake definitely expects to be paired with a discrete GPU for gaming. Understandably, that means Intel’s iGPU is pretty far down on on the priority list when it comes to power and die area allocation. Smaller iGPUs will have less cache bandwidth, so let’s try to level out the comparison by using vector FP32 throughput to normalize for GPU size.

Measured bandwidth values are used. If a cache level isn’t present, the next one down is used. GTX 1070 Max-Q was tested with Vulkan, because the L1 is not used for OpenCL. Intel shows no faster cache level even with Vulkan
Intel’s cache bandwidth now looks better, at least if we compare from L2 onward. Bytes per FLOP is roughly comparable to that of other iGPUs. Its shared chip-level L3 also looks excellent, mostly because its bandwidth is over-provisioned for such a small GPU.
As far as caches are concerned, AMD is the star of the show. Renoir’s Vega iGPU enjoys higher cache bandwidth to compute ratios than Intel or Apple. But its performance will likely be dependent on cache hitrate. L2 misses go directly to memory, because AMD doesn’t have another cache behind it. And Renoir has the weakest memory setup of all the iGPUs here. DDR4 may be flexible and economical, but it’s not winning any bandwidth contests. Apple and Intel both have a stronger memory setup, augmented by a big on-chip cache. 
Local Memory Latency
GPU memory access is more complicated than on CPUs, where programs access a single pool of memory. On GPUs, there’s global memory that works like CPU memory. There’s constant memory, which is read only. And there’s local memory, which acts as a fast scratchpad shared by a small group of threads. Everyone has a different name for this scratchpad memory. Intel calls it SLM (Shared Local Memory), Nvidia calls it Shared Memory, and AMD calls it LDS (Local Data Share). Apple calls it Tile Memory. To keep things simple, we’re going to use OpenCL terminology, and just call it local memory.

Pointer chasing latency with local memory
AMD and Apple take about as long to access local memory as they do to hit their first level caches. Of course, latency isn’t the whole story here. Each of AMD’s GCN CUs has 64 KB of LDS – four times the capacity of its L1D cache. Bandwidth from local memory is likely higher too, though we currently don’t have a test for that. Clinfo on M1 shows 32 KB of local memory, so M1 has at least that much available. That figure likely only indicates the maximum local memory allocation by a group of threads, so the hardware value could be higher.
Intel meanwhile enjoys very fast access to local memory, as does Nvidia, which is here for perspective. Their story is an interesting one too. Prior to Gen10, Intel put their SLM along the iGPU’s L3, outside the the subslices (Intel’s cloest equivalent to GPU cores on Apple and CUs on AMD). For a long time, that meant Intel iGPUs had unimpressive local memory latency.

Good job Intel
Starting with Gen 11, Intel thankfully moved the SLM into the subslice, making the local memory configuration similar to AMD and Nvidia’s. Apple likely does the same (putting “tile memory” within iGPU cores) since local memory latency on Apple’s iGPU is also quite low. 
CPU to GPU Copy Bandwidth
A shared, chip-level cache can bring other benefits. In theory, transfers between CPU and GPU memory spaces can go through the shared cache, basically providing a very high bandwidth link between the CPU and GPU. Due to time and resource constraints, slightly different devices are tested here. But Renoir and Cezanne should be similar, and Intel’s behavior is unlikely to regress from Skylake’s.

Only Intel is able to take advantage of the shared cache to accelerate data movement across different blocks. As long as buffer sizes fit in L3, Skylake handles copies completely within the chip, with performance counters showing very little memory traffic. Larger copies are still limited by memory bandwidth. The Core i7-7700K tested here only has a dual channel DDR4-2400 setup, so that’s not exactly a strong point. 
Apple in theory should be able to do the same. However, we don’t see an improvement for small copy sizes that should fit within M1’s system level cache. There are a couple of explanations. One is that M1 is unable to keep CPU to GPU transfers on-die. Another is that small transfers are kept on-die, but commands to the GPU suffer from very high latency, resulting in poor performance for small copies. Intel’s Haswell iGPU suffers from the same issue, so the second is a very likely explanation. When we get to larger copy sizes, M1’s high bandwidth LPDDR4X setup does a very good job.
AMD’s performance is very easy to understand. There’s no shared cache, so bandwidth between the CPU and GPU is limited by memory bandwidth. 
Finally, it’s worth noting that all of the iGPUs here, as well as modern dedicated GPUs, can theoretically do zero-copy transfers by mapping the appropriate memory on both the CPU and GPU. But we currently don’t have a test written to analyze transfer speeds with mapped memory.
Final Words
GPUs tend to be memory bandwidth guzzlers, and feeding an integrated GPU is particularly challenging. Their memory subsystems are typically not as beefy as those of dedicated GPUs. To make matters worse, the iGPU has to fight with the CPU for memory bandwidth. 
Apple and Intel both tackle this challenge with sophisticated cache hierarchies, including a large on-chip cache that serves the CPU and GPU. The two companies take different approaches to implementing that cache, based on how they’ve evolved their designs. Intel has the most integrated solution. Its L3 cache does double duty. It’s tied very closely to the CPU cores on a high speed ring interconnect, in order to provide low latency for CPU-side accesses. The iGPU is simply another agent on the ring bus, and L3 slices handle iGPU and CPU core requests in the same way.

Intel has a fast, flexible ring bus that can connect a variety of agents. The iGPU is simply another agent on the ring bus
Apple uses more specialized caches instead of trying to optimize one cache for both the CPU and GPU. M1 implements a 12 MB L2 cache within the Firestorm CPU cluster, which fills a similar role to Intel’s L3 from the CPU’s perspective. A separate 8 MB system level cache helps reduce DRAM bandwidth demands from all blocks on the chip, and acts as a last stop before hitting the memory controller. By dividing up responsibilities, Apple can tightly optimize the 12 MB L2 for low latency to the CPU cores. Because the L2 is large enough to absorb the bulk of CPU-side requests, the system level cache’s latency can be higher in order to save power.
M1 still has a bit of room for improvement. Its cache bandwidth to compute ratio could be a touch higher. Transfers between the CPU and GPU could take full advantage of the system level cache to improve bandwidth. But these are pretty minor complaints, and overall Apple has a pretty solid setup.

Apple has a slower, SoC-wide interconnect and a variety of blocks on it. Only the Firestorm cluster and GPU are drawn to save space
AMD’s caching setup is bare-bones in comparison. Renoir (and Cezanne) are basically a CPU and GPU glued together. Extra GPU-side L2 is the only concession made to reduce memory bandwidth requirements. And “extra” here only applies in comparison to discrete GCN cards. 1 MB of L2 isn’t anything special next to Apple and Intel, both of which have 1 MB or larger caches within their iGPUs. If the L2 is missed, AMD goes straight to memory. Memory bandwidth isn’t exactly AMD’s strong point, making Renoir’s lack of cache even worse. Renoir’s CPU-side setup isn’t helping matters either. A L3 setup that’s only 1/4 the size of desktop Zen 2’s will lead to additional memory traffic from CPU cores, putting even more pressure on the memory controller. 

Cezanne has a pretty similar layout, but with Zen 3 cores in a single core complex
AMD’s APU caching setup leaves a lot to be desired. Somehow, AMD’s iGPU still manages to be competitive against Intel’s Tiger Lake iGPU, which speaks to the strength of their GCN graphics architecture. I just wish they took advantage of that potential to deliver a killer APU. After all, AMD has a lot of low hanging fruit to improve with. RDNA 2 based discrete GPUs use a large “Infinity Cache” sitting behind Infinity Fabric to reduce memory bandwidth requirements. Experience gained implementing that cache could trickle down to AMD’s integrated GPUs. 
AMD’s RDNA2 uses a giant 128 MB Infinity Cache to compete with Nvidia while having a lot less memory bandwidth
It’s easy to imagine an Infinity Cache delivering benefits beyond reducing GPU memory bandwidth requirements too. For example, the cache could enable faster copies between GPU and CPU memory. And it could benefit CPU performance, especially since AMD likes to give their APUs less CPU-side L3 compared to desktop chips.
But such a move is unlikely with in the next generation or two. With AMD moving to LP/DDR5, the bandwidth boost along with large architecture changes allowed AMD to double iGPU performance with Rembrandt. Factor in Renoir and Cezanne’s already adequate graphics performance, Intel’s inability to capitalize on their superior cache setup, and Apple’s closed ecosystem, there’s little pressure on AMD to make aggressive moves. 
Infinity cache on an APU will also require significant die area to be effective. Hitrate with a 8 MB system level cache is abysmal:

28% bandwidth reduction implies a 28% hitrate. Source – ARM
Cache hitrate tends to increase with the logarithm of size, so AMD would probably want to start with at least 32 MB of cache to make it worth the effort. That means a bigger die, and unfortunately, I’m not sure if there’s a market for a powerful APU in the consumer x86 realm.
If you like our articles and journalism and you want to support us in our endeavors then consider heading over to our Patreon or our PayPal if you want to toss a few bucks our way or if you would like to talk with the Chips and Cheese staff and the people behind the scenes then consider joining our Discord.
Test Setup
Memory SetupNotesAMD Ryzen 4800H (Renoir)Dual channel DDR4-3200 22-22-22-52Eluktronics RP-15 laptopIntel Core i5-12600K (Alder Lake)2x DDR5-4800 CL40Thanks to Luma for running the testsApple M1On-package LPDDR4XMacbook Air with 7 core GPU, thanks to Longhorn for running tests

Author







clamchowder



View all posts
















Please leave this field empty Don’t miss our articles!

Email Address *






Check your inbox or spam folder to confirm your subscription.

 




Related Posts








Post navigation
← Examining Centaur CHA’s Die and Implementation GoalsGraviton 3: First Impressions →



1 thought on “iGPU Cache Setups Compared, Including M1” 





 Alan says: 

January 16, 2023 at 7:33 pm 


> as modern dedicated GPUs, can theoretically do zero-copy transfers by mapping the appropriate memory on both the CPU and GPU.
Is this true for dgpus? How does this work?

Reply 



Leave a Reply Cancel reply










This site uses Akismet to reduce spam. Learn how your comment data is processed.

"
https://news.ycombinator.com/rss,Granian – a Rust HTTP server for Python applications,https://github.com/emmett-framework/granian,Comments,"








emmett-framework

/

granian

Public







 

Notifications



 

Fork
    9




 


          Star
 415
  









        A Rust HTTP server for Python applications
      
License





     BSD-3-Clause license
    






415
          stars
 



9
          forks
 



 


          Star

  





 

Notifications












Code







Issues
8






Pull requests
0






Discussions







Actions







Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Discussions
 


                  Actions
 


                  Security
 


                  Insights
 







emmett-framework/granian









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











master





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





13
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




gi0baro

Update CI release workflow




        …
      




        b843a90
      

Jan 13, 2023





Update CI release workflow


b843a90



Git stats







132

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github



Update CI release workflow



Jan 13, 2023









benchmarks



Update benchmarks



Jan 13, 2023









docs/spec



Fix typos (#14)



Nov 17, 2022









granian



Follow WSGI spec on response iterable (#29)



Jan 13, 2023









lib/pyo3-asyncio



Bump pyo3-asyncio to 0.17



Oct 25, 2022









src



Code cleanup



Jan 13, 2023









tests



Fix wsgi.input out of spec (close #24)



Jan 12, 2023









.gitignore



first implementation



Apr 15, 2022









Cargo.lock



Add PyPy support



Jan 3, 2023









Cargo.toml



Add PyPy support



Jan 3, 2023









LICENSE



first implementation



Apr 15, 2022









README.md



Update benchmarks results



Dec 24, 2022









build.rs



Add PyPy support



Jan 3, 2023









pyproject.toml



Add PyPy support



Jan 3, 2023









setup.py



review package meta



Apr 18, 2022




    View code
 















Granian
Rationale
Features
Quickstart
Project status
License





README.md




Granian
A Rust HTTP server for Python applications.
Rationale
The main reasons behind Granian design are:

Have a single, correct HTTP implementation, supporting versions 1, 2 (and eventually 3)
Provide a single package for several platforms
Avoid the usual Gunicorn + uvicorn + http-tools dependency composition on unix systems
Provide stable performance when compared to existing alternatives

Features

Supports ASGI/3, RSGI and WSGI interface applications
Implements HTTP/1 and HTTP/2 protocols
Supports HTTPS
Supports Websockets over HTTP/1 and HTTP/2

Quickstart
You can install Granian using pip:
$ pip install granian

Create an ASGI application in your main.py:
async def app(scope, receive, send):
    assert scope['type'] == 'http'

    await send({
        'type': 'http.response.start',
        'status': 200,
        'headers': [
            [b'content-type', b'text/plain'],
        ],
    })
    await send({
        'type': 'http.response.body',
        'body': b'Hello, world!',
    })
and serve it:
$ granian --interface asgi main:app

You can also create an app using the RSGI specification:
async def app(scope, proto):
    assert scope.proto == 'http'

    proto.response_str(
        status=200,
        headers=[
            ('content-type', 'text/plain')
        ],
        body=""Hello, world!""
    )
and serve it using:
$ granian --interface rsgi main:app

Project status
Granian is currently under active development.
Granian is compatible with Python 3.7 and above versions on unix platforms and 3.8 and above on Windows.
License
Granian is released under the BSD License.









About

      A Rust HTTP server for Python applications
    
Topics



  python


  rust


  http


  http-server


  asyncio


  asgi



Resources





      Readme
 
License





     BSD-3-Clause license
    



Stars





415
    stars

Watchers





7
    watching

Forks





9
    forks







    Releases
      13







Granian 0.2.1

          Latest
 
Jan 13, 2023

 

        + 12 releases





Sponsor this project



 

 

 Sponsor
 
Learn more about GitHub Sponsors







    Packages 0


        No packages published 







        Used by 6
 




























    Contributors 4








gi0baro
Giovanni Barillari

 






kianmeng
Kian-Meng Ang

 






shulcsm
Mārtiņš Šulcs

 






cirospaciari
Ciro Spaciari

 





Languages












Rust
83.1%







Python
16.5%







Other
0.4%











"
https://news.ycombinator.com/rss,Adventures in Mastoland,https://searchtodon.social/Adventures-in-Mastoland.html,Comments,"    
 Adventures in Mastoland

We strongly advise you to read up on the myriad of failed experiments in this space.



Great! Where can I find them?



…hm?

This is a retrospective post about my experiment Searchtodon, an attempt at building a privacy conscious personal timeline search tool for Mastodon. I’m intentionally vague about people and projects relevant to this story, to protect the innocent. Titles used are lighthearted pop-culture references and do not semantically reflect on the content or my opinion.
Last updated 2023-01-16.
Introduction
I’ve been online since ~1997, AOL Chat, Forums, AIM, jabber, IRC, the usual, and then Twitter since 2007, did a first Mastodon exploration in 2017 and finally got swept over fully in November 2022. By all accounts, I’m new here, but none of this is new to me.
A few years ago, I built a toy Twitter web client for myself for experimenting with getting more out of my timeline: e.g. don’t show RTs, but show my top 10 RTs for the last 12/24 hours. Something similar I hear Twitter Pro/Blue/whatever has got now, by way of acquiring nuzzle, with top links posted from your timeline.
I’m not a very good frontend web developer, so this didn’t go far, but one thing that this left me convinced with is that there is a distinct lack of things you can do with your timeline. I put ~15 years into meeting lovely folks around the globe and connecting with them over Twitter, but the default experience does not allow me to get the most out of it. E.g. say I follow more people than I can possibly read all posts from, it’d be great if there was a “slow” section, where people that rarely post are listed, so I don’t miss any of their posts while other folks are more busy, or just 12 hours worth of time zones away, without me having to put that together as a list manually.
As an aside, I know I could follow fewer people, but let’s be real, that’s not gonna happen. I have about a dozen of other ideas what useful things can be done with your carefully curated timeline and I believe there is an opportunity for a lot of fun and useful things that can be done with using someone’s social timeline as the data source and better connect people that way.
One of the things I’ve missed on Twitter for the longest time is being able to recall what I’ve seen before. While Twitter has/had search, to my knowledge there is no way to filter by tweets that have shown up in your timeline. This is honestly baffling to me, and I don’t see this getting fixed any time soon. With my custom client, I could have built that, but never got around to, mainly because I feared it not being worth it (the other work having been a nice learning exercise), because Twitter could take it out any time, as shown with the native clients’ debacle just this week. — I’m not interested in building anything for that platform for the time being.
Into the Mastoverse
With taking the plunge into moving most of my social online activity to Mastodon in late 2022, I am intrigued by the possibilities of an open platform. I believe the biggest long-term impact this has is finally getting the social graph into the open, that’s amazing. But with my small demo on Twitter I knew it was feasible to bring some of my ideas over here and I don’t have to worry about losing access because of the whims of the centralised owners of a commercial platform. This is very very appealing for a lot of tinkery-folk like me. If you are reading this, you are probably one of them.
Since Ivory timeline filtering is already mature enough to cover the needs of the client I wrote for Twitter (no boosts, no replies), I had no need to port that over, and instead thought I go after the next most pressing thing: recalling things I have seen in my timeline. In my case, I follow a lot of techies in very different communities (Web dev, JavaScript, Erlang, Python, Rust, macOS, FreeBSD, Databases etc.) and as a result I sweep up a lot of information that I’m generally interested in, but only occasionally dip into more deeply. The effect here is that when I know my macOS folks talk about certain issues for developers a new OS version brings, a while later, the JS community starts running into this issue, and now I can bridge that gap and can help out. But I can’t remember all the little details and references about everything, so I need a way to find things that I’ve seen before.
Yes, there are favs and bookmarks, but if I knew something was gonna be important, I’d have filed it away already otherwise, so that’s not really helping. Plus not all instances enable search on bookmarks.
I fully realise that many people have none of these problems, or are fine using bookmarks, and that’s great, but it doesn’t solve my problem, and I know now that I’m not the only one.
Act Two
I’ll spare you the details of technically getting to a point where I could search my timeline, but let’s just say a little service that runs as an OAuth app dumping plaintext files into a directory on my Mac and then using it’s built-in Spotlight search was done in about an hour or two.
However, it was clear that a “runs on your Mac” solution doesn’t work when your computer is asleep, and with the somewhat exasperating default 400 post timeline limit, not being able to catch up things properly would defeat the usefulness of this. — There is an argument to be made for: well if you are asleep and miss posts, you didn’t see them, so you can’t know to search for them. And while that reasoning is certainly correct, it misses that folks use multiple clients to access Mastodon at different times and that I might see a post in Ivory before going to bed, but after I closed my computer for the day. — I now concede however, that a “really only the posts I’ve seen” indexing would be preferable, but there is no cross-client standard for reporting that anywhere, so I’m not hopeful this will get anywhere. The only other option is: index your entire home timeline.
Attentive readers will point out that at the time of all this, I was on an instance who’s ToS state:

Content on this instance must not be archived or indexed wholesale by automated means by any user or service. Active users may export their data through the export functionality or the API.

When reviewing the feasibility of my project I read “wholesale” as ”everybody’s all the time” and “export their data through […] the API” as “keep a copy of my timeline” as covered as allowed. — I know now that I was mistaken in my, granted, optimistic reading of this, and that I should have reached out to the admins there to double check.
Back to November 2022: when first signing up for that instance I distinctly remember that one of the top configuration options for the profile after sign up was “Opt-out of search engine indexing” and it was checked by default for me (from here on out, I’ll refer to this feature as the noindex flag). I thought that it was really nice that this is such a prominent control feature empowering users and choosing a safe default.
This was my first mistake.
It turns out, not only do few people seem to know about this, almost nobody made a conscious choice here. And if I read things correctly, this flag does not federate, which seems like a tremendous oversight.
I reached out to a few folks that I saw have this enabled and discussed whether what I was building was considered “a search engine” (I didn’t think so), and I learned that a) there are folks that would continue to to use the noindex flag to not get indexed by Google, but they’d be fine being part of a “scoped to a user’s timeline and no one else” type of search (I too fell into this category at the time), and b) on properly considering this setting for the first time, and while agreeing that my thing wasn’t a Google-like public search engine, they’d expect the noindex flag to cause their posts to be excluded. — I did fear that this would diminish the usefulness of the search tool, but eventually came around to this point of view.
While talking to more folks, I was introduced to the #nobot hashtag that accounts use to indicate they don’t want to have anything to do with any bots, which I made to behave like the noindex flag, and I added #nosearch in case folks wanted to be more fine-grained with this.
Threats
Next, I needed to consider if I was adding new vectors of abuse to Mastodon (or strengthening old ones). My reasoning went thus:

if I was a bad actor, my 1-hour-index-to-spotlight experiment would give me all the benefits without anyone knowing about this. I am sadly convinced this is already happening, it is just so simple.

running a custom instance gives you admin rights to all posts that are being read on that instance and those posts live in a database that supports searching. And the ecosystem in general seems to fine with folks running their own instances.


I still don’t think that anything I did would make any of this easier for bad actors or worse for the community.
While considerng  other prior experiences that could inform this, I thought of Twitter’s third party thread unrollers that rehost tweets with advertising next to them. They were genuinely useful before Twitter fixed its rendering of threads, but it was annoying to me. Eventually, the services added a feature where the thread-poster could block the unroll-account and unrolling would no longer work. While annoyed that I had to do that four times, I could live with the power balance here. 

Mastodon isn’t Twitter, but I also thought this set enough of a precedent that honouring noindex, #nobot and #nosearch would be a decent enough equivalent.
This was my second mistake.
Prior Art
One of the biggest volume of feedback on this was that I should have looked at the search projects that came before. If those folks had taken the time to read the associated website, they’d have seen that I did, alas.
My calculus was:

noindex & friends are well established
no additional threat vector is introduced
indexing is limited to (mostly) what a human has seen
and searching is limited only ever to that one human
open to feedback & suggestions, especially critical
and promise to shut it down with enough negative signal

This is substantially different from the other “fediverse crawlers” (as later confirmed by some of the most fierce critics) that I saw, technically and in framing. I didn’t think this even qualifies as a crawler, but I did get some early feedback that folks would consider this a crawler, but I was optimistic that this would at least bring out valuable feedback for future iterations.
Finally, for the quick demo to validate the user experience and basic functionality to work, I put this together as an OAuth app that runs as web service. That way you can just sign into the UI and start using Mastodon as before, without having to run anything yourself. To give this an honest shot, I believed this needed to be easy to get started with.
A consequence of this is that, for the multiple hours this experiment ran, all indexing happened on one of my servers. Given the framing of this as an experiment with a direct goal to inform operational overhead for Mastodon operators (more on this below), and given that I generally know what I’m doing running web services, I thought that was a valuable trade-off to make for now.
That was my third mistake.
The Experiment
From the get-go, I framed Searchtodon as an experiment with three hypotheses to validate (or not). The quotes are from the Searchtodon website.

1. User experience: can private search for Mastodon be done in a functional way and will folks find this useful?

First, quantities.
The post announcing Searchtodon has received more interactions than I could have imagined:

Comments: 129
Boosts: 700
Favs: 938

The number of signups at the end of the experiment:

304

While not all boosts should be counted as support, as some folks are just interested in the experiment, this still proves to me that a feature that lets you search through your home timeline would be popular enough. I hope the folks at Mastodon see this as encouragement in this direction.
A lot of acceptance surely comes from using an excellent open source web client, as the “Home Search” addition done by me was minimal, and limited in functionality.
But numbers aren’t everything, what about the qualitative angle? — From the folks interested in the functionality, the feedback was predominantly positive. It didn’t work for a few folks, and their feedback was understanding of the early stage of the project.
A good number of people didn’t quite understand the difference between this and what Mastodon already supports. I could clearly have explained things a little better.
Finally, some folks wrote in that they have no need for this as they don’t have this problem, or the regularly provided tools are enough for them. This was entirely expected. It is good to keep in mind that this is not something “for everybody” when arguing for its general acceptance. But enough people liked it.
Hypothesis one: confirmed. Enough people find this useful.

2. Operational feasibility: index data costs storage, search costs compute, etc. Even if the Searchtodon stack is slightly different from Mastodon, it plays in the same ballpark. In case premise #1 comes back positive, we can learn what additional resources will instance operators need to provide private search.

I’m happy to report that things look entirely feasible.
The way things worked, I stored the full post JSON with inline account and reblog (with its own reblog.account) as individual entries in CouchDB. This made the indexing and retrieving side extremely simple, but duplicated quite a bit of data. If two users received the same post, that was saved twice in strictly separate database files, to make account deletions easy.
From that base storage, I created Lucene search indexes partitioned by user timeline.
There are three obvious ways to improve on this:

deduplicate data, all those account records could have been exfiltrated into their own representations.
deduplicate posts across all users.
or forego the additional JSON store by side-caring this to Mastodon directly and only add the search indexes (Lucene or Postgres should both work. Given everything else, this is the probably best way forward).

That said, here’s the data usage as implemented:

~ 1GB JSON + index storage per 100 users per 24 hours. The optimisations outlined above could reduce this need consevatively by 50% – 80%

used ZFS block level lz4 compression, with compressratio ranging between 3.5x – 4.3x . One could go with a higher compression level with, say gzip-5 (higher gzip settings are not worth the CPU/space trade-off). Additional space savings from experience with gzip-5 could be another ~4x.


Hypothesis two: Doable. I’ve since found some demo implementations using Postgres for search in Mastodon, but no serious take-up as far as I can tell. Worth watching though. — Running this as a free-forever not-part-of-Mastodon service would likely not be sustainable, and it remains unclear if folks would pay for storage costs over multiple years (or if the index should go back that far, for that matter).
Additional learnings: Some Mastodon admins are usually not computing resource constrained, but person-time constrained. They just run the bare minimum Mastodon setup and that’s already enough work for a team of two admins. If those admins encounter optional pieces of infrastructure, they will skip them, as each piece adds to the complexity of the system operated with regards to software updates and interoperability. The only way forward here would be having this feature available in the Mastodon core distribution, using the Postgres full text search feature.

3. Community (most important): is private search for Mastodon actually something that can be done in a way that gels with the community rather than against it? — The folks behind Searchtodon do not wanna fight anyone and if there is enough negative signal, we’ll shut it down. — Open questions include: how to handle reporting, defederation after the fact of instance-to-instance trust, are noindex, #nobot & #nosearch good enough account markers or will the Mastodon community have to invent more mechanics around delegating trust from a user’s timeline to tools operating on their timeline?

Hypothesis three: Nope. It is safe to conclude that as implemented, Searchtodon does not “gel with the community”. This warrants its own section:
The Feedback
As outlined above, the support was tremendous and encouraging. I genuinely wanted to help out folks with a problem and Searchtodon found an audience.
Among the supporters were folks that said they liked the idea, but wouldn’t want to sign up for an experimental service run by one person, which is more than fair, this is me most of the time these days.
The next biggest caveat mentioned here however was the implementation choice of making this a standalone service. As that has two distinct consequences:

data that is previously known and trusted to only exist in one instance now lives in a second place that needs to be trusted to keep that data safe.

folks’ posts that were stored and indexed didn’t opt into that (c.f. noindex & friends above).


It also highlighted a distinct lack of mechanics around this in the Mastodon ecosystem, more on that later.
From here on out it got a little more erratic, as folks commented and criticised things that didn’t match reality, or only confirmed their preconceived notions. Some folks have a reflex to react negatively when hearing “mastodon” and “search” in the same sentence without actually evaluating what is proposed. Names were called.
After the Shutdown
The post-shutdown feedback also came in a bunch of relevant flavours:

thanks for taking this down

thanks for listening, this is rare

thanks for paving the way, I’ll rethink my approach (good!)

thanks for trying this, I’ll find a better loophole (don’t do this)

thanks for trying this, I hope this problem will be solved eventually


Lessons Learned

Enough folks of the current Fediverse want opt-in experimenting rather than opt-out experimenting.

There is a difference between what a thing is vs. what people perceive that thing to be. For success, it is important for whoever makes the thing to respect both of these positions.
Profile hashtags for opt in/out are limited in usefulness:

many experiments are not viable if there are not enough posts to operate and reaching a critical mass of opt-ins limits progress. Some folks think this is good.

with account metadata limits, this can only go so far. Adding more and more hashtags is not a scalable solution. And forcing new users through a gauntlet of choices with hard-to-grasp consequences is not gonna make this place more popular.

maybe the Mastodon onboarding experience could include a “allow my posts to participate in experiments” like operating systems ask to “send crash logs to the developer” on signup/upgrade. Then we could formulate a set of guidelines that experiments must adhere to. Maybe some more differentiation like CC-BY-NC where folks can say “you can do whatever you want”, “experimentation is okay, but no commercial exploitation (ads etc) of my posts”, or “nope, nothing” (the default), or anything in between.



Folks don’t trust a central service and/or closed source (and rightfully so). Ideally start as open source.

Folks value the trust relationship they have with their instance admins and the admins of their followers. Things like OAuth apps that take a delegation of that trust, especially without an opt-in process, are suspect under certain circumstances.

I think this area benefits most from clarification in the larger ecosystem. Consider a web or native client that uses the Mastodon REST API and OAuth to authenticate a user.

They get a copy of all the data that the users have access to and usually present them in an ephemeral manner (old content gets eventually pushed out by new posts coming in). This seems to be a largely accepted use of posts, APIs and terms of service.

To offer a good application experience, the client app, in both the web and native scenario, keeps a local cache of all the data it is currently accessing, so restarting the app is instantaneous and no new data has to be downloaded from the internet. This still is largely a reality today and appears to be largely accepted.

Depending on the implementation, it could be that an app not merely caches the data, but stores it permanently, which makes it part of the device’s backup. These backups more often than not are Google or Apple cloud backups under control by, admittedly very capable, admins that the original poster has no trust relationship with, and no option to opt in or out. This too seems to be generally accepted.

Next is a client that just keeps all data stored it’s ever seen. I’m not aware of a public client that does this, but there are a few folks looking to experiment with this (not me). This would probably violate some instance’s ToS, but I haven’t seen this as grounds for defederation yet. I’m relatively certain, someone is doing this privately today. — I’ve seen responses go in either direction here, with folks saying that seems fair and absolutely not. For people in the second camp, Mastodon has no effective way to protect them, but I predict will need one before long.

The one exception of this is single-user instances, which are commonly accepted, it seems. Unless the person running one doesn’t fall out of anyone’s good graces, nobody will even know what they are doing with the data. A few people have privately reached out to me that they used this exact loophole to find things in their timeline in the past (before you look, none of them are in my follower/followee lists). I like the idea of single-user instances, and I’d prefer them not to get ostracised because some of them might be bad actors.


Next is a server-side project, like Searchtodon. We don’t have to rehash that this implementation didn’t meet the community’s standards, but changes could be made:

make this an open source tool that anyone can choose to run

there would have to be an easy way to blanket opt out of these. Maybe a good-faith list that an operator can add themselves to that dissenting instances or users then can defederate or block automatically. Bad actors wouldn’t do this of course, but that’s nothing new.

and/or there could be a “this user just signed up for service X” message going out to all followers and giving them a choice to opt-in. Services (especially new ones) sending messages on the user’s behalf has always been icky on Twitter, but maybe this is a good enough cause to establish this practice here.





And then: build this into Mastodon. I think for timeline search, this is the inevitable future, IF this feature gets demanded enough. This would solve any trust issues, consent could be handled on the regular instance-to-instance trust/defederation network, and operationally, the experiment (above) showed that the resource overhead is manageable.

there are details to be discussed about how long these indexes should go back. Realistically I think 3 – 6 months probably addresses ~80% – 90% of my needs, so maybe that’s an acceptable convention (same for permacaching clients).




Finally, we can choose not do to any of these things and leave everything as is in Mastoland, but I’m certain the Fediverse at large will move on eventually. And from some of the responses I’ve got is that is what old-school folks here desperately wish for: get the new influx of users move on as fast as possible.




The Way Forward
I am still optimistic about Mastodon’s potential to become a lot better for a lot more people and I hope this write-up helps folks to not make the same mistakes again. But I’m also a bit disheartened by the response to this endeavour. I think Mastodon culture needs to become better at handling these kinds of things. Not for my sake, but for the culmination of these two factors:

If a big enough “bad actor” joins here, and we are on the brink of this with major platforms having announced upcoming ActivityPub interop, whatever they do will happen regardless of the consent given by folks who think they can federate content but restrict where it goes. I’m not suggesting this as a justification to just make away with people’s data, but realistically this is going to happen eventually and I’d rather Mastodon has tooling and conventions available to deal with this rather than “we always hoped this would never happen”. The only alternative to this is further hard splintering of communities that have more in common than they don’t and I really hope we can avoid that.

The Fediverse at large will only get more and more popular and mainstream. This means that these new people’s needs will want addressing, and I’d rather have a system in place where we can experiment safely to help those people rather than “pressuring the good actors out until only the bad ones are left” (paraphrasing a few responses I got). And while not claiming to be one or the other, I agree with this sentiment.


—
I regret my mistakes and I apologise to everyone harmed by this experiment.
Thank you for reading.
❧"
https://news.ycombinator.com/rss,The Cloud Conundrum: S3 Encryption,https://www.secwale.com/p/encryption,Comments,"























The Cloud Conundrum: S3 Encryption - by Aditya Patel











Security WaleSubscribeSign inShare this postThe Cloud Conundrum: S3 Encryptionwww.secwale.comCopy linkTwitterFacebookEmailThe Cloud Conundrum: S3 EncryptionAWS will now encrypt all new data in its Amazon S3 storage service by default. Huge announcement, secure default for the win, sure, but it gives a false sense of security. Here’s how.Aditya Patel3 hr ago1Share this postThe Cloud Conundrum: S3 Encryptionwww.secwale.comCopy linkTwitterFacebookEmailRiver Landscape with Cows, 1645/1650 by Aelbert Cuyp👋 Dear reader: Hope you’re staying safe, and going strong with your new year resolutions. This is first part of a series of posts I wish to write on peculiar cloud security challenges. In this post, I will cover:Encryption at rest in cloudAmazon S3 and its encryption optionsHow cloud’s server side encryption can give a false sense of security, and what you can do about itEncryption is a tricky concept. It’s simple at the surface, but dig a level deeper and it unravels like Game of Thrones subplots.Let’s take AWS' recent announcement that all new objects in Amazon S3 (Simple Storage Service) will now be encrypted by default.Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 will be automatically encrypted at no additional cost and with no impact on performance. (Source: AWS docs)What was earlier a 1-click setup, is now zero-click. AWS, and its S3 service specially, operate at a mind boggling scale. There are 280 trillion objects in S3, averaging over 100 million requests per second. To be able to support transparent encryption on all new objects, while not breaking any existing functionality, dependencies and applications - is impressive to say the least. Kudos to the engineering teams.But.. does the default SSE-S3 encryption provide any real security? And, does it protect from one of the primary causes of CISO migraines, i.e., data leakage via intentional or accidental public S3 buckets?Short answer: No.Let’s dive in to understand why and what can be done instead.Cryptography - a quick primerEncryption converts readable data to a random looking blob. It is the reason we can watch dog videos in private, or bitch about things on group chats (well, mostly), or buy toilet paper online securely. On a serious note, encryption is a fundamental tool for cybersecurity to the extent that it can be an enabler of human rights, by allowing freedom of speech through end-to-end encryption. It is a big deal to get it right.Technically, encryption is the process of converting plaintext data into ciphertext. There are 2 types of encryption: symmetric and asymmetric. Symmetric encryption uses the same key to encrypt and decrypt the data. Asymmetric encryption uses separate keys to do the same, and is also called public key cryptography. We will limit this blog post to symmetric encryption of data at rest, which is the data stored on disks. encrypt(plaintext, key) → ciphertextThe efficacy of a good encryption scheme depends upon the strength of encryption algorithm (the lock) and the encryption key (key).🔒 First is the lock, i.e., the algorithm itself. There are many encryption algorithms available, the most common type today is Advanced Encryption Standard with 256 bit key (AES-256). There is enough evidence, backed by gory mathematical proofs, to safely assume that the AES-256 encryption algorithm is not broken, for now. From one estimate, if we use the combined compute power of every PC on earth (estimated 2 billion PCs), it’d take 13,689 trillion trillion trillion trillion years to brute force AES-256. For comparison, the age of the universe is a meager 14 billion years. Quantum computers might change the equation sooner than later, but for now AES-256 is considered a quantum resistant algorithm. Moral of the story here is to trust the researchers and don’t invent your crypto.🔑 Then comes the key. As a wise man once said: if a thief has your key, no lock is strong enough. That’s why protecting the key is the most important part of a secure encryption scheme. So in theory, it’s pretty simple to protect your data. Choose a vetted encryption algorithm, and protect the key. In practice, things are more complex.Any modern production application usually has many different data sources, and hence many encryption schemes and keys. For instance, Pinterest currently stores and manages 1 exabyte of data on AWS. Nope - that’s not a typo, that is one frickin’ exabyte, or 1 billion gigabytes of data, which needs to be protected.This humongous amount of data is unlikely to be in a single data store. So now, you need to manage encryption across all the applicable data stores, equating to potentially thousands of encryption keys. Add to it the security best practice of rotating the keys periodically, or in case of an incident, deleting a whole bunch of keys. Doing this on your own is a nightmare. Ask Harry Potter.Luckily, you don’t have to. There are key management services both for on premise and cloud. In AWS, the service is called AWS Key Management Service (AWS KMS).This brings us to the types of encryption choices available in S3.Encryption in Amazon S3You can either do Server Side Encryption (SSE), in which Amazon S3 encrypts your data as it writes it to disks in its data centers and decrypts it for you when you access it. With server side encryption, there are 3 broad ways to manage your encryption keys.One option is for S3 to fully manage the encryption keys (SSE-S3). This option places the most trust in AWS, and is the reason I’m writing this post. A second option is for customers to use a key that is managed by the Amazon Key Management Service (SSE-KMS). This option gives customers control and transparency over access to their keys with strong auditing. Spoiler: this is my recommendation for most use-cases. Third option is for the customer to provide and manage the key, but have S3 perform the actual encryption and decryption (SSE-C). This gives customers a level of separation between themselves and AWS; do note that there’s a small window where the encryption key will be present on AWS servers to do encryption and decryption. Using either of these 3 ways, you can choose to give all the encryption, decryption and associated compute headaches to AWS.Or, you can say hey AWS, I don’t trust you, I will do the Client Side Encryption (CSE), in which you encrypt your data locally and pass it to the Amazon S3 service for storage and retrieval. You’ve 2 further options here: Use a key stored in AWS Key Management Service (AWS KMS). Or, use a key that you store within your application.Encryption-at-rest options in Amazon S3Security is a tradeoff problem. Your security decisions may come at the cost of convenience or performance or a higher spend. If you can safely create and manage your own keys in your applications for instance, you, and only you, will have access to the unencrypted material (assuming your access controls are rock solid). Choose client side encryption for highly regulated industries, business critical and the most paranoid of use cases. For the rest, the tradeoff problem may lean towards using the other option.As per an AWS blog, server side encryption may be the way to go. And I agree.While client-side encryption still has an important role in security and data protection, two of its disadvantages are that it depends on clients having a secure source of randomness, which is not always easy, and it is CPU intensive on the client. For more simplicity and efficiency, our services also offer server-side encryption.(Source: AWS blog)Now, let’s go back to the news announcement, that AWS now encrypts all new object uploads with SSE-S3 server side encryption. So does it provide any meaningful security?So does it?No, the SSE-S3 server side encryption does not provide any meaningful security assurance.It is mostly a checkbox exercise. This may appease some auditors but not all (disclaimer: nothing against auditors, I work very closely with them at Amazon, and they understand security better than most). For example, SSE-S3 meets PCI DSS’ encryption requirement but not the segregation of duties requirement.Next, at best SSE-S3 adds a defense in depth protection against a physical loss, theft or confiscation of an AWS hard drive storing your data. Think crazy scenarios like a tornado or fire, followed by more chaos and somehow the AWS hard drive landing at Goodwill. If the data on it is unencrypted, game over. As you can imagine, the likelihood of this happening is about the same as that of the United States winning a cricket world cup.More importantly, in SSE-S3, since S3 encrypts and decrypts the data transparently to anyone with access to the bucket, it will not protect leaked S3 buckets’ contents from being read. This is unfortunately still a very common scenario. Why does AWS even provide this option then? For one, some encryption is better than no encryption. Few compliance attestations will be happy with it, since it gives you a defense in depth option. It also provides some practical benefits for AWS to wipe out the hard drives more easily and securely (delete the key and you get crypto shredding). Also worth noting, there are no additional costs for using SSE-S3.What should you do instead? My suggestion is to go with any of the other options. At a minimum, go with the server side encryption with KMS keys, SSE-KMS.SSE-KMS provides a good balance between security and usability. For reading and writing contents of S3, it requires users to have access to both the object and the key. Enter multiple permission policies at IAM, S3 and KMS level, and hence segregation of duties. Now if a bucket is made public, and if it’s encrypted with SSE-KMS, it’s a very low likelihood that its contents will be world readable. Win!TakeawaysIf you’re new to AWS, you might be wondering, wow this shit is complicated. It is, and I didn’t even cover all the scenarios. Here’re the takeaways for meaningfully doing encryption at rest in the cloud.Don’t invent your crypto. Choose a cryptographic algorithm vetted by academia and industry such as AES-256.Outsource key generation and management. Prefer not to create and manage your own cryptographic keys if it’s not your core competency. Use the cloud service provider’s key management service instead.SSE-KMS for the win. That means, in AWS for your data in S3, prefer the server side encryption with KMS keys (SSE-KMS) for most use cases.SSE-S3 is misleading. Server side encryption with S3 keys (SSE-S3) shows AWS’ commitment to security, but to customers it doesn’t provide any real security benefit beyond a compliance checkbox.To wrap it up, here’s a relevant quote, attributed to Amazon CTO Werner Vogels: “Dance like nobody's watching. Encrypt like everyone is.”📕 Security Wale is a blog about cloud, cybersecurity, and in between - written by Aditya Patel. This is a passion project, where Aditya shares his learnings, opinions and rants from over a decade of working in the IT industry in United States. For a living, currently, he protects ☁️ cloudy things at Amazon/AWS. Earlier, Aditya has done software security consulting, masters in Information Security from Johns Hopkins, and computer science engineering. To support this effort, consider subscribing (it’s free) and spreading the word.Share this postThe Cloud Conundrum: S3 Encryptionwww.secwale.comCopy linkTwitterFacebookEmailPreviousCommentsTopNewNo postsReady for more?Subscribe© 2023 Aditya PatelPrivacy ∙ Terms ∙ Collection notice Start WritingGet the appSubstack is the home for great writing









        This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts
    



"
https://news.ycombinator.com/rss,GPT-3 is the best journal I’ve used,https://every.to/superorganizers/gpt-3-is-the-best-journal-you-ve-ever-used,Comments,"


GPT-3 Is the Best Journal I've Ever Used - Superorganizers - Every






































Subscribe





≡


About
Founders‘ Letter
Publications
Collections

Contact Us
Become a Sponsor
Login











Superorganizers




          GPT-3 Is the Best Journal I’ve Ever Used
        
My slow and steady progression to living out the plot of the movie 'Her'

by Dan Shipper
January 13, 2023
♥ 226





Listen







This is a joke, but it's not entirely wrong either.





Sponsor Every

Do you run a software company looking to reach an audience of early-adopters? Consider sponsoring our smart long-form essays on tech, AI, and productivity:

﻿Sponsor Every﻿


Want to hide ads? Become a subscriber

For the past few weeks, I’ve been using GPT-3 to help me with personal development. I wanted to see if it could help me understand issues in my life better, pull out patterns in my thinking, help me bring more gratitude into my life, and clarify my values.I’ve been journaling for 10 years, and I can attest that using AI is journaling on steroids. To understand what it’s like, think of a continuum plotting levels of support you might get from different interactions:Talking to GPT-3 has a lot of the same benefits of journaling: it creates a written record, it never gets tired of listening to you talk, and it’s available day or night. If you know how to use it correctly and you want to use it for this purpose, GPT-3 is pretty close, in a lot of ways, to being at the level of an empathic friend:If you know how to use it right, you can even push it toward some of the support you’d get from a coach or therapist. It’s not a replacement for those things, but given its rate of improvement, I could see it being a highly effective adjunct to them over the next few years. People who have been using language models for much longer than I have seem to agree:
Nick@nickcammarata

Replying to @nickcammarata

@krismartens I'm afraid of seeming hyperbolic, but also don't want to lie or hide information. GPT-3 is really just an incredible therapist, and is able to uncover complex patterns in my thinking and distill clean narratives that helps me a lot. It's also a lot warmer than most therapists

July 17th 2020, 4:55am EST

6 Retweets36 Likes

(Nick is a researcher at OpenAI. He’s also into meditation and is generally a great follow on Twitter.)It sounds wild and weird, but I think language models can have a productive, supportive role in any personal development practice. Here’s why I think it works.Why chatbots are great for journalingJournaling is already an effective personal development practice. It can help you get your thoughts out of your head, rendering them less scary. It shows you patterns in your thinking, which increases your self-awareness and makes it easier for you to change.It creates a record of your journey through life, which can tell you who you are at crucial moments. It can help you create a new narrative or storyline for life events so that you can make meaning out of them.It can also guide your focus toward emotional states like gratitude, or directions you want your life to go in, rather than letting you get swept up in whatever is currently going on in your life. But journaling has a few problems. For one, it’s sometimes hard to sit down and do it. It can be difficult to stare at a blank page and know what to write. For another, sometimes it feels a little silly—is summarizing my day really worth something?Once you get over those hurdles, as a practice it tends to get stale. You don’t read through your old entries that often, so the act of writing down your thoughts and experiences doesn’t compound in the way that it should. The prompts you use often get old: one like, “What are you grateful for today?” might work for the first few weeks, but after a while you need something fresh in order for the question to feel genuine.You want your journal to feel like an intimate friend that you can confide in—someone who’s seen you in different situations and can reflect back to you what’s important in crucial moments. You want your journal to be personal to you, and the act of journaling to feel fresh and full of hope and possibility every time you do.Unfortunately, paper isn’t great at those things. But GPT-3 is. Journaling in GPT-3 feels more like a conversation, so you don’t have to stare at a blank page or feel silly because you don’t know what to say. The way it reacts to you depends on what you say to it, so it’s much less likely to get stale or old. (Sometimes it does repeat itself, which is annoying but I think long-term solvable.) It can summarize things you’ve said to it in new language that helps you look at yourself in a different light and reframe situations more effectively. In this way, GPT-3 is a mashup of journaling and more involved forms of support like talking to a friend. It becomes a guide through your mind—one that shows unconditional positive regard and acceptance for whatever you’re feeling. It asks thoughtful questions, and doesn’t judge. It’s around 24/7, it never gets tired or sick, and it’s not very expensive.Let me tell you about how I use it, what its limitations are, and where I think it might be going.How I started with GPT-3 journalingI didn’t think of using GPT-3 in this way myself. I saw Nick Cammarata’s tweets about it over the years first. My initial reaction was a lot of skepticism mixed with some curiosity. After we launched Lex and I got more interested in AI, I remembered those tweets and decided to play around for myself. I started in the OpenAI playground—a text box where you input a prompt that tells GPT-3 how you want it to behave, and then interact with it:I had a bunch of ideas to start. I tried one from a Facebook PM, Mina Fahmi, whom I met at the AI hackathon I wrote about a few weeks ago. He suggested telling GPT-3 to take on a persona, and told me that he’d had great results asking it to be Socrates.GPT-3 as famous compassionate figureI started experimenting with prompts like this:The green messages are responses from GPT-3. I tried Socrates, the Buddha, Jesus, and a few others, and found I liked Socrates the best (apologies to my Christian and Buddhist readers). The GPT-3 version of him is effective at driving toward the root of an issue and helping you figure out small steps to take to resolve it.There’s a long tradition in various religions of visualizing and interacting with a divine, compassionate figure as a way of getting support—and this was a surprisingly successful alternative route to a similar experience.After a while, though, I became a little bored of Socrates. I’m a verified therapy nerd, so the obvious next step was to try asking GPT-3 to do interactions based on various therapy modalities.GPT-3 as therapy modality expertI tried asking GPT-3 to become a bot that’s well-versed in Internal Family Systems—a style of therapy that emphasizes the idea that the self is composed of many different parts or sub-personalities, and that a lot of growth comes from learning to understand and integrate those parts. It turns out, GPT-3 isn’t bad at that:﻿I also tried asking it to be a psychoanalyst and a cognitive behavioral therapist, both of which were interesting and useful. I even asked it to do Jungian dream interpretation:I don’t know what to make of the efficacy of dream interpretation in general, nor do I know what an actual Jungian might say about this interpretation. But I have found that having dreams reflected back to me in this way can help me understand some of what I’ve been feeling day to day but haven’t been able to put into words. GPT-3 as gratitude journalAnother thing I tried is asking GPT-3 to help me increase my sense of gratitude and joy—like a better gratitude journal:You’ll notice it starts by acting like a normal gratitude journal, asking me to list three things I’m grateful for. But once I respond, it probes about details of what you’re grateful for to get you past your stock answers and into the emotional experience of gratitude. GPT-3 as values coachOne of my favorite therapy modalities is ACT—acceptance and commitment therapy—because I love its focus on values. ACT emphasizes helping people understand what’s most important to them and uses that knowledge to help them navigate difficult emotions and experiences in their lives.Values work is challenging because sometimes it’s hard to connect your day-to-day experiences to your values. So I wanted to see if GPT-3 could help. This is one of the experiments I tried:This works well, and one of the cool things about it is how the prompt works. I took a sample therapy dialog from an ACT-focused values book that I love, Values in Therapy, and asked GPT-3 to generalize from that dialog to learn how to talk to me about values.It worked—successfully guiding our conversation toward talking about what was most important to me. It’s not perfect, but it suggests interesting possibilities for things to try going forward.Problems and limitationsWhile I liked these early experiments, they had a few significant problems.First, the OpenAI playground isn’t designed to facilitate chats, so it’s hard to use. Second, it doesn’t record inputs between sessions, so I ended up having to re-explain myself every time I started a new session. Third, it sometimes gets repetitive and asks the same questions.These are solvable, though. I know because I built a solution: a web app with a chatbot interface that remembers what I say in every session so I never have to repeat myself.  The bot lets me select a persona—like Socrates or an Internal Family Systems therapist—which corresponds to the prompts above. Then I can have a conversation with it. It will help me work through something I’m dealing with, or set goals, or bring my attention to something I’m grateful for. It can even output and save a summary of the session to help me notice patterns in my thinking over time. It’s still early and there are a lot of problems to fix, but I find myself gravitating toward it every day. I feel like I’m building up a record of myself and my patterns over time, and the more I write in it, the more it compounds.I’ll be releasing the bot soon for paying Every members, so if you want access, make sure to subscribe. What’s nextHere’s what I’ve learned so far through all of these experiments with GPT-3 as a journaling tool.There is something innately appealing about  building a relationship with an empathetic friend that you can talk to any time. It’s comforting to know that it’s available, and it’s exciting to think about all of the different prompts you can experiment with to help it support you in the way you need.There is also something weird about all of this. Spilling your guts to a robot somehow cheapens the experience because it doesn’t cost much for a robot to tell you it understands you. This mix of feelings is reflected in this Twitter thread by Rob Morris, the founder of a peer-to-peer support app called Koko:
Rob Morris@RobertRMorris

We provided mental health support to about 4,000 people — using GPT-3. Here’s what happened 👇

January 6th 2023, 2:50pm EST

1k Retweets6k Likes

When people were using GPT-3 to help them provide support to peers, their responses were rated significantly more highly than responses that were generated by humans alone:
Rob Morris@RobertRMorris

Replying to @RobertRMorris

Messages composed by AI (and supervised by humans) were rated significantly higher than those written by humans on their own (p < .001). Response times went down 50%, to well under a minute.

January 6th 2023, 2:50pm EST

66 Retweets785 Likes

But they had to stop using the GPT-3 integration because people felt like getting a response from GPT-3 wasn’t genuine and ruined the experience. Those feelings are understandable, but whether or not they ruin the experience depends on how the interaction is framed to you, and how familiar you are with these tools.I don’t think these objections will last over time for most people. It’s more likely a temporary result of contact with new technology. When you see a movie that you loved, does it cheapen the experience to know that you were touched by a set of pixels moving in the correct sequence over the course of a few hours? Obviously not, but if I had to bet, when movies were first introduced many people probably felt it was a cheaper version of a live performance experience.As these kinds of bots get more common, and we learn to interact with them and depend on them for different parts of our lives, we’ll be less likely to feel that our interactions with them are cheap or stilted.(None of this, by the way, means that in-person interactions aren’t valuable anymore—just that there’s probably more room for bot interactions in your life than you might realize.)If you're someone that's journaled for a long time, you'll find a lot of value in trying GPT-3 out as an alternative to your day-to-day practice. And if you've never journaled before this might be a good way to get started.I’ll be experimenting with this a lot more over the coming weeks and months, and I’ll be sharing everything I learn with you here. I’m excited for what’s next.




What did you think of this post?

Amazing
Good
Meh
Bad





Send Privately

      Your feedback has been saved anonymously. If you want it to be attributed to you, login or sign up.
    



Like this?Become a subscriber.
Subscribe →
Or, learn more.




Read this next:








Superorganizers


How Josh Kaufman Does Research 
The author of The Personal MBA shares his process for finding answers hiding in plain sight

♥ 184
🔒 
          Aug 20, 2020
      










Superorganizers


The Fall of Roam
I don’t use Roam anymore. Why?

♥ 208

          Feb 12, 2022
          by Dan Shipper











Superorganizers


The End of Organizing
How GPT-3 will turn your notes into an *actual* second brain

♥ 160

          Jan 6, 2023
          by Dan Shipper










The End of Moore’s Law?
As chips are getting smaller, prices are going up

♥ 91

          Jan 9, 2023
          by Anna-Sofia Lesiv











The Sunday Digest


Toward the Unoptimized Life, GPT-3 Journaling, and More
Everything we published this week.

♥ 12

          Jan 15, 2023
      






Comments







Post









Post



    You need to login before you can comment.
    Don't have an account? Sign up!














@nattaliehartwig
1 day ago


Loved this, are any of your projects available to try?



♡ 0

      ·
      Reply










✕
Thanks for reading Every!
Sign up for our daily email featuring the most interesting thinking (and thinkers) in tech.
Subscribe
Already a subscriber? Login




Contact Us ·
            Become a Sponsor ·
            Search ·
            Terms

©2023 Every Media, Inc





"
https://news.ycombinator.com/rss,The IAB loves tracking users but hates users tracking them,https://shkspr.mobi/blog/2023/01/the-iab-loves-tracking-users-but-it-hates-users-tracking-them/,Comments,"The IAB loves tracking users. But it hates users tracking them.By 
@edent

 on 
2023-01-16
advertising email privacy13 comments550 words
The Interactive Advertising Bureau (IAB) is a standards development group for the advertising industry. Their members love tracking users. They want to know where you are, who you're with, what you're buying, and what you think. All so they can convince you to spend slightly more on toothpaste.  Or change your political opinions. Either way, they are your adversaries.The IAB's tech lab is working on a system called UID2. It's a more advanced way to track you no matter what you do and no matter what steps you take to avoid it.
UID2 is a framework that enables deterministic identity for advertising opportunities on the open internet for many participants across the advertising ecosystem. The UID2 framework enables logged-in experiences from publisher websites, mobile apps, and Connected TV (CTV) apps to monetize through programmatic workflows.Basically, they tie your email address to everything you do. Signed in to watch a TV show? Better sell that info to the advertisers so when you sign in to a different site they can send you targetted messages. Yuck.One of the ways privacy conscious users normally avoid this is by subtly altering their email addresses for each service they use.  For example, GMail ignores any dots in your username. So if you are Han.Solo@gmail.com you can also use H.ansolo@gmail.com or ha.ns.ol.o@gmail.com.  A user might sign up to a service and use a specifically ""dotted"" email address.  If they later start receiving spam to that address, they know the service has leaked or sold their info.You can go one step further and use plus addressing.  For example han.solo+amazon@gmail.com and han.solo+github@gmail.com. They both will appear in your normal inbox, but are unique for every service you use. Again, this is great for making sure that someone hasn't sold your email address to spammers.The IAB hates this.As part of the UID2 API they specifically describe how an advertiser must ""normalise"" their users' email addresses.This means h.a.n.solo+iab@gmail.com becomes plain old hansolo@gmail.comI think this is pretty shitty behaviour. If someone has deliberately set their email address in this form it is because the user does not want their identities to be commingled.Last year, I asked them to respect users' privacy and reverse this change.  They finally responded:
Thank you for your input, we thought long about this update and ultimately as it stands today it is not a change we would like to add.So, there you have it. If you want to take even the smallest step to preserve your privacy - tough.
If you want to track which IAB members are using your data - tough.
If you want to track users even if they don't want to be tracked - the IAB is happy to help.If you want to opt out of this - and you trust the IAB to handle your data safely - you can submit your email address and phone number to https://transparentadvertising.org/.Personally, I recommend installing the uBlock advert blocker on all devices which support it.Share the love:MastodonTwitterFacebookLinkedInRedditHackerNewsLobstersEmailPocketWhatsAppTelegramMore posts from around the site:
13 thoughts on “The IAB loves tracking users. But it hates users tracking them.”

2023-01-16 12:38

 Ian Betteridge says:@Edent I’ve noticed several brands now blocking services like iCloud’s relay, which lets you sign up with a random email address that’s not related to yours. Firefox relays ducks around that by letting you use your own domain, which makes it much harder for them to block sign-ups, but that’s obviously only applicable to a few users.Reply

2023-01-16 12:55

 Gabor says:I've been loving fastmail's masked email functionality, which gives you a random email alias like ""salty.hotdog8233@fastmail.com"", plus it has 1password integration, so signing up to places is fairly straightforward if you use 1p.Reply

2023-01-16 13:00

 That Privacy Guy says:I just read this and the solution I use is my own instance of AnonAddy - and I create a new and unique email address for every site/service I use. If you don't want to run it yourself there is a SaaS version - plus it is FOSS.


Reply

2023-01-16 13:40

 HackerNewsTop10 says:The IAB loves tracking users. But it hates users tracking them
Link: shkspr.mobi/blog/2023/01/t…
Comments: news.ycombinator.com/item?id=344000…Reply

2023-01-16 13:42

 Kazaii says:@Edent wow, that's rather unsettling. Thanks for shedding light on this.Reply

2023-01-16 13:49

 That Privacy Guy says:I have been using my own installation of AnonAddy for a couple of years now. I used to just have a catchall in my mail server which would forward anything which was sent to a non existing email address to a delegated account



Reply

2023-01-16 13:55

 Fazal Majid says:The plus convention is not specific to GMail (Sendmail, MS Exchange, Postfix and other email software have it), but they only require stripping it for @gmail.com domains. I have my own dedicated domain for vendors so I won't be impacted, and Apple's email masking feature will do the same, along with competing offerings from DuckDuckGo et al.Hashing PII like an email is also PII and this proposal is a blatant violation of GDPR, of course.Reply

2023-01-16 13:55

 Nikki says:Personally my opinion of anyone involved in advertising is so poor that I'd probably not be allowed to express it here. I can easily imagine a world without advertising as the web allows you to find anything you want without having someone trying to force it down your throat. Also the idea that many parts of the web could not exist without advertising support is facile. It's a bit like saying that free and open parks cannot exist without employing pick pockets to gather funds to pay for maintenance. If there are any parts of the web that really can not exist without advertising, they must be so bankrupt of alternatives ideas that their services could not be trusted to be useful.Reply

2023-01-16 15:09

 Anonymous says:A link says uBlock but points to uBlock Origin. uBlock is different from uBlock Origin: https://github.com/gorhill/uBlock/wiki/uBlock-Origin-is-completely-unrelated-to-the-web-site-ublock.orgReply

2023-01-16 15:21

 Oli says:I’m a big fan of Fastmail’s masked addresses for this reason.Word dot word four digit number at my own domain, goes in the password manager, never thought about again!Reply

2023-01-16 15:44

 Privacy Matters says:Hi @IABTechLab  What is the legal basis relied on  to alter the email identities of individuals who will be targeted by those using UID2?Oh, & I note domain reg details for transparentadvertising.org are redacted for privacy reasons. Who owns the domain pls?

Reply

2023-01-16 15:56

 trinity says:I own my name dot [tld] so I can do slingshit@me.com. Looks like I'm still gonna be doing alright. Cloudflare's mail forwarding works well for this, before that I used ImprovMX. Both just point the proper DNS records from your site to someone's mail server for quick relay+disposal. I imagine having all mail filter through a magic box is technically A Bit Troublesome but it's still better than Google Mail!Reply

2023-01-16 20:19

 Duane Johnson :verified: says:@Edent The issue at the core of privacy is dignity--to hide or reveal parts of ourselves as we create relationships. In this case, advertisers want a kind of ""forced intimacy"" with all of humankind--to prevent people from hiding parts of themselves--so they can offer goods and services. The difference between a friend recommended something--because they know you well--and the IAB or others advertising to you, is that a friend actually has your long-term best interest in mind.ReplyLeave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment *Name * Email * Website  Notify me of follow-up comments by email. Notify me of new posts by email. 
Δ
To respond on your own website, enter the URL of your response which should contain a link to this post's permalink URL. Your response will then appear (possibly after moderation) on this page. Want to update or remove your response? Update or delete your post and re-enter your post's URL again. (Learn More)



"
https://news.ycombinator.com/rss,"Why Alexa won't wake up when hearing ""Alexa"" in Amazon's Super Bowl ad (2019)",https://www.amazon.science/blog/why-alexa-wont-wake-up-when-she-hears-her-name-in-amazons-super-bowl-ad,Comments,"


Conversational AI / Natural-language processing


        Why Alexa won't wake up when she hears her name in Amazon's Super Bowl ad
    



By Mike Rodehorst

        January 31, 2019
    







Share



Share




Copy link




Email



Twitter



LinkedIn


 
Facebook


 
Line

 
Reddit

 
QZone

 
Sina Weibo

 
WeChat

 
WhatsApp






                        分享到微信
                    
x




















This Sunday's Super Bowl between the New England Patriots and the Los Angeles Rams is expected to draw more than 100 million viewers, some of whom will have Alexa-enabled devices within range of their TV speakers. When Amazon's new Alexa ad airs, and Forest Whitaker asks his Alexa-enabled electric toothbrush to play his podcast, how will we prevent viewers’ devices from mistakenly waking up?












                  Related content
              
How AWS scientists help create the NFL’s Next Gen Stats
In its collaboration with the NFL, AWS contributes cloud computing technology, machine learning services, business intelligence services — and, sometimes, the expertise of its scientists.






With the Super Bowl ad — as with thousands of other media mentions of Alexa tracked by our team — we teach Alexa what individual recorded instances of her name sound like, so she will know to ignore them. We can also apply this technique, known as acoustic fingerprinting, on the fly to recognize when multiple devices from different households are hearing the same command at around the same time. This is crucial to preventing Alexa from responding to pranks on TV, references to people named Alexa, or other instances of her name in broadcast media that we don't know about in advance.












                  Related content
              
Audio watermarking algorithm is first to solve ""second-screen problem"" in real time
Audio watermarking is the process of adding a distinctive sound pattern — undetectable to the human ear — to an audio signal to make it identifiable to a computer. It’s one of the ways that video sites recognize copyrighted recordings that have been posted illegally. To identify a watermark, a computer usually converts a digital file into an audio signal, which it processes internally.






Our approach to matching audio recordings is based on classic acoustic-fingerprinting algorithms like that of Haitsma and Kalker in their 2002 paper “A Highly Robust Audio Fingerprinting System”. Such algorithms are designed to be robust to audio distortion and interference, such as those introduced by TV speakers, the home environment, and our microphones.To produce an acoustic fingerprint, we first derive a grid of log filter-bank energies (LFBEs) for the acoustic signal, which represent the amounts of energy in multiple overlapping frequency bands in a series of overlapping time windows. The algorithm steps through the grid in two-by-two blocks and adds and subtracts the measurements in the grid cells in a standardized way. (Technically, it computes the 2-D gradient of each block.) The sign of the result — positive or negative — provides a one-bit summary of the values in the block. The summaries of all the blocks in the grid constitute the acoustic fingerprint, and two fingerprints are deemed to match if the fraction of bits that are different (the “bit error rate”) is small enough.






An illustration of how fingerprints are used to match audio. Different instances of Alexa’s name result in a bit error rate of about 50% (random bit differences). A bit error rate significantly lower than 50% indicates two recordings of the same instance of Alexa’s name.

When we have audio samples in advance — as we do with the Super Bowl ad — we fingerprint the entire sample and store the result. With audio that’s streaming to the cloud from Alexa-enabled devices, we build up fingerprints piecemeal, repeatedly comparing them to other fingerprints as they grow.If a match is found, the incoming request is ignored. Noisy audio may yield a match, but it requires the accumulation of more data (a larger fingerprint) than clean audio does.Using this matching algorithm, we have built a system with multiple layers to protect customers at multiple stages:On-device: On most Echo devices, every time the wake word “Alexa” is detected, the audio is checked against a small set of known instances where Alexa is mentioned in commercials. Due to the limits of device CPU, this set is generally restricted to commercials we expect to be currently airing on TV.In the cloud: Every audio request to Alexa that starts with a wake word is checked in two ways:Known media: the audio is checked against a large set of fingerprints for known instances of “Alexa” and other wake words in commercials and other media. These fingerprints can also make use of the audio that follows the wake word.Unknown media: the audio is checked against a fraction of other Alexa requests arriving at around the same time. If the audio of a request matches that of requests from at least two other customers, we identify it as a media event. We also check incoming audio against a small cache of fingerprints discovered on the fly (the cached fingerprints are averages of the fingerprints that were declared matches). The cache allows Alexa to continue to ignore spurious wake words even when they no longer occur simultaneously.Ideally, a device will identify media audio using locally stored fingerprints, so it does not wake up at all. If it does wake up, and we match the media event in the cloud, the device will quickly and quietly turn back off.In addition to tracking new media mentions of Alexa’s name and updating our library of fingerprints accordingly, our team works continuously to improve the accuracy and efficiency of the fingerprinting system. We’re also exploring complementary technologies, such as machine learning systems that can distinguish media audio more generally from live human speech.Acknowledgments: Joe Wang, Aaron Challenner, Mike Peterson, Michael Rudeen, Naresh Narayanan, Liangwei Guo, and the rest of the team






        Research areas
    


Conversational AI / Natural-language processing







        Tags
    


Alexa


Keyword spotting


Signal processing




About the Author




Mike Rodehorst


                    Mike Rodehorst is a machine learning scientist in the Alexa Speech group.
                




"
https://news.ycombinator.com/rss,Brad Feld: I Don't Hate Crypto,https://feld.com/archives/2023/01/i-dont-hate-crypto/,Comments,"   Back to Blog I Don’t Hate Crypto Jan 16, 2023  Category   Investments  #crypto #ponziWell, that was interesting.I get many more private emails in response to blog posts than comments. Yesterday, in response to Reflecting on Ponzi Schemes, I got a few that said anyone under 35 needs a net native currency, and that’s crypto. A few others said some versions of all governments are Ponzi schemes. And I got a few that implied I hated crypto.Earlier last year, one of my partners told me that I’d developed a reputation with other VCs (presumably our partner funds) that I hate crypto. At the time, I deflected and said that I didn’t hate crypto; I just thought there was considerable Ponzi-like behavior in crypto. I’m regularly cynical about things on our internal Slack channel and periodically post about big blowups, including in crypto.I realize that I’m conflating speculation vs. investment. The part of crypto I don’t like is the rampant speculation. This morning, a friend of mine sent me an email about some money I owed him for a thing we are doing together. He said, “If you paypal me I’ll buy some bitcoin with it. Looks like it’s starting to firm up.”Here were the bitcoin prices when he sent me the email and when I Paypalled him the money ($1,456.42).1/15/23 9:51 PM MT: $21,158.551/16/23 7:28 PM MT: $20,879.14That’s a 1.33% difference. It cost me nothing to Paypal him the money. It would have cost me $19.37 to pay him via Bitcoin just because of the timing difference. That has nothing to do with the transaction cost. It’s entirely a result of speculative activity.I mean, c’mon. Yeah, I know credit cards have fees, and endless payment rails in the system extract money along the way. But there are also ACH and Debit Cards. And free checking accounts, although I guess it would cost me $0.60 for a stamp. Wait, $0.60 for a stamp? The last time I bought a stamp, they were $0.29. And yes, I know some of you out there have never bought a stamp.It’s hard for me to hate crypto. It’s been economically very good to me. I accidentally bought twice as many bitcoins as I needed for an online programming course I took in 2013 for about $100 each. I sold the FIL I got from investing in their SAFT as it vested (daily) and was amazed at how much money resulted. The Helium that I earned, which seemed to have no functional utility whatsoever, generated a nice multiple on the cost of all the routers I bought, even though today I earn nothing because of whatever algorithm changes they’ve made, so the network is now functionally and economically worthless. And, the crypto funds we have invested in have done exceptionally well … mostly.I regularly hear to be patient. It’s like the Internet was in 1999 – ahead of its time. The builders are building, and it’ll take over everything in the future.Ok. That’s cool. Just beware of the Ponzi schemes. Previous Post"
https://news.ycombinator.com/rss,mRNA vaccines induce higher long-term IgG4 response versus adenovirus vaccines,https://www.frontiersin.org/articles/10.3389/fimmu.2022.1020844/full?s=09#B75,Comments,"








                                ORIGINAL RESEARCH article
                            



                            Front. Immunol., 12 January 2023Sec. Vaccines and Molecular Therapeutics

                                
https://doi.org/10.3389/fimmu.2022.1020844



mRNA vaccines against SARS-CoV-2 induce comparably low long-term IgG Fc galactosylation and sialylation levels but increasing long-term IgG4 responses compared to an adenovirus-based vaccineJana Sophia Buhre1†, Tamas Pongracz2†, Inga Künsting1, Anne S. Lixenfeld1, Wenjun Wang2, Jan Nouta2, Selina Lehrian1, Franziska Schmelter3, Hanna B. Lunding1, Lara Dühring1, Carsten Kern1, Janina Petry1, Emily L. Martin1, Bandik Föh3, Moritz Steinhaus1,4, Vera von Kopylow1, Christian Sina3, Tobias Graf5, Johann Rahmöller1,4, Manfred Wuhrer2*‡ and Marc Ehlers1,6*‡1Laboratories of Immunology and Antibody Glycan Analysis, Institute of Nutritional Medicine, University of Lübeck and University Medical Center Schleswig-Holstein, Lübeck, Germany2Center for Proteomics and Metabolomics, Leiden University Medical Center, Leiden, Netherlands3Institute of Nutritional Medicine, University of Lübeck and University Medical Center Schleswig-Holstein, Lübeck, Germany4Department of Anesthesiology and Intensive Care, University of Lübeck and University Medical Center Schleswig-Holstein, Lübeck, Germany5Medical Department 2, University Heart Center of Schleswig-Holstein, Lübeck, Germany6Airway Research Center North (ARCN), University of Lübeck, German Center for Lung Research (DZL), Lübeck, GermanyBackground: The new types of mRNA-containing lipid nanoparticle vaccines BNT162b2 and mRNA-1273 and the adenovirus-based vaccine AZD1222 were developed against SARS-CoV-2 and code for its spike (S) protein. Several studies have investigated short-term antibody (Ab) responses after vaccination.Objective: However, the impact of these new vaccine formats with unclear effects on the long-term Ab response – including isotype, subclass, and their type of Fc glycosylation – is less explored.Methods: Here, we analyzed anti-S Ab responses in blood serum and the saliva of SARS-CoV-2 naïve and non-hospitalized pre-infected subjects upon two vaccinations with different mRNA- and adenovirus-based vaccine combinations up to day 270.Results: We show that the initially high mRNA vaccine-induced blood and salivary anti-S IgG levels, particularly IgG1, markedly decrease over time and approach the lower levels induced with the adenovirus-based vaccine. All three vaccines induced, contrary to the short-term anti-S IgG1 response with high sialylation and galactosylation levels, a long-term anti-S IgG1 response that was characterized by low sialylation and galactosylation with the latter being even below the corresponding total IgG1 galactosylation level. Instead, the mRNA, but not the adenovirus-based vaccines induced long-term IgG4 responses – the IgG subclass with inhibitory effector functions. Furthermore, salivary anti-S IgA levels were lower and decreased faster in naïve as compared to pre-infected vaccinees. Predictively, age correlated with lower long-term anti-S IgG titers for the mRNA vaccines. Furthermore, higher total IgG1 galactosylation, sialylation, and bisection levels correlated with higher long-term anti-S IgG1 sialylation, galactosylation, and bisection levels, respectively, for all vaccine combinations.Conclusion: In summary, the study suggests a comparable “adjuvant” potential of the newly developed vaccines on the anti-S IgG Fc glycosylation, as reflected in relatively low long-term anti-S IgG1 galactosylation levels generated by the long-lived plasma cell pool, whose induction might be driven by a recently described TH1-driven B cell response for all three vaccines. Instead, repeated immunization of naïve individuals with the mRNA vaccines increased the proportion of the IgG4 subclass over time which might influence the long-term Ab effector functions. Taken together, these data shed light on these novel vaccine formats and might have potential implications for their long-term efficacy.IntroductionThe rapid spread of severe acute respiratory syndrome coronavirus type 2 (SARS-CoV-2), the cause of Coronavirus Disease 2019 (COVID-19), has led to a global health threat (1). The virus employs a transmembrane spike (S) protein that interacts through its receptor-binding domain (RBD) with the host membrane-bound angiotensin-converting enzyme 2 (ACE2) to enter cells of the respiratory tract (2). A range of different intramuscularly administered vaccines inducing an immune response against the S-protein have been developed, led by novel mRNA-containing lipid nanoparticle (LNP) vaccines such as the BNT162b2 and the mRNA-1273 vaccines from BioNTech/Pfizer (3) and Moderna (4), respectively, as well as replication-deficient adenovirus-based vaccines such as the ChAdOx1 nCoV-19 (AZD1222) vaccine from AstraZeneca (5).Neutralizing anti-S IgG and IgA antibodies (Abs) inhibiting the interaction of the viral S protein with ACE2 have been identified both in blood and in the respiratory tract after SARS-CoV-2 infection (6–9). Persisting neutralizing Abs in the respiratory tract likely constitute the first line of defense that protect from a subsequent SARS-CoV-2 re-infection and spreading (8, 10).Although both mRNA vaccines induce strong initial neutralizing anti-S IgG and IgA Ab responses in the blood, all three vaccines seem to be only moderately or temporarily protective against SARS-CoV-2 infection and spreading (3, 7, 9–15). Moreover, better vaccine-induced protection from infection and spreading has been described for previously (pre)-infected vaccinees (with SARS-CoV-2 history) as compared to naïve vaccinees (without SARS-CoV-2 history) (16, 17). However, long-term anti-S IgG and IgA levels in the blood and the respiratory tract have hardly been investigated in naïve and pre-infected subjects upon vaccination against SARS-CoV-2.Nevertheless, all three vaccines seem to induce high protection from severe disease conditions in the next weeks after a second immunization (5, 18, 19), assuming a robust long-term systemic T and B cell response – also against non-RBD parts of the virus and virus escape variants (15). However, the influence of the different new vaccine formats with unclear co-stimulatory/”adjuvant” effects on the long-term B cell and Ab Fc response remains unknown.IgG Fc-mediated effector functions are influenced by the induced IgG subclass and the IgG Fc N-glycosylation pattern. Human IgG1 and IgG3 subclasses have been described to convey the highest potential to activate immune cells via classical activating Fcγ receptors (FcγRs) and the classical complement pathway via C1q (20–24). These IgG subclasses can form hexamers, thereby facilitating the interaction with the six-arm C1q molecule (21, 25–29). IgG2 hardly interacts with classical FcyRs and C1q and its effector function-inducing capacity needs further investigation (20, 22, 23). In contrast, IgG4 shows higher affinity to the classical IgG inhibitory receptor FcyRIIB than to classical activating FcyRs (20, 22, 23). Furthermore, IgG4 cannot activate C1q but instead is able to disturb the hexamer formation of the C1q-activating IgG subclasses (21). Furthermore, IgG4 can generate Fab arm-exchange, meaning that heavy chains with different specificities can dimerize resulting in bispecific Abs, which reduces their ability to form immune-complexes (30). Thus, IgG1 and IgG3 are the IgG subclasses with the highest potential to activate the immune system, whereas IgG4 has less activating potential and can even inhibit the effector functions of IgG1 and IgG3.Both SARS-CoV-2 infection and vaccination initially induce the IgG1 and IgG3 subclasses against the S protein (5, 31–35).Another factor known to influence IgG Fc-mediated effector functions is the type of IgG Fc N-glycosylation. The highly conserved glycosylation site at Asn297 in the Fc moiety of IgG carries a complex type N-glycan characterized by a core structure, that can be further modified with a core fucose, a bisecting N-acetylglucosamine (GlcNAc) as well as one or two galactose residues, each of which can further be capped by a sialic acid (36–38) (Table S2).IgG Abs lacking fucose are known to have an increased affinity to activating FcyRIIIa and are linked to enhanced tumor-fighting potential as well as protection against HIV and malaria infection (38–43).Agalactosylated (G0) IgG Abs have been linked to severe conditions in inflammatory (auto-) immune diseases, whereas IgG sialylation has been associated with a decreased affinity of IgG to classical activating FcyRs and lower or anti-inflammatory effects (21, 24, 36, 37, 44–52). The functional analysis of differently glycosylated IgG Abs is complex because single terminal glycan residues may in addition interact with glycan binding receptors, such as galectins, siglecs, and C-type lectin receptors (37, 44, 53–55). In vivo, immune inhibitory functions have been described for sialylated as well as terminally galactosylated antigen-specific and total IgG Abs (36, 44–48, 52, 54, 56). IgG Fc bisection is often increased in inflammatory autoimmune diseases (57). However, the biological significance of IgG bisection is less clear and remains to be investigated.In the context of SARS-CoV-2 infection, transient afucosylated and more persistent agalactosylated anti-S IgG (1) Abs have been linked to pro-inflammatory disease conditions in COVID-19 patients without (non-ICU), but in particular in those with the need of intensive care unit (ICU) admission (32–35, 58–61). Noteworthy, severe COVID-19 conditions have also been linked to the appearance of broad autoreactive IgG Abs (62). It remains unclear whether a combination of inflammatory Fc glycosylation patterns and broad auto-reactivity of the induced IgG Abs contributes to the severe inflammatory complications of COVID-19. However, it has rather been investigated whether the inflammatory IgG glycosylation patterns contribute to the inflammatory complications than to the elimination of the virus. Afucosylated as well as agalactosylated anti-S IgG Abs may also strengthen the anti-viral response (63).A recent mouse vaccination study with different adjuvants showed that the early, likely extrafollicularly induced IgG Abs were characterized by high Fc galactosylation and sialylation levels (64). Over time, the potential of each adjuvant/co-stimulus to induce an IFNγ- and IL-17-producing T follicular helper (TFH1 and TFH17) cell-dependent germinal center (GC) B cell and Ab response became visible that correlated with lower long-term IgG Fc galactosylation and sialylation levels (64). A recent vaccination study against simian immunodeficiency virus in rhesus macaques with two different adjuvant formats found comparable results. In that study, the adjuvant, which correlated with better protection, also correlated with lower induced IgG galactosylation and sialylation levels (65).The new mRNA vaccines against SARS-CoV-2 generate an initial anti-S and -RBD IgG (1) Ab response with transient afucosylation, but high galactosylation and sialylation levels (32, 34, 35).However, little is known about repeated immunizations and the potential of the different new vaccine formats with unclear “adjuvant” effects on the long-term IgG subclass and IgG Fc glycosylation response.Here, we present a comprehensive analysis of serum-derived and salivary anti-S1 IgG (subclass) and IgA Ab responses as well as anti-S serum IgG1 Fc N-glycosylation patterns of SARS-CoV-2 naïve and pre-infected individuals vaccinated with different vaccine combinations over time to compare and characterize the long-term Ab responses up to day 270 post-immunization.Materials and methodsStudy cohortSARS-CoV-2 naïve (with no known SARS-CoV-2 history) and non-hospitalized pre-infected (with past SARS-CoV-2 infection history) subjects were recruited at the University of Lübeck and the University Medical Center Schleswig-Holstein (Lübeck, Germany) since December 2020. The regimens for six vaccination cohorts varied as follows: (i) 48 naïve individuals received two doses of the BioNTech/Pfizer vaccine BNT162b2 (each 30 µg) (3); (ii) 25 naïve individuals received two doses of the Moderna vaccine mRNA-1273 (each 100 µg) (4); (iii) 14 naïve individuals received two doses of the adenovirus-based vaccine ChAdOx1 nCoV-19 (AZD1222) from AstraZeneca (each 5x1010 virus particle with not less than 2.5x108 infectious units) (5); (iv) 12 naïve individuals received one dose of AZD1222 and subsequently one dose of BNT162b2; (v) 44 naïve individuals received one dose of AZD1222 and subsequently one dose of mRNA-1273; and (vi) 14 non-hospitalized pre-infected individuals received one or two doses of BNT162b2 (Table 1 and Table S1).TABLE 1Table 1 Vaccination study groups.The mRNA and adenovirus vaccinees received their second dose between day 21 and 45 or between day 70 and 84 (except for five vaccinees with AZD1222 that received their second dose between day 35 and 61), respectively, and were sampled (blood serum and/or saliva) once or multiple times up to 270 days after the first immunization.In addition to pre-vaccination samples of the pre-infected subjects, 2 further non-hospitalized pre-infected non-vaccinated individuals as positive controls, and 8 non-vaccinated naïve subjects as negative controls were recruited.No selection criteria were used and participants as well as repeated sampling were selected at random. However, the preferred vaccination strategies at the University Medical Center Schleswig-Holstein and the University of Lübeck were vaccination with two doses of BNT162b2 or the first vaccination with AZD1222 followed by a booster injection of mRNA-1273, respectively, explaining the comparably high numbers of individuals/samples in these two groups.The identification of pre-infected individuals was limited by their low incidence in the catchment area when the project was started in December 2020 and the number of vaccinated pre-infected individuals was even lower because of unclear recommendations regarding vaccination after infection.All recruited pre-infected individuals have had a mild pre-infection meaning that they neither had to go to the hospital (non-hospitalized) nor showed signs of shortness of breath or abnormal chest imaging during infection. Furthermore, most recruited pre-infected individuals were vaccinated with BNT162b2 and not with mRNA-1273 or AZD1222. So, we included only BNT162b2-vaccinated pre-infected in this study. Most of these pre-infected individuals were tested positive for SARS-CoV-2 in a narrow time window between 200 and 150 days before the first vaccination (only three individuals were infected earlier). This is why we decided not to investigate the influence of this period on the vaccine-induced Ab response.To verify and recognize pre-infected individuals, previous positive SARS-CoV-2 PCR results were considered, together with anti-viral nucleocapsid protein (anti-NCP) and anti-S1 serum IgG responses (Figure S1F).Blood samples and saliva were collected after obtaining written informed consent according to the Declaration of Helsinki in accordance with the local ethics board-approved protocol 20-123 (Ethics Committee of the University of Lübeck, Germany).Blood serum and saliva antibody detectionBlood samples were collected as described earlier (31). Salivary samples were collected with the Saliva Collection system-ORACOL Plus S14 (Malvern Medical Developments, United Kingdom) and frozen before usage.Enzyme-linked Immunosorbent Assays (ELISA) were used to detect always anti-S1 (the extracellular part of S containing the RBD) Abs. The EUROIMMUN SARS-CoV-2 S1 IgG (EUROIMMUN, Lübeck, Germany; #EI 2606-9601-2 G), the EUROIMMUN SARS-CoV-2 S1 IgA (#EI 2606-9601-2 A), and the EUROIMMUN SARS-CoV-2-NCP IgG (#EI 2606-9601-2 G) ELISA were performed according to manufacturer’s instructions; serum dilution: 1/101. A ratio to reference value was calculated by dividing the sample OD (450 nm) value by the OD (450 nm) value of a reference sample provided by the manufacturer.Alternatively, 96-well ELISA plates were coated with 4 µg/mL of SARS-CoV-2-S1 antigen (ACROBiosystems, Newark, DE 19711, USA; #S1N-C52H3) to identify anti-S1 serum IgG and IgG1-4 (Hansestadt Lübeck (HL)-1 ELISA), or anti-S1 salivary IgG and IgA (HL-2 ELISA) levels as recently described (31), or J-chain-coupled salivary Abs (HL-2 ELISA). These HL ELISA protocols were established in-house. Briefly (HL-1 and HL-2 ELISA), the plates were washed with 0.05% Tween 20 in PBS to remove unbound antigens. In case of anti-S1 salivary IgG and IgA detection additional blocking (HL-2 ELISA) was performed with 0.05% Tween 20, and 3% BSA in PBS. Subsequently, serum (diluted 1/1000 for IgG and IgG1, 1/100 for IgG2-4, and in addition 1/10 for IgG1 (Figure S2B) detection) or saliva (diluted 1/10 for IgG and IgA detection) in 0.05% Tween 20, 3% BSA in PBS were added. Bound Abs were detected with horseradish peroxidase (HRP)-coupled polyclonal goat anti-human IgG Fc (#A80-104P) or IgA (#A80-102P)-specific Abs purchased from Bethyl Laboratories (Montgomery, TX, USA) or monoclonal anti-human IgG1 (clone HP-6001), IgG2 (clone HP-6014), IgG3 (clone HP-6050), or IgG4 (clone HP-6025)-specific Abs purchased from Southern Biotech (Birmingham, AL, USA) or anti-J-chain Ab (clone F-12) obtained from Santa Cruz Biotechnology (Dallas, TX, USA; #sc-133177) in 0.05% Tween 20, 3% BSA in PBS. After incubation with the 3,3′,5,5′-tetramethylbenzidine (TMB) substrate (BD Biosciences, San Diego, CA, USA), the optical density (OD) was measured at 450 nm. Secondary Ab specificity was verified recently (31). OD (450 nm) values are shown or alternatively, a ratio to reference value was calculated by dividing the sample OD (450 nm) value through the OD (450 nm) value of an internal reference sample of an individual with a historic non-hospitalized SARS-CoV-2 infection.IgG Fc glycosylation analysisTotal IgG Abs were affinity-captured from sera using Protein G Sepharose 4 Fast Flow beads (GE Healthcare, Uppsala, Sweden) in a 96-well filter plate (Millipore Multiscreen, Amsterdam, Netherlands), as described (60, 66). Eluates from total IgG affinity-purification were dried by vacuum centrifugation and subjected to tryptic cleavage followed by liquid chromatography (LC)-mass spectrometry (MS) analysis according to established procedures (60). Using this method, IgG1 glycoforms were assigned based on accurate mass and specific migration position in LC, excluding the possible glycopeptide-level interference of IgG3 with IgG2 and IgG4 (66).LC-MS data processingRaw LC-MS spectra were converted to mzXML files. LaCyTools, an in-house developed software was used for the alignment and targeted extraction of raw data (67). Alignment was performed based on the average retention time of at least three highly abundant glycoforms. The analyte list for targeted extraction of the 2+ and 3+ charge states was based on manual annotation as well as on literature reports (60, 64).The inclusion of an analyte for the final data analysis was based on quality criteria including signal-to-noise (higher than 9), isotopic pattern quality (less than 25% deviation from the theoretical isotopic pattern), and mass error (within ±20 parts per million range) leading to a final analyte list (Table S2). The relative intensity of each glycan form in the final analyte list was calculated by normalizing it to the sum of their total glycoform areas. Normalized intensities were used to calculate fucosylation, bisection, galactosylation, and sialylation (Tables S2, S3). Serum samples with low anti-S Ab levels shortly after the first vaccination did not always result in sufficient signal strengths and hence were excluded from the analysis.Anti-S IgG1 Fc N-glycosylation patterns from unvaccinated, hospitalized non-ICU and ICU SARS-CoV-2 patients were used for comparison. Therefore, anti-S IgG1 Fc N-glycopeptide raw data of our previous study (60) were used to calculate the glycosylation traits based on the analyte list described above (Tables S2, S3).Statistical analysisStatistical analyses were performed using GraphPad Prism v6.0 and v9.0 (GraphPad, La Jolla, CA), and MatLab (The MathWorks Inc., Massachusetts, NE). The smoothed mean curves shown in scatter plots were created by polynomial regression. Confidence bands were plotted by using a confidence level of 95%. Data in bar graphs were presented as mean values ± SD. Differences between two groups were assessed with the Mann-Whitney U test. Differences between more than two groups were assessed with the Kruskal-Wallis test. Pearson correlation was done to measure the strength of the linear relationship between two variables. p-values < 0.05 were considered significant as follows: *, **, ***, ****: p-value < 0.05, 0.01, 0.001, and 0.0001 respectively. Principal component analyses (PCA) were performed in GraphPad Prism v9.0, while the partial least square-discriminant analyses (PLS-DA) were performed in PLS-Toolbox (Eigenvector Research Inc., Wenatchee, WA) in MatLab. For cross-validation, venetian blinds were used and the area under the receiver operating characteristic curve (AUROC) was calculated.ResultsSix study groupsNaïve and non-hospitalized SARS-CoV-2 pre-infected individuals were recruited in Lübeck, Germany, that received one of the six mRNA and adenovirus-based SARS-CoV-2 vaccine combinations shown in Table 1, and sampled (blood serum and/or saliva) once or multiple times up to 270 days after the first immunization.Anti-S1 serum and salivary IgG Ab responsesFirst, we analyzed anti-S1 serum IgG Ab levels by commercially available (EUROIMMUN) and in-house developed (HL) ELISA methods (Figures 1A–D, S1). Early anti-S1 serum IgG levels of naïve and pre-infected vaccinees were similar to those described earlier (9, 31, 34, 68). All three vaccines induced higher anti-S1 serum IgG levels after the second as compared to the first vaccination indicating a re-activation of memory B cells (Figures 1A, S1). Two vaccinations with an mRNA vaccine induced higher anti-S1 serum IgG levels than two vaccinations with the adenovirus-based vaccine AZD1222 (Figures 1A, S1). Among the mRNA vaccines, mRNA-1273 appeared to induce higher anti-S1 IgG titers than BNT162b2 (Figures 1A, S1). Individuals who received the first immunization with the adenovirus-based vaccine and the second with either one or the other mRNA vaccine reached IgG levels comparable to the levels induced with two mRNA vaccinations (Figures 1B, S1). High anti-S1 serum IgG levels were observed early on after the first vaccination with BNT162b2 in pre-infected individuals (Figures 1C, S1).FIGURE 1Figure 1 Anti-S1 serum and salivary IgG levels. (A-C) Anti-S1 serum IgG (HL-1 ELISA) levels (ratios to reference value) of the indicated six vaccination groups. (D) Color legend of the six study groups. (E–G) Anti-S1 salivary IgG levels (OD 450 nm values) of the indicated six groups. Gray bars: time windows of the second shot after the first shot with an mRNA (between day 21 and 45) or adenovirus-based (between day 70 and 84) vaccine. Dashed (— (C)) and dotted (…) lines indicate the corresponding average anti-S1 IgG levels of pre-infected individuals without/before vaccination or non-vaccinated healthy (negative) controls, respectively. (H, I) Pearson correlations between anti-S1 serum IgG (HL-1) levels and anti-S1 salivary IgG levels (y-axis as in (E)) of all paired samples from once and twice BNT162b2-vaccinated naïve and pre-infected individuals. p-values of the indicated correlations are shown.Subsequently, however, the high mRNA-induced anti-S1 serum IgG levels of naïve and pre-infected individuals waned and approached over time the long-term levels observed following two adenovirus-based vaccinations (Figures 1A–C, S1). Furthermore, anti-S1 IgG time courses were similar for and highly correlated between serum and saliva (Figures 1E–I, S1) assuming passive transfer of IgG between blood and lumen/mucosa of the respiratory tract (8, 69).Anti-S1 serum IgG subclass responsesNext, we analyzed anti-S1 serum IgG subclass abundances over time (Figures 2, S2, S3). Our observations confirmed recent findings describing that the mRNA vaccines initially induce anti-S1 IgG1 followed by IgG3 and IgG2 and hardly any IgG4 responses (31, 32, 34), whereas vaccination with AZD1222 mainly results in anti-S IgG1 and IgG3, but hardly any IgG2 and IgG4 (5) in the first weeks after immunization of naïve individuals (Figures 2, S2, S3).FIGURE 2Figure 2 Anti-S1 serum IgG1-4 subclass levels. Anti-S1 serum (A–C) IgG1, (F–H) IgG3, (K–M) IgG4, and (R, S) IgG2 levels (ratios to reference values) of the indicated six vaccination groups. The A+B and A+M groups were not analyzed for IgG2. The used color codes are identical to Figure 1D. Gray bars: time windows of the second shot after the first shot with an mRNA (between day 21 and 45) or adenovirus-based (between day 70 and 84) vaccine. Dotted lines indicate the corresponding average anti-S1 IgG1, IgG3, IgG4, or IgG2 levels of non-vaccinated healthy (negative) controls. (D, E, I, J, N–Q, T, U) Pearson correlations between anti-S1 serum IgG levels (HL-1) and anti-S1 serum (D, E) IgG1, (I, J) IgG3, (N–Q) IgG4, or (T, U) IgG2 levels (y-axis as in (A, F, K or R, respectively)) of all paired samples from one and two-times BNT162b2-vaccinated (D, I, N, T) naïve and (E, J, O, U) pre-infected individuals, and (P, Q) two-times mRNA-1273-vaccinated naïve individuals. The IgG to IgG4 correlation in (Q) was only done with paired long-term samples of two times mRNA-1273-vaccinated naïve individuals collected between day (d) 209 and 256 upon the first immunization. p-values of the indicated correlations are shown.Furthermore, we observed more intense differences in early anti-S1 IgG1 than in early IgG3 levels between the naïve mRNA and AZD1222 groups, with the high initial mRNA-induced anti-S1 IgG response dominated by IgG1 (Figures 2A, F, S2). Over time, anti-S1 IgG1 and IgG3 levels became comparable between the different naïve vaccination groups (Figures 2A, B, F, G, S2).Interestingly, the two times mRNA-1273 vaccination group and to a lesser extent also the two times BNT162b2 vaccination group generated long-term IgG4 responses (Figures 2K, S3). This long-term IgG4 response also developed in vaccinees receiving a combination of AZD1222 and mRNA-1273 as well as AZD1222 and BNT162b2 but to a lesser extent (Figures 2L, S3). Notably, the two times AZD1222 vaccination group did not show this long-term IgG4 response (Figures 2K, L, S3). The initial IgG2 response after mRNA vaccination kept higher over time as compared to the adenovirus-based vaccination in naïve individuals (Figures 2R, S, S3).In pre-infected individuals mostly an increase of IgG1 levels was observed after vaccination with BNT162b2. Although this group showed comparable long-term IgG subclass levels when compared to naïve individuals immunized with BNT162b2 (Figures 2C, H, M, S, S2, S3), their IgG4 response seemed not to be or barely induced (Figures 2M, O).The anti-S1 serum IgG1-3 levels of naïve and the IgG1 levels of pre-infected vaccinees positively correlated with their total anti-S1 serum IgG levels, whereas the IgG2-4 levels of pre-infected vaccinees did not show such a significant positive correlation (Figures 2D, E, I, J, T, U, S2, S3). In contrast, the anti-S1 serum IgG4 levels of naïve individuals vaccinated with mRNA vaccines showed a significant or in tendency negative correlation with their total anti-S1 serum IgG levels (Figures 2N–P, S3). However, anti-S1 serum IgG4 levels of naïve individuals vaccinated twice with mRNA-1273 significantly correlated with their total anti-S1 serum IgG levels, when only long-term samples (between day 209-256 upon the first immunization) were considered (Figures 2Q, S3).Thus, two immunizations or at least a second immunization with an mRNA vaccine generated detectable long-term IgG4 responses in naïve individuals. Notably, the mRNA-1273 vaccine showed a higher potential to generate such a late IgG4 response than the BNT162b2 vaccine.Anti-S1 serum and salivary IgA Ab responsesIn naïve individuals, two vaccinations with mRNA-1273 induced higher anti-S1 IgA levels in the serum and saliva than BNT162b2, and both mRNA vaccines induced higher levels than two doses with AZD1222 (Figures 3A, E, D, S4). In contrast to the anti-S1 IgG levels, the anti-S1 IgA levels in serum and saliva of naïve individuals seemed to be boosted less and decreased faster (Figures 3A, E, S4). Further, the mRNA vaccines induced only a reduced anti-S1 IgA response as compared to the anti-S1 IgG response when the individuals were first vaccinated with AZD1222 (Figures 3B, F, S4). Over time, the mRNA vaccine-induced IgA levels gradually approached the rather low IgA levels resulting from two AZD1222 vaccinations (Figures 3A, E, S4).FIGURE 3Figure 3 Anti-S1 serum and salivary IgA levels. (A-C) Anti-S1 serum IgA levels (ratios to reference value) of the indicated six vaccination groups. (D) Color legend of the six study groups. (E-G) Anti-S1 salivary IgA levels (OD 450 nm values) of the indicated six groups. Gray bars: time windows of the second shot after the first shot with an mRNA (between day 21 and 45) or adenovirus-based (between day 70 and 84) vaccine. Dashed and dotted lines indicate the corresponding anti-S1 IgA average levels of pre-infected individuals without/before vaccination or non-vaccinated healthy (negative) controls, respectively. (H, I) Pearson correlations between anti-S1 serum IgA and salivary IgA levels (y-axis as in (E)) of all paired samples from once and twice BNT162b2-vaccinated naïve and pre-infected individuals. p-values of the indicated correlations are shown.In contrast, pre-infected individuals vaccinated with BNT162b2 reached and maintained higher anti-S1 IgA levels both in serum and saliva over time compared to naïve individuals vaccinated with BNT162b2 (Figures 3C, G, S4). Anti-S1 saliva IgA levels correlated in pre-infected as well as naïve vaccinees with anti-S1 saliva J-chain levels (Figure S4) suggesting that the anti-S1 salivary IgA is mostly dimeric J-chain-coupled secretory (s)IgA in all groups. While anti-S1 salivary IgA levels correlated with anti-S1 serum IgA levels in naïve vaccinees, they did not display such a significant correlation in pre-infected ones (Figures 3H, I, S4). The findings suggest a proper, but more decoupled re-activation of local respiratory and systemic S1-reactive IgA+ B cells in pre-infected vaccinees.Anti-S serum IgG1 Fc N-glycosylationFinally, we analyzed the Fc N-glycosylation patterns of anti-S and total serum IgG1 over time up to day 270 by LC-MS (Figures 4, 5, S5, S6 and Tables S2, S3). The analysis resulted in the identification of 12 IgG1 Fc glycopeptide species (the six major glycan species are schematically shown in Figure 4A), from which glycosylation traits of fucosylation, bisection, sialylation, and galactosylation were calculated (Tables S2, S3). The development of the anti-S IgG1 glycosylation from the vacinees were compared to the anti-S IgG1 glycosylation from unvaccinated, hospitalized non-ICU and ICU SARS-CoV-2 patients investigated in the context of our previous study (60) (Figures 4E, I, 5D, H).FIGURE 4Figure 4 Anti-S serum IgG1 fucosylation and bisection. (A) The six major IgG Fc N-glycans attached to Asn 297 of IgG1 with an average relative abundance of more than 3% (Table S2): Galactose: G, yellow circle; sialic acid: S, purple diamond; fucose: F, red triangle; mannose: green circle; N-acetylglucosamine: GlcNAc and bisecting GlcNAc, N, blue square. (B–D) Anti-S serum IgG1 Fc N-fucosylation and (F–H) anti-S serum IgG1 Fc N-bisection of the indicated six vaccination groups. The used color codes are identical to Figure 1D. Gray bars: time windows of the second shot after the first shot with an mRNA (between day 21 and 45) or adenovirus-based (between day 70 and 84) vaccine. Dashed lines indicate the average level of total IgG1 Fc fucosylation or bisection, respectively (Figure S6). (E) Anti-S serum IgG1 Fc N-fucosylation and (I) anti-S serum IgG1 Fc N-bisection of unvaccinated, hospitalized non-ICU and ICU SARS-CoV-2 patients from our previous study (60) for comparison.FIGURE 5Figure 5 Anti-S serum IgG1 sialylation and galactosylation. (A–C) Anti-S serum IgG1 Fc N-sialylation and (E–G) anti-S serum IgG1 Fc N-galactosylation of the indicated six vaccination groups. The used color codes are identical to Figure 1D. Gray bars: time windows of the second shot after the first shot with an mRNA (between day 21 and 45) or adenovirus-based (between day 70 and 84) vaccine. Dashed lines indicate the average level of total IgG1 Fc sialylation and galactosylation, respectively (Figure S6). (D) Anti-S serum IgG1 Fc N-sialylation and (H) anti-S serum IgG1 Fc N-galactosylation of unvaccinated, hospitalized non-ICU and ICU SARS-CoV-2 patients from our previous study (60) for comparison. (I, J) Comparison of anti-S and total (Figure S6) IgG1 (I) sialylation and (J) galactosylation levels of samples from mRNA (B/M+B/M; B: blue; M: pink) or adenovirus (A+A) vaccinated individuals collected late between day (d) 190 and 256 upon first immunization. The green and red lines indicate the mean of the anti-S IgG1 sialylation and galactosylation levels from the hospitalized non-ICU and ICU patients, respectively, upon day 50 post-infection (D, H).FucosylationIn line with recent findings, the first immunization with BNT162b2 induced an initial anti-S IgG1 response with a temporarily afucosylated glycosylation pattern, with fucosylation levels as low as 80% (34) (Figures 4B, S5). The fucosylation levels steadily increased in the next few weeks to about 98%, surpassing the corresponding total IgG1 fucosylation levels (92-94%), and stagnated over the study period (Figures 4B, S5, S6). A comparable longitudinal fucosylation pattern was observed upon one and two mRNA-1273 immunizations (Figures 4B, S5, S6). On the other hand, one immunization with AZD1222 seemed to induce a less pronounced early afucosylated anti-S IgG1 response, only down to 95% fucosylation, that also increased upon a booster immunization, irrespective of the vaccine type, to about 98% (Figures 4B, C, S5, S6).In contrast, BNT162b2-vaccinated non-hospitalized pre-infected individuals maintained stable anti-S IgG1 fucosylation levels over time (96-97%), which was lower than the long-term anti-S IgG1 fucosylation level of naïve vaccinees and might have had developed already before vaccination as hypothesized previously (34) (Figures 4D, S5, S6). In comparison, the early, very low anti-S IgG1 fucosylation levels (down to 70% fucosylation) of the unvaccinated hospitalized patients vastly increased and reached fucosylation levels higher than 95% already 50 days post-infection (60) (Figure 4E).BisectionFollowing booster immunization with an mRNA vaccine, anti-S IgG1 bisection levels had fallen, then slightly increased, but remained below their total counterpart in naïve individuals throughout the study period (34) (Figures 4F, S5, S6).One dose of AZD1222 induced higher early anti-S IgG1 bisection levels than one dose of an mRNA vaccine (Figures 4F, S5) but after a second immunization irrespective of the vaccine type the levels became comparable to the levels upon two mRNA doses (Figures 4G, S5, S6). However, the two times AZD1222 vaccination group showed a slightly higher long-term anti-S IgG1 bisection level than the other vaccine combinations throughout the study period (Figures 4F, G, S5). Upon day 100, the anti-S IgG1 bisection levels of pre-infected vaccinees surpassed those of naïve individuals upon two BNT162b2 vaccinations (Figures 4H, S5, S6). The unvaccinated, hospitalized non-ICU and ICU patients showed very low early anti-S IgG1 bisection levels that increased over time (60) (Figure 4I).Galactosylation and sialylationBoth one and two immunizations of naïve individuals with an mRNA vaccine led to initial anti-S IgG1 sialylation as well as galactosylation levels higher than their corresponding total IgG1 sialylation and galactosylation levels, a finding consistent with our recent report (34) (Figures 5A, E, S5, S6). However, these anti-S IgG1 sialylation and galactosylation levels decreased with the passage of time and the anti-S1 galactosylation levels even fell below their corresponding total IgG1 levels, but still remained above the very low anti-S1 IgG1 galactosylation levels of the unvaccinated, hospitalized ICU patients that prevailed upon day 50 post-infection (60) (Figures 5A, E, D, H, I, J, S5, S6).Although the initial anti-S IgG1 galactosylation and sialylation levels were lower after one AZD1222 dose compared to one mRNA vaccine dose, the AZD1222-induced levels upon a second dose with AZD1222, BNT162b2, or mRNA-1273 became comparable to the mRNA-induced ones over time up to day 270 (Figure 5, S5, S6). BNT162b2-vaccinated, pre-infected individuals showed a comparable anti-S IgG1 sialylation and a slightly higher anti-S IgG1 galactosylation course to naïve individuals vaccinated with BNT162b2 (Figures 5C, G, S5, S6).Potential predictive parameters and long-term differences in the antibody response upon vaccinationNext, we explored associations between the data at baseline and at later time points to identify potential predictive correlations. Interestingly, age negatively correlated with the long-term anti-S1 serum IgG levels between day 190 and 256 in naïve individuals after two mRNA vaccinations but not after two AZD1222 vaccinations (Figures 6A, B). Furthermore, total IgG1 galactosylation, sialylation, and bisection levels positively correlated significantly or in tendency with the long-term anti-S IgG1 galactosylation, sialylation, and bisection levels, respectively, between day 190 and 256 of individuals from all groups (Figure 6C), suggesting an influence of the total IgG galactosylation, sialylation, and bisection levels on the induced anti-S T and B cell responses in all vaccination groups.FIGURE 6Figure 6 Investigation of potential predictive parameters. (A) Negative correlations between age and late anti-S1 IgG titers (samples collected between day (d) 190 and 256 upon first vaccination) (Left: HL-1 ELISA data; Right: EUROIMMUN (EI) ELISA data) of the indicated vaccination groups. (B) Color legend of the six naïve groups; the mRNA group includes both naïve B+B- and M+M-vaccinated individuals. (C) Positive correlations between total and anti-S IgG1 galactosylation, sialylation, and bisection levels from samples collected late between day (d) 190 and 256.Finally, we verified long-term outcomes between the vaccination groups by performing principal component analyses (PCA) and partial least square-discriminant analyses (PLS-DA) (Figures 7, S7). Conversely to PCA, PLS-DA considers the initial separation in vaccination groups. Both analyses resulted in comparable parameter-dependent separations, albeit with slightly stronger separations with the PLS-DA analysis (Figures 7, S7).FIGURE 7Figure 7 Partial least squares-discriminant analysis (PLS-DA) of the study groups (A) PLS-DA of data collected from the naïve and pre-infected B+(B) vaccination cohorts between day (d) 100 and 170 upon first vaccination. Latent variable (LV) scores and loadings are shown. The area under the receiver operating characteristic curve (AUROC) for cross-validation is 0.8810 for both groups. (B) Color legend of the six vaccination groups. (C) PLS-DA of data collected from all naïve vaccination cohorts between day (d) 209 and 256 upon first vaccination. The AUROC values for cross-validation are B+B: 0.8304, M+M: 0.8449, A+A: 0.7416, A+B: 0.8773, and A+M: 0.9094.Naïve and pre-infected vaccinees that received two BNT162b2 immunizations were separated between day 100 and 170 as follows: pre-infected vaccines showed higher anti-S1 serum and saliva IgA as well as anti-S IgG1 bisection and galactosylation levels, whereas naïve vaccines showed higher anti-S1 IgG4 and anti-S IgG1 fucosylation levels (Figures 7A, B, S7).Naïve individuals that received two AZD1222 immunizations were separated between day 209 and 256 by higher anti-S IgG1 bisection levels and lower anti-S1 IgG4 titers from two times mRNA vaccinated individuals (Figures 7C, S7). AZD1222 vaccinees that received an mRNA booster were separated between day 209 and 256 by higher anti-S1 Ab titers and anti-S IgG1 galactosylation and sialylation levels, likely because of later boosting (Figures 7C, S7).DiscussionOur study shows that the mRNA-containing LNP vaccines BNT162b2 and mRNA-1273 induce high anti-S1 IgG and IgA levels in the blood as well as in the saliva, but these Ig levels steadily decrease over time and approach levels that are comparable to the long-term levels induced by two immunizations with the adenovirus-based vaccine AZD1222. In the long run, such pronounced anti-S1 IgG and (s)IgA reductions in the saliva likely reflect the declining protection against infection and from spreading in the respiratory tract of naïve individuals (16, 17). On the other hand, the observed stronger anti-S1 (s)IgA response in the saliva of previously infected vaccinees – likely generated by re-activation of infection-induced local (s)IgA+ memory B cells – might explain their recently described higher protection from infection and spreading (16, 17, 70).Neutralizing mucosal Abs play a crucial role in preventing infections of the respiratory tract. Therefore, optimized vaccination strategies against pathogens of the respiratory tract should enhance local antigen-specific long-lived PC and memory B cell responses for generating an improved, long-lasting Ab frontline defense response in the mucosa of the respiratory tract (71–73).Both mRNA- and adenovirus-based vaccines generate comparable long-term anti-S1 IgG1 and IgG3 levels up to day 270; whereas the IgG2 levels remained higher after mRNA vaccination. Very interestingly, two mRNA immunizations as well as one AZD1222 immunization with an mRNA booster, in particular with the mRNA-1273 vaccine, induced long-term anti-S1 IgG4 responses – the IgG subclass with inhibitory effector functions – in naïve subjects. In contrast, we could not observe such an increase upon two immunizations with the AZD1222 vaccine in naïve individuals up to day 270, suggesting that only mRNA vaccines generate detectable long-term IgG4 responses at least until day 270.Supporting evidence stems from another study, where comparable results were observed, additionally with a further increase of IgG4 after the third vaccination with BNT162b2 (74). Similarly to our results, no IgG4 response was detected after two AZD1222 vaccinations. The authors observed a slight IgG4 response after immunization with AZD1222 and boosting with BNT162b2, while the effects of the mRNA-1273 vaccine were not analyzed.In a setting of HIV vaccination, a study compared repeated immunizations with two related HIV vaccine formats. The authors described that the protection of one vaccine composition correlated with the induction of IgG1 and IgG3 Abs, whereas the other vaccine composition hardly showed any protection, which correlated with the generation of IgG4 upon repeated immunizations instead (75). Interestingly, repeated immunizations by allergen-specific immunotherapy also induce IgG4 responses over time (50). Thus, the vaccine composition might influence the IgG4-inducing capacity upon repeated immunization.The mRNA-1273 vaccine (100 µg) contained higher amounts of mRNA than the BNT162b2 vaccine (30 µg) that will influence the amount and probably also the duration of S protein expression. One or both circumstances might contribute to the higher induction of anti-S1 IgG4 by the mRNA-1273 vaccine. Furthermore, the induced IgG4 class switching might occur directly from IgM or via other IgG subclass intermediates, as the Igg4 gene locus is the most downstream of the IgG subclass gene loci.Notably, in pre-infected individuals no IgG4 responses were observed upon one or two doses of BNT162b2. However, since the case number and observation period for pre-infected individuals were limited, further studies involving more long-term samples are needed to verify this observation. In the future, the potential non-neutralizing effects of IgG4 on the elimination of SARS-CoV-2 require further investigation.Two mRNA vaccinations and also an AZD1222 vaccination with an mRNA booster induce massive, but temporary anti-S1 IgG(1) responses, presumably generated by short-term PCs. Accordingly, IgG Abs generated by long-term PC responses become noticeable only after several months, when the short-lived PC responses had already faded. In contrast, the adenovirus-based vaccine induced a weaker short-term Ab response, and Abs generated by long-term PC subsets likely become dominant more rapidly.Early and late IgG responses may convey functionally divergent roles. Recent studies suggest that highly galactosylated and sialylated short-term IgG responses might facilitate antigen-delivery to GC reactions in a sialylation-dependent manner for aiding affinity maturation and therewith the induction of IgG Abs with high neutralizing potential (76, 77). In contrast, long-term IgG Ab responses with lower galactosylation and sialylation levels might thereby induce a stronger immune cell activation, also, after subsequent infection with emerging SARS-CoV-2 variants, when neutralizing capacities of the existing Abs are diminished due to their potentially reduced RBD-specificities (15).Although the fighting potential of differently galactosylated and sialylated IgG Abs against pathogens has to be further investigated, it is important to comprehend how vaccine compositions influence IgG Fc galactosylation and sialylation levels in both short- and long-term PC responses (78).A recent mouse immunization study has shown that the (inflammatory) potential of an adjuvant (co-stimulus) reflects the qualitative potential of a vaccine composition to determine the IgG Fc galactosylation and sialylation levels during the GC response and thereby the GC-derived long-term IgG response (64). In summary, the results suggested distinct adjuvant-specific (inflammatory) potentials of different adjuvants to induce GC-driven antigen-specific IgG Abs with low galactosylation and sialylation levels: CFA (complete Freund’s adjuvant; water-in-oil adjuvant+M.tb.) > IFA, Montanide (both water-in-oil adjuvants) > Alum (aluminum hydroxide) > AddaVax (similar to MF59; squalene-based), Toll-like-receptor ligands (64).Furthermore, the adjuvant Alum induced better protection from subsequent SIV infection than MF59, correlating with Alum-induced anti-gp120 IgG Abs with lower galactosylation and sialylation levels than MF59 (65).Mechanistically, the induction of antigen-specific IgG Abs with low galactosylation and sialylation levels in the GC (e.g. with IFA) was linked to the induction of IL-6-dependent IFNγ-producing T follicular helper TFH1 cells. CFA-induced very low IgG galactosylation and sialylation levels have further been linked to the additional induction of (inflammatory) IL-17-producing TFH17 cells (64).Strong inflammatory immune responses induced in ICU-admitted SARS-CoV-2-infected patients were characterized by high IL-6 and IL-17 levels (79), CCR6+ circulating (c)TFH17 cells in the blood (80, 81), and anti-S IgG1 Abs with very low galactosylation levels that prevailed upon day 50 post-infection (60) (Figure 5H). In contrast, non-hospitalized SARS-CoV-2 patients were characterized by CXCR3+ cTFH1 cells (82), and relatively higher anti-S IgG1 galactosylation and sialylation levels (34).The mRNA and adenovirus-based vaccines induced comparable, relatively low long-term anti-S IgG1 galactosylation levels both in naïve vaccinees and vaccinees with past infection. These anti-S IgG1 galactosylation levels were lower than their corresponding total IgG1 galactosylation levels, but not as low as the presumably pro-inflammatory anti-S IgG1 galactosylation levels of ICU patients that were prevailing upon day 50 post-infection (60).These findings suggest that the adenovirus-based and the two mRNA-containing LNP vaccine formats have a comparable, strong potential to influence the quality of the long-term anti-S IgG (1) response, as reflected by the low long-term anti-S IgG1 Fc galactosylation levels.The stimulatory “adjuvant” potential” of the adenovirus-based vaccine might be induced by the activation of pattern-recognition receptors (PRRs) on immune cells, due to its adenovirus-inherent activation nature. Although the mRNAs in the mRNA vaccines have been modified to reduce their interaction with PRRs, residual activation of PRRs cannot be excluded and potentially co-stimulate immune cells. In addition, it has been described that LNPs can also have a co-stimulatory adjuvant effect (83, 84).Recent mouse studies have shown that an mRNA-containing LNP vaccine is inducing rather a TFH1- than an IL-4-producing TFH2-driven GC response, whereas an RBD protein-AddaVax vaccination rather induced a TFH2-driven GC response (85). Furthermore, both mRNA vaccines have induced GC B cells and TFH1 > TFH2, but hardly TFH17 cells in human lymph nodes (86) as well as cTFH1 cells in the blood (82). For the adenovirus-based vaccine, a strong TH1 response has been described (87).Altogether, these human data suggest, comparable to the murine data described above (64), that the induction of (c)TFH17 > (c)TFH1 > (c)TFH2 cells might correlate with lower long-term antigen-specific IgG Fc galactosylation and sialylation levels upon vaccination. Accordingly, the moderate/relatively low long-term anti-S IgG galactosylation levels in the three vaccination groups seem to correlate with the recently described (c)TFH1-driven B cell response for all three vaccines.Predictively, age correlated with lower long-term IgG Ab titers for the mRNA vaccines. Furthermore, total IgG1 galactosylation, sialylation, and bisection levels correlated with higher long-term anti-S IgG1 sialylation, galactosylation, and bisection levels, respectively, for all vaccine combinations suggesting that total IgG Fc glycosylation patterns might influence the glycosylation patterns of antigen-specific immune responses upon vaccination.In summary, the data indicate that the high initial mRNA vaccine-induced anti-S1 IgG(1) and IgA responses decrease over time and approach levels induced with the adenovirus-based vaccine up to day 270. Higher and more stable anti-S1 (s)IgA levels in the saliva of pre-infected vaccinees might explain their higher protection from infection and spread of SARS-CoV-2.Intriguingly, the mRNA vaccines, and in particular the mRNA-1273 vaccine, induced increasing long-term anti-S1 serum IgG4 levels in naïve individuals with hitherto unclear influences on the fight against the pathogen. Naïve individuals vaccinated with the adenovirus-based vaccine did not show such long-term anti-S1 IgG4 response at least after two vaccinations until day 270.Instead, both the mRNA-containing LNP and adenovirus-based vaccines induced comparable anti-S IgG1 glycosylation responses over time up to day 270 as reflected in relatively low anti-S IgG(1) galactosylation levels. This low galactosylation level might reflect the stimulatory “adjuvant” potential of the new vaccine formats and a previously described, primarily TH1-driven B cell response, which overall may contribute to the described efficient protection from severe infections. Understanding the long-term adjuvant effects of mRNA and adenovirus-based vaccinations against SARS-CoV-2 will have potential implications on future vaccine designs.Data availability statementThe original contributions presented in the study are included in the article/Supplementary Material. Further inquiries can be directed to the corresponding authors.Ethics statementThe studies involving human participants were reviewed and approved by Ethics Committee of the University of Lübeck, Germany. The patients/participants provided their written informed consent to participate in this study.Author contributionsOrganization of blood and saliva sampling: JB, IK, AL, BF, EM, SL, MS, VK, JR, CS, TG, and ME. Serum and saliva ELISA analysis: JB, IK, AL, EM, SL, HL, CK, LD, JP, and JR. IgG glycosylation analysis: TP, WW, JN, and MW. Statistical analysis: JB, FS, CS, JR, TP, and ME. Supervision: MW and ME. Writing - original draft: JB, TP, MW, and ME. Initial submission: JB. All authors contributed to the article and approved the submitted version.FundingThis project received funding from the Deutsche Forschungsgemeinschaft ((DFG, German Research Foundation): grants 398859914 (EH 221/10-1); 400912066 (EH 221/11-1); 429175970 (RTG 2633); and 390884018 (Germany`s Excellence Strategies - EXC 2167, Precision Medicine in Chronic Inflammation (PMI)) (ME), the Federal State Schleswig-Holstein, Germany (“COVID-19 Research Initiative Schleswig-Holstein”): grant DOI4-Nr. 3 (ME) and the European Union’s Horizon 2020 research and innovation program H2020-MSCA-ITN: grant 721815) (TP). We acknowledge financial support by Land Schleswig-Holstein within the funding program Open Access Publication Fond. JB was a PhD student of the RTG 2633.Conflict of interestThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.Publisher’s noteAll claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.Supplementary materialThe Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/fimmu.2022.1020844/full#supplementary-materialReferences 1. Acter T, Uddin N, Das J, Akhter A, Choudhury TR, Kim S. Evolution of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) as coronavirus disease 2019 (COVID-19) pandemic: A global health emergency. Sci Total Environ (2020) 730:138996. doi: 10.1016/j.scitotenv.2020.138996PubMed Abstract | CrossRef Full Text | Google Scholar 2. Letko M, Marzi A, Munster V. Functional assessment of cell entry and receptor usage for SARS-CoV-2 and other lineage b betacoronaviruses. Nat Microbiol (2020) 5(4):562–9. doi: 10.1038/s41564-020-0688-yPubMed Abstract | CrossRef Full Text | Google Scholar 3. Polack FP, Thomas SJ, Kitchin N, Absalon J, Gurtman A, Lockhart S, et al. C4591001 clinical trial group. safety and efficacy of the BNT162b2 mRNA covid-19 vaccine. N Engl J Med (2020) 383(27):2603–15. doi: 10.1056/NEJMoa2034577PubMed Abstract | CrossRef Full Text | Google Scholar 4. Jackson LA, Anderson EJ, Rouphael NG, Roberts PC, Makhene M, Coler RN, et al. mRNA-1273 study group. an mRNA vaccine against SARS-CoV-2 - preliminary report. N Engl J Med (2020) 383(20):1920–31. doi: 10.1056/NEJMoa2022483PubMed Abstract | CrossRef Full Text | Google Scholar 5. Barrett JR, Belij-Rammerstorfer S, Dold C, Ewer KJ, Folegatti PM, Gilbride C, et al. Oxford COVID vaccine trial group. phase 1/2 trial of SARS-CoV-2 vaccine ChAdOx1 nCoV-19 with a booster dose induces multifunctional antibody responses. Nat Med (2021) 27(2):279–88. doi: 10.1038/s41591-021-01372-zPubMed Abstract | CrossRef Full Text | Google Scholar 6. Sterlin D, Mathian A, Miyara M, Mohr A, Anna F, Claër L, et al. IgA dominates the early neutralizing antibody response to SARS-CoV-2. Sci Transl Med (2021) 13(577):eabd2223. doi: 10.1126/scitranslmed.abd2223PubMed Abstract | CrossRef Full Text | Google Scholar 7. Seow J, Graham C, Merrick B, Acors S, Pickering S, Steel KJA, et al. Longitudinal observation and decline of neutralizing antibody responses in the three months following SARS-CoV-2 infection in humans. Nat Microbiol (2020) 5(12):1598–607. doi: 10.1038/s41564-020-00813-8PubMed Abstract | CrossRef Full Text | Google Scholar 8. Isho B, Abe KT, Zuo M, Jamal AJ, Rathod B, Wang JH, et al. Persistence of serum and saliva antibody responses to SARS-CoV-2 spike antigens in COVID-19 patients. Sci Immunol (2020) 5(52):eabe5511. doi: 10.1126/sciimmunol.abe5511PubMed Abstract | CrossRef Full Text | Google Scholar 9. Wang Z, Schmidt F, Weisblum Y, Muecksch F, Barnes CO, Finkin S, et al. mRNA vaccine-elicited antibodies to SARS-CoV-2 and circulating variants. Nature (2021) 592(7855):616–22. doi: 10.1038/s41586-021-03324-6PubMed Abstract | CrossRef Full Text | Google Scholar 10. Krammer F. SARS-CoV-2 vaccines in development. Nature (2020) 586:516–27. doi: 10.1038/s41586-020-2798-3PubMed Abstract | CrossRef Full Text | Google Scholar 11. Dagan N, Barda N, Kepten E, Miron O, Perchik S, Katz MA, et al. BNT162b2 mRNA covid-19 vaccine in a nationwide mass vaccination setting. N Engl J Med (2021) 384(15):1412–23. doi: 10.1056/NEJMoa2101765PubMed Abstract | CrossRef Full Text | Google Scholar 12. Shrotri M, Navaratnam AMD, Nguyen V, Byrne T, Geismar C, Fragaszy E, et al. Virus Watch Collaborative. Spike-antibody waning after second dose of BNT162b2 or ChAdOx1. Lancet (2021) 398(10298):385–7. doi: 10.1016/S0140-6736(21)01642-1PubMed Abstract | CrossRef Full Text | Google Scholar 13. Mizrahi B, Lotan R, Kalkstein N, Peretz A, Perez G, Ben-Tov A, et al. Correlation of SARS-CoV-2-breakthrough infections to time-from-vaccine. Nat Commun (2021) 12(1):6379. doi: 10.1038/s41467-021-26672-3PubMed Abstract | CrossRef Full Text | Google Scholar 14. Levine-Tiefenbrun M, Yelin I, Alapi H, Katz R, Herzel E, Kuint J, et al. Viral loads of delta-variant SARS-CoV-2 breakthrough infections after vaccination and booster with BNT162b2. Nat Med (2021) 27(12):2108–10. doi: 10.1038/s41591-021-01575-4PubMed Abstract | CrossRef Full Text | Google Scholar 15. Bartsch YC, Tong X, Kang J, Avendaño MJ, Serrano EF, García-Salum T, et al. Omicron variant spike-specific antibody binding and fc activity is preserved in recipients of mRNA or inactivated COVID-19 vaccines. Sci Trans Med (2022) 14(642):eabn9243. doi: 10.1126/scitranslmed.abn9243CrossRef Full Text | Google Scholar 16. Petráš M, Lesná IK, Večeřová L, Nyčová E, Malinová J, Klézl P, et al. The effectiveness of post-vaccination and post-infection protection in the hospital staff of three Prague hospitals: A cohort study of 8-month follow-up from the start of the COVID-19 vaccination campaign (COVANESS). Vaccines (Basel) (2021) 10(1):9. doi: 10.3390/vaccines10010009PubMed Abstract | CrossRef Full Text | Google Scholar 17. Abu-Raddad LJ, Chemaitelly H, Ayoub HH, Yassine HM, Benslimane FM, Al Khatib HA, et al. Association of prior SARS-CoV-2 infection with risk of breakthrough infection following mRNA vaccination in Qatar. JAMA (2021) 326(19):1930–9. doi: 10.1001/jama.2021.19623PubMed Abstract | CrossRef Full Text | Google Scholar 18. Tenforde MW, Self WH, Adams K, Gaglani M, Ginde AA, McNeal T, et al. Influenza and other viruses in the acutely ill (IVY) network. association between mRNA vaccination and COVID-19 hospitalization and disease severity. JAMA (2021) 326(20):2043–54. doi: 10.1001/jama.2021.19499PubMed Abstract | CrossRef Full Text | Google Scholar 19. Andrews N, Tessier E, Stowe J, Gower C, Kirsebom F, Simmons R, et al. Duration of protection against mild and severe disease by covid-19 vaccines. Engl J Med (2022) 386(4):340–50. doi: 10.1056/NEJMoa2115481CrossRef Full Text | Google Scholar 20. Nimmerjahn F, Ravetch JV. Fcgamma receptors as regulators of immune responses. Nat Rev Immunol (2008) 8(1):34–47. doi: 10.1038/nri2206PubMed Abstract | CrossRef Full Text | Google Scholar 21. Lilienthal GM, Rahmöller J, Petry J, Bartsch YC, Leliavski A, Ehlers M. Potential of murine IgG1 and human IgG4 to inhibit the classical complement and fcγ receptor activation pathways. Front Immunol (2018) 9:958. doi: 10.3389/fimmu.2018.00958PubMed Abstract | CrossRef Full Text | Google Scholar 22. Bruhns P, Iannascoli B, England P, Mancardi DA, Fernandez N, Jorieux S, et al. Specificity and affinity of human fcgamma receptors and their polymorphic variants for human IgG subclasses. Blood (2009) 113(16):3716–25. doi: 10.1182/blood-2008-09-179754PubMed Abstract | CrossRef Full Text | Google Scholar 23. Wang Y, Kreímer V, Iannascoli B, Goff OR, Mancardi DA, Ramke L, et al. Specificity of mouse and human fcgamma receptors and their polymorphic variants for IgG subclasses of different species. Eur J Immunol (2022) 52(5):753–9. doi: 10.1002/eji.202149766PubMed Abstract | CrossRef Full Text | Google Scholar 24. Buhre JS, Becker M, Ehlers M. IgG subclass and fc glycosylation shifts are linked to the transition from pre- to inflammatory autoimmune conditions. Front Immunol (2022) 13:1006939. doi: 10.3389/fimmu.2022.1006939PubMed Abstract | CrossRef Full Text | Google Scholar 25. Diebolder CA, Beurskens FJ, de Jong RN, Koning RI, Strumane K, Lindorfer MA, et al. Complement is activated by IgG hexamers assembled at the cell surface. Science (2014) 343:1260–3. doi: 10.1126/science.1248943PubMed Abstract | CrossRef Full Text | Google Scholar 26. Melis JP, Strumane K, Ruuls SR, Beurskens FJ, Schuurman J, Parren PW. Complement in therapy and disease: regulating the complement system with antibody-based therapeutics. Mol Immunol (2015) 67:117–30. doi: 10.1016/j.molimm.2015.01.028PubMed Abstract | CrossRef Full Text | Google Scholar 27. Wang G, de Jong RN, van den Bremer ET, Beurskens FJ, Labrijn AF, Ugurlar D, et al. Molecular basis of assembly and activation of complement component C1 in complex with immunoglobulin G1 and antigen. Mol Cell (2016) 63:135–45. doi: 10.1016/j.molcel.2016.05.016PubMed Abstract | CrossRef Full Text | Google Scholar 28. de Jong RN, Beurskens FJ, Verploegen S, Strumane K, van Kampen MD, Voorhorst M, et al. A novel platform for the potentiation of therapeutic antibodies based on antigen-dependent formation of IgG hexamers at the cell surface. PloS Biol (2016) 14:e1002344. doi: 10.1371/journal.pbio.1002344PubMed Abstract | CrossRef Full Text | Google Scholar 29. Cook EM, Lindorfer MA, van der Horst H, Oostindie S, Beurskens FJ, Schuurman J, et al. Antibodies that efficiently form hexamers upon antigen binding can induce complement-dependent cytotoxicity under complement-limiting conditions. J Immunol (2016) 197:1762–75. doi: 10.4049/jimmunol.1600648PubMed Abstract | CrossRef Full Text | Google Scholar 30. van der Neut Kolfschoten M, Schuurman J, Losen M, Bleeker WK, Martınez-Mart í ınez P, Vermeulen E, et al. Anti-in í flammatory activity of human IgG4 antibodies by dynamic fab arm exchange. Science (2007) 317(5844):1554–7. doi: 10.1126/science.1144603PubMed Abstract | CrossRef Full Text | Google Scholar 31. Lixenfeld AS, Künsting I, Martin EL, von Kopylow V, Lehrian S, Lunding HB, et al. The BioNTech/Pfizer vaccine BNT162b2 induces class-switched SARS-CoV-2-specific plasma cells and potential memory b cells as well as IgG and IgA serum and IgG saliva antibodies upon the first immunization. MedRxiv (2021) 2021:03. doi: 10.1101/2021.03.10.21252001CrossRef Full Text | Google Scholar 32. Farkash I, Feferman T, Cohen-Saban N, Avraham Y, Morgenstern D, Mayuni G, et al. Anti-SARS-CoV-2 antibodies elicited by COVID-19 mRNA vaccine exhibit a unique glycosylation pattern. Cell Rep (2021) 37(11):110114. doi: 10.1016/j.celrep.2021.110114PubMed Abstract | CrossRef Full Text | Google Scholar 33. Chakraborty S, Gonzalez J, Edwards K, Mallajosyula V, Buzzanco AS, Sherwood R, et al. Proinflammatory IgG fc structures in patients with severe COVID-19. Nat Immunol (2021) 22(1):67–73. doi: 10.1038/s41590-020-00828-7PubMed Abstract | CrossRef Full Text | Google Scholar 34. van Coillie V, Pongracz T, Rahmöller J, Chen H-JGeyer C, van Vlught LA, Buhre JS, et al. The BNT162b2 mRNA SARS-CoV-2 vaccine induces transient afucosylated IgG1 in naïve but not antigen-experienced vaccines. eBioMedicine (2023) 87:104408. doi: 10.1016/j.ebiom.2022.104408CrossRef Full Text | Google Scholar 35. Chakraborty S, Gonzalez JC, Sievers BL, Mallajosyula V, Chakraborty S, Dubey M, et al. Early non-neutralizing, afucosylated antibody responses are associated with COVID-19 severity. Sci Transl Med (2022) 14(635):eabm7853. doi: 10.1126/scitranslmed.abm7853PubMed Abstract | CrossRef Full Text | Google Scholar 36. Kaneko Y, Nimmerjahn F, Ravetch JV. Anti-inflammatory activity of immunoglobulin G resulting from fc sialylation. Science (2006) 313:670–3. doi: 10.1126/science.1129594PubMed Abstract | CrossRef Full Text | Google Scholar 37. Arnold JN, Wormald MR, Sim RB, Rudd PM, Dwek RA. The impact of glycosylation on the biological function and structure of human immunoglobulins. Annu Rev Immunol (2007) 25:21–50. doi: 10.1146/annurev.immunol.25.022106.141702PubMed Abstract | CrossRef Full Text | Google Scholar 38. Nimmerjahn F, Ravetch JV. Anti-inflammatory actions of intravenous immunoglobulin. Annu Rev Immunol (2008) 26:513–33. doi: 10.1146/annurev.immunol.26.021607.090232PubMed Abstract | CrossRef Full Text | Google Scholar 39. Shields RL, Lai J, Keck R, O’Connell LY, Hong K, Meng YG, et al. Lack of fucose on human IgG1 n-linked oligosaccharide improves binding to human FcγRIII and antibody-dependent cellular toxicity. J Biol Chem (2002) 277:26733–40. doi: 10.1074/jbc.M202069200PubMed Abstract | CrossRef Full Text | Google Scholar 40. Ferrara C, Grau S, Jäger C, Sondermann P, Brünker P, Waldhauer I, et al. Unique carbohydrate-carbohydrate interactions are required for high affinity binding between FcγRIII and antibodies lacking core fucose. PNAS (2011) 108:12669–74. doi: 10.1073/pnas.1108455108PubMed Abstract | CrossRef Full Text | Google Scholar 41. Ackerman ME, Crispin M, Yu X, Baruah K, Boesch AW, Harvey DJ, et al. Natural variation in fc glycosylation of HIV-specific antibodies impacts antiviral activity. J Clin Invest (2013) 123(5):2183–92. doi: 10.1172/JCI65708PubMed Abstract | CrossRef Full Text | Google Scholar 42. Larsen MD, Lopez-Perez M, Dickson EK, Ampomah P, Tuikue Ndam N, Nouta J, et al. Afucosylated plasmodium falciparum-specific IgG is induced by infection but not by subunit vaccination. Nat Commun (2021) 12(1):5838. doi: 10.1038/s41467-021-26118-wPubMed Abstract | CrossRef Full Text | Google Scholar 43. Bharadwaj P, Shrestha S, Pongracz T, Concetta C, Sharma S, Le Moine A, et al. Afucosylation of HLA-specific IgG1 as a potential predictor of antibody pathogenicity in kidney transplantation. medRxiv (2022). doi: 10.1101/2022.03.09.22272152CrossRef Full Text | Google Scholar 44. Anthony RM, Kobayashi T, Wermeling F, Ravetch JV. Intravenous gammaglobulin suppresses inflammation through a novel T(H)2 pathway. Nature (2011) 475(7354):110–3. doi: 10.1038/nature10134PubMed Abstract | CrossRef Full Text | Google Scholar 45. Oefner CM, Winkler A, Hess C, Lorenz AK, Holecska V, Huxdorf M, et al. Tolerance induction with T cell-dependent protein antigens induces regulatory sialylated IgGs. J Allergy Clin Immunol (2012) 129(6):1647–55. doi: 10.1016/j.jaci.2012.02.037PubMed Abstract | CrossRef Full Text | Google Scholar 46. Hess C, Winkler A, Lorenz AK, Holecska V, Blanchard V, Eiglmeier S, et al. T Cell-independent b cell activation induces immunosuppressive sialylated IgG antibodies. J Clin Invest (2013) 123(9):3788–96. doi: 10.1172/JCI65938PubMed Abstract | CrossRef Full Text | Google Scholar 47. Pincetic A, Bournazos S, DiLillo DJ, Maamary J, Wang TT, Dahan R, et al. Type I and type II fc receptors regulate innate and adaptive immunity. Nat Immunol (2014) 15(8):707–16. doi: 10.1038/ni.2939PubMed Abstract | CrossRef Full Text | Google Scholar 48. Ohmi Y, Ise W, Harazono A, Takakura D, Fukuyama H, Baba Y, et al. Sialylation converts arthritogenic IgG into inhibitors of collagen-induced arthritis. Nat Commun (2016) 7:11205. doi: 10.1038/ncomms11205PubMed Abstract | CrossRef Full Text | Google Scholar 49. Pfeifle R, Rothe T, Ipseiz N, Scherer HU, Culemann S, Harre U, et al. Regulation of autoantibody activity by the IL-23-TH17 axis determines the onset of autoimmune disease. Nat Immunol (2017) 18(1):104–13. doi: 10.1038/ni.3579PubMed Abstract | CrossRef Full Text | Google Scholar 50. Epp A, Hobusch J, Bartsch YC, Petry J, Lilienthal G-M, Koeleman CAM, et al. Sialylation of IgG antibodies inhibits IgG-mediated allergic reactions. J Allergy Clin Immunol (2018) 141(1):399–402.e8. doi: 10.1016/j.jaci.2017.06.021PubMed Abstract | CrossRef Full Text | Google Scholar 51. Bartsch YC, Rahmöller J, Mertes MMM, Eiglmeier S, Lorenz FKM, Stoehr AD, et al. Sialylated autoantigen-reactive IgG antibodies attenuate disease development in autoimmune mouse models of lupus nephritis and rheumatoid arthritis. Front Immunol (2018) 9:1183. doi: 10.3389/fimmu.2018.01183PubMed Abstract | CrossRef Full Text | Google Scholar 52. Petry J, Rahmöller J, Dühring L, Lilienthal G-M, Lehrian S, Buhre JS, et al. Enriched blood IgG sialylation attenuates IgG-mediated and IgGcontrolled-IgE-mediated allergic reactions. J Allergy Clin Immunol (2021) 147(2):763–7. doi: 10.1016/j.jaci.2020.05.056PubMed Abstract | CrossRef Full Text | Google Scholar 53. Banda NK, Wood AK, Takahashi K, Levitt B, Rudd PM, Royle L, et al. Initiation of the alternative pathway of murine complement by immune complexes is dependent on n-glycans in IgG antibodies. Arthritis Rheumatol (2008) 58:3081–9. doi: 10.1002/art.23865CrossRef Full Text | Google Scholar 54. Karsten CM, Pandey MK, Figge J, Kilchenstein R, Taylor PR, Rosas M, et al. Anti-inflammatory activity of IgG1 mediated by fc galactosylation and association of FcγRIIB and dectin-1. Nat Med (2012) 18(9):1401–6. doi: 10.1038/nm.2862PubMed Abstract | CrossRef Full Text | Google Scholar 55. van Osch TLJ, Nouta J, Derksen NIL, van Mierlo G, van der Schoot CE, Wuhrer M, et al. Fc galactosylation promotes hexamerization of human IgG1, leading to enhanced classical complement activation. J Immunol (2021) 207(6):1545–54. doi: 10.4049/jimmunol.2100399PubMed Abstract | CrossRef Full Text | Google Scholar 56. Ito K, Furukawa J, Yamada K, Tran NL, Shinohara Y, Izui S. Lack of galactosylation enhances the pathogenic activity of IgG1 but not IgG2a anti-erythrocyte autoantibodies. J Immunol (2014) 192(2):581–8. doi: 10.4049/jimmunol.1302488PubMed Abstract | CrossRef Full Text | Google Scholar 57. Flevaris K, Kontoravdi C. Immunoglobulin G n-glycan biomarkers for autoimmune diseases: Current state and a glycoinformatics perspective. Int J Mol Sci (2022) 23(9):5180. doi: 10.3390/ijms23095180PubMed Abstract | CrossRef Full Text | Google Scholar 58. Hoepel W, Chen HJ, Geyer CE, Allahverdiyeva S, Manz XD, de Taeye SW, et al. High titers and low fucosylation of early human anti-SARS-CoV-2 IgG promote inflammation by alveolar macrophages. Sci Transl Med (2021) 13(596):eabf8654. doi: 10.1126/scitranslmed.abf8654PubMed Abstract | CrossRef Full Text | Google Scholar 59. Larsen MD, de Graaf EL, Sonneveld ME, Plomp HR, Nouta J, Hoepel W, et al. Afucosylated IgG characterizes enveloped viral responses and correlates with COVID-19 severity. Science (2021) 371(6532):eabc8378. doi: 10.1126/science.abc8378PubMed Abstract | CrossRef Full Text | Google Scholar 60. Pongracz T, Nouta J, Wang W, van Meijgaarden KE, Linty F, Vidarsson G, et al. Immunoglobulin G1 fc glycosylation as an early hallmark of severe COVID-19. EBioMedicine (2022) 78:103957. doi: 10.1016/j.ebiom.2022.103957PubMed Abstract | CrossRef Full Text | Google Scholar 61. Vicente MM, Alves I, Gaifem J, Rodrigues CS, Fernandes Â, Dias AM, et al. Altered IgG glycosylation at COVID-19 diagnosis predicts disease severity. Eur J Immunol (2022) 52(6):946–57. doi: 10.1002/eji.202149491PubMed Abstract | CrossRef Full Text | Google Scholar 62. Woodruff MC, Ramonell RP, Saini AS, Haddad NS, Anam FA, Rudolph ME, et al. Relaxed peripheral tolerance drives broad de novo autoreactivity in severe COVID-19. medRxiv (2021) 2020:10. doi: 10.1101/2020.10.21.20216192CrossRef Full Text | Google Scholar 63. de Jong SE, Selman MH, Adegnika AA, Amoah AS, van Riet E, Kruize YC, et al. IgG1 fc n-glycan galactosylation as a biomarker for immune activation. Sci Rep (2016) 6:28207. doi: 10.1038/srep28207PubMed Abstract | CrossRef Full Text | Google Scholar 64. Bartsch YC, Eschweiler S, Leliavski A, Lunding HB, Wagt S, Petry J, et al. IgG fc sialylation is regulated during the germinal center reaction following immunization with different adjuvants. J Allergy Clin Immunol (2020) 146(3):652–666.e11. doi: 10.1016/j.jaci.2020.04.059PubMed Abstract | CrossRef Full Text | Google Scholar 65. Vaccari M, Gordon SN, Fourati S, Schifanella L, Liyanage NP, Cameron M, et al. Adjuvant-dependent innate and adaptive immune signatures of risk of SIVmac251 acquisition. Nat Med (2016) 22(7):762–70. doi: 10.1038/nm1016-1192aPubMed Abstract | CrossRef Full Text | Google Scholar 66. Falck D, Jansen BC, de Haan N, Wuhrer M. High-throughput analysis of IgG fc glycopeptides by LC-MS. Methods Mol Biol (2017) 1503:31–47. doi: 10.1007/978-1-4939-6493-2_4PubMed Abstract | CrossRef Full Text | Google Scholar 67. Jansen BC, Falck D, De Haan N, Ederveen ALH, Razdorov G, Lauc G, et al. LaCyTools: A targeted liquid chromatography-mass spectrometry data processing package for relative quantitation of glycopeptides. J Proteome Res (2016) 15:2198–210. doi: 10.1021/acs.jproteome.6b00171PubMed Abstract | CrossRef Full Text | Google Scholar 68. Adjobimey T, Meyer J, Sollberg L, Bawolt M, Berens C, Kovačević P, et al. Comparison of IgA, IgG and neutralizing antibody responses following immunization with moderna, BioNTech, AstraZeneca, Sputnik-V, Johnson and Johnson, and sinopharm’s COVID-19 vaccines. Res Square (2021) 13. doi: 10.21203/rs.3.rs-1197023/v1CrossRef Full Text | Google Scholar 69. Hettegger P, Huber J, Paßecker K, Soldo R, Kegler U, Nöhammer C, et al. High similarity of IgG antibody profiles in blood and saliva opens opportunities for saliva based serology. PloS One (2019) 14(6):e0218456. doi: 10.1371/journal.pone.0218456PubMed Abstract | CrossRef Full Text | Google Scholar 70. Azzi L, Dalla Gasperina D, Veronesi G, Shallak M, Ietto G, Iovino D, et al. Mucosal immune response in BNT162b2 COVID-19 vaccine recipients. EBioMedicine (2022) 75:103788. doi: 10.1016/j.ebiom.2021.103788PubMed Abstract | CrossRef Full Text | Google Scholar 71. Hopkins S, Kraehenbuhl JP, Schödel F, Potts A, Peterson D, de Grandi P, et al. A recombinant salmonella typhimurium vaccine induces local immunity by four different routes of immunization. Infect Immun (1995) 63(9):3279–86. doi: 10.1128/iai.63.9.3279-3286.1995PubMed Abstract | CrossRef Full Text | Google Scholar 72. Kantele A, Häkkinen M, Moldoveanu Z, Lu A, Savilahti E, Alvarez RD. Differences in immune responses induced by oral and rectal immunizations with salmonella typhi Ty21a: Evidence for compartmentalization within the common mucosal immune system in humans. Infect Immun (1998) 66(12):5630–5. doi: 10.1128/IAI.66.12.5630-5635.1998PubMed Abstract | CrossRef Full Text | Google Scholar 73. Eriksson K, Quiding-Järbrink M, Osek J, Möller A, Björk S, Holmgren J, et al. Specific-antibody-secreting cells in the rectums and genital tracts of nonhuman primates following vaccination. Infect Immun (1998) 66(12):5889–96. doi: 10.1128/IAI.66.12.5889-5896.1998PubMed Abstract | CrossRef Full Text | Google Scholar 74. Irrgang P, Gerling J, Kocher K, Lapuente D, Steininger P, Wytopil M, et al. Class switch towards non-inflammatory IgG isotypes after repeated SARS-CoV-2 mRNA vaccination. medRxiv (2022). doi: 10.1101/2022.07.05.22277189CrossRef Full Text | Google Scholar 75. Chung AW, Ghebremichael M, Robinson H, Brown E, Choi I, Lane S, et al. Polyfunctional fc-effector profiles mediated by IgG subclass selection distinguish RV144 and VAX003 vaccines. Sci Transl Med (2014) 6(228):228ra38. doi: 10.1126/scitranslmed.3007736PubMed Abstract | CrossRef Full Text | Google Scholar 76. Wang TT, Maamary J, Tan GS, Bournazos S, Davis CW, Krammer F, et al. Anti-HA glycoforms drive b cell affinity selection and determine influenza vaccine efficacy. Cell (2015) 162(1):160–9. doi: 10.1016/j.cell.2015.06.026PubMed Abstract | CrossRef Full Text | Google Scholar 77. Lofano G, Gorman MJ, Yousif AS, Yu WH, Fox JM, Dugast AS, et al. Antigen-specific antibody fc glycosylation enhances humoral immunity via the recruitment of complement. Sci Immunol (2018) 3(26):eaat7796. doi: 10.1126/sciimmunol.aat7796PubMed Abstract | CrossRef Full Text | Google Scholar 78. Alter G, Ottenhoff THM, Joosten SA. Antibody glycosylation in inflammation, disease and vaccination. Semin Immunol (2018) 39:102–10. doi: 10.1016/j.smim.2018.05.003PubMed Abstract | CrossRef Full Text | Google Scholar 79. Ling L, Chen Z, Lui G, Wong CK, Wong WT, Ng RWY, et al. Longitudinal cytokine profile in patients with mild to critical COVID-19. Front Immunol (2021) 12:763292. doi: 10.3389/fimmu.2021.763292PubMed Abstract | CrossRef Full Text | Google Scholar 80. Park JH, Lee HK. Delivery routes for COVID-19 vaccines. Vaccines (Basel). (2021) 9(5):524. doi: 10.3390/vaccines9050524PubMed Abstract | CrossRef Full Text | Google Scholar 81. Biswas B, Chattopadhyay S, Hazra S, Hansda AK, Goswami R. COVID-19 pandemic: the delta variant, T-cell responses, and the efficacy of developing vaccines. Inflammation Res (2022) 71(4):377–96. doi: 10.1007/s00011-022-01555-5CrossRef Full Text | Google Scholar 82. Koutsakos M, Lee WS, Wheatley AK, Kent SJ, Juno JA. T Follicular helper cells in the humoral immune response to SARS-CoV-2 infection and vaccination. J Leukoc Biol (2022) 111(2):355–65. doi: 10.1002/JLB.5MR0821-464RPubMed Abstract | CrossRef Full Text | Google Scholar 83. Alameh MG, Tombácz I, Bettini E, Lederer K, Sittplangkoon C, Wilmore JR, et al. Lipid nanoparticles enhance the efficacy of mRNA and protein subunit vaccines by inducing robust T follicular helper cell and humoral responses. Immunity (2021) 54(12):2877–2892.e7. doi: 10.1016/j.immuni.2021.11.001PubMed Abstract | CrossRef Full Text | Google Scholar 84. Bettini E, Locci M. SARS-CoV-2 mRNA vaccines: Immunological mechanism and beyond. Vaccines (Basel) (2021) 9(2):147. doi: 10.3390/vaccines9020147PubMed Abstract | CrossRef Full Text | Google Scholar 85. Lederer K, Castaño D, Gómez Atria D, Oguin TH 3rd, Wang S, Manzoni TB, et al. SARS-CoV-2 mRNA vaccines foster potent antigen-specific germinal center responses associated with neutralizing antibody generation. Immunity (2020) 53(6):1281–1295.e5. doi: 10.1016/j.immuni.2020.11.009PubMed Abstract | CrossRef Full Text | Google Scholar 86. Lederer K, Bettini E, Parvathaneni K, Painter MM, Agarwal D, Lundgreen KA, et al. Germinal center responses to SARS-CoV-2 mRNA vaccines in healthy and immunocompromised individuals. Cell (2022) 185(6):1008–1024.e15. doi: 10.1016/j.cell.2022.01.027PubMed Abstract | CrossRef Full Text | Google Scholar 87. Swanson PA 2nd, Padilla M, Hoyland W, McGlinchey K, PA F, Bibi S, et al. AstraZeneca/Oxford/VRC study Group.AZD1222/ChAdOx1 nCoV-19 vaccination induces a polyfunctional spike protein-specific TH1 response with a diverse TCR repertoire. Sci Transl Med (2021) 13(620):eabj7211. doi: 10.1126/scitranslmed.abj7211PubMed Abstract | CrossRef Full Text | Google ScholarKeywords: vaccination, antibody, SARS-CoV-2, COVID-19, IgG, IgG subclass, IgG glycosylation, IgACitation: Buhre JS, Pongracz T, Künsting I, Lixenfeld AS, Wang W, Nouta J, Lehrian S, Schmelter F, Lunding HB, Dühring L, Kern C, Petry J, Martin EL, Föh B, Steinhaus M, von Kopylow V, Sina C, Graf T, Rahmöller J, Wuhrer M and Ehlers M (2023) mRNA vaccines against SARS-CoV-2 induce comparably low long-term IgG Fc galactosylation and sialylation levels but increasing long-term IgG4 responses compared to an adenovirus-based vaccine. Front. Immunol. 13:1020844. doi: 10.3389/fimmu.2022.1020844Received: 16 August 2022; Accepted: 09 December 2022;Published: 12 January 2023.Edited by:Shikha Shrivastava, Pfizer, United StatesReviewed by:Lela Kardava, National Institute of Allergy and Infectious Diseases (NIH), United StatesJames Drew Brien, Saint Louis University, United StatesCopyright © 2023 Buhre, Pongracz, Künsting, Lixenfeld, Wang, Nouta, Lehrian, Schmelter, Lunding, Dühring, Kern, Petry, Martin, Föh, Steinhaus, von Kopylow, Sina, Graf, Rahmöller, Wuhrer and Ehlers. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.*Correspondence: Manfred Wuhrer, m.wuhrer@lumc.nl; Marc Ehlers, marc.ehlers@uksh.de†These authors share first authorship‡These authors share last authorship 



This article is part of the Research Topic



                    Current Landscape of Adjuvants for Optimizing the Immunogenicity of Vaccines
                

                        View all
4                        Articles 











"
https://news.ycombinator.com/rss,MSI's (in)Secure Boot,https://dawidpotocki.com/en/2023/01/13/msi-insecure-boot/,Comments,"

MSI's (in)Secure Boot


      posted on 2023-01-13 (UTC)
    

I guess I have found a reason to write my first blog post.
Before we start, maybe I will quickly explain what Secure Boot is. It is
a security feature, which allows our computer to decline booting
operating systems that have not been signed by a key that the firmware
trusts.
On 2022-12-11, I decided to setup Secure Boot on my new desktop with a
help of sbctl. Unfortunately I have
found that my firmware was… accepting every OS image I gave it, no
matter if it was trusted or not. It wasn't the first time that I have
been self-signing Secure Boot, I wasn't doing it wrong.
As I have later discovered on 2022-12-16, it wasn't just broken
firmware, MSI had changed their Secure Boot defaults to allow booting on
security violations(!!).
This can be changed by going to the place where the settings are for
Secure Boot on your motherboard. In my case it's in Security\Secure Boot. From this place, we can see a menu called ""Image Execution
Policy"", which is the culprit.

When we enter the menu, we can see the disappointing default settings.
It's doing no verification. It's useless. It's just there to satisfy
Windows 11 requirements. OS has no idea that Secure Boot is doing
nothing, it just knows that it's ""enabled"".

To change the settings to something saner, we have to change ""Always
Execute"" to ""Deny Execute"" for ""Removable Media"" and ""Fixed Media"".
What's funny is that ""Allow Execute"" and ""Query User"" options
are breaking UEFI specification,
though I'm not really sure what's the difference between ""Allow Execute""
and ""Always Execute"".
We can also change ""Option ROM"", about which you can read more about
here:

https://github.com/Foxboron/sbctl/wiki/FAQ#option-rom
https://learn.microsoft.com/en-us/windows-hardware/manufacture/desktop/uefi-validation-option-rom-validation-guidance


Case closed, everyone can move on, right?
Well, not really. I needed to figure out if this is only affecting my
motherboard or also other models and maybe even other vendors. And also
we need to document this, because even if I know this, there is probably
a lot of people that are not aware about this issue.
I had asked 2 users of B450 TOMAHAWK MAX (thanks Sage Hane and Daniel
Nathan Gray) to check their firmware and what? Unsurprisingly, it's also
there. We were able to determine that version 7C02v3C from 2022-01-18
introduced this issue.
Is it mentioned in the changelog? Hah, nope.

I have also received information from a user of B550-A PRO (CEC) (thanks
Joseph Richey) that they have this issue from 7C56vH1 (2021-12-20)
onwards.
While I was able to extrapolate this information to guess which versions
for other boards have introduced this issue, that isn't really enough.
We need to go deeper.
I have tried extracting some information from MSI's binary firmware
files, but to no avail. I tried using binwalk, UEFITool and others, but
I didn't really find what I wanted. Until one day I have found out that
UEFI has a thing called ""UEFI Internal Form Representation"" or in short
""IFR"". It's a way to describe firmware configuration options. This is
exactly what I need to look for! Now, what do I do with this knowledge?
Once we extract files from the firmware using UEFIExtract from
UEFITool project, we can find a
file called
Section_PE32_image_899407D7-99FE-43D8-9A21-79EC328CAC21_Setup_body.bin.
It contains most of UEFI GUI stuff and seems to be available on all
firmware from all major desktop motherboard makers, though ASUS decided
to remove ""Setup"" from the name for some reason or maybe it has to do
something to do with the UEFIExtract, not sure.
Now once we have this file, we have to extract IFR data from it, to do
it we can use IFRExtractor RS.
Funnily enough, it's made by the same people as UEFITool. Thanks guys
for your hard work, otherwise I would have to do it myself ;p.
Now with IFR extracted, we have what we wanted. We can see all the
UEFI settings available, including ""Image Execution Policy"".
Form FormId: 0x2A79, Title: ""Image Execution Policy""
	Text Prompt: ""Internal FV"", Help: """", Text: ""Always Execute""
	OneOf Prompt: ""Option ROM"", Help: ""Image Execution Policy on Security Violation per Device Path"", QuestionFlags: 0x10, QuestionId: 0x1116, VarStoreId: 0x28, VarOffset: 0x4, Flags: 0x10, Size: 8, Min: 0x0, Max: 0x5, Step: 0x0
		Default DefaultId: 0x0 Value: 0
		OneOfOption Option: ""Always Execute"" Value: 0
		OneOfOption Option: ""Always Deny"" Value: 1
		OneOfOption Option: ""Allow Execute"" Value: 2
		OneOfOption Option: ""Defer Execute"" Value: 3
		OneOfOption Option: ""Deny Execute"" Value: 4
		OneOfOption Option: ""Query User"" Value: 5
	End
	OneOf Prompt: ""Removable Media"", Help: ""Image Execution Policy on Security Violation per Device Path"", QuestionFlags: 0x10, QuestionId: 0x1117, VarStoreId: 0x28, VarOffset: 0x5, Flags: 0x10, Size: 8, Min: 0x0, Max: 0x5, Step: 0x0
		Default DefaultId: 0x0 Value: 0
		OneOfOption Option: ""Always Execute"" Value: 0
		OneOfOption Option: ""Always Deny"" Value: 1
		OneOfOption Option: ""Allow Execute"" Value: 2
		OneOfOption Option: ""Defer Execute"" Value: 3
		OneOfOption Option: ""Deny Execute"" Value: 4
		OneOfOption Option: ""Query User"" Value: 5
	End
	OneOf Prompt: ""Fixed Media"", Help: ""Image Execution Policy on Security Violation per Device Path"", QuestionFlags: 0x10, QuestionId: 0x1118, VarStoreId: 0x28, VarOffset: 0x6, Flags: 0x10, Size: 8, Min: 0x0, Max: 0x5, Step: 0x0
		Default DefaultId: 0x0 Value: 0
		OneOfOption Option: ""Always Execute"" Value: 0
		OneOfOption Option: ""Always Deny"" Value: 1
		OneOfOption Option: ""Allow Execute"" Value: 2
		OneOfOption Option: ""Defer Execute"" Value: 3
		OneOfOption Option: ""Deny Execute"" Value: 4
		OneOfOption Option: ""Query User"" Value: 5
	End
End
I have checked if other vendors (ASRock, ASUS, Biostar, EVGA, Gigabyte
and NZXT) have the same thing and I wasn't able to find anything like
that in their IFR. Also MSI's laptops are not affected by this issue.
I'm gonna assume that they figured that Microsoft wouldn't approve it
and/or that they had less tickets from people about Secure Boot related
issues for their laptops.
Now, doing this manually would be kinda annoying, so I made a small
little shell script which checks if ""Image Execution Policy"" menu is
available and if any of the three options are set to ""Always Execute"".
#!/bin/sh

if [ ! -d ""$1"" ]; then
	[ ! -f ""$1.zip"" ] && curl ""https://download.msi.com/bos_exe/mb/$1.zip"" -O -#
	bsdtar xf ""$1.zip""
fi

cd ""$1"" || exit
UEFIExtract ./*MS.* unpack 1>/dev/null
ifrextractor ./*.dump/Section_PE32_image_899407D7-99FE-43D8-9A21-79EC328CAC21_Setup_body.bin 1>/dev/null
output=""$(grep -A1 -E 'OneOf Prompt: ""(Option ROM|Removable Media|Fixed Media)"", Help: ""Image Execution Policy' ./*.dump/Section_PE32_image_899407D7-99FE-43D8-9A21-79EC328CAC21_Setup_body.bin.*ifr.txt)""
clear

if echo ""$output"" | grep -q ""DefaultId: 0x0""; then
	printf ""\033[1;31m%s: Bad\033[0m\n"" ""$1""
else
	printf ""\033[1;32m%s: Good\033[0m\n"" ""$1""
fi
Now this is where the fun part ends. Now I had to check firmware for
MSI's somewhat recent boards.
While we can get most of the firmware just by going to motherboard's
support page, MSI usually only lists stable firmware and the newest
beta. The problem is that I need to figure out the earliest affected
version of the firmware for each board, which means that I have to guess
what the betas were called (if they even existed). At least MSI doesn't
remove most of its beta firmware from their servers, so they are still
accessible if you know the link.
For some AMD boards, I have found a list of beta firmware on some German
forum. Thankfully, I didn't have to read any German, because contrary to
the popular belief, I don't know German or Russian, I'm Polish.
This… took forever. I checked every motherboard for:

AMD: TRX40, X670, X570, X470, X370, B650, B550, B450, B350, A520, A320
Intel: X399, X299, Z790, Z690, Z590, Z490, Z390, Z370, B760, B660, B560, B460, B360, H670, H510, H410, H370, H310

It's… a lot of motherboards. For a full list of affected motherboards
and their firmware versions, visit
sbctl#181.
And now it's time for some ""fun"" statistic:
# The amount of times I ran the script
$ history 0 | grep ""  msi "" | wc -l
1989
According to Wikipedia in 1989:

The first commercial Internet service providers surfaced in this year,
as well as the first written proposal for the World Wide Web and New
Zealand, Japan and Australia's first Internet connections.

Well, that was a mistake.
You could ask me, why didn't I automate it? The reason is… well… some of
it is not really easy to as some beta names have arbitrary suffixes
which was faster for me to guess than having a script bruteforce its way
in. Also some boards weren't listed on their motherboard list page.
Now, after doing all the work for MSI, I think I should bill them, that
or they should give me a lifetime supply of their motherboards.
If you are curious, yes, I have tried contacting MSI about this issue,
but they ignored my emails and other forms of communication I have
tried.

Conclusion

Don't trust that whatever security features you enabled are working,
TEST THEM! Somehow I was the first person to document this, even though
it has been first introduced somewhere in 2021 Q3.

Quiz time!

What's the difference between these 3 boards:

MSI B360 GAMING ARCTIC
MSI B360 GAMING PLUS
MSI B360-A PRO

Heatsink and PCB colours! They are the same board and share the same
firmware! But hey, the red and white one is only for gamers but black is
only suitable for ""professionals"".

"
https://news.ycombinator.com/rss,Project Mage is an effort to build a power-user environment in Common Lisp,https://project-mage.org,Comments,"




Project Mage













Project Mage
Home
About
Campaign
Code
Contact
RSS









nil


The Power of Structure →



Hi, and welcome! Project Mage is an effort to build a power-user environment and a set of applications in Common Lisp. To get an overview, see The Power of Structure. Otherwise, the essays listed below may be read in any order.








The Power of Structure (12 January 2023) Emacs is Not Enough (12 January 2023) On Flexibility and Software Temples (12 January 2023) Overcoming the Print-Statement (12 January 2023) Data-Supplied Web as a Model of Free Web (12 January 2023) Isn't It Obvious That C Programmers Wrote Git? (12 January 2023) Epilogue (12 January 2023) [Appendix] Why Common Lisp for This Project (12 January 2023) [Appendix] All Else is Not Enough (12 January 2023)







...proudly created, delivered and presented to you by: some-mthfka. Mthfka!



"
https://news.ycombinator.com/rss,Quitting the rat race,https://seanbarry.dev/posts/quitting-the-rat-race/,Comments,"Quitting the Rat Race - Seán BarryAll ArticlesQuitting the Rat RaceSome readers may be familiar with the cartoon “Happiness” by Steve Cutts. If you’re not, it’s embedded below. Please take four minutes to give it a watch if you haven’t already:
  
Around the 30 second mark is a scene where the protaganist rat is waiting for a train to arrive at a packed platform. I recently had a sobering realisation while standing waiting at the platform for the Waterloo & City Line: this had become my life.
I grew up in one of the least densely populated parts of the United Kingdom. My childhood was a happy one: full of trees, hills, mountains, lakes, beaches, and all of the adventures to be found in such places. I now live in one of the most nature depleted parts of an already nature depleted country. 
There’s so much noise in the city around me: planes approaching Heathrow, scooters whizzing around to deliver bags of fast food, cars revving while breaking the speed limit to get to the next red light, people playing music out loud while travelling and rowdy groups heading home from the pub.
When I cycle I need to cycle through kilometre after kilometre of urban jungle before I see anything resembling a field. When I run I have to stop frequently to cross dangerous roads. I’m constantly aware that the air I’m breathing is polluted and the water I drink has been recycled many times.
I’m currently working at a top tier investment bank as a software engineer. I’m an insignificant cog in a machine that skims the cream from the milk. I’m earning the most money I’ve ever made and yet I’m the least fulfilled I’ve ever been. 
Almost everything around me is designed to addict me. Every storefront specifically engineered to attract me inside with gimics like flashing lights. There are countless places I can go to buy experiences - simulations supposed to release some chemicals in my brain and give me a thrill.
The truth is: nothing I’ve done or experienced in this place has given me any experience comparable to walking along the ridge between two mountains. Nothing has made me feel alive like getting in to freezing cold water despite my body screaming at me not to. Nothing has made me feel anything like that feeling when you summit a mountain after 2 hours of solid climbing in the rain, and the clouds part to reveal the most spectacular and breathtaking view you’ve ever seen.
The best part about those things is that there is no booking system. There is no door security choosing who gets in because there is no door. It’s all there, ready to be experienced, and free.
Right now in the UK and across the world, things are uncertain. Companies are laying workers off, there’s a cost of living and energy crisis. I’m excruciatingly lucky to have been in the right place at the right time to develop the skillset I’ve got. In times like these every signal is telling me to stay on the path I’m on, enjoying the comfort and safety of a high paying job. 
I’m not going to listen. I’m quitting and I’m leaving this place. I’ll see you in the mountains.



If this resonated with you - reach out to me on Twitter - @SeanBarryUK.Get notified when I post a new articleYour email address will never be shared or spammed. You'll receive a notification when I post a new article and that's it.SubscribePublished 16 January 2023PersonalBlogI'm a front office Software Engineer, building risk systems in London. I've been coding since 2013 and have worked in a number of startups/scaleups.Seán Barry on Twitter"
https://news.ycombinator.com/rss,Major standard library changes in Go 1.20,https://blog.carlmjohnson.net/post/2023/golang-120-arenas-errors-responsecontroller/,Comments,"What’s New in Go 1.20, Part II: Major Standard Library ChangesTuesday, January 17, 2023In the first part of What’s New in Go 1.20, we looked at language changes. For part two, I would like to introduce three changes to the standard library that address problems that the community has been thinking about and debating solutions to for years.First of all, a whole new package has been added. But you can’t import it by default, and you probably shouldn’t be using it at all. It’s the new experimental arena package.The arena package was proposed by Dan Scales and has been added to the Go standard library in 1.20. But if you just try to add import ""arena"" to a program, you get the following, somewhat cryptic error message:imports arena: build constraints exclude all Go files in GOROOT/src/arena
To opt into using arenas, you need to set GOEXPERIMENT=arenas when calling the go tool, like GOEXPERIMENT=arenas go build ..So what are arenas and why is the Go team trying so hard to keep you from using them? I asked ChatGPT, and this is what it said (this is the equivalent of quoting Webster’s Dictionary for the 21st century):Memory arenas are a memory management technique used in some programming languages and libraries to allocate and deallocate large blocks of memory efficiently. They are typically used in situations where the program needs to frequently allocate and deallocate a large number of small objects. By allocating and deallocating memory in large blocks, rather than individually for each object, memory arenas can reduce the overhead associated with memory management and improve performance.If you want to go more in-depth, Uptrace has a nice guide to the arena package (presumably written by a human, but who knows nowadays), but I’ll try to just give a basic overview here.As you probably know, Go is a garbage collected language. This means that when you refer to a variable, the compiler and the runtime automatically keep track of the uses of that variable to see when it comes into use and when it is no longer being used. Once a variable is no longer used, it is “garbage” waiting to be collected.For many kinds of applications, garbage comes in waves. For example, if you have a web server, it may allocate a lot of memory in order to build up a response to some user request, but once it responds, it no longer needs any of the memory that it allocated, so it can all be returned to the system at once. Another example is a game might want to free all of the objects created for a level once the level is over. The arena package lets Gophers opt into this approach to memory management in performance critical code. Instead of having the garbage collector start a root and then travel down to “mark and sweep” the live memory and return the dead objects, the whole arena can be marked as dead all at once. The release notes for Go 1.20 claim thatWhen used appropriately, [using package arena] has the potential to improve CPU performance by up to 15% in memory-allocation-heavy applications.This is highly efficient, but also highly dangerous. What if the programmer makes a mistake, and for example, adds some strings to a logger call that outlives the request? The log might be overwritten by a subsequent request and the string become replaced with junk data, leading to crashes or worse—security exploits.To mitigate the risk of these kinds of bugs, the arena package will deliberately cause a panic if can detect someone reusing memory after it has been freed. Dan Scales explains,Each arena A uses a distinct range in the 64-bit virtual address spaceA.Free unmaps the virtual address range for arena AThe physical pages for the arena can then be reused by the operating system for other arenas.If a pointer to an object in arena A still exists and is dereferenced, it will get a memory access fault, which will cause the Go program to terminate. Because the implementation knows the address ranges of arenas, it can give an arena-specific error message during the termination.There is a similar comment in the Go runtime package that implements memory arenas:// What makes the arenas here safe is that once they are freed, accessing the
// arena's memory will cause an explicit program fault, and the arena's address
// space will not be reused until no more pointers into it are found. There's one
// exception to this: if an arena allocated memory that isn't exhausted, it's placed
// back into a pool for reuse. This means that a crash is not always guaranteed.
So, it is still possible to write buggy code with arenas, but hopefully, the bugs will translate into simple crashes rather than full blown memory corruption or security exploits.The arena package has a fairly simple API. Here’s some example code from arenas_test.go:a := arena.NewArena()
defer a.Free()

tt := arena.New[T1](a)
tt.n = 1

ts := arena.MakeSlice[T1](a, 99, 100)
// …
There is also an arena.Clone function for when you want to move an object out of an arena and onto the regular Go memory heap.With luck, the arena experiment will succeed, and we will see it introduced as a regular package in a future version of Go.While most Go programmers probably will never need to use the arena package directly, I suspect virtually all Go programmers will have some occasion to use a different new feature in Go 1.20: multierrors.The concept of multierrors in Go is not new. Hashicorp’s go-multierror package goes all the way back to 2014 and there was at least one proposal to add multierrors to the standard library by 2017.Multierrors also exist in other languages. Python added exception groups to Python 3.11, for example. In the case of Python, while there was a popular third party MultiError class, it ultimately needed to be added to the language for full operability:Changes to the language are required in order to extend support for exception groups in the style of existing exception handling mechanisms. At the very least we would like to be able to catch an exception group only if it contains an exception of a type that we choose to handle. Exceptions of other types in the same group need to be automatically reraised, otherwise it is too easy for user code to inadvertently swallow exceptions that it is not handling.Unlike Python, in Go, errors are just values, so it was easy enough to create your own multierror type and expose it using errors.As. Indeed, I wrote my own multierror package that worked this same way. This was clearly an idea that was being created and recreated by the community, so did it really need to be solved at the level of the standard library?Suppose I have some code like this:a := errors.New(""a"")
b := errors.New(""b"")
c := join(a, b)
d := fmt.Errorf(""more context: %w"", c)
e := errors.New(""e"")
f := join(d, e)
If join flattens the error list, then d (“more context”) will be lost from the resulting multierror. Inside of f will be a, b, and e, but not d.afbeIf you are just using your own multierror package in your own application, this is basically a theoretical concern, because you can make sure not to add context around a multierror wherever it might be lost. But if multierrors are part of the standard library and can be expected to be used regularly, then losing d is a real problem that could pop up at unwanted times and places. This problem is what sunk one of the more recent multierror proposals.The reason that Damien Neil’s latest multierror proposal has succeeded where other multierror proposals did not is that it creates a tree of errors, rather than a slice. The accepted errors.Join code instead represents the error hierarchy like this:adcfbeThis means that all of the nodes in the tree are still available for inspection using errors.Is or errors.As.In Go 1.20, multierrors can be created either with errors.Join(errs ...error) error or by using multiple %w verbs with fmt.Errorf like fmt.Errorf(""%w: %w"", notFoundErr, dbErr). Once created, errors.Is and errors.As can extract any of the values in the tree, whether they are in a leaf like a, b and e or a branch like d. Users can also create their own multierror types by adding an Unwrap() []error method to their custom error type.One wrinkle in the implementation is that for now at least, there is no errors.Split(error) []error function. This is a deliberate omission, since that would flatten the tree. Instead, if you need to inspect every node in a tree (for example, for logging purposes), you are encouraged to traverse the tree yourself. I suspect that if we ever see a generic iterator type in Go, something that automatically walks the tree might be added then.Finally, lets look at http.ResponseController.Interfaces are one Go’s defining features as a language. With an interface, you can specify that your code can accept any value that has a certain method, no matter what its concrete type is.From the early days of Go, interfaces have also been used for what Chris Siebenmann has called “interface smuggling”:In interface smuggling, the actual implementation is augmented with additional well known APIs, such as io.ReaderFrom and io.WriterTo. Functions that want to work more efficiently when possible, such as io.Copy(), attempt to convert the io.Reader or io.Writer they obtained to the relevant API and then use it if the conversion succeeded:if wt, ok := src.(WriterTo); ok {
   return wt.WriteTo(dst)
}
if rt, ok := dst.(ReaderFrom); ok {
   return rt.ReadFrom(src)
}
[... do copy ourselves ...]
I call this interface smuggling because we are effectively smuggling a different, more powerful, and broader API through a limited one. In the case of types supporting io.WriterTo and io.ReaderFrom, io.Copy completely bypasses the nominal API; the .Read() and .Write() methods are never actually used, at least directly by io.Copy (they may be used by the specific implementations of .WriteTo() or .ReadFrom(), or more interface smuggling may take place).Russ Cox deemed this pattern the somewhat less pejorative sounding “extension interface pattern”. Whatever you call it, this pattern can be a great way to expose a simple API and still leave room for adding more complicated extensions later.Extension interfaces are useful, but not without their pitfalls. There can be be cases where it would be good to implement an extended interface, but an implementation author isn’t aware of it, so they fail to implement it. This can be addressed with documentation, but it’s not as clear as using the type system directly.What if you want to create a wrapper a simple interface that you do know might also implement an extended interface? How to do this depends on what exactly you’re wrapping and why. If you were creating a new version of io.LimitedReader, it wouldn’t make sense to add a WriteTo method. The whole point is to put a cap on how much can be read from the source reader, not bypass it and hook it up to the writer directly. As an implementation author, you need to think carefully about how extended interfaces interact with your type.Another problem is if the extended interface can only be tested for through a type assertion. In that case, as the author of a wrapper, you need to provide two wrapping types: one that has the extended interface and passes calls through to the underlying type, and one that doesn’t have the method, so it won’t spuriously trigger the type assertion. Worse still, if there are multiple extended interfaces you want to be able to wrap, you need to provide a 2N number of types to provide for every variation of extended interfaces coexisting! This may seem absurd, but this is exactly the situation that library authors who wanted to wrap http.ResponseWriter found themselves in.http.ResponseWriter is a fairly simple interface used for HTTP servers in the Go standard library:type ResponseWriter interface {
    Header() Header
    WriteHeader(statusCode int)
    io.Writer
}
You can set the headers on a response. You can set the status code on the response (which also causes the headers to be written on the wire). And you can write the body of a response. Simple!But of course, the Go http package also supports a number of extended interfaces for ResponseWriter. These are http.Flusher (which lets you flush an in progress write to its clients), http.Pusher (which lets you do HTTP/2 server push requests), http.Hijacker (which provides access to the underlying net.Conn), and io.ReaderFrom (which allows for nice things like automatic sendfile support). As a result, the go-chi project has six different types to implement its WrapResponseWriter interface type. (This is cut down from 24 to 23 on the theory that anything which implements HTTP/2 server push must be maximally fancy.)So then all the way back in 2016, Filippo Valsorda opened an issue about setting timeouts in an http.Handler. This was clearly a real need the Go community had, but it was hard to see how to make it work while retrofitting it into the existing http.ResponseWriter interface. It would be great to set server timeouts for a client based on what we know about that client, but how can this functionality be exposed? Do we just go straight from 3 to 5 optional methods defined in the http package?Despite a lot of careful thought about the problem, this was the situation until Go 1.20, when Damien Neil successfully landed the http.ResponseController proposal. As he wrote,A problem is that we have no good place at the moment to add functions that adjust these timeouts. We might add methods to the ResponseWriter implementation and access them via type assertions (as is done with the existing Flush and Hijack methods), but this proliferation of undiscoverable magic methods scales poorly and does not interact well with middleware which wraps the ResponseWriter type.The solution he came up with based on prior discussions was to add a new concrete type, http.ResponseController:// NewResponseController creates a ResponseController for a request.
//
// The ResponseWriter should be the original value passed to the Handler.ServeHTTP method,
// or have an Unwrap method returning the original ResponseWriter.
//
// If the ResponseWriter implements any of the following methods, the ResponseController
// will call them as appropriate:
//
//  Flush()
//  FlushError() error // alternative Flush returning an error
//  Hijack() (net.Conn, *bufio.ReadWriter, error)
//  SetReadDeadline(deadline time.Time) error
//  SetWriteDeadline(deadline time.Time) error
//
// If the ResponseWriter does not support a method, ResponseController returns
// an error matching ErrNotSupported.
func NewResponseController(rw ResponseWriter) *ResponseController
There are two aspects of http.ResponseController that fix the problems with the earlier extension interfaces. One, the addition of an Unwrap() http.ResponseWriter method allows middleware to easily wrap a ResponseWriter without needing to provide all the extended interface methods. Two, the addition of ErrNotSupported makes it easy for types that do have extended methods to signal to their callers when the types they wrap don’t have the same extended methods they do. These are best practices for extended interfaces that have emerged from experience using them. If you provide an extended interface, also provide an escape hatch for wrapper types that don’t know if their wrapped types will have the extended types or not.This is my blog, so I will immodestly mention that I had a proposal for adding an Unwrap method to ResponseWriter back in 2020, but my proposal didn’t have a concrete type to handle the unwrapping or ErrNotSupported, and I’m not sufficiently in the weeds of the http package to have been able to implement read/write deadlines if I had known to suggest it as a motivating problem. The 3 line long http.MaxBytesHandler is about the limit of my ability to contribute. 😆For all three of these changes written about above, the moral of the story is that when it comes to the Go standard library, it can take sometimes years for all the pieces of a good solution to come together in one place, but when they do, it can solve a longstanding problem in a way that makes things easier for everyone involved going forward. Even the authors of Go didn’t know what idiomatic Go code looked like when they wrote the standard library, but working together today we can evolve the standard library in a way that adds new capabilities while preserving backwards compatibility, so that everyone benefits.See more in this series…ChatGPTChris SiebenmannDamien NeilFilippo ValsordagolangprogrammingRuss CoxUptraceRelated ArticlesWhat’s New in Go 1.20, Part I: Language ChangesWhat’s new in Go 1.19?Three Minor Features in Go 1.18Adding Some Func to Go’s Flag PackageEven More Minor Features in Go 1.18
SubscribeComments"
https://news.ycombinator.com/rss,TI-Basic interpreter written in JavaScript,https://www.davidtorosyan.com/ti-js/,Comments,"


















TI-JS | TI-Basic interpreter written in JavaScript


















TI-JS

TI-Basic interpreter written in JavaScript

View on GitHub











What is this?
If you’re familiar with the TI-84 graphing calculator,
you might know that you can program it with code that looks like this:

Input

2->X
Disp X+3
Well, this project aims to implement that language in JavaScript
so that it can run in the browser.
Try changing the input above to see this output change:

Output



ERR: Looks like JavaScript is disabled. As this is a JS library, the demo won't work!


But why?
I wrote a lot of programs in TI-Basic a long time ago,
and I’d like to be able to preserve and look back on them.
You can see my collection at
ti84-entertainment;
I hope to show all of those off here some day.
And it works?
Kinda. Check out the tests to see what’s supported,
or experiment in the playground.
Can I use this?
Sure, check out the README on GitHub
for instructions.
Note that it’s still in prerelease and isn’t even versioned yet.
Also maybe reach out and tell me why you want to!
What’s next?
I’ve got some of TI-Basic implemented, but there’s more to be done.
As certain milestones are hit I’ll update this page.
Anything else?
See the rest of my projects at davidtorosyan.com




"
https://news.ycombinator.com/rss,Intel Core i9-13900T CPU benchmarks show faster than 12900K 125W performance,https://wccftech.com/intel-core-i9-13900t-cpu-benchmarks-show-faster-than-12900k-125w-performance-at-35w/,Comments,"

HardwareReport
Intel Core i9-13900T CPU Benchmarks Show Faster Than 12900K 125W Performance at 35W

Hassan Mujtaba •
Jan 14, 2023 02:44 PM EST

•
Copy Shortlink
























Intel recently introduced brand new 13th Gen T-series chips which feature the Core i9-13900T that operates at a 35W TDP. The new chip has been benchmarked within Geekbench 5 and showcases impressive performance given its limited power budget.
Intel's 13th Gen Core i9-13900T 35W CPU Beats The 125W Core i9-12900K In Geekbench 5 Benchmark
Starting with the specifications, the Intel Core i9-13900T is a variation of the Core i9-13900 series that comes with a limited TDP design. While the standard chips boast 125W TDP in the unlocked and 65W TDP on the Non-K SKUs, the T-series chip is limited to a 35W TDP.  The Unlocked CPU is rated at up to 253W, the Non-K is rated at up to 219W while the T-series chip is rated at up to 106 Watts which is less than half the power budget of its higher-end siblings.
Related StoryHassan MujtabaIntel Core i9-13900KS, World’s First 6 GHz CPU, Now Available For $699 USThe Intel Core i9-13900T retains the same core configuration with 24 cores that are made up of 8 P-Cores and 16 E-Cores with 32 threads, a base clock of 1.10 GHz, a boost of up to 5.30 GHz & 68 MB of cache (L2+L3). The CPU also comes at a slightly lower price point of $549.00 US. Now the CPU is tested within the Geekbench 5 benchmark using an ASUS TUF Gaming B660M-PLUS WIFI board and coupled with 64 GB of DDR5 memory.

The CPU scored 2178 points in the single-core and 17339 points in the multi-core tests. We used the Intel Core i9-12900K for comparison which scores 1901 points in single-core and 17272 points in multi-core tests. This puts the Intel Core i9-13900T up to 15% faster in single-core and slightly faster in multi-threaded tests which is very impressive considering the Core i9-12900K also has a higher 125W base TDP (3.58x higher) and a peak TDP rating of 241W (2.27x higher).

Intel Core i9-13900KS Single-Thread CPU Benchmark (Geekbench 5)

Single-Core


050010001500200025003000




050010001500200025003000





Core i9-13900KS

2.3k


Core i9-13900K

2.2k


Ryzen 9 7900X

2.2k


Ryzen 9 7950X

2.2k


Ryzen 7 7700X

2.2k


Core i9-13900T

2.2k


Ryzen 5 7600X

2.2k


Ryzen 9 7900

2.1k


Core i9-13900

2.1k


Ryzen 7 7700

2.1k


Core i9-12900KS

2.1k


Core i9-13900HX

2k


Ryzen 5 7600

2k


Core i7-13700K

2k


Core i5-13600K

1.9k


Core i9-12900K

1.9k


Core i7-12700K

1.9k


M2 Max

1.9k


M1 Max

1.8k


Core i5-12600K

1.7k


Ryzen 9 5950X

1.7k


Ryzen 7 5800X

1.7k


Ryzen 9 5900X

1.7k


Ryzen 5 5600X

1.6k







Intel Core i9-13900KS Multi-Thread CPU Benchmark (Geekbench 5)

Multi-Core


050001000015000200002500030000




050001000015000200002500030000





Core i9-13900KS

26.8k


Core i9-13900K

24.3k


Ryzen 9 7950X

24.4k


Core i9-13900HX

20.9k


Core i9-13900

20.1k


Core i7-13700K

19.8k


Ryzen 9 7900X

19.3k


Core i9-12900KS

19k


Ryzen 9 7900

18.6k


Core i9-13900T

17.3k


Core i9-12900K

17.3k


Ryzen 9 5950X

16.5k


Core i5-13600K

16.1k


M2 Max

14.6k


Core i7-12700K

14.1k


Ryzen 7 7700X

14.1k


Ryzen 9 5900X

14k


Ryzen 7 7700

12.7k


M1 Max

12.3k


Core i5-12600K

11.6k


Ryzen 5 7600X

11.4k


Ryzen 5 7600

11.3k


Ryzen 7 5800X

10.3k


Ryzen 5 5600X

8.2k






This goes off to show the immense efficiency that Intel's 10nm ESF process node and the new hybrid architecture packs and we will also get to see some similar results with the mobility lineup, especially the 13th Gen HX parts which are going to ship in enthusiast-grade gaming laptops in the coming months. AMD also introduced its brand new 65W Ryzen 7000 Non-X CPUs which have been showcasing some impressive efficiency feats on their own with the Zen 4 core architecture.
News Source: Benchleaks
				
				Share this story
				 Facebook
 Twitter






Deal of the Day











Further Reading




 AMD Ryzen 9 7950X3D CPU Shown To Beat Intel Core i9-13900K In Games With Up To 24% Lead


 Intel Core i9-13980HX CPU Powered MSI Raider GE78HX Laptop Matches High-End Desktop CPUs In Performance


 Intel Core i9-13980HX Flagship Raptor Lake-HX CPU Spotted In ASUS’s Next-Gen ROG STRIX Laptop


 It’s Over 9000! Intel Core i9-13900KS Becomes The First CPU To Achieve 9 GHz Frequency World Record













Comments




Please enable JavaScript to view the comments.










Trending Stories


NASA Captures Star Eaten By Black Hole 300 Million Light Years Away



				86 Active Readers



SpaceX’s Rockets Split Up In Mid Air For Rare & Stunning Views At 5,000 Km/h+



				78 Active Readers



Apple To Bring microLED Technology To All Products Eventually, Starting With Apple Watch Ultra; Complete Transition May Take 10 Years



				54 Active Readers



PlayStation 5 Vertical Orientation Issue Clarified by Technician; Issue Happens on “Unopened” Consoles



				41 Active Readers



Intel Core i9-13900T CPU Benchmarks Show Faster Than 12900K 125W Performance at 35W



				30 Active Readers








Popular Discussions


AMD Radeon RX 7900 XTX Failure Rates Reportedly At 11%, RMA’s Piling Up But Users Not Receiving Cards



				3100 Comments



AMD Radeon RX 6000 GPUs Mysteriously Start Dying, German Repair Shop Receives 48 Cards With Cracked Chips



				3020 Comments



Intel Lunar Lake To Feature A Brand New CPU Architecture Built From The Ground-Up, Perf/Watt Focused at Mobile



				2757 Comments



AMD To Give The Love of 3D V-Cache This Valentines With Its Ryzen 7000 X3D CPUs Launch



				2021 Comments



Intel Arc A770 Performs Above AMD & NVIDIA In DirectStorage 1.1 Performance Benchmark



				1802 Comments








	 







"
https://news.ycombinator.com/rss,Show HN: Sketch – AI code-writing assistant that understands data content,https://github.com/approximatelabs/sketch,Comments,"








approximatelabs

/

sketch

Public




 

Notifications



 

Fork
    9




 


          Star
 346
  









        AI code-writing assistant that understands data content
      





346
          stars
 



9
          forks
 



 


          Star

  





 

Notifications












Code







Issues
0






Pull requests
0






Actions







Projects
0






Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Actions
 


                  Projects
 


                  Security
 


                  Insights
 







approximatelabs/sketch









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











main





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





6
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




bluecoconut

update readme wording




        …
      




        9d567ec
      

Jan 16, 2023





update readme wording


9d567ec



Git stats







133

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github/workflows



remove python 3.7, add tests, remove uneeded code



Dec 15, 2022









sketch



some bug fix and copy addition



Jan 16, 2023









tests



remove python 3.7, add tests, remove uneeded code



Dec 15, 2022









.gitignore



update with edit and work on text2sql



Oct 13, 2022









README.md



update readme wording



Jan 16, 2023









dev-requirements.txt



starting to rebuild sketch



Dec 15, 2022









pyproject.toml



rename to pandas extension, add missing requirement, use base64 encoding



Jan 11, 2023









setup.py



starting to rebuild sketch



Dec 15, 2022




    View code
 


















sketch
Demo
How to use
.sketch.ask
.sketch.howto
.sketch.apply
Sketch currently uses prompts.approx.dev to help run with minimal setup
How it works





README.md




sketch
Sketch is an AI code-writing assistant for pandas users that understands the context of your data, greatly improving the relevance of suggestions. Sketch is usable in seconds and doesn't require adding a plugin to your IDE.
pip install sketch
Demo
Here we follow a ""standard"" (hypothetical) data-analysis workflow, showing a Natural Language interace that successfully navigates many tasks in the data stack landscape.

Data Catalogging:

General tagging (eg. PII identification)
Metadata generation (names and descriptions)


Data Engineering:

Data cleaning and masking (compliance)
Derived feature creation and extraction


Data Analysis:

Data questions
Data visualization








sketch-demo.mp4





Try it out in colab: 
How to use
It's as simple as importing sketch, and then using the .sketch extension on any pandas dataframe.
import sketch
Now, any pandas dataframe you have will have an extension registered to it. Access this new extension with your dataframes name .sketch
.sketch.ask
Ask is a basic question-answer system on sketch, this will return an answer in text that is based off of the summary statistics and description of the data.
Use ask to get an understanding of the data, get better column names, ask hypotheticals (how would I go about doing X with this data), and more.
df.sketch.ask(""Which columns are integer type?"")
.sketch.howto
Howto is the basic ""code-writing"" prompt in sketch. This will return a code-block you should be able to copy paste and use as a starting point (or possibly ending!) for any question you have to ask of the data. Ask this how to clean the data, normalize, create new features, plot, and even build models!
df.sketch.howto(""Plot the sales versus time"")
.sketch.apply
apply is a more advanced prompt that is more useful for data generation. Use it to parse fields, generate new features, and more. This is built directly on lambdaprompt. In order to use this, you will need to set up a free account with OpenAI, and set an environment variable with your API key. OPENAI_API_KEY=YOUR_API_KEY
df['review_keywords'] = df.sketch.apply(""Keywords for the review [{{ review_text }}] of product [{{ product_name }}] (comma separated):"")
df['capitol'] = pd.DataFrame({'State': ['Colorado', 'Kansas', 'California', 'New York']}).sketch.apply(""What is the capitol of [{{ State }}]?"")
Sketch currently uses prompts.approx.dev to help run with minimal setup
In the future, we plan to update the prompts at this endpoint with our own custom foundation model, built to answer questions more accurately than GPT-3 can with its minimal data context.
You can also directly call OpenAI directly (and not use our endpoint) by using your own API key. To do this, set 2 environment variables.
(1) SKETCH_USE_REMOTE_LAMBDAPROMPT=False
(2) OPENAI_API_KEY=YOUR_API_KEY
How it works
Sketch uses efficient approximation algorithms (data sketches) to quickly summarize your data, and feed that information into language models. Right now it does this by summarizing the columns and writing these summary statistics as additional context to be used by the code-writing prompt. In the future we hope to feed these sketches directly into custom made ""data + language"" foundation models to get more accurate results.









About

      AI code-writing assistant that understands data content
    
Topics



  python


  data-science


  data


  ai


  tabular-data


  pandas


  df


  sketches


  dataframe


  copilot


  codex


  ds


  datasketches


  gpt3


  lambdaprompt


  datasketch



Resources





      Readme
 


Stars





346
    stars

Watchers





4
    watching

Forks





9
    forks







    Releases





6
tags







    Packages 0


        No packages published 







        Used by 22
 




























            + 14
          







    Contributors 2








bluecoconut
Justin Waugh

 






jmbiven
Mike Biven

 





Languages










Python
100.0%











"
https://news.ycombinator.com/rss,The hardest part of being a junior developer,https://rachsmith.com/the-hardest-part-of-being-a-jnr/,Comments,"



The hardest part of being a Junior Developer


Added:
        January 16, 2023
      

Tags:

  work


  development





Even though it was 12 years ago, I still remember what the most stressful part of being a junior developer was. I was reminded of it when I saw this meme in my Twitter timeline yesterday:

In my first developer job, I had no previous experience - just some basic front-end coding and design classes at Uni. I had been hired by an advertising agency to animate flash banners and code marketing emails. The work atmosphere was fast-paced but I enjoyed the challenge. There was a lot to learn but I was loving everything I was learning. A lot of the people who worked there - ‘ad people’ were intense personalities but I grew used to working with them.
I loved that job, and couldn’t believe I was being paid more to do it than in any hospitality role I had worked prior. There was just one part that stressed me out: knowing how long I should work on figuring out something by myself vs. asking for help. I had no context for how long anything should take. I didn’t want to be judged harshly for asking too many dumb questions, but I also didn’t want to appear to be slow. I had the added pressure of feeling like I was representing my entire gender with my performance.
My manager at the time must have noticed my anxiety around this, and he did a wonderful thing. He made the timing aspect of it super explicit for me. He would do things like give me my next task and then say “try and get this done, if at any point you have spent an hour without making any progress, come and ask for help”. Then I knew how long was too long to spin my wheels on something on my own. Most of the time, I could keep figuring things out on my own, but when I couldn’t I could approach my manager without worrying whether I was bothering him “too soon”.
By the time I moved on from that job, with the help of that manager, I had enough experience to be able to judge when it is time to ask for help on something and didn’t have to worry about it again.
As I’ve always worked on small teams, I haven’t had the opportunity to manage a green developer that needs a lot of direction. But I thought I would just share this in case it is useful to anyone who does. Try and be very specific when you share your expectations around timing, it might help them out. It certainly helped me.



Thanks for reading! If you'd like to share your thoughts you can leave a comment, send me an email, Tweet at me, or add an issue on GitHub.



  Comments



Please enable Javascript to view comments.

          The Comments system is powered by a third party service - Talkyard. Sometimes they don't load 😞. If you're having trouble leaving a comment you can send me an email.
        



"
https://news.ycombinator.com/rss,Show HN: Cross-Platform GitHub Action,https://github.com/marketplace/actions/cross-platform-action,Comments,"





Marketplace
Actions
Cross Platform Action






play-circle





GitHub Action
Cross Platform Action




v0.9.0
Latest version







    Use latest version
 









play-circle






Cross Platform Action
Provides cross platform runner


Installation
Copy and paste the following snippet into your .yml file.












- name: Cross Platform Action
  uses: cross-platform-actions/action@v0.9.0



          Learn more about this action in cross-platform-actions/action












Choose a version







v0.9.0

                Cross Platform Action 0.9.0
              

 




v0.8.0

                Cross Platform Action 0.8.0
              

 




v0.7.0

                Cross Platform Action 0.7.0
              

 




v0.6.2

                Cross Platform Action 0.6.2
              

 




v0.6.1

                Cross Platform Action 0.6.1
              

 




v0.6.0

                Cross Platform Action 0.6.0
              

 




v0.5.0

                Cross Platform Action 0.5.0
              

 




v0.4.0

                Cross Platform Action 0.4.0
              

 




v0.3.1

                Cross Platform Action 0.3.1
              

 




v0.3.0

                Cross Platform Action 0.3.0
              

 








Cross-Platform GitHub Action
This project provides a GitHub action for running GitHub Action workflows on
multiple platforms. This includes platforms that GitHub Actions doesn't
currently natively support.
Features
Some of the features that are supported include:

Multiple operating system with one single action
Multiple versions of each operating system
Allows to use default shell or Bash shell
Low boot overhead
Fast execution

Usage
Here's a sample workflow file which will setup a matrix resulting in four jobs.
One which will run on FreeBSD 13.1, one which runs OpenBSD 7.2, one which runs
NetBSD 9.2 and one which runs OpenBSD 7.2 on ARM64.
name: CI

on: [push]

jobs:
  test:
    runs-on: ${{ matrix.os.host }}
    strategy:
      matrix:
        os:
          - name: freebsd
            architecture: x86-64
            version: '13.1'
            host: macos-12

          - name: openbsd
            architecture: x86-64
            version: '7.2'
            host: macos-12

          - name: openbsd
            architecture: arm64
            version: '7.2'
            host: ubuntu-latest

          - name: netbsd
            architecture: x86-64
            version: '9.2'
            host: ubuntu-latest

    steps:
      - uses: actions/checkout@v2

      - name: Test on ${{ matrix.os.name }}
        uses: cross-platform-actions/action@v0.9.0
        env:
          MY_ENV1: MY_ENV1
          MY_ENV2: MY_ENV2
        with:
          environment_variables: MY_ENV1 MY_ENV2
          operating_system: ${{ matrix.os.name }}
          architecture: ${{ matrix.os.architecture }}
          version: ${{ matrix.os.version }}
          shell: bash
          run: |
            uname -a
            echo $SHELL
            pwd
            ls -lah
            whoami
            env | sort
Different platforms need to run on different runners, see the
Runners section below.
Inputs
This section lists the available inputs for the action.



Input
Required
Default Value
Description




run
✓
✗
Runs command-line programs using the operating system's shell. This will be executed inside the virtual machine.


operating_system
✓
✗
The type of operating system to run the job on. See Supported Platforms.


version
✓
✗
The version of the operating system to use. See Supported Platforms.


shell
✗
default
The shell to use to execute the commands. Defaults to the default shell for the given operating system. Allowed values are: default, sh and bash


environment_variables
✗
""""
A list of environment variables to forward to the virtual machine. The list should be separated with spaces.



All inputs are expected to be strings. It's important that especially the
version is explicitly specified as a string, using single or double quotes.
Otherwise YAML might interpet the value as a numeric value instead of a string.
This might lead to some unexpected behavior. If the version is specified as
version: 13.0, YAML will interpet 13.0 as a floating point number, drop the
fraction part (because 13 and 13.0 are the same) and the GitHub action will
only see 13 instead of 13.0. The solution is to explicitly state that a
string is required by using quotes: version: '13.0'.
Supported Platforms
This sections lists the currently supported platforms by operating system. Each
operating system will list which versions are supported.
OpenBSD (openbsd)



Version
x86-64
arm64




7.2
✓
✓


7.1
✓
✓


6.9
✓
✓


6.8
✓
✗



FreeBSD (freebsd)



Version
x86-64




13.1
✓


13.0
✓


12.4
✓


12.2
✓



NetBSD (netbsd)



Version
x86-64




9.2
✓



Runners
This section list the different combinations of platforms and on which runners
they can run.



Runner
OpenBSD
FreeBSD
NetBSD




Linux
✓
✓
✓


macos-10.15, macos-11, macos-12
✓
✓
✗



Under the Hood
GitHub Actions currently only support the following platforms: macOS, Linux and
Windows. To be able to run other platforms, this GitHub action runs the
commands inside a virtual machine (VM). If the host platform is macOS the
hypervisor can take advantage of nested virtualization.
The FreeBSD and OpenBSD VMs run on the xhyve hypervisor (on a macOS
host), while the other platforms run on the QEMU hypervisor (on a Linux
host). xhyve is built on top of Apple's Hypervisor
framework. The Hypervisor framework allows to implement hypervisors with
support for hardware acceleration without the need for kernel extensions. xhyve
is a lightweight hypervisor that boots the guest operating systems quickly and
requires no dependencies outside of what's provided by the system. QEMU is a
more general purpose hypervisor that runs on most host platforms and supports
most guest systems. It's a bit slower than xhyve because it's general purpose
and it cannot use nested virtualization on the Linux hosts provided by GitHub.
The VM images running inside the hypervisor are built using Packer.
It's a tool for automatically creating VM images, installing the guest
operating system and doing any final provisioning.
The GitHub action uses SSH to communicate and execute commands inside the VM.
It uses rsync to share files between the guest VM and the host. xhyve
does not have any native support for sharing files. To authenticate the SSH
connection a unique key pair is used. This pair is generated each time the
action is run. The public key is added to the VM image and the private key is
stored on the host. Since xhyve does not support file sharing, a secondar hard
drive, which is backed by a file, is created. The public key is stored on this
hard drive, which is then mounted by the VM. At boot time, the secondary hard
drive will be identified and the public key will be copied to the appropriate
location.
To reduce the time it takes for the GitHub action to start executing the
commands specified by the user, it aims to boot the guest operating systems as
fast as possible. This is achieved in a couple of ways:


By downloading resources, like the hypervisor and a few other
tools, instead of installing them through a package manager


No compression is used for the resources that are downloaded. The size is
small enough anyway and it's faster to download the uncompressed data than
it is to download compressed data and then uncompress it.


It leverages async/await to perform tasks asynchronously. Like
downloading the VM image and other resources at the same time


It performs as much as possible of the setup ahead of time when the VM image
is provisioned


Local Development
Prerequisites

NodeJS
npm
git

Instructions


Install the above prerequisites


Clone the repository by running:
git clone https://github.com/cross-platform-actions/action



Navigate to the newly cloned repository: cd action


Install the dependencies by running: npm install


Run any of the below npm commands


npm Commands
The following npm commands are available:

build - Build the GitHub action
format - Reformat the code
lint - Lint the code
package - Package the GitHub action for distribution and end to end testing
test - Run unit tests
all - Will run all of the above commands

Running End to End Tests
The end to end tests can be run locally by running it through Act. By
default, resources and VM images will be downloaded from github.com. By running
a local HTTP server it's possible to point the GitHub action to local resources.
Prerequisites

Docker
Act

Instructions


Install the above prerequisites


Copy test/workflows/ci.yml.example to
test/workflows/ci.yml


Make any changes you like to test/workflows/ci.yml, this is file ignored by
Git


Build the GitHub action by running: npm run build


Package the GitHub action by running: npm run package


Run the GitHub action by running: act --privileged -W test/workflows


Providing Resources Locally
The GitHub action includes a development dependency on a HTTP server. The
test/http directory contains a skeleton of a directory structure
which matches the URLs that the GitHub action uses to download resources. All
files within the test/http are ignore by Git.


Add resources as necessary to the test/http directory


In one shell, run the following command to start the HTTP server:
./node_modules/http-server/bin/http-server test/http -a 127.0.0.1

The -a flag configures the HTTP server to only listen for incoming
connections from localhost, no external computers will be able to connect.


In another shell, run the GitHub action by running:
act --privileged -W test/workflows --env CPA_RESOURCE_URL=<url>

Where <url> is the URL inside Docker that points to localhost of the host
machine, for macOS, this is http://host.docker.internal:8080. By default,
the HTTP server is listening on port 8080.






Stars

 


          Star
 40
  





Contributors



 

 


Categories


  Continuous integration


  Testing




Links



cross-platform-actions/action
    



Open issues
        1




Pull requests
      1




Report abuse
 

Cross Platform Action is not certified by GitHub. It is provided by a third-party and is governed by separate terms of service, privacy policy, and support documentation.
    




"
https://news.ycombinator.com/rss,Will Floating Point 8 Solve AI/ML Overhead?,https://semiengineering.com/will-floating-point-8-solve-ai-ml-overhead/,Comments,"












Will Floating Point 8 Solve AI/ML Overhead?

























































































 
 
 












 










 


Search for:



 Subscribe

中文 English 


















Home
Systems & Design
Low Power - High Performance
Manufacturing, Packaging & Materials
Test, Measurement & Analytics
Auto, Security & Pervasive Computing




Special Reports

Business & Startups
Jobs
Knowledge Center
Technical Papers 

Home';
				AI/ML/DLArchitecturesAutomotiveCommunication/Data MovementDesign & VerificationLithographyManufacturingMaterialsMemoryOptoelectronics / PhotonicsPackagingPower & PerformanceQuantumSecurityTest & AnalyticsTransistorsZ-End Applications


Events & Webinars 

Events
Webinars



Videos & Research

Videos
Industry Research



Newsletters





MENU 

Home
Special Reports
Systems & Design
Low Power-High Performance
Manufacturing, Packaging & Materials
Test, Measurement & Analytics
Auto, Security & Pervasive Computing
Knowledge Center
Videos
Startup Corner
Business & Startups
Jobs
Technical Papers 
Events
Webinars
Industry Research
Special Reports







































Home >
                                                                    Low Power-High Performance >
                                                                Will Floating Point 8 Solve AI/ML Overhead?                                                    

















Low Power-High Performance

Will Floating Point 8 Solve AI/ML Overhead?









Less precision equals lower power, but standards are required to make this work.





								January 12th, 2023 - 

								By: Karen Heyman








While the media buzzes about the Turing Test-busting results of ChatGPT, engineers are focused on the hardware challenges of running large language models and other deep learning networks. High on the ML punch list is how to run models more efficiently using less power, especially in critical applications like self-driving vehicles where latency becomes a matter of life or death.
AI already has led to a rethinking of computer architectures, in which the conventional von Neumann structure is replaced by near-compute and at-memory floorplans. But novel layouts aren’t enough to achieve the power reductions and speed increases required for deep learning networks. The industry also is updating the standards for floating-point (FP) arithmetic.
“There is a great deal of research and study on new data types in AI, as it is an area of rapid innovation,” said David Bell, product marketing director, Tensilica IP at Cadence. “Eight-bit floating-point (FP8) data types are being explored as a means to minimize hardware — both compute resources and memory — while preserving accuracy for network models as their complexities grow.”
As part of that effort, researchers at Arm, Intel, and Nvidia published a white paper proposing “FP8 Formats for Deep Learning.” [1]
“Bit precision has been a very active topic of debate in machine learning for several years,” said Steve Roddy, chief marketing officer at Quadric. “Six or eight years ago when models began to explode in size (parameter count), the sheer volume of shuffling weight data into and out of training compute (either CPU or GPU) became the performance limiting bottleneck in large training runs. Faced with a choice of ever more expensive memory interfaces, such as HBM, or cutting bit precision in training, a number of companies experimented successfully with lower-precision floats. Now that networks have continued to grow exponentially in size, the exploration of FP8 is the next logical step in reducing training bandwidth demands.”
How we got here
Floating-point arithmetic is a kind of scientific notation, which condenses the number of digits needed to represent a number. This trick is pulled off by an arithmetic expression first codified by IEEE working group 754 in 1986, when floating-point operations generally were performed on a co-processor.
IEEE 754 describes how the radix point (more commonly known in English as the “decimal” point) doesn’t have a fixed position, but rather “floats” where needed in the expression. It allows numbers with extremely long streams of digits (whether originally to the left or right of a fixed point) to fit into the limited bit-space of computers. It works in either base 10 or base 2, and it’s essential for computing, given that binary numbers extend to many more digits than decimal numbers (100 = 1100100).
 

Fig. 1: 12.345 as a base-10 floating-point number. Source: Wikipedia
 
While this is both an elegant solution and the bane of computer science students worldwide, its terms are key to understanding how precision is achieved in AI. The statement has three parts:

A sign bit, which determines whether the number is positive (0) or negative (1);
An exponent, which determines the position of the radix point, and
A mantissa, or significand, which represents the most significant digits of the number.


Fig. 2: IEEE 754 floating-point scheme. Source: WikiHow
As shown in figure 2, while the exponent gains 3 bits in a 64-bit representation, the mantissa jumps from 32 bits to 52 bits. Its length is key to precision.
IEEE 754, which defines FP32 bit and FP64, was designed for scientific computing, in which precision was the ultimate consideration. Currently, IEEE working group P3109 is developing a new standard for machine learning, aligned with the current (2019) version of 754. P3109 aims to create a floating-point 8 standard.
Precision tradeoffs
Machine learning often needs less precision than a 32-bit scheme. The white paper proposes two different flavors of FP8: E4M3 (4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa).
“Neural networks are a bit strange in that they are actually remarkably tolerant to relatively low precision,” said Richard Grisenthwaite, executive vice president and chief architect at Arm. “In our paper, we showed you don’t need 32 bits of mantissa for precision. You can use only two or three bits, and four or five bits of exponent will give you sufficient dynamic range. You really don’t need the massive precision that was defined in 754, which was designed for finite element analysis and other highly precise arithmetic tasks.”
Consider a real-world example: A weather forecast needs the extreme ranges of 754, but a self-driving car doesn’t need the fine-grained recognition of image search. The salient point is not whether it’s a boy or girl in the middle of the road. It’s just that the vehicle must immediately stop, with no time to waste on calculating additional details. So it’s fine to use a floating point with a smaller exponent and much smaller mantissa, especially for edge devices, which need to optimize energy usage.
“Energy is a fundamental quantity and no one’s going to make it go away as an issue,” said Martin Snelgrove, CTO of Untether AI. “And it’s also not a narrow one. Worrying about energy means you can’t afford to be sloppy in your software or your arithmetic. If doing a 32-bit floating point makes everything easier, but massively more power consuming, you just can’t do it. Throwing an extra 1,000 layers at something makes it slightly more accurate, but the value for power isn’t there. There’s an overall discipline about energy — the physics says you’re going to pay attention to this, whether you like it or not.”
In fact, to save energy and performance overhead, many deep learning networks had already shifted to an IEEE-approved 16-bit floating point and other formats, including mantissa-less integers. [2]
“Because compute energy and storage is at a premium in devices, nearly all high-performance device/edge deployments of ML always have been in INT8,” Quadric’s Roddy said. “Nearly all NPUs and accelerators are INT-8 optimized. An FP32 multiply-accumulate calculation takes nearly 10X the energy of an INT8 MAC, so the rationale is obvious.”
Why FP8 is necessary
The problem starts with the basic design of a deep learning network. In the early days of AI, there were simple, one-layer models that only operated in a feedforward manner. In 1986, David Rumelart, Geoffrey Hinton, and Ronald Williams published a breakthrough paper on back-propagation [3] that kicked off the modern era of AI. As their abstract describes, “The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units, which are not part of the input or output, come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units.”
In other words, they created a system in which better results could be achieved by adding more and more layers into a model, which would be improved by incorporating “learned” adjustments. Decades later, their ideas so vastly improved machine translation and transcription that college professors remain unsure whether undergraduates’ essays have been written by bots.
But additional layers require additional processing power. “Larger networks with more and more layers were found to be progressively more successful at neural networks tasks, but in certain applications this success came with an ultimately unmanageable increase in memory footprint, power consumption, and compute resources. It became imperative to reduce the size of the data elements (activations, weights, gradients) from 32 bits, and so the industry started using 16-bit formats, such as Bfloat16 and IEEE FP16,” according to the paper jointly written by Arm/Intel/Nvidia.
“The tradeoff fundamentally is with an 8-bit floating-point number compared to a 32-bit one,” said Grisenthwaite. “I can have four times the number of weights and activations in the same amount of memory, and I can get far more computational throughput as well. All of that means I can get much higher performance. I can make the models more involved. I can have more weights and activations at each of the layers. And that’s proved to be more useful than each of the individual points being hyper-accurate.”
Behind these issues are the two basic functions in machine learning, training and inference. Training is the first step in which, for example, the AI learns to classify features in an image by reviewing a dataset. With inference, the AI is given novel images outside of the training set and asked to classify them. If all goes as it should, the AI should distinguish that tails and wings are not human features, and at finer levels, that airplanes do not have feathers and a tube with a tail and wings is not a bird.
“If you’re doing training or inference, the math is identical,” said Ron Lowman, strategic marketing manager for IoT at Synopsys. “The difference is you do training over a known data set thousands of times, maybe even millions of times, to train what the results will be. Once that’s done, then you take an unknown picture and it will tell you what it should be. From a math perspective, a hardware perspective, that’s the big difference. So when you do training, you want to do that in parallel, rather than doing it in a single hardware implementation, because the time it takes to do training is very costly. It could take weeks or months, or even years in some cases, and that just costs too much.”
In industry, training and inference have become separate specialties, each with its own dedicated teams.
“Most companies that are deploying AI have a team of data scientists that create neural network architectures and train the networks using their datasets,” said Bob Beachler, vice president of product at Untether AI. “Most of the autonomous vehicle companies have their own data sets, and they use that as a differentiating factor. They train using their data sets on these novel network architectures that they come up with, which they feel gives them better accuracy. Then that gets taken to a different team, which does the actual implementation in the car. That is the inference portion of it.”
Training requires a wide dynamic range for the continual adjustment of coefficients that is the hallmark of backpropagation. The inference phase is computing on the inputs, rather than learning, so it needs much less dynamic range. “Once you’ve trained the network, you’re not tweaking the coefficients, and the dynamic range required is dramatically reduced,” explained Beachler.
For inference, continuing operations in FP32 or FP16 is just unnecessary overhead, so there’s a quantization step to shift the network down to FP8 or Integer 8 (Int8), which has become something of a de facto standard for inference, driven largely by TensorFlow.
“The idea of quantization is you’re taking all the floating point 32 bits of your model and you’re essentially cramming it into an eight-bit format,” said Gordon Cooper, product manager for Synopsys’ Vision and AI Processor IP. “We’ve done accuracy tests and for almost every neural network-based object detection. We can go from 32-bit floating point to Integer 8 with less than 1% accuracy loss.”
For quality/assurance, there’s often post-quantization retraining to see how converting the floating-point value has affected the network, which could iterate through several passes.
This is why training and inference can be performed using different hardware. “For example, a common pattern we’ve seen is accelerators using NVIDIA GPUs, which then end up running the inference on general purpose CPUs,” said Grisenthwaite.
The other approach is chips purpose-built for inference.
“We’re an inference accelerator. We don’t do training at all,” says Untether AI’s Beachler. “We place the entire neural network on our chip, every layer and every node, feed data at high bandwidth into our chip, resulting in each and every layer of the network computed inside our chip. It’s massively parallelized multiprocessing. Our chip has 511 processors, each of them with single instruction multiple data (SIMD) processing. The processing elements are essentially multiply/accumulate functions, directly attached to memory. We call this the Energy Centric AI computing architecture. This Energy Centric AI Computing architecture results in a very short distance for the coefficients of a matrix vector to travel, and the activations come in through each processing element in a row-based approach. So the activation comes in, we load the coefficients, do the matrix mathematics, do the multiply/accumulate, store the value, move the activation to the next row, and move on. Short distances of data movement equates to low power consumption.”
In broad outline, AI development started with CPUs, often with FP co-processors, then moved to GPUs, and now is splitting into a two-step process of GPUs (although some still use CPUs) for training and CPUs or dedicated chips for inference.
The creators of general-purpose CPU architectures and dedicated inference solutions may disagree on which approach will dominate. But they all agree that the key to a successful handoff between training and inference is a floating-point standard that minimizes the performance overhead and risk of errors during quantization and transferring operations between chips. Several companies, including NVIDIA, Intel, and Untether, have brought out FP8-based chips.
“It’s an interesting paper,” said Cooper. “8-bit floating point, or FP8, is more important on the training side. But the benefits they’re talking about with FP8 on the inference side is that you possibly can skip the quantization. And you get to match the format of what you’ve done between training and inference.”
Nevertheless, as always, there are still many challenges still to consider.
“The cost is one of model conversion — FP32 trained model converted to INT8. And that conversion cost is significant and labor intensive,” said Roddy. “But if FP8 becomes real, and if the popular training tools begin to develop ML models with FP8 as the native format, it could be a huge boon to embedded inference deployments. Eight-bit weights take the same storage space, whether they are INT8 or FP8. The energy cost of moving 8 bits (DDR to NPU, etc.) is the same, regardless of format. And a Float8 multiply-accumulate is not significantly more power consumptive than an INT8 MAC. FP8 would rapidly be adopted across the silicon landscape.  But the key is not whether processor licensors would rapidly adopt FP8. It’s whether the mathematicians building training tools can and will make the switch.”
Conclusion
As the quest for lower power continues, there’s debate about whether there might even be a FP4 standard, in which only 4 bits carry a sign, an exponent, and mantissa. People who follow a strict neuromorphic interpretation have even discussed binary neural networks, in which the input functions like an axon spike, just 0 or 1.
“Our sparsity level is going to go up,” said Untether’s Snelgrove. “There are hundreds of papers a day on new neural net techniques. Any one of them could completely revolutionize the field. If you talk to me in a year, all of these words could mean different things.”
At least at the moment, it’s hard to imagine that lower FPs or integer schemes could contain enough information for practical purposes. Right now, various flavors of FP8 are undergoing the slow grind towards standardization. For example, Graphcore, AMD, and Qualcomm have also brought a detailed FP8 proposal to the IEEE. [4]
“The advent of 8-bit floating point offers tremendous performance and efficiency benefits for AI compute,” said Simon Knowles, CTO and co-founder of Graphcore. “It is also an opportunity for the industry to settle on a single, open standard, rather than ushering in a confusing mix of competing formats.”
Indeed, everyone is optimistic there will be a standard — eventually. “We’re involved in IEEE P3109, as are many, many companies in this industry,” said Arm’s Grisenthwaite. “The committee has looked at all sorts of different formats. There are some really interesting ones out there. Some of them will stand the test of time, and some of them will fall by the wayside. We all want to make sure we’ve got complete compatibility and don’t just say, ‘Well, we’ve got six different competing formats and it’s all a mess, but we’ll call it a standard.”
References 

Micikevicius, P., et al. FP8 Formats for Deep Learning. Last revised Sep 29 2022 arXiv:2209.05433v2. https://doi.org/10.48550/arXiv.2209.05433
Sapunov, G. FP64, FP32, FP16, BFLOAT16, TF32, and other members of the ZOO. Medium. May 16, 2020. https://moocaholic.medium.com/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407
Rumelhart, D., Hinton, G. & Williams, R. Learning representations by back-propagating errors. Nature 323, 533–536 (1986). https://doi.org/10.1038/323533a0
Noune, B. 8-bit Numerical Formats for Deep Neural Networks. Submitted June 6 2022 arXiv:2206.02915 https://doi.org/10.48550/arXiv.2206.02915

Additional Reading:
How to convert a number from decimal to IEEE 754 Floating Point Representation.
Number Representation and Computer Arithmetic
https://web.ece.ucsb.edu/~parhami/pubs_folder/parh02-arith-encycl-infosys.pdf
Computer Representation of Numbers and Computer Arithmetic
https://people.cs.vt.edu/~asandu/Courses/CS3414/comp_arithm.pdf











 Tags: 8-bit floating point AI AI models AI/ML/DL AMD ARM base 10 base 2 BFLOAT16 Cadence Cadence Design Systems ChatGPT deep learning E4M3 E5M2 edge inference floating point FP16 FP32 FP64 FP8 Graphcore IEEE 754 inference INT8 Integer 8 Intel machine learning mantissa ML training neural network Nvidia P3109 Quadric Quadric.io Qualcomm Synopsys training Untether AI





Karen Heyman   (all posts)


							Karen Heyman is a technology editor at Semiconductor Engineering.
						





Leave a Reply Cancel replyComment * Name*(Note: This name will be displayed publicly)
Email*(This will not be displayed publicly) 
 

Δ 




 Knowledge Centers Blogs 
Spiking Neural Network (SNN)

Published on June 23, 2021



Recurrent Neural Network (RNN)

Published on June 25, 2019



Edge Computing

Published on April 4, 2019



Neural Networks

Published on July 25, 2017



Architectures

Published on 



Machine Learning (ML)

Published on May 10, 2017



Convolutional Neural Network (CNN)

Published on 



Artificial Intelligence (AI)

Published on 




Technical Papers
Manycore-FPGA Architecture Employing Novel Duet Adapters To Integrate eFPGAs in a Scalable, Non-Intrusive, Cache-Coherent Manner (Princeton) January 16, 2023 by Technical Paper LinkHardware Trojan Detection Case Study Based on 4 Different ICs Manufactured in Progressively Smaller CMOS Process Technologies January 11, 2023 by Technical Paper LinkQuantum Computing Architecture Enabling  Communication Between Superconducting Quantum Processors (MIT) January 11, 2023 by Technical Paper LinkArbitrary Precision DNN Accelerator Controlled by a RISC-V CPU (Ecole Polytechnique Montreal, IBM, Mila, CMC) January 10, 2023 by Technical Paper LinkTechnique For Printing Electronic Circuits Onto Curved & Corrugated Surfaces Using Metal Nanowires (NC State) January 10, 2023 by Technical Paper Link 

  Trending Articles

RISC-V Pushes Into The Mainstream

Open-source processor cores are beginning to show up in heterogeneous SoCs and packages.


by Marie C. Baca and Ed Sperling



How Secure Are RISC-V Chips?

Open source by itself doesn’t guarantee security. It still comes down to the fundamentals of design.


by Jeff Goldman



Will Floating Point 8 Solve AI/ML Overhead?

Less precision equals lower power, but standards are required to make this work.


by Karen Heyman



RISC-V decoupled Vector Processing Unit (VPU) For HPC



by Technical Paper Link



Startup Funding: December 2022

Wafer manufacturing and GPUs draw investment; 106 companies raise $2.8B.


by Jesse Allen






Knowledge Centers Entities, people and technologies explored
Learn More



Related Articles

Foundational Changes In Chip Architectures

New memory approaches and challenges in scaling CMOS point to radical changes — and potentially huge improvements — in semiconductor designs. 


by Brian Bailey



How Memory Design Optimizes System Performance

Changes are steady in the memory hierarchy, but how and where that memory is accessed is having a big impact.


by John Koon



Will Floating Point 8 Solve AI/ML Overhead?

Less precision equals lower power, but standards are required to make this work.


by Karen Heyman



Startup Funding: October 2022

113 startups raise $3.5B; batteries, AI, and new architectures top the list.


by Jesse Allen



Startup Funding: November 2022

127 startups raise $2.6B; data center connectivity, quantum computing, and batteries draw big funding.


by Jesse Allen



IC Stresses Affect Reliability At Advanced Nodes

Thermal mismatch in heterogeneous designs, different use cases, can impact everything from accelerated aging to warpage and system failures.


by Ann Mutschler



3D-IC Reliability Degrades With Increasing Temperature

Electromigration and other aging factors become more complicated along the z axis.


by Ann Mutschler



On-Chip Power Distribution Modeling Becomes Essential Below 7nm

Why and when it’s needed, and what tools and technologies are required.


by Ann Mutschler











Sponsors






























Advertise with us





Advertise with us





Advertise with us





Newsletter Signup



Popular Tags2.5D
5G
7nm
advanced packaging
AI
ANSYS
Apple
Applied Materials
ARM
Atrenta
automotive
business
Cadence
EDA
eSilicon
EUV
finFETs
GlobalFoundries
Google
IBM
imec
Intel
IoT
IP
Lam Research
machine learning
memory
Mentor
Mentor Graphics
MIT
Moore's Law
Nvidia
NXP
Qualcomm
Rambus
Samsung
security
SEMI
Siemens
Siemens EDA
software
Sonics
Synopsys
TSMC
verification
Recent CommentsWZIS on Arbitrary Precision DNN Accelerator Controlled by a RISC-V CPU (Ecole Polytechnique Montreal, IBM, Mila, CMC)Rama Chaganti on Growing System Complexity Drives More IP ReuseTL on How Secure Are RISC-V Chips?Frank on The Good And Bad Of Bi-Directional ChargingSandeep Dixit on The Good And Bad Of Bi-Directional ChargingHertz on How Secure Are RISC-V Chips?Andrew on How Software Utilizes CoresAsaf Jivilik on Cybord: Electronic Component TraceabilitySantosh Kurinec on Where All The Semiconductor Investments Are Goingdick freebird on Designing And Securing Chips For Outer SpaceAkshay on Designing And Securing Chips For Outer SpaceRaj on Is UCIe Really Universal?Andrew TAM on How Software Utilizes CoresRiko R on Designing For Multiple DieDan Ganousis on RISC-V Pushes Into The MainstreamIvan Batinic on IC Stresses Affect Reliability At Advanced NodesGiovanni Lostumbo on A Power-First ApproachMohammed Zakir Hussain on Embracing the Challenges Of Cybersecurity In Automotive ApplicationsLaura Peters on Week In Review: Manufacturing, TestAiv on Week In Review: Manufacturing, TestRoss Youngblood on High Voltage Testing Races AheadMark Olivas on Cybord: Electronic Component TraceabilityKarl Stevens on The Drive Toward Virtual PrototypesRon Lavallee on The Politics Of StandardsDenis McCarthy on Hot Trends In Semiconductor Thermal ManagementTom Smith on Are We Too Hard On Artificial Intelligence For Autonomous Driving?Maya F on Where All The Semiconductor Investments Are GoingSaikatm on Balancing Power And Heat In Advanced Chip DesignsDoug L. on Holistic 3D-IC Interposer Analysis In Product DesignsAndy Deng on Post-Quantum And Pre-Quantum Security Issues GrowJohn Dunn on Post-Quantum And Pre-Quantum Security Issues Growmadmax2069 on Chip Design Shifts As Fundamental Laws Run Out Of SteamMatthew Slyman on Chip Design Shifts As Fundamental Laws Run Out Of SteamDouglas MacIntyre on Chip Design Shifts As Fundamental Laws Run Out Of SteamJose on Universal Verification Methodology Running Out Of SteamZhengji Lu on Moving From AMBA ACE to CHI For CoherencyJohn Bennice on A Power-First Approach[email protected] on Chip Design Shifts As Fundamental Laws Run Out Of SteamMatthew on Chip Design Shifts As Fundamental Laws Run Out Of SteamKarthik Krishnamoorthy on AI-Powered VerificationCPlusPlus4Ever on Chip Design Shifts As Fundamental Laws Run Out Of SteamDouglas on Chip Design Shifts As Fundamental Laws Run Out Of SteamBowie Poag on Chip Design Shifts As Fundamental Laws Run Out Of SteamEugene on Startup Funding: October 2022Wesley Sung on Fan-Out And Packaging ChallengesHong Xiao on Chip Design Shifts As Fundamental Laws Run Out Of SteamRobert Anderson on Chip Design Shifts As Fundamental Laws Run Out Of SteamMike Frank on A Power-First ApproachWilliam Ruby on A Power-First ApproachPeter C Salmon on A Power-First ApproachDr. Dev Gupta on Which Foundry Is In The Lead? It Depends.Steve Hoover on A Power-First ApproachDylanP on Which Foundry Is In The Lead? It Depends.Asaf Jivilik on Cybord: Electronic Component TraceabilityChris @ crossPORt on Foundational Changes In Chip ArchitecturesMark Olivas on Cybord: Electronic Component TraceabilityAri ben David on Constraints On The Electricity GridJeff Zika on Auto Safety Tech Adds New IC Design ChallengesJung Yoon on Foundational Changes In Chip ArchitecturesSchrodinger's Cat's Advocate on Foundational Changes In Chip ArchitecturesRigTig on Foundational Changes In Chip ArchitecturesSteve on Foundational Changes In Chip ArchitecturesPrashant Purwar on Why Mask Blanks Are CriticalMostafa Abdelgawwad on Radar For Automotive: How Far Can A Radar See?yieldWerx on Managing Wafer RetestJohn Horner on A Brief History of TestLakshm J on ESD Requirements Are ChangingDr. Dev Gupta on Improving Redistribution Layers for Fan-out Packages And SiPsAkarsh on Better PMIC Design Using Multi-Physics SimulationTodd Bermensolo on Reducing Schedule Slips With Automated Post-Route Verification Of SerDes High Speed Serial LinksLaur Rizzatti on Why Geofencing Will Enable L5Raj Raghuram on The Complex Art Of Handling S-ParametersStevo on CHIPS Act: U.S. Releases New Implementation StrategySantosh Kurinec on Quantum Research Bits: Sept. 12Lewis Sternberg on ML And UVM Share Same FlawsRoger Stierman on L5 Adoption Hinges on 5G/6GMarcel on MicroLEDs Move Toward CommercializationRagu Athreya on Is There A Limit To The Number of Layers In 3D-NAND?Brian Bailey on AI Power Consumption ExplodingDavid S on AI Power Consumption ExplodingMike Cormack on Cryogenic CMOS Becomes CoolLance Harvie on New Uses For AI In ChipsDoc R on Electronics And Its Role In Climate ChangeMagdy Abadir on Is Standardization Required For Security?guest on How Overlay Keeps Pace With EUV PatterningSantosh Kurinec on Week In Review, Manufacturing, Testsravani on Timing Library LVF Validation For Production Design FlowsDr. F on A Sputnik Moment For ChipsGary Dagastine on A Sputnik Moment For ChipsMike Sottak on A Sputnik Moment For ChipsRobert Pearson on A Sputnik Moment For ChipsRaye E. Ward on A Sputnik Moment For ChipsMichael Williams on A Look Inside RF DesignSURESHBABU CHILUGODU on Week In Review: Manufacturing, TestJC Bouzigues, Menta on Customizing ProcessorsSteve Swendrowski on IC Package Illustrations, From 2D To 3DEMV on Hybrid Bonding Moves Into The Fast LaneDr. Appo van der Wiel on Variation Making Trouble In Advanced Packageswang yu on Verification Of Functional SafetyFrederick Chen on High-NA EUV May Be Closer Than It AppearsFact Cheq on The Week In Review: DesignShiwen Huang on E-beam’s Role Grows For Detecting IC DefectsAdele Hars on Wafer Shortage Improvement In Sight For 300mm, But Not 200mmDavid A. Humphreys on IMS2022 Booth Tour: EDA And Measurement Science ConvergeMerritt on Can Analog Make A Comeback?subra ganesan on Meeting Processor Performance And Safety Requirements For New ADAS & Autonomous Vehicle SystemsGeorge on Building A More Secure SoCAmit Garg on A New Breed Of EDA RequiredKarl Stevens on A Minimal RISC-VKarl Stevens on EDA Gaps At The Leading EdgeMicah Forstein MS. on Risks Rise As Robotic Surgery Goes MainstreamDr. Punam Raskar on Who Does Processor Validation?Dr. Dev Gupta on Variation Making Trouble In Advanced PackagesCox on DRAM Thermal Issues Reach Crisis PointDavid Leary on DRAM Thermal Issues Reach Crisis PointGeeeeeee on DRAM Thermal Issues Reach Crisis PointPedro Ferro Laks on SOT-MRAM To Challenge SRAMObviously silly on DRAM Thermal Issues Reach Crisis PointSimon on DRAM Thermal Issues Reach Crisis PointGareth on Energy Harvesting Starting To Gain Traction

 





Manycore-FPGA Architecture Emplo... Technical Paper LinkSelecting The Right RISC-V Core Brian Bailey 










  










About

About us
Contact us
Advertising on SemiEng
Newsletter SignUp



Navigation



Homepage
Special Reports
Systems & Design
Low Power-High Perf
Manufacturing, Packaging & Materials
Test, Measurement & Analytics
Auto, Security & Pervasive Computing




Videos
Jobs
Technical Papers
Events
Webinars
Knowledge Centers

Industry Research
Business & Startups
Newsletters





Connect With Us

Facebook
Twitter  @semiEngineering
LinkedIn
YouTube




Copyright ©2013-2023 SMG   |  Terms of Service  |  Privacy Policy





This site uses cookies. By continuing to use our website, you consent to our Cookies PolicyACCEPT Manage consent




Close






Privacy Overview 
This website uses cookies to improve your experience while you navigate through the website. The cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. We do not sell any personal information.

By continuing to use our website, you consent to our Privacy Policy. If you access other websites using the links provided, please be aware they may have their own privacy policies, and we do not accept any responsibility or liability for these policies or for any personal data which may be collected through these sites. Please check these policies before you submit any personal information to these sites.

 





								Necessary							


Necessary

Always Enabled




									Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.								






								Non-necessary							


Non-necessary





									Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website.								












SAVE & ACCEPT










 

 
 

 
























 








"
https://news.ycombinator.com/rss,Microsoft returns to the Altair,https://hackaday.com/2023/01/15/microsoft-returns-to-the-altair/,Comments,"


Microsoft Returns To The Altair


                18 Comments            

by:
Al Williams



January 15, 2023















Title:


Copy

Short Link:


Copy






The Altair 8800 arguably launched Microsoft. Now [Dave Glover] from Microsoft offers an emulated and potentially cloud-based Altair emulation with CP/M and Microsoft Basic. You can see a video of the project below. One thing that makes it a bit odd compared to other Altair clones we’ve seen is that the emulator runs in a Docker environment and is fully cloud-enabled. You can interact with it via a PCB front panel, or a terminal running in a web browser.
The core emulator is MIT-licensed and seems like it would run nearly everywhere. We were a little surprised there wasn’t an instance in the Azure cloud that you could spin up to test drive. Surely a few hundred Altairs running at once wouldn’t even make a dent in a modern CPU.

There are plenty of Altair emulators and even replicas with authentic CPUs out there. But we have to admit the Wiki documentation on this one is uncommonly well done. Even if you don’t want to use this emulator, you might find the collection of data about the Altair useful.
Don’t know how to use a computer front panel? Learn on the Altair or a PDP/8, even if you don’t have a real one. For simulated hardware, the project that turns an Arduino Due into an Altair works well. If you just want to play Zork, you can do that in your browser, for sure.





 



















Posted in RetrocomputingTagged altair 8800 


Post navigation

← A Number Maze For Younger HackersA Flex Sensor For A Glove Controller Using An LDR → 






            18 thoughts on “Microsoft Returns To The Altair”        





paulvdh says: 



							January 15, 2023 at 1:24 am						




@05:42 “I don’t have the source code for CP/M, I’m not even sure it exists anymore.
https://hackaday.com/2014/10/06/cpm-source-code-released/
This video is apparently only made as entertainment, just some guys blabbing and not caring about facts, That may be all right for some, it makes me wonder what else is off and  I lost interest.


Report comment 
Reply 





MG says: 



							January 15, 2023 at 1:51 am						




Even just off the top of my head, the fact that it’s MIT-licensed is nothing special; the entire core of MAME and a significant number of machine drivers are all MIT/3-clause BSD, which includes the Intel 8080A core that one would need to spin up an Altair 8800 emulator as well.


Report comment 
Reply 





stappers says: 



							January 15, 2023 at 3:59 am						




Thanks for the click bait warning.


Report comment 
Reply 





me says: 



							January 16, 2023 at 2:33 am						




fortunately i read here, so i re-read witho more attention the initial post, so I avoid to lose some minutes of my life looking at another-not-wanted-at-all-cliclbite video. Thanks yo you, dude!


Report comment 
Reply 







Joshua says: 



							January 15, 2023 at 1:58 pm						




What’s also kind of forgotten:
CP/M was thoroughly understood.
It was cloned and derived a dozen times, if not more.
People who worked with assembly language could literally “read” how CP/M worked and flowed through the system.
My father, among many others, wrote his own floppy controller routines for CP/M.
It’s kind of sad to see how incompetent the matter is being handled these days. Such a blunder. *sigh*
Back in the 70s and 80s, it was just natural to know about the internals of the Z-80 and/or CP/M. 
There were hundreds of books being written about all the deepest mechanisms of them.


Report comment 
Reply 





bruceeifler@gmail.com says: 



							January 15, 2023 at 6:15 pm						




“How to program the z80.  Roger Zaks


Report comment 
Reply 





Mr Name Required says: 



							January 15, 2023 at 11:14 pm						




Rodnay Zaks, with the ‘a’. A well-thumbed book for me.


Report comment 
Reply 











daveboltman says: 



							January 15, 2023 at 4:50 am						




Has Bill Gates really got over himself now?
https://en.wikipedia.org/wiki/An_Open_Letter_to_Hobbyists


Report comment 
Reply 





Ken says: 



							January 15, 2023 at 7:37 am						




You must not have been involved in the hobby “back in the day” – the copying of commercial products like Microsoft BASIC was rampant, almost celebrated in the community.
This was before Microsoft had even started licensing ROM BASIC to the major home/personal computer manufacturers, so stolen paper tapes were a serious hit to MS revenues.


Report comment 
Reply 





stappers says: 



							January 15, 2023 at 7:53 am						




And the problem of lack of wallet-voting still exists.  Might be because of the same word for libre and gratuit.
Where I do agree with https://justforfunnoreally.dev/  I also do agree with young Bill Gates that people should do some kind of payment for what they want.


Report comment 
Reply 







Joshua says: 



							January 15, 2023 at 1:48 pm						




Out of context. 
I’m no expert of American copyright law, but as far as I understand:
Material released before 1977 was considered Public Domain, unless the copyright is/was explicitly expressed.
I assume this covers material/works of engineers, students, universities etc. which didn’t mention the copyright in their papers. 
Go double check yourself, if you wish. I’m lazy right now. 😁


Report comment 
Reply 





The Commenter Formerly Known As Ren says: 



							January 15, 2023 at 4:29 pm						




No, he hasn’t, he is now a James Bond level super villain.


Report comment 
Reply 







Sok Puppette says: 



							January 15, 2023 at 6:39 am						




If it cannot play tunes on an AM radio, it is not a satisfying Altair emulator


Report comment 
Reply 





Ken says: 



							January 15, 2023 at 8:13 am						




Neat project, I too am impressed by the level of documentation. While I’m not too interested in running weather/climate change analysis programs on an emulated 50 year-old system, I realize it was important to inject some cloud activity to jazz up the project.
Interesting to note that you need to install Linux on a windows machine to run the code.
Neat how the sense hat reduces the need for ‘blinkin lights’ to its most basic elements, like the computer prop in the bat cave or any 1960s sci-fi movie/tv show…


Report comment 
Reply 





paulvdh says: 



							January 15, 2023 at 9:25 am						




At 5:42 he says he does not know whether the CP/M source code still exists…
He clearly has not checked Hackaday lately…
https://hackaday.com/2014/10/06/cpm-source-code-released/


Report comment 
Reply 





Joshua says: 



							January 15, 2023 at 1:51 pm						




Not just CP/M, also MP/M, the power user version of CP/M. The real thing, so to say. :)


Report comment 
Reply 







SayWhat? says: 



							January 15, 2023 at 12:16 pm						




I remember hacking “protected” MS Basic programs back in the day. All it took was finding the one byte in memory  that set the code to be not listable LOL.


Report comment 
Reply 





demon256 says: 



							January 15, 2023 at 3:45 pm						




Heh, my first computer ran CP/M.


Report comment 
Reply 



Leave a Reply					Cancel reply










Please be kind and respectful to help make the comments section excellent. (Comment Policy)This site uses Akismet to reduce spam. Learn how your comment data is processed.







Search

Search for:



Never miss a hack
Follow on facebook
Follow on twitter
Follow on youtube
Follow on rss
Contact us
Subscribe










If you missed it







AI-Controlled Twitch V-Tuber Has More Followers Than You


 39 Comments				








Ask Hackaday: What’s Your Worst Repair Win?


 173 Comments				








The Surprisingly Simple Way To Steal Cryptocurrency


 65 Comments				








Excuse Me, Your Tie Is Unzipped


 100 Comments				








All About USB-C: Resistors And Emarkers


 16 Comments				



More from this category





Our Columns







2022 FPV Contest: Congratulations To The Winners!


 1 Comment				








Machining With Electricity Hack Chat


 5 Comments				








Hackaday Links: January 15, 2023


 16 Comments				








Too Many Pixels


 34 Comments				








Hackaday Podcast 201: Faking A Transmission, Making Nuclear Fuel, And A Slidepot With A Twist


 2 Comments				



More from this category

"
https://news.ycombinator.com/rss,How my brother's iCloud account was stolen,https://7c0h.com/blog/new/stolen_iphone.html,Comments,"






Martin's blog - How my brother's iCloud account was stolen


















Martin's blog 



april_2020
april_2022
august_2015
august_2018
august_2020
december_2018
december_2020
december_2021
december_2022
february_2017
february_2018
february_2020
february_2022
january_2016
january_2020
january_2021
january_2022
january_2023
july_2015
july_2016
july_2019
july_2020
july_2021
july_2022
june_2015
june_2018
june_2020
june_2021
march_2015
march_2018
march_2020
march_2021
may_2015
may_2016
may_2020
may_2021
misc
november_2017
november_2019
november_2020
november_2021
november_2022
october_2015
october_2020
october_2021
october_2022
september_2015
september_2016
september_2021
september_2022






7c0h

Home
About me
Blog
Projects
Research





Martin's blog







RSS


Archive


Mastodon












How my brother's iCloud account was stolen

				Posted on 2023-01-16 under 					iphone
support




My brother got his iPhone stolen at gunpoint. This is the story of how he lost
control of his iCloud account first (along with years of priceless memories,
including my nephew's first steps) and how this couldn't have happened without
Apple's incompetent support.
But before that, a plea for help: if you know of anyone who can help us get
the account back (or rather, the priceless photos locked inside) please
get in touch with the links at the end of the article.
Part 1: Locked out of iCloud
The first thing the thieves did (after running away, of course) was changing
the phone number associated with the iCloud account. I do not know how they
did this - it has been suggested that Apple will send a code to your phone,
which the thieves obviously had. In any case, as soon as my brother tried to
log into ""Find my Phone"" he was faced with a screen asking him to verify
the phone number associated with the account, which was set to a number we
do not recognize.
It didn't matter that we still had the proper password for the iCloud account,
nor that we still have control of the e-mail associated with the account.
As far as Apple is concerned, if you don't know the phone number (which, again,
the thieves changed) you cannot access your iCloud account.
This is a known issue with iCloud security.
Next we got in touch with Apple, both via phone and Twitter.
Phone support was useless, as all they would say was that they couldn't help
us unless we knew the phone number. The closest I got to a victory was
getting the phone representative to say out loud that she wanted me to provide
the phone number the thieves gave, but that's about it.
Even if we have the old phone number, the iCloud e-mail, the iCloud
password, a police report and multiple IDs, Apple will not budge.
Twitter support was even worse: after repeatedly asking them to read what I
said two messages ago, I ended up getting this pointless, infuriating response:

We completely understand the concern with how important it is to have this resolved as soon as possible, and we were able to locate the cases from when you had previously reached out. Based on the information that you've provided, and the steps you and your brother have gone through, if your brother is unable to regain access to his Apple ID we would be unable to provide any additional methods to regain access to the account, and we would be unable to change the trusted phone number on the account. If you have any other questions or concerns regarding this issue, the best option would be to reach back out to our Apple ID experts at the link provided in our previous message (Note: that's the phone we called
before).

Part 2: Losing iCloud for good
But the story gets even worse (better?). While we were stuck in phone support
hell, my brother got a phishing SMS. He didn't recognize it as such, and lost
the phone for good. The trick works like this: once you get a new SIM card
(which the thieves can tell because the old one stops working) the
thieves send you a phishing e-mail pretending to be from Apple.
You follow the link, give your iCloud username and password,
and now the thieves can unlock your iPhone and resell it.
Crucially, this step only works because thieves know that Apple support will
not help you: if Apple had been of ANY help then we would have recovered
access long before the SMS and my brother wouldn't have followed the link.
According to Gizmodo (in Spanish) the next step would have been
a phishing call with spoofed caller ID. But we will never know.
Sidenote: I reported both URLs (https://apple.iforgot-ip.info and
https://apple.located-maps.info) to their hosting providers.
Results have been mixed. PublicDomainRegistry.com (which belongs to
Webhostbox) will not take down the hosting addresses unless they can see the
phishing attack themselves (they won't check logs), but good luck getting
a one-time link to work twice. UnifiedLayer.com was helpful,
and I believe that GoDaddy revoked their domains.
Part 3: No more Apple
The iCloud website promises
""all your stuff — photos, files, notes, and more — is safe, up to date, and available wherever you are"".
You have now seen what ""safe"" means in this context: it will be in the cloud,
yes, but that doesn't mean you'll have access to it.
The question now is: would my brother choose Apple again?
An iPhone is not cheap in general, and in Argentina less so. The current price
for an iPhone 13 is ca. 400.000 ARS, which roughly translates to 2200 USD
or 1300 USD at the unofficial rate (it's complicated). With an average
monthly salary of 427 USD (according to Numbeo) you can see that getting a new
iPhone is not a choice to take lightly.
What will my brother do? Paraphrasing from a conversation we had:
""if I could get my account back I could consider getting a new iPhone.
But if I have to start from scratch then it doesn't make sense.
If I can't get my data back I'll probably get an Android phone instead"".
Part 4: Conclusion and next steps
I have not entirely given up, but I'm not keeping my hopes up either. We are
currently looking into whether my brother's wife can get access to his files
(they had some kind of shared access), whether his iWatch can be of any use
(it was logged into the iCloud account), whether small claims court is likely to help (I know
it would work in the EU but
Argentina is trickier), and whether anyone in my extended network can reach
someone at Apple who is not an AI (thanks for nothing, LinkedIn).
As for next steps, I will be gifting my family access to some cloud storage,
but unless I can get a service with competent tech support
I may end up setting up a cloud of my own.
Hopefully the loss of my nephew's first steps will not be in vain.
Do you have any ideas? Do you work for Apple? Then send
me an e-mail or get in
touch in Mastodon or even
Twitter.








Pure.css
Pelican






					All content in this blog is licensed under a
Creative Commons Attribution-ShareAlike 4.0 International License






"
https://news.ycombinator.com/rss,Marriage rules in Minoan Crete revealed by ancient DNA analysis,https://phys.org/news/2023-01-marriage-minoan-crete-revealed-ancient.html,Comments,"400 Bad request
Your browser sent an invalid request.

"
https://news.ycombinator.com/rss,Boris Yeltsin's visit to a suburban Houston supermarket in 1989,http://beelineblogger.blogspot.com/2016/01/how-supermarket-visit-brought-down.html,Comments,"


















BeeLine: How A Supermarket Visit Brought Down The Soviet Union








































































BeeLine




The Shortest Route To What You Need To Know




















Scott Beeken





BeeLine



View my complete profile








































































Tuesday, January 5, 2016








How A Supermarket Visit Brought Down The Soviet Union





Many point to the fact that the Soviet Union collapse occurred as the Soviets were baited into trying to compete with the defense build-up instituted by Ronald Reagan.

The Soviets just did not have the financial resources to match the United States in defense spending while also tending to the needs of its citizenry. The Soviets spent money on guns rather than butter. Something had to give and the Soviet people were the ones that suffered.

However, a little known visit to a suburban Houston supermarket in 1989 by Boris Yeltsin appears to have been the catalyst that ended up bringing down the Soviet Union.

Yeltsin visited the Johnson Space Center in Houston in September, 1989 to tour mission control and to view a model of the planned International Space Station.

After visiting the Space Center, Yeltsin made an unplanned stop at a local Randall's grocery store that was close by before heading to the airport.

That visit changed the course of history.

At the time, Yeltsin was a newly elected member of the Soviet Parliament and the Supreme Soviet and had been a key ally of the General Secretary of the Communist Party Mikhail Gorbachev, who was initiating reforms but the pace of which was too slow for Yeltsin.

Houston Chronicle reporter Stephanie Asin was with Yeltsin on the visit to the grocery store that day.


Yeltsin, then 58, “roamed the aisles of Randall’s nodding his head in amazement,” wrote Asin. He told his fellow Russians in his entourage that if their people, who often must wait in line for most goods, saw the conditions of U.S. supermarkets, “there would be a revolution.”

“Even the Politburo doesn’t have this choice. Not even Mr. Gorbachev,” he said.

The fact that stores like these were on nearly every street corner in America amazed him. They even offered free cheese samples. According to Asin, Yeltsin didn’t leave empty-handed, as he was given a small bag of goodies to enjoy on his trip.

This is a picture of Yeltsin touring the grocery store.



Credit: Houston Chronicle


This Houston Chronicle story from 2014 fills in the rest of the story.


About a year after the Russian leader left office, a Yeltsin biographer later wrote that on the plane ride to Yeltsin’s next destination, Miami, he was despondent. He couldn’t stop thinking about the plentiful food at the grocery store and what his countrymen had to subsist on in Russia.

In Yeltsin’s own autobiography, he wrote about the experience at Randall’s, which shattered his view of communism, according to pundits. Two years later, he left the Communist Party and began making reforms to turn the economic tide in Russia. 

“When I saw those shelves crammed with hundreds, thousands of cans, cartons and goods of every possible sort, for the first time I felt quite frankly sick with despair for the Soviet people,” Yeltsin wrote. “That such a potentially super-rich country as ours has been brought to a state of such poverty! It is terrible to think of it.”

To give you some perspective on what was available in the Soviet Union at that time, here is a picture of a Russian store from that era.




Credit: Gennady Galperin/Reuters


An aide to Yeltsin later reported that in that visit to the grocery store in Houston “the last vestige of Bolshevism collapsed” inside his boss.

Two years later Yeltsin was elected to the newly created office of President of the Russian Federation after the collapse of the Soviet Union with Gorbachev.

Yeltsin immediately began dismantling the socialist economic system and introducing capitalism to the Russians. In the process he attempted to convert the world's largest command economy into a free-market one. 

The results of that transition were rocky in large part to cronyism in the break-up of many of the large state-owned businesses. In the process, many Russian oligarchs were created and Yeltsin eventually resigned his office in 1999 haunted by charges of corruption and incompetence.

His successor?

Vladimir Putin.

The falling price of oil has put a similar squeeze on Putin and the Russians today. Putin has been popular with the Russian people based on his macho style and nationalistic bombast. However, potential trouble lurks for Putin because of the Russian economy.

The Russian consumer is being squeezed with annual inflation of almost 20% and the average Russian spends about 50% of their income on food.

By comparison, the average American spends only 8% of income on groceries.

Will groceries once again determine the future of Russia?






Posted by



BeeLine




at

8:21 PM
















Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest














42 comments:




andreSeptember 14, 2017 at 4:52 AMobat viagraviagra asliReplyDeleteRepliesUnknownOctober 29, 2021 at 10:14 AMGreat Article Artificial Intelligence Projects Project Center in Chennai JavaScript Training in Chennai JavaScript Training in Chennai Project Centers in Chennai DeleteRepliesReplyReplychegekhanMarch 28, 2018 at 8:11 AMUseful Information, your blog is sharing unique information....Thanks for sharing!!! buy bakery products online south-cbuy branded food online in panganiReplyDeleteRepliesReplyUnknownSeptember 27, 2018 at 7:24 AMThank you for your post. This is excellent information. It is amazing and wonderful to visit your site.buy bakery products online south-c ReplyDeleteRepliesReplyYK AgencyDecember 25, 2018 at 11:30 AMSupermarket in Dubai Great article. Cool.ReplyDeleteRepliesReplyLuck CityFebruary 11, 2019 at 3:50 AMWithin this webpage, you'll see the page, you need to understand this data. https://digitalglobal.comReplyDeleteRepliesReplyjames brownNovember 22, 2019 at 6:45 AMAwesome blog. I enjoyed reading your articles. This is truly a read for me. I have bookmarked it and I am looking forward to reading new articles. Keep up the good work!Kroger customer surveyReplyDeleteRepliesReplyKroger experienceDecember 10, 2019 at 8:05 AMPlease share more like that.Kroger experienceReplyDeleteRepliesReplyAnonymousDecember 30, 2019 at 7:47 PMGreat article! Yeltsin revealed as a realist! I never knew this ...ReplyDeleteRepliesReplyDavid Grant Stewart, Sr., EgyptologistJanuary 1, 2020 at 3:17 PMWho paid you to write this drivel? You are either incredibly gullible or on the take from the USSR propaganda machine. You look and say the Soviet civilian economy is bad. There is no civilian economy in the USSR. The country does not have a war machine. The country is a war machine.ReplyDeleteRepliesReplyNeha UppalJanuary 9, 2020 at 3:33 AMThanks for sharing such beautiful information with us. I hope you will share some more information about best grocery shopping app. Please keep sharing.ReplyDeleteRepliesReplyjames brownFebruary 13, 2020 at 1:15 PMHey There. I found your blog using This is a very well written article. I’ll be sure to bookmark it and come back to read more of your useful info. Thanks for the post. I’ll definitely return.https://krogerexperiencee.com/ReplyDeleteRepliesReplyjames brownFebruary 18, 2020 at 7:24 AMGreat post I would like to thank you for the efforts you have made in writing this interesting and knowledgeable article.https://tellthebelll.usReplyDeleteRepliesReplysurvey monkey usaFebruary 19, 2020 at 7:45 AMGreat things you’ve always with us. Just keep writing this kind of posts.The time which was wasted in traveling for tuition now it can be used for studies.Thankssurvey monkey usaReplyDeleteRepliesReplydanielwilsonnFebruary 19, 2020 at 3:43 PM Thank you again for all the knowledge u distribute,Good post. I was very interested in the article, it's quite inspiring I should admit. I like visiting you site since I always come across interesting articles like this one.Great Job, I greatly appreciate that.Do Keep sharing! Regards,https://krogerfeeedback.us/ReplyDeleteRepliesReplydahliaApril 23, 2020 at 3:21 AM Thanks for sharing this information. I really like your post very much. You have really shared an informative and interesting post with people  TellTheBell ReplyDeleteRepliesReplypatronsurveysApril 24, 2020 at 8:23 AMAfter Kroger and Walmart, I prefer to go to Tesco supermarket. Do you know what? there is quality in products with an extraordinary service tescoviews com offers a platform to complete the Tesco customer satisfaction survey to win £1000 Gift Card & 25 Club Points. Survey site is giving a lot of store surveys at one place to complete.ReplyDeleteRepliesReplyjames brownJuly 14, 2020 at 3:24 AMThis comment has been removed by the author.ReplyDeleteRepliesReplyjames brownJuly 14, 2020 at 3:25 AMIts a great pleasure reading your post.Its full of information I am looking for and I love to post a comment that ""The content of your post is awesome"" Great work.https://krogerexperiencee.com/greatpeople-me-kroger-employee-login-portal/ReplyDeleteRepliesReplyNFL FanSeptember 20, 2020 at 4:46 PMThe official source for NFL news, video highlights, fantasy football, game-day coverage, schedules, stats, scores and more. Ravens FootballReplyDeleteRepliesReplytellthebellSeptember 22, 2020 at 11:42 AM Excellent website you have  so much cool information!..tellthebellReplyDeleteRepliesReplydgcustomerfirstJune 25, 2021 at 6:49 AMTo take an interest in the Dollar general super market survey, it is important to arrange a few things at one of its Branches initially. Visit the authority survey site of Dollar General survey at Dgcustomerfirst.Com and Win A $100 gift voucher. Then, at that point, you need to save the receipt of the store. Then, at that point, go to the authority survey site of dollar general. The Dg survey is accessible in both English or Spanish.ReplyDeleteRepliesReplymybkexperienceAugust 3, 2021 at 2:30 AMDollar general survey is an online platform that collects customers' most recent shopping experiences and overall customer satisfaction. Participate in the survey and be the lucky person to get enlisted in dg customer first winners. ReplyDeleteRepliesReplyarnavharperAugust 30, 2021 at 12:33 PMRemote for Fire TV is designed specifically to control Fire TV, Fire TV Cube and Fire TV Stick. Just connect mobile device and a TV or media player to the Firestick Remote.ReplyDeleteRepliesReplySEOOctober 12, 2021 at 3:37 AMdelta international recruitment agency in pakistanReplyDeleteRepliesReplyHealthandBeautyTipsNovember 6, 2021 at 5:24 PMEmployees can Perform Kroger E-Schedule Login at the Feed Kroger Login Portal once their Schedule Credentials are verified. If you are unable to sign in to Kroger Login then you need to contact the branch manager.Feed KrogerReplyDeleteRepliesReplyUnknownJanuary 27, 2022 at 5:48 AMThe NASA dark Brant IX sounding rocket conveyed the payload to an apogee of 177 miles prior to plunging by parachute and arriving at White Sands. Dgcustomerfirst.com Survey ReplyDeleteRepliesReplydgcustomerfirst.comMay 19, 2022 at 3:35 AMThe dgcustomerfirst.com criticism review permits customers to enter the Dollar General Sweepstakes of Cash $100 in the wake of finishing the overview.dgcustomerfirstwin.shop Survey  ReplyDeleteRepliesReplywww.DGCustomerFirst.comJune 6, 2022 at 10:50 AMDollar general survey is an online platform that collects customers' most recent shopping experiences and overall customer satisfaction. https://idgcustomerfirst.org/ReplyDeleteRepliesReplydgcustomerfirsts.shopJune 10, 2022 at 6:07 AMAlso, you have a superb opportunity to partake in the client criticism overview. DGCustomerFirst 2022 or Dollar General Survey is a study led by Dollar General's authorities for all United States inhabitants.dgcustomerfirsts executed a connected with the WWW review to take an arrangement about dgcustomerfirsts Helpline notwithstanding your support level thereafter visiting service  Click here dgcustomerfirsts ReplyDeleteRepliesReplySteveJune 16, 2022 at 9:58 AMGreat Article, it was very informative. That was such thought-provoking content. I enjoyed reading your content. Every week, I look forward to your column. In my opinion, this one is one of the best articles you have written so far.How to Change Instagram PasswordChange Windows 10 PasswordSubwaylistensHome Depot SurveyDQFanFeedback.com ReplyDeleteRepliesReplyUmairJune 30, 2022 at 6:44 AMirescopk.comReplyDeleteRepliesReplyDgcustomerfirstscom.shopJuly 9, 2022 at 6:51 AMFormerly referred to as J L Turner, Dollar General has numerous subsidiaries viz Dollar General Market, Dollar General Financial, Dollar General Global Sourcing, and lots extra.dgcustomerfirstscom executed a connected with the WWW review to take an arrangement about dgcustomerfirstscom Helpline notwithstanding your support level thereafter visiting service  Click here dgcustomerfirstscom ReplyDeleteRepliesReplyAnonymousJuly 19, 2022 at 1:35 PMLiveTheOrangeLife – Official Portal www.LiveTheOrangeLife.comWalmartOne Login - Walmartone.com Login Guidemyaccountaccess.comonevanillaJCP Associate KioskReplyDeleteRepliesReplySmith AdomJuly 30, 2022 at 12:03 PMTalkToWendys executed a connected with the WWW review to take an arrangement about TalkToWendys Helpline notwithstanding your support level thereafter visiting service  Click here TalkToWendys  It is mandatory to make a purchase at Wendy’s once before being a participant in this survey.ReplyDeleteRepliesReplyInformTarget.comAugust 10, 2022 at 3:38 AMRules are guidelines, and they are set to be accompanied. If you want to participate within the survey effectively, you need to adhere to the rules and policies set apart via the informtarget.Com remarks survey.informtargets executed a connected with the WWW review to take an arrangement about informtargets Helpline notwithstanding your support level thereafter visiting service  Click here informtargets.shop ReplyDeleteRepliesReplyTellBaskinRobbinsAugust 19, 2022 at 5:39 AMThere are some basic rules and requirements of this Baskin Robbins Customer Satisfaction Survey which I even have furnished in this newsletter. tellbaskinrobbins executed a connected with the WWW review to take an arrangement about tellbaskinrobbins Helpline notwithstanding your support level thereafter visiting service  Click here tellbaskinrobbins ReplyDeleteRepliesReplyTalktofoodlionAugust 20, 2022 at 2:37 AMTalktofoodlion The company's full name is general Dollar, and it is offering a $100 incentive to customers who take the time to participate in this little survey. visit here Talktofoodlion ReplyDeleteRepliesReplyFaiz IsrailiAugust 21, 2022 at 5:02 AMThe procedure is requesting information from individuals using a questionnaire, which may be completed offline or online. New technologies, however, are frequently disseminated via digital channels like social media, email, QR codes, or URLs. dgcustomerfirstReplyDeleteRepliesReplyTellBaskinRobbinsSeptember 8, 2022 at 7:11 AMTellBaskinRobbins After responding to the feedback questions, participants are expected to rate their experience shopping at Baskin Robbins.The feedback survey is sponsored by Baskin Robbins in order to better understand its service quality TellBaskinRobbinsReplyDeleteRepliesReplyNikithaOctober 8, 2022 at 1:31 AMmybkexperience customer satisfaction survey which is an online platform to get timely feedback from their customers about the food and services. This can improve their services according to customer’s needs and at the same time, rewards their customer for their time and loyalty towards the restaurant. So you can answer the questions and eat delicious food for free at the same time. Now you might be wondering about how to participate in the survey or what are the requirements and much more.ReplyDeleteRepliesReplySEOOctober 14, 2022 at 5:59 AMtop recruitment agencies in pakistan for saudi arabiaReplyDeleteRepliesReplyAdd commentLoad more...























Newer Post


Older Post

Home




Subscribe to:
Post Comments (Atom)















BeeLine Email Subscription

Get new posts by email:  Subscribe




Follow @BeeLineBlog




Followers











Blog Archive








        ► 
      



2023

(5)





        ► 
      



January

(5)









        ► 
      



2022

(139)





        ► 
      



December

(11)







        ► 
      



November

(10)







        ► 
      



October

(12)







        ► 
      



September

(12)







        ► 
      



August

(13)







        ► 
      



July

(11)







        ► 
      



June

(12)







        ► 
      



May

(12)







        ► 
      



April

(12)







        ► 
      



March

(12)







        ► 
      



February

(10)







        ► 
      



January

(12)









        ► 
      



2021

(131)





        ► 
      



December

(15)







        ► 
      



November

(12)







        ► 
      



October

(9)







        ► 
      



September

(13)







        ► 
      



August

(14)







        ► 
      



July

(11)







        ► 
      



June

(10)







        ► 
      



May

(6)







        ► 
      



April

(10)







        ► 
      



March

(11)







        ► 
      



February

(7)







        ► 
      



January

(13)









        ► 
      



2020

(154)





        ► 
      



December

(11)







        ► 
      



November

(12)







        ► 
      



October

(14)







        ► 
      



September

(11)







        ► 
      



August

(12)







        ► 
      



July

(13)







        ► 
      



June

(14)







        ► 
      



May

(12)







        ► 
      



April

(16)







        ► 
      



March

(16)







        ► 
      



February

(10)







        ► 
      



January

(13)









        ► 
      



2019

(145)





        ► 
      



December

(14)







        ► 
      



November

(11)







        ► 
      



October

(9)







        ► 
      



September

(12)







        ► 
      



August

(13)







        ► 
      



July

(12)







        ► 
      



June

(12)







        ► 
      



May

(14)







        ► 
      



April

(13)







        ► 
      



March

(12)







        ► 
      



February

(10)







        ► 
      



January

(13)









        ► 
      



2018

(139)





        ► 
      



December

(11)







        ► 
      



November

(10)







        ► 
      



October

(10)







        ► 
      



September

(10)







        ► 
      



August

(12)







        ► 
      



July

(13)







        ► 
      



June

(12)







        ► 
      



May

(12)







        ► 
      



April

(13)







        ► 
      



March

(12)







        ► 
      



February

(9)







        ► 
      



January

(15)









        ► 
      



2017

(132)





        ► 
      



December

(9)







        ► 
      



November

(10)







        ► 
      



October

(15)







        ► 
      



September

(9)







        ► 
      



August

(13)







        ► 
      



July

(12)







        ► 
      



June

(9)







        ► 
      



May

(13)







        ► 
      



April

(12)







        ► 
      



March

(10)







        ► 
      



February

(10)







        ► 
      



January

(10)









        ▼ 
      



2016

(119)





        ► 
      



December

(11)







        ► 
      



November

(15)







        ► 
      



October

(15)







        ► 
      



September

(10)







        ► 
      



August

(2)







        ► 
      



July

(9)







        ► 
      



June

(11)







        ► 
      



May

(6)







        ► 
      



April

(9)







        ► 
      



March

(11)







        ► 
      



February

(12)







        ▼ 
      



January

(8)

In the Middle of 5th Avenue
Risky Business
That's for the Birds
An Inconvenient Truth +10
Citizen Cruz
Context on Guns
How A Supermarket Visit Brought Down The Soviet Union
A Humble Servant?










        ► 
      



2015

(71)





        ► 
      



December

(9)







        ► 
      



November

(5)







        ► 
      



October

(2)







        ► 
      



September

(2)







        ► 
      



August

(9)







        ► 
      



July

(7)







        ► 
      



June

(6)







        ► 
      



May

(3)







        ► 
      



April

(10)







        ► 
      



March

(6)







        ► 
      



February

(4)







        ► 
      



January

(8)









        ► 
      



2014

(88)





        ► 
      



December

(5)







        ► 
      



November

(7)







        ► 
      



October

(10)







        ► 
      



September

(9)







        ► 
      



August

(5)







        ► 
      



July

(8)







        ► 
      



June

(7)







        ► 
      



May

(9)







        ► 
      



April

(7)







        ► 
      



March

(6)







        ► 
      



February

(5)







        ► 
      



January

(10)









        ► 
      



2013

(115)





        ► 
      



December

(8)







        ► 
      



November

(5)







        ► 
      



October

(13)







        ► 
      



September

(9)







        ► 
      



August

(7)







        ► 
      



July

(10)







        ► 
      



June

(10)







        ► 
      



May

(9)







        ► 
      



April

(13)







        ► 
      



March

(10)







        ► 
      



February

(7)







        ► 
      



January

(14)









        ► 
      



2012

(139)





        ► 
      



December

(6)







        ► 
      



November

(16)







        ► 
      



October

(16)







        ► 
      



September

(9)







        ► 
      



August

(14)







        ► 
      



July

(13)







        ► 
      



June

(8)







        ► 
      



May

(11)







        ► 
      



April

(9)







        ► 
      



March

(9)







        ► 
      



February

(11)







        ► 
      



January

(17)









        ► 
      



2011

(188)





        ► 
      



December

(12)







        ► 
      



November

(10)







        ► 
      



October

(10)







        ► 
      



September

(10)







        ► 
      



August

(17)







        ► 
      



July

(18)







        ► 
      



June

(11)







        ► 
      



May

(3)







        ► 
      



April

(16)







        ► 
      



March

(19)







        ► 
      



February

(25)







        ► 
      



January

(37)









About Me





BeeLine


Scott Beeken has practiced as an attorney, CPA and has been an officer with two Fortune 500 companies overseeing diverse functions such as Taxation, Employee Benefits, Human Resources, Real Estate Facilities, Risk Management, Corporate Communications, Marketing and Advertising. In addition to writing BeeLine, he is a Keynote Speaker, Author  and Strategic Consultant.

View my complete profile




























Total Pageviews

























Theme images by luoman. Powered by Blogger.

























"
https://news.ycombinator.com/rss,"Show HN: Vento, Screen Recorder that lets you rewind and re-record over mistakes",https://vento.so,Comments,"ventoNew RecordingLog inStress-free Screen Recording Constantly restarting your screen recordings? With Vento, just pause, rewind, and carry on instead - keeping calm helps too :)Come back on a desktop computer to try us out!Install Chrome ExtensionStart Recording“To record great videos, one must first rewind.” - Ghandi, probably.Video Not Availablevento© 2023 Vento. All rights reserved.Say hello! We don’t bite. Well, maybe one of us does.hello@vento.so"
https://news.ycombinator.com/rss,Granian – a Rust HTTP server for Python applications,https://github.com/emmett-framework/granian,Comments,"








emmett-framework

/

granian

Public







 

Notifications



 

Fork
    9




 


          Star
 475
  









        A Rust HTTP server for Python applications
      
License





     BSD-3-Clause license
    






475
          stars
 



9
          forks
 



 


          Star

  





 

Notifications












Code







Issues
7






Pull requests
0






Discussions







Actions







Security







Insights



 
 



More


 


                  Code
 


                  Issues
 


                  Pull requests
 


                  Discussions
 


                  Actions
 


                  Security
 


                  Insights
 







emmett-framework/granian









This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.











master





Switch branches/tags










Branches
Tags














View all branches















View all tags













Name already in use









      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?



    Cancel

    Create








1
branch





14
tags







    Code
 







Local



 Codespaces



  










  Clone





            HTTPS
 
            GitHub CLI
 













        Use Git or checkout with SVN using the web URL.
    













      Work fast with our official CLI.
      Learn more.
    








    Open with GitHub Desktop






    Download ZIP



 
Sign In Required

                Please
                sign in
                to use Codespaces.
              



Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching GitHub Desktop

    If nothing happens, download GitHub Desktop and try again.
  




Launching Xcode

    If nothing happens, download Xcode and try again.
  





Launching Visual Studio Code
Your codespace will open once ready.
There was a problem preparing your codespace, please try again.










Latest commit






 




gi0baro

Refactor response build in RSGI protocol




        …
      




        e5139e2
      

Jan 16, 2023





Refactor response build in RSGI protocol


e5139e2



Git stats







136

                      commits
                    







Files
Permalink




  
    Failed to load latest commit information.


  
 


Type
Name
Latest commit message
Commit time








.github



Update CI release workflow



Jan 13, 2023









benchmarks



Update benchmarks



Jan 13, 2023









docs/spec



Fix typos (#14)



Nov 17, 2022









granian



wsgi: ensure to close the app response iterator (#29)



Jan 16, 2023









lib/pyo3-asyncio



Bump pyo3-asyncio to 0.17



Oct 25, 2022









src



Refactor response build in RSGI protocol



Jan 16, 2023









tests



Fix wsgi.input out of spec (close #24)



Jan 12, 2023









.gitignore



first implementation



Apr 15, 2022









Cargo.lock



Upgrade cargo dependencies



Jan 16, 2023









Cargo.toml



Upgrade cargo dependencies



Jan 16, 2023









LICENSE



first implementation



Apr 15, 2022









README.md



Update benchmarks results



Dec 24, 2022









build.rs



Add PyPy support



Jan 3, 2023









pyproject.toml



Add PyPy support



Jan 3, 2023









setup.py



review package meta



Apr 18, 2022




    View code
 















Granian
Rationale
Features
Quickstart
Project status
License





README.md




Granian
A Rust HTTP server for Python applications.
Rationale
The main reasons behind Granian design are:

Have a single, correct HTTP implementation, supporting versions 1, 2 (and eventually 3)
Provide a single package for several platforms
Avoid the usual Gunicorn + uvicorn + http-tools dependency composition on unix systems
Provide stable performance when compared to existing alternatives

Features

Supports ASGI/3, RSGI and WSGI interface applications
Implements HTTP/1 and HTTP/2 protocols
Supports HTTPS
Supports Websockets over HTTP/1 and HTTP/2

Quickstart
You can install Granian using pip:
$ pip install granian

Create an ASGI application in your main.py:
async def app(scope, receive, send):
    assert scope['type'] == 'http'

    await send({
        'type': 'http.response.start',
        'status': 200,
        'headers': [
            [b'content-type', b'text/plain'],
        ],
    })
    await send({
        'type': 'http.response.body',
        'body': b'Hello, world!',
    })
and serve it:
$ granian --interface asgi main:app

You can also create an app using the RSGI specification:
async def app(scope, proto):
    assert scope.proto == 'http'

    proto.response_str(
        status=200,
        headers=[
            ('content-type', 'text/plain')
        ],
        body=""Hello, world!""
    )
and serve it using:
$ granian --interface rsgi main:app

Project status
Granian is currently under active development.
Granian is compatible with Python 3.7 and above versions on unix platforms and 3.8 and above on Windows.
License
Granian is released under the BSD License.









About

      A Rust HTTP server for Python applications
    
Topics



  python


  rust


  http


  http-server


  asyncio


  asgi



Resources





      Readme
 
License





     BSD-3-Clause license
    



Stars





475
    stars

Watchers





7
    watching

Forks





9
    forks







    Releases
      14







Granian 0.2.2

          Latest
 
Jan 16, 2023

 

        + 13 releases





Sponsor this project



 

 

 Sponsor
 
Learn more about GitHub Sponsors







    Packages 0


        No packages published 







        Used by 6
 




























    Contributors 4





 



 



 



 







Languages












Rust
83.1%







Python
16.5%







Other
0.4%











"
https://news.ycombinator.com/rss,"Late Hokusai: Thought, technique, society",https://www.britishmuseum.org/research/projects/late-hokusai-thought-technique-society,Comments,"




405 Not allowed


Error 405 Not allowed
Not allowed
Error 54113
Details: cache-sjc10059-SJC 1673915968 1469586448

Varnish cache server


"
https://news.ycombinator.com/rss,Apple TV prompt requires another Apple device,https://twitter.com/hugelgupf/status/1614794516309118977,Comments,"














JavaScript is not available.
We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.
Help Center

Terms of Service
Privacy Policy
Cookie Policy
Imprint
Ads info
      © 2023 Twitter, Inc.
    

Something went wrong, but don’t fret — let’s give it another shot."
