{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# フィードの収集\n",
    "\n",
    "英語フィードの収集\n",
    "- 1_1_feeds.py と同じ内容\n",
    "\n",
    "feedparserパッケージが無いとエラーが出る場合は、端末（ターミナル等）で以下のコマンドによりインストール：\n",
    "```\n",
    "conda install feedparser\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feedparser\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pdfminer.high_level import extract_text\n",
    "import io\n",
    "\n",
    "def get_webpage_content(url):\n",
    "    # URLからデータ取得 (GET)\n",
    "    print('URL # {}'.format(url), flush=True)\n",
    "    res = requests.get(url)\n",
    "    content_type = res.headers['Content-Type']\n",
    "    print('content_type # {}'.format(content_type), flush=True)\n",
    "    if 'text/html' in content_type:\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        main_txt = soup.main\n",
    "        # main_text == Noneならmainタグがない\n",
    "        if main_txt == None:\n",
    "            text = soup.get_text()\n",
    "            text.replace('\\n', '').replace('\\u3000', ' ')\n",
    "        else:\n",
    "            text = soup.main.get_text()\n",
    "            text.replace('\\n', '').replace('\\u3000', ' ')\n",
    "    elif 'application/pdf':\n",
    "        text2 = []\n",
    "        ext_text = extract_text(io.BytesIO(res.content))\n",
    "        \n",
    "        # 行ごとに分割してから不要な改行コードを削除\n",
    "        # extract_text.replace('\\n', '')\n",
    "        for l in ext_text.split('\\n\\n'):\n",
    "            text2.append(l.replace('\\n', ''))\n",
    "        # その後もう一度結合し直す\n",
    "        text = ' '.join(text2)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feeds(n, url):\n",
    "    \"\"\"\n",
    "    urlから取得したフィードをリストにして返す。\n",
    "    \"\"\"\n",
    "    # print('URL #{} {}'.format(n, url), flush=True)\n",
    "    # 初期化\n",
    "    feeds = []\n",
    "    # urlからフィードを取得\n",
    "    try:\n",
    "        f = feedparser.parse(url)\n",
    "        if f.bozo != 0 and f.bozo != True:\n",
    "            print('Error(bozo) url:', url, flush=True)\n",
    "            return feeds\n",
    "    except:\n",
    "        print('Error(exception) url:', url, flush=True)\n",
    "        return feeds\n",
    "    # f.entries 内の各要素について処理\n",
    "    # - title: タイトル\n",
    "    # - summary, description: 内容\n",
    "    for e in f.entries:\n",
    "        # タイトル\n",
    "        if 'title' in e:\n",
    "            title = e.title\n",
    "        else:\n",
    "            title = ''\n",
    "\n",
    "        # リンク(コンテンツ本体のURL)\n",
    "        if 'link' in e:\n",
    "            link = e.link\n",
    "        else:\n",
    "            link = ''\n",
    "\n",
    "        # 内容：summary または description\n",
    "        if 'summary' in e:\n",
    "            body = e.summary\n",
    "        elif 'description' in e:\n",
    "            body = e.description\n",
    "        else:\n",
    "            body = ''\n",
    "        \n",
    "        # title と body の両方が空ならば追加しない\n",
    "        if title == '' and body == '':\n",
    "            continue\n",
    "\n",
    "        # HTML 形式の場合があるため <...> を削除\n",
    "        body = re.compile(r'<[^>]+>').sub('', body)\n",
    "        # feeds に URL, タイトル、内容 を追加\n",
    "        # - body.strip(): 先頭、末尾の改行・空白文字を削除\n",
    "\n",
    "        content = get_webpage_content(link)\n",
    "\n",
    "        feeds.append([url, title, link, body.strip(), content])\n",
    "\n",
    "    return feeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_feeds(feedlist, output, simple_output):\n",
    "    \"\"\"\n",
    "    feedlistに記載のURLからフィードを取得し、CSV形式でoutputファイルに書き出す。\n",
    "    outputファイルが既にあれば、読み込み、重複排除を行う。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # outputファイル（CSV形式）から読み込み\n",
    "        df = pd.read_csv(output)\n",
    "        s_df = pd.read_csv(simple_output)\n",
    "    except:\n",
    "        # outputファイルがなかった場合、DataFrameを作成\n",
    "        df = pd.DataFrame([], columns=['url', 'title', 'link', 'summary', 'content'])\n",
    "        s_df = pd.DataFrame([], columns=['title', 'link'])\n",
    "\n",
    "    # feedlistに記載のURLからフィードを取得\n",
    "    urls = [line.strip() for line in open(feedlist)]\n",
    "    for i, url in enumerate(urls):\n",
    "        feeds = get_feeds(i, url)\n",
    "        feeds_df = pd.DataFrame(feeds, columns=['url', 'title', 'link', 'summary', 'content'])\n",
    "        df = pd.concat([df, feeds_df])\n",
    "        s_df = pd.concat([s_df, feeds_df.loc[:, ['title', 'link']]])\n",
    "\n",
    "    # 重複排除\n",
    "    df = df.drop_duplicates()\n",
    "    s_df = s_df.drop_duplicates()\n",
    "    # CSV形式でoutputファイルに書き出し\n",
    "    df.to_csv(output, index=False)\n",
    "    s_df.to_csv(simple_output, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL # https://daverupert.com/2023/01/shadow-banned-by-duckduckgo-and-bing/\n",
      "content_type # text/html; charset=UTF-8\n",
      "URL # https://github.com/Enerccio/SLT\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://news.ycombinator.com/item?id=34388866\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://utcc.utoronto.ca/~cks/space/blog/linux/Ubuntu2204ServerPhasedUpdates\n",
      "content_type # text/html; charset=UTF-8\n",
      "URL # https://dbohdan.com/scripts-with-dependencies\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://blog.revolutionanalytics.com/2014/01/the-fourier-transform-explained-in-one-sentence.html\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://gofoss.net/\n",
      "content_type # text/html; charset=UTF-8\n",
      "URL # https://github.com/andrewcmyers/constrain\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://jobs.flightcontrol.dev/developer-advocate\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://gitlab.com/tsoding/porth\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://old.reddit.com/r/MagicArena/comments/b21u3n/i_analyzed_shuffling_in_a_million_games/\n",
      "content_type # text/html; charset=UTF-8\n",
      "URL # https://www.sqlite.org/fasterthanfs.html\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://spectrum.ieee.org/open-circuits\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://cordlandwehr.wordpress.com/2023/01/14/running-plasma-on-visionfive-2/\n",
      "content_type # text/html; charset=UTF-8\n",
      "URL # https://leocaussan.itch.io/the-bibites\n",
      "content_type # text/html\n",
      "URL # https://github.com/williamyang1991/VToonify\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://github.com/jakkra/ZSWatch\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://leebyron.com/4000/\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://acko.net/blog/use-gpu-goes-trad/\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://www.phoronix.com/news/NetBSD-HAMMER2-Port\n",
      "content_type # text/html; charset=UTF-8\n",
      "URL # https://www.prospectmagazine.co.uk/arts-and-books/the-joy-of-sets\n",
      "content_type # text/html\n",
      "URL # https://utcc.utoronto.ca/~cks/space/blog/sysadmin/BMCsCanNeedRebooting\n",
      "content_type # text/html; charset=UTF-8\n",
      "URL # https://statmodeling.stat.columbia.edu/2023/01/14/bayesian-statistics-and-machine-learning-how-do-they-differ/\n",
      "content_type # text/html\n",
      "URL # https://www.bankofengland.co.uk/-/media/boe/files/quarterly-bulletin/2014/money-creation-in-the-modern-economy.pdf?la=en&hash=9A8788FD44A62D8BB927123544205CE476E01654\n",
      "content_type # application/pdf\n",
      "URL # https://www.polygon.com/zelda/23540526/legend-of-zelda-cartoon-oral-history-zeldathon\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://developer.twitter.com/apitools\n",
      "content_type # text/html;charset=utf-8\n",
      "URL # https://www.1-9-9-1.com/\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://eos.org/articles/nasas-double-asteroid-redirection-test-is-a-smashing-success\n",
      "content_type # text/html; charset=utf-8\n",
      "URL # https://www.nplusonemag.com/online-only/book-review/malcolm-on-the-stand/\n",
      "content_type # text/html; charset=UTF-8\n",
      "URL # https://techcrunch.com/2023/01/14/circleci-hackers-stole-customer-source-code/\n",
      "content_type # text/html; charset=UTF-8\n"
     ]
    }
   ],
   "source": [
    "# フィードの取得、書き出し\n",
    "# ハッカーニュースのRSSの場合、その時点での人気の記事が順番に表示されている\n",
    "write_feeds('feedlist_en.txt', 'output_en.csv', 'index_output.csv')\n",
    "# write_feeds('feedlist_jp.txt', 'output_jp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL # https://www.sqlite.org/fasterthanfs.html\n",
      "content_type # text/html; charset=utf-8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n35% Faster Than The Filesystem\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSmall. Fast. Reliable.Choose any three.\\n\\n\\n\\nHome\\nMenu\\nAbout\\nDocumentation\\nDownload\\nLicense\\nSupport\\nPurchase\\n\\nSearch\\n\\n\\n\\n\\nAbout\\nDocumentation\\nDownload\\nSupport\\nPurchase\\n\\n\\n\\n\\n\\nSearch Documentation\\nSearch Changelog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n35% Faster Than The Filesystem\\n\\n\\n\\n►\\nTable Of Contents\\n\\n1. Summary\\n1.1. Caveats\\n1.2. Related Studies\\n2. How These Measurements Are Made\\n2.1. Read Performance Measurements\\n2.2. Write Performance Measurements\\n2.3. Variations\\n3. General Findings\\n4. Additional Notes\\n4.1. Compiling And Testing on Android\\n\\n\\n\\n\\n1. Summary\\nSQLite reads and writes small blobs (for example, thumbnail images)\\n35% faster¹ than the same blobs\\ncan be read from or written to individual files on disk using\\nfread() or fwrite().\\n\\nFurthermore, a single SQLite database holding\\n10-kilobyte blobs uses about 20% less disk space than\\nstoring the blobs in individual files.\\n\\nThe performance difference arises (we believe) because when\\nworking from an SQLite database, the open() and close() system calls\\nare invoked only once, whereas\\nopen() and close() are invoked once for each blob\\nwhen using blobs stored in individual files.  It appears that the\\noverhead of calling open() and close() is greater than the overhead\\nof using the database.  The size reduction arises from the fact that\\nindividual files are padded out to the next multiple of the filesystem\\nblock size, whereas the blobs are packed more tightly into an SQLite\\ndatabase.\\n\\n\\nThe measurements in this article were made during the week of 2017-06-05\\nusing a version of SQLite in between 3.19.2 and 3.20.0.  You may expect\\nfuture versions of SQLite to perform even better.\\n\\n1.1. Caveats\\n\\n\\n¹The 35% figure above is approximate.  Actual timings vary\\ndepending on hardware, operating system, and the\\ndetails of the experiment, and due to random performance fluctuations\\non real-world hardware.  See the text below for more detail.\\nTry the experiments yourself.  Report significant deviations on\\nthe SQLite forum.\\n\\n\\nThe 35% figure is based on running tests on every machine\\nthat the author has easily at hand.\\nSome reviewers of this article report that SQLite has higher \\nlatency than direct I/O on their systems.  We do not yet understand\\nthe difference.  We also see indications that SQLite does not\\nperform as well as direct I/O when experiments are run using\\na cold filesystem cache.\\n\\n\\nSo let your take-away be this: read/write latency for\\nSQLite is competitive with read/write latency of individual files on\\ndisk.  Often SQLite is faster.  Sometimes SQLite is almost\\nas fast.  Either way, this article disproves the common\\nassumption that a relational database must be slower than direct\\nfilesystem I/O.\\n\\n1.2. Related Studies\\n\\nJim Gray\\nand others studied the read performance of BLOBs\\nversus file I/O for Microsoft SQL Server and found that reading BLOBs \\nout of the \\ndatabase was faster for BLOB sizes less than between 250KiB and 1MiB.\\n(Paper).\\nIn that study, the database still stores the filename of the content even\\nif the content is held in a separate file.  So the database is consulted\\nfor every BLOB, even if it is only to extract the filename.  In this\\narticle, the key for the BLOB is the filename, so no preliminary database\\naccess is required.  Because the database is never used at all when\\nreading content from individual files in this article, the threshold\\nat which direct file I/O becomes faster is smaller than it is in Gray\\'s\\npaper.\\n\\n\\nThe Internal Versus External BLOBs article on this website is an\\nearlier investigation (circa 2011) that uses the same approach as the\\nJim Gray paper — storing the blob filenames as entries in the\\ndatabase — but for SQLite instead of SQL Server.\\n\\n\\n\\n2. How These Measurements Are Made\\nI/O performance is measured using the\\nkvtest.c program\\nfrom the SQLite source tree.\\nTo compile this test program, first gather the kvtest.c source file\\ninto a directory with the SQLite amalgamation source\\nfiles \"sqlite3.c\" and \"sqlite3.h\".  Then on unix, run a command like\\nthe following:\\n\\ngcc -Os -I. -DSQLITE_DIRECT_OVERFLOW_READ \\\\\\n  kvtest.c sqlite3.c -o kvtest -ldl -lpthread\\n\\nOr on Windows with MSVC:\\n\\ncl -I. -DSQLITE_DIRECT_OVERFLOW_READ kvtest.c sqlite3.c\\n\\nInstructions for compiling for Android\\nare shown below.\\n\\n\\nUse the resulting \"kvtest\" program to\\ngenerate a test database with 100,000 random uncompressible\\nblobs, each with a random\\nsize between 8,000 and 12,000 bytes\\nusing a command like this:\\n\\n./kvtest init test1.db --count 100k --size 10k --variance 2k\\n\\n\\nIf desired, you can verify the new database by running this command:\\n\\n./kvtest stat test1.db\\n\\n\\nNext, make copies of all the blobs into individual files in a directory\\nusing a command like this:\\n\\n./kvtest export test1.db test1.dir\\n\\n\\nAt this point, you can measure the amount of disk space used by\\nthe test1.db database and the space used by the test1.dir directory\\nand all of its content.  On a standard Ubuntu Linux desktop, the\\ndatabase file will be 1,024,512,000 bytes in size and the test1.dir\\ndirectory will use 1,228,800,000 bytes of space (according to \"du -k\"),\\nabout 20% more than the database.\\n\\n\\nThe \"test1.dir\" directory created above puts all the blobs into a single\\nfolder.  It was conjectured that some operating systems would perform \\npoorly when a single directory contains 100,000 objects.  To test this,\\nthe kvtest program can also store the blobs in a hierarchy of folders with no\\nmore than 100 files and/or subdirectories per folder.  The alternative\\non-disk representation of the blobs can be created using the --tree\\ncommand-line option to the \"export\" command, like this:\\n\\n./kvtest export test1.db test1.tree --tree\\n\\n\\nThe test1.dir directory will contain 100,000 files\\nwith names like \"000000\", \"000001\", \"000002\" and so forth but the\\ntest1.tree directory will contain the same files in subdirectories like\\n\"00/00/00\", \"00/00/01\", and so on.  The test1.dir and test1.test\\ndirectories take up approximately the same amount of space, though\\ntest1.test is very slightly larger due to the extra directory entries.\\n\\n\\nAll of the experiments that follow operate the same with either \\n\"test1.dir\" or \"test1.tree\".  Very little performance difference is\\nmeasured in either case, regardless of operating system.\\n\\n\\nMeasure the performance for reading blobs from the database and from\\nindividual files using these commands:\\n\\n./kvtest run test1.db --count 100k --blob-api\\n./kvtest run test1.dir --count 100k --blob-api\\n./kvtest run test1.tree --count 100k --blob-api\\n\\n\\nDepending on your hardware and operating system, you should see that reads \\nfrom the test1.db database file are about 35% faster than reads from \\nindividual files in the test1.dir or test1.tree folders.  Results can vary\\nsignificantly from one run to the next due to caching, so it is advisable\\nto run tests multiple times and take an average or a worst case or a best\\ncase, depending on your requirements.\\n\\nThe --blob-api option on the database read test causes kvtest to use\\nthe sqlite3_blob_read() feature of SQLite to load the content of the\\nblobs, rather than running pure SQL statements.  This helps SQLite to run\\na little faster on read tests.  You can omit that option to compare the\\nperformance of SQLite running SQL statements.\\nIn that case, the SQLite still out-performs direct reads, though\\nby not as much as when using sqlite3_blob_read().\\nThe --blob-api option is ignored for tests that read from individual disk\\nfiles.\\n\\n\\nMeasure write performance by adding the --update option.  This causes\\nthe blobs are overwritten in place with another random blob of\\nexactly the same size.\\n\\n./kvtest run test1.db --count 100k --update\\n./kvtest run test1.dir --count 100k --update\\n./kvtest run test1.tree --count 100k --update\\n\\n\\nThe writing test above is not completely fair, since SQLite is doing\\npower-safe transactions whereas the direct-to-disk writing is not.\\nTo put the tests on a more equal footing, add either the --nosync\\noption to the SQLite writes to disable calling fsync() or\\nFlushFileBuffers() to force content to disk, or using the --fsync option\\nfor the direct-to-disk tests to force them to invoke fsync() or\\nFlushFileBuffers() when updating disk files.\\n\\n\\nBy default, kvtest runs the database I/O measurements all within\\na single transaction.  Use the --multitrans option to run each blob\\nread or write in a separate transaction.  The --multitrans option makes\\nSQLite much slower, and uncompetitive with direct disk I/O.  This\\noption proves, yet again, that to get the most performance out of\\nSQLite, you should group as much database interaction as possible within\\na single transaction.\\n\\n\\nThere are many other testing options, which can be seen by running\\nthe command:\\n\\n./kvtest help\\n\\n2.1. Read Performance Measurements\\nThe chart below shows data collected using \\nkvtest.c on five different\\nsystems:\\n\\n\\nWin7: A circa-2009 Dell Inspiron laptop, Pentium dual-core\\n    at 2.30GHz, 4GiB RAM, Windows7.\\nWin10: A 2016 Lenovo YOGA 910, Intel i7-7500 at 2.70GHz,\\n    16GiB RAM, Windows10.\\nMac: A 2015 MacBook Pro, 3.1GHz intel Core i7, 16GiB RAM,\\n    MacOS 10.12.5\\nUbuntu: Desktop built from Intel i7-4770K at 3.50GHz, 32GiB RAM,\\n    Ubuntu 16.04.2 LTS\\nAndroid: Galaxy S3, ARMv7, 2GiB RAM\\n\\nAll machines use SSD except Win7 which has a\\nhard-drive. The test database is 100K blobs with sizes uniformly\\ndistributed between 8K and 12K, for a total of about 1 gigabyte\\nof content.  The database page size\\nis 4KiB.  The -DSQLITE_DIRECT_OVERFLOW_READ compile-time option was\\nused for all of these tests.\\nTests were run multiple times.\\nThe first run was used to warm up the cache and its timings were discarded.\\n\\n\\nThe chart below shows average time to read a blob directly from the\\nfilesystem versus the time needed to read the same blob from the SQLite \\ndatabase.\\nThe actual timings vary considerably from one system to another \\n(the Ubuntu desktop is much\\nfaster than the Galaxy S3 phone, for example).  \\nThis chart shows the ratio of the\\ntimes needed to read blobs from a file divided by the time needed to\\nfrom the database.  The left-most column in the chart is the normalized\\ntime to read from the database, for reference.\\n\\n\\nIn this chart, an SQL statement (\"SELECT v FROM kv WHERE k=?1\") \\nis prepared once.  Then for each blob, the blob key value is bound \\nto the ?1 parameter and the statement is evaluated to extract the\\nblob content.\\n\\n\\nThe chart shows that on Windows10, content can be read from the SQLite\\ndatabase about 5 times faster than it can be read directly from disk.\\nOn Android, SQLite is only about 35% faster than reading from disk.\\n\\n\\n\\n\\n\\n\\nChart 1:  SQLite read latency relative to direct filesystem reads.\\n100K blobs, avg 10KB each, random order using SQL\\n\\n\\nThe performance can be improved slightly by bypassing the SQL layer\\nand reading the blob content directly using the\\nsqlite3_blob_read() interface, as shown in the next chart:\\n\\n\\n\\n\\n\\n\\nChart 2:  SQLite read latency relative to direct filesystem reads.\\n100K blobs, avg size 10KB, random order\\nusing sqlite3_blob_read().\\n\\n\\nFurther performance improves can be made by using the\\nmemory-mapped I/O feature of SQLite.  In the next chart, the\\nentire 1GB database file is memory mapped and blobs are read\\n(in random order) using the sqlite3_blob_read() interface.\\nWith these optimizations, SQLite is twice as fast as Android\\nor MacOS-X and over 10 times faster than Windows.\\n\\n\\n\\n\\n\\n\\nChart 3:  SQLite read latency relative to direct filesystem reads.\\n100K blobs, avg size 10KB, random order\\nusing sqlite3_blob_read() from a memory-mapped database.\\n\\n\\nThe third chart shows that reading blob content out of SQLite can be\\ntwice as fast as reading from individual files on disk for Mac and\\nAndroid, and an amazing ten times faster for Windows.\\n\\n2.2. Write Performance Measurements\\n\\nWrites are slower.\\nOn all systems, using both direct I/O and SQLite, write performance is\\nbetween 5 and 15 times slower than reads.\\n\\n\\nWrite performance measurements were made by replacing (overwriting)\\nan entire blob with a different blob.  All of the blobs in these\\nexperiment are random and incompressible.  Because writes are so much\\nslower than reads, only 10,000 of the 100,000 blobs in the database\\nare replaced.  The blobs to be replaced are selected at random and\\nare in no particular order.\\n\\n\\nThe direct-to-disk writes are accomplished using fopen()/fwrite()/fclose().\\nBy default, and in all the results shown below, the OS filesystem buffers are\\nnever flushed to persistent storage using fsync() or\\nFlushFileBuffers().  In other words, there is no attempt to make the\\ndirect-to-disk writes transactional or power-safe.\\nWe found that invoking fsync() or FlushFileBuffers() on each file\\nwritten causes direct-to-disk storage\\nto be about 10 times or more slower than writes to SQLite.\\n\\n\\nThe next chart compares SQLite database updates in WAL mode\\nagainst raw direct-to-disk overwrites of separate files on disk.\\nThe PRAGMA synchronous setting is NORMAL.\\nAll database writes are in a single transaction.\\nThe timer for the database writes is stopped after the transaction\\ncommits, but before a checkpoint is run.\\nNote that the SQLite writes, unlike the direct-to-disk writes,\\nare transactional and power-safe, though because the synchronous\\nsetting is NORMAL instead of FULL, the transactions are not durable.\\n\\n\\n\\n\\n\\n\\nChart 4:  SQLite write latency relative to direct filesystem writes.\\n10K blobs, avg size 10KB, random order,\\nWAL mode with synchronous NORMAL,\\nexclusive of checkpoint time\\n\\n\\nThe android performance numbers for the write experiments are omitted\\nbecause the performance tests on the Galaxy S3 are so random.  Two\\nconsecutive runs of the exact same experiment would give wildly different\\ntimes.  And, to be fair, the performance of SQLite on android is slightly\\nslower than writing directly to disk.\\n\\n\\nThe next chart shows the performance of SQLite versus direct-to-disk\\nwhen transactions are disabled (PRAGMA journal_mode=OFF)\\nand PRAGMA synchronous is set to OFF.  These settings put SQLite on an\\nequal footing with direct-to-disk writes, which is to say they make the\\ndata prone to corruption due to system crashes and power failures.\\n\\n\\n\\n\\n\\n\\nChart 5:  SQLite write latency relative to direct filesystem writes.\\n10K blobs, avg size 10KB, random order,\\njournaling disabled, synchronous OFF.\\n\\n\\nIn all of the write tests, it is important to disable anti-virus software\\nprior to running the direct-to-disk performance tests.  We found that\\nanti-virus software slows down direct-to-disk by an order of magnitude\\nwhereas it impacts SQLite writes very little.  This is probably due to the\\nfact that direct-to-disk changes thousands of separate files which all need\\nto be checked by anti-virus, whereas SQLite writes only changes the single\\ndatabase file.\\n\\n2.3. Variations\\nThe -DSQLITE_DIRECT_OVERFLOW_READ compile-time option causes SQLite\\nto bypass its page cache when reading content from overflow pages.  This\\nhelps database reads of 10K blobs run a little faster, but not all that much\\nfaster.  SQLite still holds a speed advantage over direct filesystem reads\\nwithout the SQLITE_DIRECT_OVERFLOW_READ compile-time option.\\n\\nOther compile-time options such as using -O3 instead of -Os or\\nusing -DSQLITE_THREADSAFE=0 and/or some of the other\\nrecommended compile-time options might help SQLite to run even faster\\nrelative to direct filesystem reads.\\n\\nThe size of the blobs in the test data affects performance.\\nThe filesystem will generally be faster for larger blobs, since\\nthe overhead of open() and close() is amortized over more bytes of I/O,\\nwhereas the database will be more efficient in both speed and space\\nas the average blob size decreases.\\n\\n\\n3. General Findings\\n\\n\\nSQLite is competitive with, and usually faster than, blobs stored in\\nseparate files on disk, for both reading and writing.\\n\\n\\nSQLite is much faster than direct writes to disk on Windows\\nwhen anti-virus protection is turned on.  Since anti-virus software\\nis and should be on by default in Windows, that means that SQLite\\nis generally much faster than direct disk writes on Windows.\\n\\n\\nReading is about an order of magnitude faster than writing, for all\\nsystems and for both SQLite and direct-to-disk I/O.\\n\\n\\nI/O performance varies widely depending on operating system and hardware.\\nMake your own measurements before drawing conclusions.\\n\\n\\nSome other SQL database engines advise developers to store blobs in separate\\nfiles and then store the filename in the database.  In that case, where\\nthe database must first be consulted to find the filename before opening\\nand reading the file, simply storing the entire blob in the database\\ngives much faster read and write performance with SQLite.\\nSee the Internal Versus External BLOBs article for more information.\\n\\n4. Additional Notes\\n\\n4.1. Compiling And Testing on Android\\n\\nThe kvtest program is compiled and run on Android as follows.\\nFirst install the Android SDK and NDK.  Then prepare a script\\nnamed \"android-gcc\" that looks approximately like this:\\n\\n#!/bin/sh\\n#\\nNDK=/home/drh/Android/Sdk/ndk-bundle\\nSYSROOT=$NDK/platforms/android-16/arch-arm\\nABIN=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin\\nGCC=$ABIN/arm-linux-androideabi-gcc\\n$GCC --sysroot=$SYSROOT -fPIC -pie $*\\n\\nMake that script executable and put it on your $PATH.  Then\\ncompile the kvtest program as follows:\\n\\nandroid-gcc -Os -I. kvtest.c sqlite3.c -o kvtest-android\\n\\nNext, move the resulting kvtest-android executable to the Android\\ndevice:\\n\\nadb push kvtest-android /data/local/tmp\\n\\nFinally use \"adb shell\" to get a shell prompt on the Android device,\\ncd into the /data/local/tmp directory, and begin running the tests\\nas with any other unix host.\\nThis page last modified on  2021-03-01 12:55:48 UTC \\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = 'https://www.sqlite.org/fasterthanfs.html'\n",
    "get_webpage_content(link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d451e6fd3e3a90eb92742bb146e8e20c8a00180dad068226f141e79828259f8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
